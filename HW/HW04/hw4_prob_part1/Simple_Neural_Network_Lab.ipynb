{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# T1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(x):\n",
    "    return np.maximum(0, x)\n",
    "\n",
    "def forward_backward_pass(x0, w0, w1, b0, b1):\n",
    "    # Forward Pass\n",
    "    x1 = relu(x0 * w0 + b0)\n",
    "    y1 = x1 * w1 + b1\n",
    "    z = relu(y1 + x0)\n",
    "\n",
    "    # Backward Pass\n",
    "    dz_dy1 = 1 if y1 + x0 > 0 else 0\n",
    "    dy1_dx1 = w1\n",
    "    dy1_dw1 = x1\n",
    "    dy1_db1 = 1\n",
    "    dx1_dx0 = 1 if x0 * w0 + b0 > 0 else 0\n",
    "    dx1_dw0 = x0 if x0 * w0 + b0 > 0 else 0\n",
    "\n",
    "    # Gradients\n",
    "    dz_dw0 = dz_dy1 * dy1_dx1 * dx1_dw0\n",
    "    dz_dw1 = dz_dy1 * dy1_dw1\n",
    "    dz_db0 = dz_dy1 * dy1_dx1 * dx1_dx0\n",
    "    dz_db1 = dz_dy1 * dy1_db1\n",
    "\n",
    "    return z, x1, y1, dz_dy1, dy1_dx1, dy1_dw1, dy1_db1, dx1_dx0, dx1_dw0, dz_dw0, dz_dw1, dz_db0, dz_db1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Forward Pass - x1: 0.4\n",
      "Forward Pass - y1: -0.38\n",
      "Forward Pass - z: 0.62\n",
      "\n",
      "Backward Pass - Gradient dz_dy1: 1\n",
      "Backward Pass - Gradient dy1_dx1: -0.2\n",
      "Backward Pass - Gradient dy1_dw1: 0.4\n",
      "Backward Pass - Gradient dy1_db1: 1\n",
      "Backward Pass - Gradient dx1_dx0: 1\n",
      "Backward Pass - Gradient dx1_dw0: 1.0\n",
      "\n",
      "Backward Pass - Gradients z w.r.t. w0:: -0.2\n",
      "Backward Pass - Gradients z w.r.t. w1: 0.4\n",
      "Backward Pass - Gradients z w.r.t. b0: -0.2\n",
      "Backward Pass - Gradients z w.r.t. b1: 1\n"
     ]
    }
   ],
   "source": [
    "# Given values\n",
    "x0 = 1.0\n",
    "w0 = 0.3\n",
    "w1 = -0.2\n",
    "b0 = 0.1\n",
    "b1 = -0.3\n",
    "\n",
    "# Compute forward and backward pass\n",
    "output, x1, y1, dz_dy1, dy1_dx1, dy1_dw1, dy1_db1, dx1_dx0, dx1_dw0, dz_dw0, dz_dw1, dz_db0, dz_db1 = forward_backward_pass(x0, w0, w1, b0, b1)\n",
    "\n",
    "# Print results\n",
    "print(\"Forward Pass - x1:\", x1)\n",
    "print(\"Forward Pass - y1:\", y1)\n",
    "print(\"Forward Pass - z:\", output)\n",
    "\n",
    "print(\"\\nBackward Pass - Gradient dz_dy1:\", dz_dy1)\n",
    "print(\"Backward Pass - Gradient dy1_dx1:\", dy1_dx1)\n",
    "print(\"Backward Pass - Gradient dy1_dw1:\", dy1_dw1)\n",
    "print(\"Backward Pass - Gradient dy1_db1:\", dy1_db1)\n",
    "print(\"Backward Pass - Gradient dx1_dx0:\", dx1_dx0)\n",
    "print(\"Backward Pass - Gradient dx1_dw0:\", dx1_dw0)\n",
    "\n",
    "print(\"\\nBackward Pass - Gradients z w.r.t. w0::\", dz_dw0)\n",
    "print(\"Backward Pass - Gradients z w.r.t. w1:\", dz_dw1)\n",
    "print(\"Backward Pass - Gradients z w.r.t. b0:\", dz_db0)\n",
    "print(\"Backward Pass - Gradients z w.r.t. b1:\", dz_db1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Two-Layer Neural Networks\n",
    "\n",
    "In this part of the homework, we will work on building a simple Neural Network to classify digits using MNIST dataset. There are many powerful neural network frameworks nowadays that are relatively straightforward to use. However, we believe that in order to understand the theory behind neural networks, and to have the right intuitions in order to build, use, and debug more complex architectures, one must first start from the basics.\n",
    "\n",
    "Your task is to complete the missing codes needed for training and testing of a simple fully-connected neural network.\n",
    "\n",
    "The missing parts will be marked with TODO#N, where N represents the task number.\n",
    "\n",
    "Many parts in this homework are modified from Stanford's cs231n assignments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from __future__ import print_function\n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (10.0, 8.0) # set default size of plots\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'\n",
    "\n",
    "from cattern.neural_net import TwoLayerNet\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Just a function that verifies if your answers are correct\n",
    "\n",
    "def rel_error(x, y):\n",
    "    \"\"\" returns relative error \"\"\"\n",
    "    return np.max(np.abs(x - y) / (np.maximum(1e-8, np.abs(x) + np.abs(y))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use the class `TwoLayerNet` in the file `cattern/neural_net.py` to represent instances of our network. The network parameters (weights and biases) are stored in the instance variable `self.params` where the keys are parameter names and values are numpy arrays. Below, we initialize some toy data and a toy model that will guide you with your implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create a small net and some toy data to check your implementations.\n",
    "# Note that we set the random seed for repeatable experiments.\n",
    "\n",
    "input_size = 4\n",
    "hidden_size = 10\n",
    "num_classes = 3\n",
    "num_inputs = 5\n",
    "\n",
    "def init_toy_model():\n",
    "    np.random.seed(0)\n",
    "    return TwoLayerNet(input_size, hidden_size, num_classes, std=1e-1)\n",
    "\n",
    "def init_toy_data():\n",
    "    np.random.seed(1)\n",
    "    X = 10 * np.random.randn(num_inputs, input_size)\n",
    "    y = np.array([0, 1, 2, 2, 1])\n",
    "    return X, y\n",
    "\n",
    "net = init_toy_model()\n",
    "X, y = init_toy_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Forward pass: compute scores\n",
    "Open the file `cattern/neural_net.py` and look at the method `TwoLayerNet.loss`. This function takes the data and weights and computes the class scores, the loss, and the gradients on the parameters. \n",
    "\n",
    "Complete TODO#1 in `TwoLayerNet.loss`, by implementing the first part of the forward pass which uses the weights and biases to compute the scores for all inputs. The scores refer to the output of the network just before the softmax layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your scores:\n",
      "[[-0.81233741 -1.27654624 -0.70335995]\n",
      " [-0.17129677 -1.18803311 -0.47310444]\n",
      " [-0.51590475 -1.01354314 -0.8504215 ]\n",
      " [-0.15419291 -0.48629638 -0.52901952]\n",
      " [-0.00618733 -0.12435261 -0.15226949]]\n",
      "\n",
      "correct scores:\n",
      "[[-0.81233741 -1.27654624 -0.70335995]\n",
      " [-0.17129677 -1.18803311 -0.47310444]\n",
      " [-0.51590475 -1.01354314 -0.8504215 ]\n",
      " [-0.15419291 -0.48629638 -0.52901952]\n",
      " [-0.00618733 -0.12435261 -0.15226949]]\n",
      "\n",
      "Difference between your scores and correct scores:\n",
      "3.680272093239262e-08\n"
     ]
    }
   ],
   "source": [
    "scores = net.loss(X)\n",
    "print('Your scores:')\n",
    "print(scores)\n",
    "print()\n",
    "print('correct scores:')\n",
    "correct_scores = np.asarray([\n",
    "  [-0.81233741, -1.27654624, -0.70335995],\n",
    "  [-0.17129677, -1.18803311, -0.47310444],\n",
    "  [-0.51590475, -1.01354314, -0.8504215 ],\n",
    "  [-0.15419291, -0.48629638, -0.52901952],\n",
    "  [-0.00618733, -0.12435261, -0.15226949]])\n",
    "print(correct_scores)\n",
    "print()\n",
    "\n",
    "# The difference should be very small. We get < 1e-7\n",
    "print('Difference between your scores and correct scores:')\n",
    "print(np.sum(np.abs(scores - correct_scores)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Forward pass: compute loss\n",
    "In the same function, complete TODO#2 by implementing the second part that computes the data and regularizaion loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Difference between your loss and correct loss:\n",
      "1.794120407794253e-13\n",
      "1.3037878913298206 1.30378789133\n"
     ]
    }
   ],
   "source": [
    "loss, _ = net.loss(X, y, reg=0.1)\n",
    "correct_loss = 1.30378789133\n",
    "\n",
    "# should be very small, we get < 1e-12\n",
    "print('Difference between your loss and correct loss:')\n",
    "print(np.sum(np.abs(loss - correct_loss)))\n",
    "print(loss, correct_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Backward pass\n",
    "Implement the rest of the function by completing TODO#3. This will compute the gradient of the loss with respect to the variables `W1`, `b1`, `W2`, and `b2`. Now that you (hopefully!) have a correctly implemented forward pass, you can debug your backward pass using a numeric gradient check:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W2 max relative error: 3.440708e-09\n",
      "b2 max relative error: 4.447687e-11\n",
      "W1 max relative error: 3.561318e-09\n",
      "b1 max relative error: 2.738421e-09\n"
     ]
    }
   ],
   "source": [
    "from cattern.gradient_check import eval_numerical_gradient\n",
    "\n",
    "# Use numeric gradient checking to check your implementation of the backward pass.\n",
    "# If your implementation is correct, the difference between the numeric and\n",
    "# analytic gradients should be less than 1e-8 for each of W1, W2, b1, and b2.\n",
    "\n",
    "loss, grads = net.loss(X, y, reg=0.05)\n",
    "\n",
    "# these should all be less than 1e-8 or so\n",
    "for param_name in grads:\n",
    "    f = lambda W: net.loss(X, y, reg=0.05)[0]\n",
    "    param_grad_num = eval_numerical_gradient(f, net.params[param_name], verbose=False)\n",
    "    print('%s max relative error: %e' % (param_name, rel_error(param_grad_num, grads[param_name])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train the network\n",
    "To train the network we will use stochastic gradient descent (SGD). Look at the function `TwoLayerNet.train` and fill in the missing sections to implement the training procedure (TODO#4-6). You will also have to implement `TwoLayerNet.predict` (TODO#7), as the training process periodically performs prediction to keep track of accuracy over time while the network trains.\n",
    "\n",
    "Once you have implemented the method, run the code below to train a two-layer network on toy data. You should achieve a training loss less than 0.05."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 0 / 100: loss 1.241992\n",
      "Final training loss:  0.017143643532923733\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x800 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "net = init_toy_model()\n",
    "stats = net.train(X, y, X, y,\n",
    "            learning_rate=1e-1, reg=5e-6,\n",
    "            num_iters=100, verbose=True)\n",
    "\n",
    "print('Final training loss: ', stats['loss_history'][-1])\n",
    "\n",
    "# plot the loss history\n",
    "plt.plot(stats['loss_history'])\n",
    "plt.xlabel('iteration')\n",
    "plt.ylabel('training loss')\n",
    "plt.title('Training Loss history')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the data\n",
    "Now that you have implemented a two-layer network that passes gradient checks and works on toy data, it's time to load up MNIST data so we can use it to train a classifier on a real dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting mnist_data/train-images-idx3-ubyte.gz\n",
      "Extracting mnist_data/train-labels-idx1-ubyte.gz\n",
      "Extracting mnist_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting mnist_data/t10k-labels-idx1-ubyte.gz\n",
      "Train data shape:  (55000, 784)\n",
      "Train labels shape:  (55000,)\n",
      "Validation data shape:  (5000, 784)\n",
      "Validation labels shape:  (5000,)\n",
      "Test data shape:  (10000, 784)\n",
      "Test labels shape:  (10000,)\n"
     ]
    }
   ],
   "source": [
    "from mnist_data import load_mnist\n",
    "\n",
    "\n",
    "def get_mnist_data(num_training=55000, num_validation=5000, num_test=10000):\n",
    "    \"\"\"\n",
    "    Load the MNIST dataset from disk and perform preprocessing to prepare\n",
    "    it for the two-layer neural net classifier.\n",
    "    \"\"\"\n",
    "    # Load the raw MNIST data\n",
    "    X_train, y_train, X_val, y_val, X_test, y_test = load_mnist.read_data_sets('mnist_data')\n",
    "\n",
    "    # Normalize the data: subtract the mean image\n",
    "    mean_image = np.mean(X_train, axis=0)\n",
    "    X_train = X_train - mean_image\n",
    "    X_val = X_val - mean_image\n",
    "    X_test = X_test - mean_image\n",
    "\n",
    "    # Reshape data to rows\n",
    "    X_train = X_train.reshape(num_training, -1)\n",
    "    X_val = X_val.reshape(num_validation, -1)\n",
    "    X_test = X_test.reshape(num_test, -1)\n",
    "\n",
    "    return X_train, y_train, X_val, y_val, X_test, y_test\n",
    "\n",
    "X_train, y_train, X_val, y_val, X_test, y_test = get_mnist_data()\n",
    "print('Train data shape: ', X_train.shape)\n",
    "print('Train labels shape: ', y_train.shape)\n",
    "print('Validation data shape: ', X_val.shape)\n",
    "print('Validation labels shape: ', y_val.shape)\n",
    "print('Test data shape: ', X_test.shape)\n",
    "print('Test labels shape: ', y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x800 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from mnist_data.vis_utils import visualize_grid\n",
    "\n",
    "# Visualize mnist data\n",
    "\n",
    "def show_mnist_image(data):\n",
    "  data = data.reshape(-1, 28, 28, 1)\n",
    "  plt.imshow(visualize_grid(data, padding=3).astype('uint8').squeeze(axis=2))\n",
    "  plt.gca().axis('off')\n",
    "  plt.show()\n",
    "\n",
    "show_mnist_image(X_train[:36])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Train a network\n",
    "To train our network we will use SGD with momentum. We will use fixed learning rate to train this model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 0 / 2000: loss 2.302592\n",
      "iteration 100 / 2000: loss 2.302232\n",
      "iteration 200 / 2000: loss 2.300876\n",
      "iteration 300 / 2000: loss 2.291126\n",
      "iteration 400 / 2000: loss 2.239348\n",
      "iteration 500 / 2000: loss 2.008828\n",
      "iteration 600 / 2000: loss 1.814898\n",
      "iteration 700 / 2000: loss 1.425479\n",
      "iteration 800 / 2000: loss 1.268225\n",
      "iteration 900 / 2000: loss 0.994899\n",
      "iteration 1000 / 2000: loss 0.847616\n",
      "iteration 1100 / 2000: loss 0.758159\n",
      "iteration 1200 / 2000: loss 0.633341\n",
      "iteration 1300 / 2000: loss 0.536447\n",
      "iteration 1400 / 2000: loss 0.478063\n",
      "iteration 1500 / 2000: loss 0.397137\n",
      "iteration 1600 / 2000: loss 0.415201\n",
      "iteration 1700 / 2000: loss 0.385851\n",
      "iteration 1800 / 2000: loss 0.517132\n",
      "iteration 1900 / 2000: loss 0.425830\n",
      "Validation accuracy:  0.8922\n"
     ]
    }
   ],
   "source": [
    "input_size = 28 * 28 * 1\n",
    "hidden_size = 50\n",
    "num_classes = 10\n",
    "net = TwoLayerNet(input_size, hidden_size, num_classes)\n",
    "\n",
    "# Train the network\n",
    "stats = net.train(X_train, y_train, X_val, y_val,\n",
    "            num_iters=2000, batch_size=200,\n",
    "            learning_rate=1e-4, learning_rate_decay=1,\n",
    "            reg=0, verbose=True)\n",
    "\n",
    "# Predict on the validation set\n",
    "val_acc = (net.predict(X_val) == y_val).mean()\n",
    "print('Validation accuracy: ', val_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learning Rate Decay\n",
    "\n",
    "In the previous run, we used the same learning rate during the whole training process. This fix-sized learning rate disregards the benefit of larger learning rate at the beginning of the training, and it might suffer from overshooting around the minima.\n",
    "\n",
    "Add learning rate decay to the train function, run the model again with larger starting learning rate and learning rate decay, then compare the losses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 0 / 2000: loss 2.302581\n",
      "iteration 100 / 2000: loss 2.119326\n",
      "iteration 200 / 2000: loss 0.804760\n",
      "iteration 300 / 2000: loss 0.492363\n",
      "iteration 400 / 2000: loss 0.407158\n",
      "iteration 500 / 2000: loss 0.286767\n",
      "iteration 600 / 2000: loss 0.318665\n",
      "iteration 700 / 2000: loss 0.363674\n",
      "iteration 800 / 2000: loss 0.273462\n",
      "iteration 900 / 2000: loss 0.278906\n",
      "iteration 1000 / 2000: loss 0.263698\n",
      "iteration 1100 / 2000: loss 0.181063\n",
      "iteration 1200 / 2000: loss 0.257531\n",
      "iteration 1300 / 2000: loss 0.353448\n",
      "iteration 1400 / 2000: loss 0.188915\n",
      "iteration 1500 / 2000: loss 0.165813\n",
      "iteration 1600 / 2000: loss 0.248227\n",
      "iteration 1700 / 2000: loss 0.259703\n",
      "iteration 1800 / 2000: loss 0.292172\n",
      "iteration 1900 / 2000: loss 0.196248\n",
      "Validation accuracy:  0.9432\n"
     ]
    }
   ],
   "source": [
    "net = TwoLayerNet(input_size, hidden_size, num_classes)\n",
    "stats_LRDecay = net.train(X_train, y_train, X_val, y_val,\n",
    "                    num_iters=2000, batch_size=200,\n",
    "                    learning_rate=5e-4, learning_rate_decay=0.95,\n",
    "                    reg=0, verbose=True)\n",
    "\n",
    "# Predict on the validation set\n",
    "val_acc_LRDecay = (net.predict(X_val) == y_val).mean()\n",
    "print('Validation accuracy: ', val_acc_LRDecay)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Debug the training\n",
    "With the default parameters we provided above, you should get a validation accuracy of about 0.94 on the validation set. This isn't very good for MNIST data which has reports of up to 0.99 accuracy.\n",
    "\n",
    "One strategy for getting insight into what's wrong is to plot the loss function and the accuracies on the training and validation sets during optimization.\n",
    "\n",
    "Another strategy is to visualize the weights that were learned in the first layer of the network. In most neural networks trained on visual data, the first layer weights typically show some visible structure when visualized. \n",
    "\n",
    "(You can think of the first layer weights as a projection W^TX. This is very similar to how we project our training data using PCA projection in our previous homework. Just like how we visualize the eigenfaces. We can also visualize the weights of the neural network in the same manner.)\n",
    "\n",
    "Below, we will also show you lossed between two models we trained above. Do you notice the difference between the two?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x800 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot the loss function and train / validation accuracies\n",
    "plt.subplot(2, 1, 1)\n",
    "plt.plot(stats['loss_history'], label='with_fixed_LR')\n",
    "plt.plot(stats_LRDecay['loss_history'], label='with_LR_decay')\n",
    "plt.title('Loss history')\n",
    "plt.legend()\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('Loss')\n",
    "\n",
    "plt.subplot(2, 1, 2)\n",
    "plt.plot(stats['train_acc_history'], label='train')\n",
    "plt.plot(stats['val_acc_history'], label='val')\n",
    "plt.plot(stats_LRDecay['train_acc_history'], label='train_LRDecay')\n",
    "plt.plot(stats_LRDecay['val_acc_history'], label='val_LRDecay')\n",
    "plt.title('Classification accuracy history')\n",
    "plt.legend()\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Clasification accuracy')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x800 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Visualize the weights of the network\n",
    "\n",
    "def show_net_weights(net):\n",
    "    W1 = net.params['W1']\n",
    "    W1 = W1.reshape(28, 28, 1, -1).transpose(3, 0, 1, 2)\n",
    "    plt.imshow(visualize_grid(W1, padding=3).astype('uint8').squeeze(axis=2))\n",
    "    plt.gca().axis('off')\n",
    "    plt.show()\n",
    "\n",
    "show_net_weights(net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tune your hyperparameters\n",
    "\n",
    "**Tuning**. Tuning the hyperparameters and developing intuition for how they affect the final performance is a large part of using Neural Networks, so we want you to get a lot of practice. Below, you should experiment with different values of the various hyperparameters, including hidden layer size, learning rate, numer of training epochs, and regularization strength. You might also consider tuning the learning rate decay, but you should be able to get good performance using the default value.\n",
    "\n",
    "**Approximate results**. You should be aim to achieve a classification accuracy of greater than 97.4% on the validation set.\n",
    "\n",
    "**Experiment**: You goal in this exercise is to get as good of a result on MNIST as you can, with a fully-connected Neural Network. Feel free implement your own techniques (e.g. PCA to reduce dimensionality, or adding dropout, or adding features to the solver, etc.)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 0 / 2000: loss 2.302584\n",
      "iteration 100 / 2000: loss 0.334746\n",
      "iteration 200 / 2000: loss 0.281170\n",
      "iteration 300 / 2000: loss 0.327581\n",
      "iteration 400 / 2000: loss 0.371246\n",
      "iteration 500 / 2000: loss 0.237536\n",
      "iteration 600 / 2000: loss 0.204962\n",
      "iteration 700 / 2000: loss 0.401765\n",
      "iteration 800 / 2000: loss 0.315085\n",
      "iteration 900 / 2000: loss 0.248659\n",
      "iteration 1000 / 2000: loss 0.235130\n",
      "iteration 1100 / 2000: loss 0.175879\n",
      "iteration 1200 / 2000: loss 0.144514\n",
      "iteration 1300 / 2000: loss 0.197938\n",
      "iteration 1400 / 2000: loss 0.247400\n",
      "iteration 1500 / 2000: loss 0.177352\n",
      "iteration 1600 / 2000: loss 0.169424\n",
      "iteration 1700 / 2000: loss 0.387722\n",
      "iteration 1800 / 2000: loss 0.347012\n",
      "iteration 1900 / 2000: loss 0.352855\n",
      "Hidden Size: 10, Learning Rate: 0.01, LR Decay: 0.99, Regularization: 0.0, Validation accuracy: 0.926\n",
      "iteration 0 / 2000: loss 2.302585\n",
      "iteration 100 / 2000: loss 0.337563\n",
      "iteration 200 / 2000: loss 0.353827\n",
      "iteration 300 / 2000: loss 0.243212\n",
      "iteration 400 / 2000: loss 0.295174\n",
      "iteration 500 / 2000: loss 0.344531\n",
      "iteration 600 / 2000: loss 0.402568\n",
      "iteration 700 / 2000: loss 0.399546\n",
      "iteration 800 / 2000: loss 0.485781\n",
      "iteration 900 / 2000: loss 0.344193\n",
      "iteration 1000 / 2000: loss 0.252596\n",
      "iteration 1100 / 2000: loss 0.364943\n",
      "iteration 1200 / 2000: loss 0.368786\n",
      "iteration 1300 / 2000: loss 0.334234\n",
      "iteration 1400 / 2000: loss 0.523270\n",
      "iteration 1500 / 2000: loss 0.352576\n",
      "iteration 1600 / 2000: loss 0.235788\n",
      "iteration 1700 / 2000: loss 0.291985\n",
      "iteration 1800 / 2000: loss 0.280625\n",
      "iteration 1900 / 2000: loss 0.298550\n",
      "Hidden Size: 10, Learning Rate: 0.01, LR Decay: 0.99, Regularization: 0.1, Validation accuracy: 0.9302\n",
      "iteration 0 / 2000: loss 2.302594\n",
      "iteration 100 / 2000: loss 0.429438\n",
      "iteration 200 / 2000: loss 0.508486\n",
      "iteration 300 / 2000: loss 0.371101\n",
      "iteration 400 / 2000: loss 0.402482\n",
      "iteration 500 / 2000: loss 0.309651\n",
      "iteration 600 / 2000: loss 0.204220\n",
      "iteration 700 / 2000: loss 0.468320\n",
      "iteration 800 / 2000: loss 0.385760\n",
      "iteration 900 / 2000: loss 0.269949\n",
      "iteration 1000 / 2000: loss 0.299661\n",
      "iteration 1100 / 2000: loss 0.336625\n",
      "iteration 1200 / 2000: loss 0.330587\n",
      "iteration 1300 / 2000: loss 0.245573\n",
      "iteration 1400 / 2000: loss 0.283490\n",
      "iteration 1500 / 2000: loss 0.423839\n",
      "iteration 1600 / 2000: loss 0.329784\n",
      "iteration 1700 / 2000: loss 0.255132\n",
      "iteration 1800 / 2000: loss 0.350565\n",
      "iteration 1900 / 2000: loss 0.427830\n",
      "Hidden Size: 10, Learning Rate: 0.01, LR Decay: 0.99, Regularization: 0.2, Validation accuracy: 0.9198\n",
      "iteration 0 / 2000: loss 2.302605\n",
      "iteration 100 / 2000: loss 0.515185\n",
      "iteration 200 / 2000: loss 0.354285\n",
      "iteration 300 / 2000: loss 0.303486\n",
      "iteration 400 / 2000: loss 0.382736\n",
      "iteration 500 / 2000: loss 0.347519\n",
      "iteration 600 / 2000: loss 0.481077\n",
      "iteration 700 / 2000: loss 0.407405\n",
      "iteration 800 / 2000: loss 0.482278\n",
      "iteration 900 / 2000: loss 0.533493\n",
      "iteration 1000 / 2000: loss 0.264531\n",
      "iteration 1100 / 2000: loss 0.324073\n",
      "iteration 1200 / 2000: loss 0.414621\n",
      "iteration 1300 / 2000: loss 0.232201\n",
      "iteration 1400 / 2000: loss 0.393298\n",
      "iteration 1500 / 2000: loss 0.327735\n",
      "iteration 1600 / 2000: loss 0.350006\n",
      "iteration 1700 / 2000: loss 0.371819\n",
      "iteration 1800 / 2000: loss 0.329771\n",
      "iteration 1900 / 2000: loss 0.263536\n",
      "Hidden Size: 10, Learning Rate: 0.01, LR Decay: 0.99, Regularization: 0.30000000000000004, Validation accuracy: 0.9218\n",
      "iteration 0 / 2000: loss 2.302592\n",
      "iteration 100 / 2000: loss 0.596359\n",
      "iteration 200 / 2000: loss 0.400242\n",
      "iteration 300 / 2000: loss 0.324612\n",
      "iteration 400 / 2000: loss 0.407425\n",
      "iteration 500 / 2000: loss 0.367545\n",
      "iteration 600 / 2000: loss 0.400415\n",
      "iteration 700 / 2000: loss 0.414069\n",
      "iteration 800 / 2000: loss 0.374708\n",
      "iteration 900 / 2000: loss 0.465241\n",
      "iteration 1000 / 2000: loss 0.329398\n",
      "iteration 1100 / 2000: loss 0.339119\n",
      "iteration 1200 / 2000: loss 0.316213\n",
      "iteration 1300 / 2000: loss 0.487716\n",
      "iteration 1400 / 2000: loss 0.661637\n",
      "iteration 1500 / 2000: loss 0.440837\n",
      "iteration 1600 / 2000: loss 0.408660\n",
      "iteration 1700 / 2000: loss 0.614999\n",
      "iteration 1800 / 2000: loss 0.350159\n",
      "iteration 1900 / 2000: loss 0.395079\n",
      "Hidden Size: 10, Learning Rate: 0.01, LR Decay: 0.99, Regularization: 0.4, Validation accuracy: 0.9186\n",
      "iteration 0 / 2000: loss 2.302579\n",
      "iteration 100 / 2000: loss 0.356417\n",
      "iteration 200 / 2000: loss 0.332188\n",
      "iteration 300 / 2000: loss 0.257946\n",
      "iteration 400 / 2000: loss 0.374541\n",
      "iteration 500 / 2000: loss 0.244114\n",
      "iteration 600 / 2000: loss 0.277117\n",
      "iteration 700 / 2000: loss 0.326960\n",
      "iteration 800 / 2000: loss 0.328726\n",
      "iteration 900 / 2000: loss 0.177420\n",
      "iteration 1000 / 2000: loss 0.196761\n",
      "iteration 1100 / 2000: loss 0.130782\n",
      "iteration 1200 / 2000: loss 0.157494\n",
      "iteration 1300 / 2000: loss 0.191539\n",
      "iteration 1400 / 2000: loss 0.087863\n",
      "iteration 1500 / 2000: loss 0.169415\n",
      "iteration 1600 / 2000: loss 0.126107\n",
      "iteration 1700 / 2000: loss 0.227276\n",
      "iteration 1800 / 2000: loss 0.134681\n",
      "iteration 1900 / 2000: loss 0.304916\n",
      "Hidden Size: 10, Learning Rate: 0.01, LR Decay: 0.8175, Regularization: 0.0, Validation accuracy: 0.9386\n",
      "iteration 0 / 2000: loss 2.302584\n",
      "iteration 100 / 2000: loss 0.486130\n",
      "iteration 200 / 2000: loss 0.411713\n",
      "iteration 300 / 2000: loss 0.358318\n",
      "iteration 400 / 2000: loss 0.411732\n",
      "iteration 500 / 2000: loss 0.241258\n",
      "iteration 600 / 2000: loss 0.191879\n",
      "iteration 700 / 2000: loss 0.224385\n",
      "iteration 800 / 2000: loss 0.355686\n",
      "iteration 900 / 2000: loss 0.285409\n",
      "iteration 1000 / 2000: loss 0.215691\n",
      "iteration 1100 / 2000: loss 0.306222\n",
      "iteration 1200 / 2000: loss 0.273263\n",
      "iteration 1300 / 2000: loss 0.181320\n",
      "iteration 1400 / 2000: loss 0.291926\n",
      "iteration 1500 / 2000: loss 0.266092\n",
      "iteration 1600 / 2000: loss 0.265864\n",
      "iteration 1700 / 2000: loss 0.231011\n",
      "iteration 1800 / 2000: loss 0.290874\n",
      "iteration 1900 / 2000: loss 0.197802\n",
      "Hidden Size: 10, Learning Rate: 0.01, LR Decay: 0.8175, Regularization: 0.1, Validation accuracy: 0.9392\n",
      "iteration 0 / 2000: loss 2.302589\n",
      "iteration 100 / 2000: loss 0.298967\n",
      "iteration 200 / 2000: loss 0.303307\n",
      "iteration 300 / 2000: loss 0.289694\n",
      "iteration 400 / 2000: loss 0.251370\n",
      "iteration 500 / 2000: loss 0.239061\n",
      "iteration 600 / 2000: loss 0.327272\n",
      "iteration 700 / 2000: loss 0.308724\n",
      "iteration 800 / 2000: loss 0.265748\n",
      "iteration 900 / 2000: loss 0.288642\n",
      "iteration 1000 / 2000: loss 0.317692\n",
      "iteration 1100 / 2000: loss 0.205988\n",
      "iteration 1200 / 2000: loss 0.278886\n",
      "iteration 1300 / 2000: loss 0.235919\n",
      "iteration 1400 / 2000: loss 0.336365\n",
      "iteration 1500 / 2000: loss 0.244165\n",
      "iteration 1600 / 2000: loss 0.263426\n",
      "iteration 1700 / 2000: loss 0.302298\n",
      "iteration 1800 / 2000: loss 0.271061\n",
      "iteration 1900 / 2000: loss 0.282889\n",
      "Hidden Size: 10, Learning Rate: 0.01, LR Decay: 0.8175, Regularization: 0.2, Validation accuracy: 0.9362\n",
      "iteration 0 / 2000: loss 2.302593\n",
      "iteration 100 / 2000: loss 0.361038\n",
      "iteration 200 / 2000: loss 0.453688\n",
      "iteration 300 / 2000: loss 0.329839\n",
      "iteration 400 / 2000: loss 0.375316\n",
      "iteration 500 / 2000: loss 0.442102\n",
      "iteration 600 / 2000: loss 0.311421\n",
      "iteration 700 / 2000: loss 0.268591\n",
      "iteration 800 / 2000: loss 0.362979\n",
      "iteration 900 / 2000: loss 0.285101\n",
      "iteration 1000 / 2000: loss 0.317478\n",
      "iteration 1100 / 2000: loss 0.362280\n",
      "iteration 1200 / 2000: loss 0.244826\n",
      "iteration 1300 / 2000: loss 0.261710\n",
      "iteration 1400 / 2000: loss 0.282942\n",
      "iteration 1500 / 2000: loss 0.418615\n",
      "iteration 1600 / 2000: loss 0.316974\n",
      "iteration 1700 / 2000: loss 0.337848\n",
      "iteration 1800 / 2000: loss 0.244982\n",
      "iteration 1900 / 2000: loss 0.222423\n",
      "Hidden Size: 10, Learning Rate: 0.01, LR Decay: 0.8175, Regularization: 0.30000000000000004, Validation accuracy: 0.9376\n",
      "iteration 0 / 2000: loss 2.302604\n",
      "iteration 100 / 2000: loss 0.329187\n",
      "iteration 200 / 2000: loss 0.380108\n",
      "iteration 300 / 2000: loss 0.293568\n",
      "iteration 400 / 2000: loss 0.364815\n",
      "iteration 500 / 2000: loss 0.333712\n",
      "iteration 600 / 2000: loss 0.280682\n",
      "iteration 700 / 2000: loss 0.414311\n",
      "iteration 800 / 2000: loss 0.269803\n",
      "iteration 900 / 2000: loss 0.320552\n",
      "iteration 1000 / 2000: loss 0.320591\n",
      "iteration 1100 / 2000: loss 0.428879\n",
      "iteration 1200 / 2000: loss 0.451762\n",
      "iteration 1300 / 2000: loss 0.475396\n",
      "iteration 1400 / 2000: loss 0.333733\n",
      "iteration 1500 / 2000: loss 0.308911\n",
      "iteration 1600 / 2000: loss 0.267454\n",
      "iteration 1700 / 2000: loss 0.329671\n",
      "iteration 1800 / 2000: loss 0.361346\n",
      "iteration 1900 / 2000: loss 0.323294\n",
      "Hidden Size: 10, Learning Rate: 0.01, LR Decay: 0.8175, Regularization: 0.4, Validation accuracy: 0.9306\n",
      "iteration 0 / 2000: loss 2.302590\n",
      "iteration 100 / 2000: loss 0.331710\n",
      "iteration 200 / 2000: loss 0.327289\n",
      "iteration 300 / 2000: loss 0.371328\n",
      "iteration 400 / 2000: loss 0.224044\n",
      "iteration 500 / 2000: loss 0.162707\n",
      "iteration 600 / 2000: loss 0.263050\n",
      "iteration 700 / 2000: loss 0.135886\n",
      "iteration 800 / 2000: loss 0.301023\n",
      "iteration 900 / 2000: loss 0.157484\n",
      "iteration 1000 / 2000: loss 0.192162\n",
      "iteration 1100 / 2000: loss 0.233311\n",
      "iteration 1200 / 2000: loss 0.165459\n",
      "iteration 1300 / 2000: loss 0.170897\n",
      "iteration 1400 / 2000: loss 0.198092\n",
      "iteration 1500 / 2000: loss 0.182749\n",
      "iteration 1600 / 2000: loss 0.195067\n",
      "iteration 1700 / 2000: loss 0.301630\n",
      "iteration 1800 / 2000: loss 0.321691\n",
      "iteration 1900 / 2000: loss 0.198527\n",
      "Hidden Size: 10, Learning Rate: 0.01, LR Decay: 0.645, Regularization: 0.0, Validation accuracy: 0.9372\n",
      "iteration 0 / 2000: loss 2.302590\n",
      "iteration 100 / 2000: loss 0.359486\n",
      "iteration 200 / 2000: loss 0.439116\n",
      "iteration 300 / 2000: loss 0.323486\n",
      "iteration 400 / 2000: loss 0.259695\n",
      "iteration 500 / 2000: loss 0.389706\n",
      "iteration 600 / 2000: loss 0.225755\n",
      "iteration 700 / 2000: loss 0.172416\n",
      "iteration 800 / 2000: loss 0.262633\n",
      "iteration 900 / 2000: loss 0.240189\n",
      "iteration 1000 / 2000: loss 0.182884\n",
      "iteration 1100 / 2000: loss 0.277404\n",
      "iteration 1200 / 2000: loss 0.124696\n",
      "iteration 1300 / 2000: loss 0.178258\n",
      "iteration 1400 / 2000: loss 0.250626\n",
      "iteration 1500 / 2000: loss 0.200621\n",
      "iteration 1600 / 2000: loss 0.285935\n",
      "iteration 1700 / 2000: loss 0.303396\n",
      "iteration 1800 / 2000: loss 0.157530\n",
      "iteration 1900 / 2000: loss 0.270821\n",
      "Hidden Size: 10, Learning Rate: 0.01, LR Decay: 0.645, Regularization: 0.1, Validation accuracy: 0.9374\n",
      "iteration 0 / 2000: loss 2.302588\n",
      "iteration 100 / 2000: loss 0.406626\n",
      "iteration 200 / 2000: loss 0.396316\n",
      "iteration 300 / 2000: loss 0.355905\n",
      "iteration 400 / 2000: loss 0.271274\n",
      "iteration 500 / 2000: loss 0.217975\n",
      "iteration 600 / 2000: loss 0.299869\n",
      "iteration 700 / 2000: loss 0.346714\n",
      "iteration 800 / 2000: loss 0.283419\n",
      "iteration 900 / 2000: loss 0.290674\n",
      "iteration 1000 / 2000: loss 0.279066\n",
      "iteration 1100 / 2000: loss 0.325550\n",
      "iteration 1200 / 2000: loss 0.190603\n",
      "iteration 1300 / 2000: loss 0.338658\n",
      "iteration 1400 / 2000: loss 0.245708\n",
      "iteration 1500 / 2000: loss 0.314725\n",
      "iteration 1600 / 2000: loss 0.219220\n",
      "iteration 1700 / 2000: loss 0.262783\n",
      "iteration 1800 / 2000: loss 0.249068\n",
      "iteration 1900 / 2000: loss 0.266122\n",
      "Hidden Size: 10, Learning Rate: 0.01, LR Decay: 0.645, Regularization: 0.2, Validation accuracy: 0.9344\n",
      "iteration 0 / 2000: loss 2.302596\n",
      "iteration 100 / 2000: loss 0.423026\n",
      "iteration 200 / 2000: loss 0.284767\n",
      "iteration 300 / 2000: loss 0.310492\n",
      "iteration 400 / 2000: loss 0.420774\n",
      "iteration 500 / 2000: loss 0.361224\n",
      "iteration 600 / 2000: loss 0.242402\n",
      "iteration 700 / 2000: loss 0.314786\n",
      "iteration 800 / 2000: loss 0.272806\n",
      "iteration 900 / 2000: loss 0.260719\n",
      "iteration 1000 / 2000: loss 0.340613\n",
      "iteration 1100 / 2000: loss 0.252760\n",
      "iteration 1200 / 2000: loss 0.285495\n",
      "iteration 1300 / 2000: loss 0.302930\n",
      "iteration 1400 / 2000: loss 0.280407\n",
      "iteration 1500 / 2000: loss 0.311259\n",
      "iteration 1600 / 2000: loss 0.316634\n",
      "iteration 1700 / 2000: loss 0.272097\n",
      "iteration 1800 / 2000: loss 0.260757\n",
      "iteration 1900 / 2000: loss 0.230341\n",
      "Hidden Size: 10, Learning Rate: 0.01, LR Decay: 0.645, Regularization: 0.30000000000000004, Validation accuracy: 0.9366\n",
      "iteration 0 / 2000: loss 2.302600\n",
      "iteration 100 / 2000: loss 0.335006\n",
      "iteration 200 / 2000: loss 0.369732\n",
      "iteration 300 / 2000: loss 0.340721\n",
      "iteration 400 / 2000: loss 0.309918\n",
      "iteration 500 / 2000: loss 0.366341\n",
      "iteration 600 / 2000: loss 0.403156\n",
      "iteration 700 / 2000: loss 0.328390\n",
      "iteration 800 / 2000: loss 0.354127\n",
      "iteration 900 / 2000: loss 0.311132\n",
      "iteration 1000 / 2000: loss 0.337419\n",
      "iteration 1100 / 2000: loss 0.391238\n",
      "iteration 1200 / 2000: loss 0.220818\n",
      "iteration 1300 / 2000: loss 0.287603\n",
      "iteration 1400 / 2000: loss 0.310128\n",
      "iteration 1500 / 2000: loss 0.332617\n",
      "iteration 1600 / 2000: loss 0.342895\n",
      "iteration 1700 / 2000: loss 0.314763\n",
      "iteration 1800 / 2000: loss 0.273219\n",
      "iteration 1900 / 2000: loss 0.340166\n",
      "Hidden Size: 10, Learning Rate: 0.01, LR Decay: 0.645, Regularization: 0.4, Validation accuracy: 0.9338\n",
      "iteration 0 / 2000: loss 2.302585\n",
      "iteration 100 / 2000: loss 0.258605\n",
      "iteration 200 / 2000: loss 0.319250\n",
      "iteration 300 / 2000: loss 0.273710\n",
      "iteration 400 / 2000: loss 0.217953\n",
      "iteration 500 / 2000: loss 0.206902\n",
      "iteration 600 / 2000: loss 0.200453\n",
      "iteration 700 / 2000: loss 0.193913\n",
      "iteration 800 / 2000: loss 0.214674\n",
      "iteration 900 / 2000: loss 0.188936\n",
      "iteration 1000 / 2000: loss 0.190708\n",
      "iteration 1100 / 2000: loss 0.231690\n",
      "iteration 1200 / 2000: loss 0.267900\n",
      "iteration 1300 / 2000: loss 0.210889\n",
      "iteration 1400 / 2000: loss 0.212100\n",
      "iteration 1500 / 2000: loss 0.346203\n",
      "iteration 1600 / 2000: loss 0.303448\n",
      "iteration 1700 / 2000: loss 0.174777\n",
      "iteration 1800 / 2000: loss 0.190523\n",
      "iteration 1900 / 2000: loss 0.261294\n",
      "Hidden Size: 10, Learning Rate: 0.01, LR Decay: 0.47250000000000003, Regularization: 0.0, Validation accuracy: 0.9312\n",
      "iteration 0 / 2000: loss 2.302590\n",
      "iteration 100 / 2000: loss 0.291590\n",
      "iteration 200 / 2000: loss 0.256885\n",
      "iteration 300 / 2000: loss 0.215487\n",
      "iteration 400 / 2000: loss 0.314213\n",
      "iteration 500 / 2000: loss 0.225846\n",
      "iteration 600 / 2000: loss 0.231551\n",
      "iteration 700 / 2000: loss 0.305550\n",
      "iteration 800 / 2000: loss 0.294018\n",
      "iteration 900 / 2000: loss 0.176813\n",
      "iteration 1000 / 2000: loss 0.215329\n",
      "iteration 1100 / 2000: loss 0.271280\n",
      "iteration 1200 / 2000: loss 0.206239\n",
      "iteration 1300 / 2000: loss 0.265107\n",
      "iteration 1400 / 2000: loss 0.240719\n",
      "iteration 1500 / 2000: loss 0.351401\n",
      "iteration 1600 / 2000: loss 0.181775\n",
      "iteration 1700 / 2000: loss 0.234074\n",
      "iteration 1800 / 2000: loss 0.244511\n",
      "iteration 1900 / 2000: loss 0.287206\n",
      "Hidden Size: 10, Learning Rate: 0.01, LR Decay: 0.47250000000000003, Regularization: 0.1, Validation accuracy: 0.9372\n",
      "iteration 0 / 2000: loss 2.302590\n",
      "iteration 100 / 2000: loss 0.370165\n",
      "iteration 200 / 2000: loss 0.362852\n",
      "iteration 300 / 2000: loss 0.253806\n",
      "iteration 400 / 2000: loss 0.293055\n",
      "iteration 500 / 2000: loss 0.226132\n",
      "iteration 600 / 2000: loss 0.251980\n",
      "iteration 700 / 2000: loss 0.317877\n",
      "iteration 800 / 2000: loss 0.247818\n",
      "iteration 900 / 2000: loss 0.386863\n",
      "iteration 1000 / 2000: loss 0.258838\n",
      "iteration 1100 / 2000: loss 0.283973\n",
      "iteration 1200 / 2000: loss 0.292096\n",
      "iteration 1300 / 2000: loss 0.253381\n",
      "iteration 1400 / 2000: loss 0.269455\n",
      "iteration 1500 / 2000: loss 0.237199\n",
      "iteration 1600 / 2000: loss 0.299581\n",
      "iteration 1700 / 2000: loss 0.281222\n",
      "iteration 1800 / 2000: loss 0.294773\n",
      "iteration 1900 / 2000: loss 0.272782\n",
      "Hidden Size: 10, Learning Rate: 0.01, LR Decay: 0.47250000000000003, Regularization: 0.2, Validation accuracy: 0.9346\n",
      "iteration 0 / 2000: loss 2.302593\n",
      "iteration 100 / 2000: loss 0.381986\n",
      "iteration 200 / 2000: loss 0.311824\n",
      "iteration 300 / 2000: loss 0.278913\n",
      "iteration 400 / 2000: loss 0.293326\n",
      "iteration 500 / 2000: loss 0.329573\n",
      "iteration 600 / 2000: loss 0.245730\n",
      "iteration 700 / 2000: loss 0.285398\n",
      "iteration 800 / 2000: loss 0.309441\n",
      "iteration 900 / 2000: loss 0.324843\n",
      "iteration 1000 / 2000: loss 0.292074\n",
      "iteration 1100 / 2000: loss 0.312936\n",
      "iteration 1200 / 2000: loss 0.240276\n",
      "iteration 1300 / 2000: loss 0.293656\n",
      "iteration 1400 / 2000: loss 0.350785\n",
      "iteration 1500 / 2000: loss 0.289948\n",
      "iteration 1600 / 2000: loss 0.339906\n",
      "iteration 1700 / 2000: loss 0.270198\n",
      "iteration 1800 / 2000: loss 0.285622\n",
      "iteration 1900 / 2000: loss 0.316809\n",
      "Hidden Size: 10, Learning Rate: 0.01, LR Decay: 0.47250000000000003, Regularization: 0.30000000000000004, Validation accuracy: 0.9348\n",
      "iteration 0 / 2000: loss 2.302602\n",
      "iteration 100 / 2000: loss 0.428961\n",
      "iteration 200 / 2000: loss 0.353620\n",
      "iteration 300 / 2000: loss 0.351201\n",
      "iteration 400 / 2000: loss 0.294048\n",
      "iteration 500 / 2000: loss 0.331771\n",
      "iteration 600 / 2000: loss 0.329378\n",
      "iteration 700 / 2000: loss 0.371289\n",
      "iteration 800 / 2000: loss 0.394202\n",
      "iteration 900 / 2000: loss 0.382000\n",
      "iteration 1000 / 2000: loss 0.251283\n",
      "iteration 1100 / 2000: loss 0.451212\n",
      "iteration 1200 / 2000: loss 0.301433\n",
      "iteration 1300 / 2000: loss 0.296127\n",
      "iteration 1400 / 2000: loss 0.303749\n",
      "iteration 1500 / 2000: loss 0.212650\n",
      "iteration 1600 / 2000: loss 0.295410\n",
      "iteration 1700 / 2000: loss 0.311154\n",
      "iteration 1800 / 2000: loss 0.341303\n",
      "iteration 1900 / 2000: loss 0.342310\n",
      "Hidden Size: 10, Learning Rate: 0.01, LR Decay: 0.47250000000000003, Regularization: 0.4, Validation accuracy: 0.937\n",
      "iteration 0 / 2000: loss 2.302588\n",
      "iteration 100 / 2000: loss 0.395616\n",
      "iteration 200 / 2000: loss 0.357068\n",
      "iteration 300 / 2000: loss 0.214925\n",
      "iteration 400 / 2000: loss 0.219563\n",
      "iteration 500 / 2000: loss 0.312838\n",
      "iteration 600 / 2000: loss 0.333038\n",
      "iteration 700 / 2000: loss 0.220488\n",
      "iteration 800 / 2000: loss 0.176220\n",
      "iteration 900 / 2000: loss 0.215718\n",
      "iteration 1000 / 2000: loss 0.239854\n",
      "iteration 1100 / 2000: loss 0.225189\n",
      "iteration 1200 / 2000: loss 0.249042\n",
      "iteration 1300 / 2000: loss 0.243991\n",
      "iteration 1400 / 2000: loss 0.214532\n",
      "iteration 1500 / 2000: loss 0.152271\n",
      "iteration 1600 / 2000: loss 0.240758\n",
      "iteration 1700 / 2000: loss 0.239933\n",
      "iteration 1800 / 2000: loss 0.279524\n",
      "iteration 1900 / 2000: loss 0.184050\n",
      "Hidden Size: 10, Learning Rate: 0.01, LR Decay: 0.3, Regularization: 0.0, Validation accuracy: 0.9316\n",
      "iteration 0 / 2000: loss 2.302602\n",
      "iteration 100 / 2000: loss 0.501411\n",
      "iteration 200 / 2000: loss 0.429395\n",
      "iteration 300 / 2000: loss 0.277119\n",
      "iteration 400 / 2000: loss 0.338386\n",
      "iteration 500 / 2000: loss 0.279492\n",
      "iteration 600 / 2000: loss 0.322100\n",
      "iteration 700 / 2000: loss 0.344987\n",
      "iteration 800 / 2000: loss 0.262198\n",
      "iteration 900 / 2000: loss 0.248169\n",
      "iteration 1000 / 2000: loss 0.311724\n",
      "iteration 1100 / 2000: loss 0.245067\n",
      "iteration 1200 / 2000: loss 0.281929\n",
      "iteration 1300 / 2000: loss 0.238975\n",
      "iteration 1400 / 2000: loss 0.231093\n",
      "iteration 1500 / 2000: loss 0.225049\n",
      "iteration 1600 / 2000: loss 0.222308\n",
      "iteration 1700 / 2000: loss 0.244931\n",
      "iteration 1800 / 2000: loss 0.264214\n",
      "iteration 1900 / 2000: loss 0.278972\n",
      "Hidden Size: 10, Learning Rate: 0.01, LR Decay: 0.3, Regularization: 0.1, Validation accuracy: 0.9262\n",
      "iteration 0 / 2000: loss 2.302585\n",
      "iteration 100 / 2000: loss 0.386500\n",
      "iteration 200 / 2000: loss 0.270572\n",
      "iteration 300 / 2000: loss 0.301188\n",
      "iteration 400 / 2000: loss 0.237383\n",
      "iteration 500 / 2000: loss 0.278599\n",
      "iteration 600 / 2000: loss 0.245659\n",
      "iteration 700 / 2000: loss 0.244054\n",
      "iteration 800 / 2000: loss 0.218135\n",
      "iteration 900 / 2000: loss 0.287011\n",
      "iteration 1000 / 2000: loss 0.345906\n",
      "iteration 1100 / 2000: loss 0.347682\n",
      "iteration 1200 / 2000: loss 0.248212\n",
      "iteration 1300 / 2000: loss 0.247922\n",
      "iteration 1400 / 2000: loss 0.322066\n",
      "iteration 1500 / 2000: loss 0.263976\n",
      "iteration 1600 / 2000: loss 0.201289\n",
      "iteration 1700 / 2000: loss 0.309555\n",
      "iteration 1800 / 2000: loss 0.260917\n",
      "iteration 1900 / 2000: loss 0.256199\n",
      "Hidden Size: 10, Learning Rate: 0.01, LR Decay: 0.3, Regularization: 0.2, Validation accuracy: 0.9312\n",
      "iteration 0 / 2000: loss 2.302599\n",
      "iteration 100 / 2000: loss 0.570122\n",
      "iteration 200 / 2000: loss 0.293589\n",
      "iteration 300 / 2000: loss 0.383727\n",
      "iteration 400 / 2000: loss 0.322273\n",
      "iteration 500 / 2000: loss 0.394264\n",
      "iteration 600 / 2000: loss 0.406430\n",
      "iteration 700 / 2000: loss 0.331096\n",
      "iteration 800 / 2000: loss 0.306463\n",
      "iteration 900 / 2000: loss 0.311777\n",
      "iteration 1000 / 2000: loss 0.357151\n",
      "iteration 1100 / 2000: loss 0.272122\n",
      "iteration 1200 / 2000: loss 0.332495\n",
      "iteration 1300 / 2000: loss 0.192417\n",
      "iteration 1400 / 2000: loss 0.426809\n",
      "iteration 1500 / 2000: loss 0.318603\n",
      "iteration 1600 / 2000: loss 0.386696\n",
      "iteration 1700 / 2000: loss 0.462499\n",
      "iteration 1800 / 2000: loss 0.347840\n",
      "iteration 1900 / 2000: loss 0.233738\n",
      "Hidden Size: 10, Learning Rate: 0.01, LR Decay: 0.3, Regularization: 0.30000000000000004, Validation accuracy: 0.9236\n",
      "iteration 0 / 2000: loss 2.302604\n",
      "iteration 100 / 2000: loss 0.453608\n",
      "iteration 200 / 2000: loss 0.460959\n",
      "iteration 300 / 2000: loss 0.296997\n",
      "iteration 400 / 2000: loss 0.280182\n",
      "iteration 500 / 2000: loss 0.389329\n",
      "iteration 600 / 2000: loss 0.293487\n",
      "iteration 700 / 2000: loss 0.331844\n",
      "iteration 800 / 2000: loss 0.316653\n",
      "iteration 900 / 2000: loss 0.357569\n",
      "iteration 1000 / 2000: loss 0.361294\n",
      "iteration 1100 / 2000: loss 0.391052\n",
      "iteration 1200 / 2000: loss 0.313646\n",
      "iteration 1300 / 2000: loss 0.278616\n",
      "iteration 1400 / 2000: loss 0.407059\n",
      "iteration 1500 / 2000: loss 0.309703\n",
      "iteration 1600 / 2000: loss 0.406611\n",
      "iteration 1700 / 2000: loss 0.274187\n",
      "iteration 1800 / 2000: loss 0.360958\n",
      "iteration 1900 / 2000: loss 0.360853\n",
      "Hidden Size: 10, Learning Rate: 0.01, LR Decay: 0.3, Regularization: 0.4, Validation accuracy: 0.922\n",
      "iteration 0 / 2000: loss 2.302578\n",
      "iteration 100 / 2000: loss 0.412333\n",
      "iteration 200 / 2000: loss 0.308441\n",
      "iteration 300 / 2000: loss 0.228705\n",
      "iteration 400 / 2000: loss 0.255772\n",
      "iteration 500 / 2000: loss 0.260516\n",
      "iteration 600 / 2000: loss 0.253806\n",
      "iteration 700 / 2000: loss 0.372603\n",
      "iteration 800 / 2000: loss 0.151482\n",
      "iteration 900 / 2000: loss 0.300031\n",
      "iteration 1000 / 2000: loss 0.335040\n",
      "iteration 1100 / 2000: loss 0.310690\n",
      "iteration 1200 / 2000: loss 0.367969\n",
      "iteration 1300 / 2000: loss 0.278581\n",
      "iteration 1400 / 2000: loss 0.342401\n",
      "iteration 1500 / 2000: loss 0.216818\n",
      "iteration 1600 / 2000: loss 0.220135\n",
      "iteration 1700 / 2000: loss 0.296287\n",
      "iteration 1800 / 2000: loss 0.234771\n",
      "iteration 1900 / 2000: loss 0.316665\n",
      "Hidden Size: 10, Learning Rate: 0.0075000250000000004, LR Decay: 0.99, Regularization: 0.0, Validation accuracy: 0.9302\n",
      "iteration 0 / 2000: loss 2.302578\n",
      "iteration 100 / 2000: loss 0.352261\n",
      "iteration 200 / 2000: loss 0.434516\n",
      "iteration 300 / 2000: loss 0.287847\n",
      "iteration 400 / 2000: loss 0.255385\n",
      "iteration 500 / 2000: loss 0.287476\n",
      "iteration 600 / 2000: loss 0.261666\n",
      "iteration 700 / 2000: loss 0.223454\n",
      "iteration 800 / 2000: loss 0.224748\n",
      "iteration 900 / 2000: loss 0.277154\n",
      "iteration 1000 / 2000: loss 0.222854\n",
      "iteration 1100 / 2000: loss 0.244806\n",
      "iteration 1200 / 2000: loss 0.282376\n",
      "iteration 1300 / 2000: loss 0.219241\n",
      "iteration 1400 / 2000: loss 0.298811\n",
      "iteration 1500 / 2000: loss 0.250180\n",
      "iteration 1600 / 2000: loss 0.321233\n",
      "iteration 1700 / 2000: loss 0.282429\n",
      "iteration 1800 / 2000: loss 0.304477\n",
      "iteration 1900 / 2000: loss 0.215125\n",
      "Hidden Size: 10, Learning Rate: 0.0075000250000000004, LR Decay: 0.99, Regularization: 0.1, Validation accuracy: 0.938\n",
      "iteration 0 / 2000: loss 2.302597\n",
      "iteration 100 / 2000: loss 0.325497\n",
      "iteration 200 / 2000: loss 0.380317\n",
      "iteration 300 / 2000: loss 0.310004\n",
      "iteration 400 / 2000: loss 0.384194\n",
      "iteration 500 / 2000: loss 0.377720\n",
      "iteration 600 / 2000: loss 0.190105\n",
      "iteration 700 / 2000: loss 0.386242\n",
      "iteration 800 / 2000: loss 0.300417\n",
      "iteration 900 / 2000: loss 0.363166\n",
      "iteration 1000 / 2000: loss 0.338117\n",
      "iteration 1100 / 2000: loss 0.380096\n",
      "iteration 1200 / 2000: loss 0.202168\n",
      "iteration 1300 / 2000: loss 0.306544\n",
      "iteration 1400 / 2000: loss 0.325927\n",
      "iteration 1500 / 2000: loss 0.319184\n",
      "iteration 1600 / 2000: loss 0.253101\n",
      "iteration 1700 / 2000: loss 0.276310\n",
      "iteration 1800 / 2000: loss 0.276624\n",
      "iteration 1900 / 2000: loss 0.237282\n",
      "Hidden Size: 10, Learning Rate: 0.0075000250000000004, LR Decay: 0.99, Regularization: 0.2, Validation accuracy: 0.9274\n",
      "iteration 0 / 2000: loss 2.302599\n",
      "iteration 100 / 2000: loss 0.315253\n",
      "iteration 200 / 2000: loss 0.304452\n",
      "iteration 300 / 2000: loss 0.284860\n",
      "iteration 400 / 2000: loss 0.396770\n",
      "iteration 500 / 2000: loss 0.402795\n",
      "iteration 600 / 2000: loss 0.388584\n",
      "iteration 700 / 2000: loss 0.347410\n",
      "iteration 800 / 2000: loss 0.429230\n",
      "iteration 900 / 2000: loss 0.325531\n",
      "iteration 1000 / 2000: loss 0.322932\n",
      "iteration 1100 / 2000: loss 0.385152\n",
      "iteration 1200 / 2000: loss 0.461674\n",
      "iteration 1300 / 2000: loss 0.247486\n",
      "iteration 1400 / 2000: loss 0.308277\n",
      "iteration 1500 / 2000: loss 0.328435\n",
      "iteration 1600 / 2000: loss 0.365447\n",
      "iteration 1700 / 2000: loss 0.331056\n",
      "iteration 1800 / 2000: loss 0.373045\n",
      "iteration 1900 / 2000: loss 0.327550\n",
      "Hidden Size: 10, Learning Rate: 0.0075000250000000004, LR Decay: 0.99, Regularization: 0.30000000000000004, Validation accuracy: 0.9282\n",
      "iteration 0 / 2000: loss 2.302605\n",
      "iteration 100 / 2000: loss 0.420149\n",
      "iteration 200 / 2000: loss 0.451600\n",
      "iteration 300 / 2000: loss 0.347250\n",
      "iteration 400 / 2000: loss 0.380382\n",
      "iteration 500 / 2000: loss 0.377198\n",
      "iteration 600 / 2000: loss 0.383102\n",
      "iteration 700 / 2000: loss 0.492470\n",
      "iteration 800 / 2000: loss 0.354952\n",
      "iteration 900 / 2000: loss 0.391833\n",
      "iteration 1000 / 2000: loss 0.341853\n",
      "iteration 1100 / 2000: loss 0.405337\n",
      "iteration 1200 / 2000: loss 0.272596\n",
      "iteration 1300 / 2000: loss 0.360444\n",
      "iteration 1400 / 2000: loss 0.399007\n",
      "iteration 1500 / 2000: loss 0.327710\n",
      "iteration 1600 / 2000: loss 0.308962\n",
      "iteration 1700 / 2000: loss 0.370976\n",
      "iteration 1800 / 2000: loss 0.314771\n",
      "iteration 1900 / 2000: loss 0.369162\n",
      "Hidden Size: 10, Learning Rate: 0.0075000250000000004, LR Decay: 0.99, Regularization: 0.4, Validation accuracy: 0.9318\n",
      "iteration 0 / 2000: loss 2.302589\n",
      "iteration 100 / 2000: loss 0.330656\n",
      "iteration 200 / 2000: loss 0.357409\n",
      "iteration 300 / 2000: loss 0.306981\n",
      "iteration 400 / 2000: loss 0.331367\n",
      "iteration 500 / 2000: loss 0.391778\n",
      "iteration 600 / 2000: loss 0.269957\n",
      "iteration 700 / 2000: loss 0.265222\n",
      "iteration 800 / 2000: loss 0.241753\n",
      "iteration 900 / 2000: loss 0.160399\n",
      "iteration 1000 / 2000: loss 0.121551\n",
      "iteration 1100 / 2000: loss 0.241269\n",
      "iteration 1200 / 2000: loss 0.200850\n",
      "iteration 1300 / 2000: loss 0.203713\n",
      "iteration 1400 / 2000: loss 0.158718\n",
      "iteration 1500 / 2000: loss 0.246072\n",
      "iteration 1600 / 2000: loss 0.262079\n",
      "iteration 1700 / 2000: loss 0.154703\n",
      "iteration 1800 / 2000: loss 0.198769\n",
      "iteration 1900 / 2000: loss 0.160229\n",
      "Hidden Size: 10, Learning Rate: 0.0075000250000000004, LR Decay: 0.8175, Regularization: 0.0, Validation accuracy: 0.9318\n",
      "iteration 0 / 2000: loss 2.302592\n",
      "iteration 100 / 2000: loss 0.314466\n",
      "iteration 200 / 2000: loss 0.341814\n",
      "iteration 300 / 2000: loss 0.392800\n",
      "iteration 400 / 2000: loss 0.282775\n",
      "iteration 500 / 2000: loss 0.276365\n",
      "iteration 600 / 2000: loss 0.228552\n",
      "iteration 700 / 2000: loss 0.250775\n",
      "iteration 800 / 2000: loss 0.313052\n",
      "iteration 900 / 2000: loss 0.198160\n",
      "iteration 1000 / 2000: loss 0.231162\n",
      "iteration 1100 / 2000: loss 0.296061\n",
      "iteration 1200 / 2000: loss 0.192034\n",
      "iteration 1300 / 2000: loss 0.214059\n",
      "iteration 1400 / 2000: loss 0.172995\n",
      "iteration 1500 / 2000: loss 0.251773\n",
      "iteration 1600 / 2000: loss 0.248461\n",
      "iteration 1700 / 2000: loss 0.206011\n",
      "iteration 1800 / 2000: loss 0.252441\n",
      "iteration 1900 / 2000: loss 0.199288\n",
      "Hidden Size: 10, Learning Rate: 0.0075000250000000004, LR Decay: 0.8175, Regularization: 0.1, Validation accuracy: 0.9358\n",
      "iteration 0 / 2000: loss 2.302600\n",
      "iteration 100 / 2000: loss 0.353392\n",
      "iteration 200 / 2000: loss 0.198448\n",
      "iteration 300 / 2000: loss 0.284368\n",
      "iteration 400 / 2000: loss 0.345288\n",
      "iteration 500 / 2000: loss 0.255143\n",
      "iteration 600 / 2000: loss 0.227940\n",
      "iteration 700 / 2000: loss 0.215875\n",
      "iteration 800 / 2000: loss 0.274242\n",
      "iteration 900 / 2000: loss 0.333340\n",
      "iteration 1000 / 2000: loss 0.239401\n",
      "iteration 1100 / 2000: loss 0.283949\n",
      "iteration 1200 / 2000: loss 0.397865\n",
      "iteration 1300 / 2000: loss 0.264245\n",
      "iteration 1400 / 2000: loss 0.245331\n",
      "iteration 1500 / 2000: loss 0.282241\n",
      "iteration 1600 / 2000: loss 0.300474\n",
      "iteration 1700 / 2000: loss 0.348231\n",
      "iteration 1800 / 2000: loss 0.310595\n",
      "iteration 1900 / 2000: loss 0.172703\n",
      "Hidden Size: 10, Learning Rate: 0.0075000250000000004, LR Decay: 0.8175, Regularization: 0.2, Validation accuracy: 0.939\n",
      "iteration 0 / 2000: loss 2.302601\n",
      "iteration 100 / 2000: loss 0.407156\n",
      "iteration 200 / 2000: loss 0.408537\n",
      "iteration 300 / 2000: loss 0.352741\n",
      "iteration 400 / 2000: loss 0.253543\n",
      "iteration 500 / 2000: loss 0.322932\n",
      "iteration 600 / 2000: loss 0.335197\n",
      "iteration 700 / 2000: loss 0.379435\n",
      "iteration 800 / 2000: loss 0.319125\n",
      "iteration 900 / 2000: loss 0.369407\n",
      "iteration 1000 / 2000: loss 0.390612\n",
      "iteration 1100 / 2000: loss 0.310442\n",
      "iteration 1200 / 2000: loss 0.231654\n",
      "iteration 1300 / 2000: loss 0.270091\n",
      "iteration 1400 / 2000: loss 0.248074\n",
      "iteration 1500 / 2000: loss 0.331266\n",
      "iteration 1600 / 2000: loss 0.335218\n",
      "iteration 1700 / 2000: loss 0.274301\n",
      "iteration 1800 / 2000: loss 0.219172\n",
      "iteration 1900 / 2000: loss 0.226353\n",
      "Hidden Size: 10, Learning Rate: 0.0075000250000000004, LR Decay: 0.8175, Regularization: 0.30000000000000004, Validation accuracy: 0.9376\n",
      "iteration 0 / 2000: loss 2.302601\n",
      "iteration 100 / 2000: loss 0.457627\n",
      "iteration 200 / 2000: loss 0.240437\n",
      "iteration 300 / 2000: loss 0.345902\n",
      "iteration 400 / 2000: loss 0.427652\n",
      "iteration 500 / 2000: loss 0.364113\n",
      "iteration 600 / 2000: loss 0.362502\n",
      "iteration 700 / 2000: loss 0.328089\n",
      "iteration 800 / 2000: loss 0.273492\n",
      "iteration 900 / 2000: loss 0.474643\n",
      "iteration 1000 / 2000: loss 0.331876\n",
      "iteration 1100 / 2000: loss 0.197855\n",
      "iteration 1200 / 2000: loss 0.310705\n",
      "iteration 1300 / 2000: loss 0.335498\n",
      "iteration 1400 / 2000: loss 0.219026\n",
      "iteration 1500 / 2000: loss 0.289951\n",
      "iteration 1600 / 2000: loss 0.353541\n",
      "iteration 1700 / 2000: loss 0.302241\n",
      "iteration 1800 / 2000: loss 0.322735\n",
      "iteration 1900 / 2000: loss 0.319312\n",
      "Hidden Size: 10, Learning Rate: 0.0075000250000000004, LR Decay: 0.8175, Regularization: 0.4, Validation accuracy: 0.9376\n",
      "iteration 0 / 2000: loss 2.302569\n",
      "iteration 100 / 2000: loss 0.299020\n",
      "iteration 200 / 2000: loss 0.301693\n",
      "iteration 300 / 2000: loss 0.423867\n",
      "iteration 400 / 2000: loss 0.317354\n",
      "iteration 500 / 2000: loss 0.368426\n",
      "iteration 600 / 2000: loss 0.263376\n",
      "iteration 700 / 2000: loss 0.199459\n",
      "iteration 800 / 2000: loss 0.298039\n",
      "iteration 900 / 2000: loss 0.131270\n",
      "iteration 1000 / 2000: loss 0.251100\n",
      "iteration 1100 / 2000: loss 0.257368\n",
      "iteration 1200 / 2000: loss 0.196071\n",
      "iteration 1300 / 2000: loss 0.225802\n",
      "iteration 1400 / 2000: loss 0.193670\n",
      "iteration 1500 / 2000: loss 0.237282\n",
      "iteration 1600 / 2000: loss 0.245471\n",
      "iteration 1700 / 2000: loss 0.170859\n",
      "iteration 1800 / 2000: loss 0.228434\n",
      "iteration 1900 / 2000: loss 0.191808\n",
      "Hidden Size: 10, Learning Rate: 0.0075000250000000004, LR Decay: 0.645, Regularization: 0.0, Validation accuracy: 0.939\n",
      "iteration 0 / 2000: loss 2.302585\n",
      "iteration 100 / 2000: loss 0.380010\n",
      "iteration 200 / 2000: loss 0.344678\n",
      "iteration 300 / 2000: loss 0.430363\n",
      "iteration 400 / 2000: loss 0.397778\n",
      "iteration 500 / 2000: loss 0.215951\n",
      "iteration 600 / 2000: loss 0.237535\n",
      "iteration 700 / 2000: loss 0.211495\n",
      "iteration 800 / 2000: loss 0.189097\n",
      "iteration 900 / 2000: loss 0.293485\n",
      "iteration 1000 / 2000: loss 0.339438\n",
      "iteration 1100 / 2000: loss 0.245489\n",
      "iteration 1200 / 2000: loss 0.228296\n",
      "iteration 1300 / 2000: loss 0.199347\n",
      "iteration 1400 / 2000: loss 0.296532\n",
      "iteration 1500 / 2000: loss 0.288007\n",
      "iteration 1600 / 2000: loss 0.264852\n",
      "iteration 1700 / 2000: loss 0.276384\n",
      "iteration 1800 / 2000: loss 0.284847\n",
      "iteration 1900 / 2000: loss 0.237924\n",
      "Hidden Size: 10, Learning Rate: 0.0075000250000000004, LR Decay: 0.645, Regularization: 0.1, Validation accuracy: 0.9364\n",
      "iteration 0 / 2000: loss 2.302591\n",
      "iteration 100 / 2000: loss 0.439751\n",
      "iteration 200 / 2000: loss 0.311256\n",
      "iteration 300 / 2000: loss 0.334256\n",
      "iteration 400 / 2000: loss 0.324675\n",
      "iteration 500 / 2000: loss 0.239464\n",
      "iteration 600 / 2000: loss 0.239104\n",
      "iteration 700 / 2000: loss 0.337092\n",
      "iteration 800 / 2000: loss 0.271974\n",
      "iteration 900 / 2000: loss 0.299685\n",
      "iteration 1000 / 2000: loss 0.185147\n",
      "iteration 1100 / 2000: loss 0.290634\n",
      "iteration 1200 / 2000: loss 0.205105\n",
      "iteration 1300 / 2000: loss 0.261146\n",
      "iteration 1400 / 2000: loss 0.314543\n",
      "iteration 1500 / 2000: loss 0.308796\n",
      "iteration 1600 / 2000: loss 0.213833\n",
      "iteration 1700 / 2000: loss 0.309149\n",
      "iteration 1800 / 2000: loss 0.244629\n",
      "iteration 1900 / 2000: loss 0.290425\n",
      "Hidden Size: 10, Learning Rate: 0.0075000250000000004, LR Decay: 0.645, Regularization: 0.2, Validation accuracy: 0.938\n",
      "iteration 0 / 2000: loss 2.302595\n",
      "iteration 100 / 2000: loss 0.377116\n",
      "iteration 200 / 2000: loss 0.389661\n",
      "iteration 300 / 2000: loss 0.290345\n",
      "iteration 400 / 2000: loss 0.359612\n",
      "iteration 500 / 2000: loss 0.336458\n",
      "iteration 600 / 2000: loss 0.341386\n",
      "iteration 700 / 2000: loss 0.366708\n",
      "iteration 800 / 2000: loss 0.217075\n",
      "iteration 900 / 2000: loss 0.277370\n",
      "iteration 1000 / 2000: loss 0.307948\n",
      "iteration 1100 / 2000: loss 0.244838\n",
      "iteration 1200 / 2000: loss 0.244540\n",
      "iteration 1300 / 2000: loss 0.323221\n",
      "iteration 1400 / 2000: loss 0.370081\n",
      "iteration 1500 / 2000: loss 0.249923\n",
      "iteration 1600 / 2000: loss 0.283547\n",
      "iteration 1700 / 2000: loss 0.298271\n",
      "iteration 1800 / 2000: loss 0.336125\n",
      "iteration 1900 / 2000: loss 0.264304\n",
      "Hidden Size: 10, Learning Rate: 0.0075000250000000004, LR Decay: 0.645, Regularization: 0.30000000000000004, Validation accuracy: 0.94\n",
      "iteration 0 / 2000: loss 2.302599\n",
      "iteration 100 / 2000: loss 0.439587\n",
      "iteration 200 / 2000: loss 0.320986\n",
      "iteration 300 / 2000: loss 0.288089\n",
      "iteration 400 / 2000: loss 0.303485\n",
      "iteration 500 / 2000: loss 0.288118\n",
      "iteration 600 / 2000: loss 0.259010\n",
      "iteration 700 / 2000: loss 0.196331\n",
      "iteration 800 / 2000: loss 0.278879\n",
      "iteration 900 / 2000: loss 0.312790\n",
      "iteration 1000 / 2000: loss 0.321946\n",
      "iteration 1100 / 2000: loss 0.281628\n",
      "iteration 1200 / 2000: loss 0.221054\n",
      "iteration 1300 / 2000: loss 0.342860\n",
      "iteration 1400 / 2000: loss 0.398157\n",
      "iteration 1500 / 2000: loss 0.327863\n",
      "iteration 1600 / 2000: loss 0.218726\n",
      "iteration 1700 / 2000: loss 0.328808\n",
      "iteration 1800 / 2000: loss 0.391149\n",
      "iteration 1900 / 2000: loss 0.328652\n",
      "Hidden Size: 10, Learning Rate: 0.0075000250000000004, LR Decay: 0.645, Regularization: 0.4, Validation accuracy: 0.9378\n",
      "iteration 0 / 2000: loss 2.302579\n",
      "iteration 100 / 2000: loss 0.324198\n",
      "iteration 200 / 2000: loss 0.342363\n",
      "iteration 300 / 2000: loss 0.373218\n",
      "iteration 400 / 2000: loss 0.452937\n",
      "iteration 500 / 2000: loss 0.340715\n",
      "iteration 600 / 2000: loss 0.346383\n",
      "iteration 700 / 2000: loss 0.244154\n",
      "iteration 800 / 2000: loss 0.132804\n",
      "iteration 900 / 2000: loss 0.405580\n",
      "iteration 1000 / 2000: loss 0.288632\n",
      "iteration 1100 / 2000: loss 0.126539\n",
      "iteration 1200 / 2000: loss 0.179172\n",
      "iteration 1300 / 2000: loss 0.185382\n",
      "iteration 1400 / 2000: loss 0.125941\n",
      "iteration 1500 / 2000: loss 0.190299\n",
      "iteration 1600 / 2000: loss 0.158020\n",
      "iteration 1700 / 2000: loss 0.196446\n",
      "iteration 1800 / 2000: loss 0.329096\n",
      "iteration 1900 / 2000: loss 0.275719\n",
      "Hidden Size: 10, Learning Rate: 0.0075000250000000004, LR Decay: 0.47250000000000003, Regularization: 0.0, Validation accuracy: 0.9382\n",
      "iteration 0 / 2000: loss 2.302584\n",
      "iteration 100 / 2000: loss 0.519387\n",
      "iteration 200 / 2000: loss 0.315404\n",
      "iteration 300 / 2000: loss 0.324226\n",
      "iteration 400 / 2000: loss 0.183239\n",
      "iteration 500 / 2000: loss 0.319673\n",
      "iteration 600 / 2000: loss 0.226022\n",
      "iteration 700 / 2000: loss 0.220619\n",
      "iteration 800 / 2000: loss 0.235574\n",
      "iteration 900 / 2000: loss 0.357512\n",
      "iteration 1000 / 2000: loss 0.293878\n",
      "iteration 1100 / 2000: loss 0.243291\n",
      "iteration 1200 / 2000: loss 0.184748\n",
      "iteration 1300 / 2000: loss 0.152617\n",
      "iteration 1400 / 2000: loss 0.241150\n",
      "iteration 1500 / 2000: loss 0.296382\n",
      "iteration 1600 / 2000: loss 0.313020\n",
      "iteration 1700 / 2000: loss 0.164518\n",
      "iteration 1800 / 2000: loss 0.292974\n",
      "iteration 1900 / 2000: loss 0.218219\n",
      "Hidden Size: 10, Learning Rate: 0.0075000250000000004, LR Decay: 0.47250000000000003, Regularization: 0.1, Validation accuracy: 0.932\n",
      "iteration 0 / 2000: loss 2.302591\n",
      "iteration 100 / 2000: loss 0.429289\n",
      "iteration 200 / 2000: loss 0.259105\n",
      "iteration 300 / 2000: loss 0.276507\n",
      "iteration 400 / 2000: loss 0.263146\n",
      "iteration 500 / 2000: loss 0.361550\n",
      "iteration 600 / 2000: loss 0.300995\n",
      "iteration 700 / 2000: loss 0.252295\n",
      "iteration 800 / 2000: loss 0.389991\n",
      "iteration 900 / 2000: loss 0.325736\n",
      "iteration 1000 / 2000: loss 0.301337\n",
      "iteration 1100 / 2000: loss 0.191434\n",
      "iteration 1200 / 2000: loss 0.290021\n",
      "iteration 1300 / 2000: loss 0.468128\n",
      "iteration 1400 / 2000: loss 0.283894\n",
      "iteration 1500 / 2000: loss 0.380740\n",
      "iteration 1600 / 2000: loss 0.283498\n",
      "iteration 1700 / 2000: loss 0.296540\n",
      "iteration 1800 / 2000: loss 0.305769\n",
      "iteration 1900 / 2000: loss 0.389313\n",
      "Hidden Size: 10, Learning Rate: 0.0075000250000000004, LR Decay: 0.47250000000000003, Regularization: 0.2, Validation accuracy: 0.929\n",
      "iteration 0 / 2000: loss 2.302590\n",
      "iteration 100 / 2000: loss 0.399929\n",
      "iteration 200 / 2000: loss 0.277390\n",
      "iteration 300 / 2000: loss 0.337179\n",
      "iteration 400 / 2000: loss 0.364087\n",
      "iteration 500 / 2000: loss 0.325517\n",
      "iteration 600 / 2000: loss 0.408465\n",
      "iteration 700 / 2000: loss 0.286157\n",
      "iteration 800 / 2000: loss 0.303273\n",
      "iteration 900 / 2000: loss 0.273576\n",
      "iteration 1000 / 2000: loss 0.295416\n",
      "iteration 1100 / 2000: loss 0.272189\n",
      "iteration 1200 / 2000: loss 0.314168\n",
      "iteration 1300 / 2000: loss 0.331918\n",
      "iteration 1400 / 2000: loss 0.267798\n",
      "iteration 1500 / 2000: loss 0.257084\n",
      "iteration 1600 / 2000: loss 0.330497\n",
      "iteration 1700 / 2000: loss 0.283602\n",
      "iteration 1800 / 2000: loss 0.276904\n",
      "iteration 1900 / 2000: loss 0.278739\n",
      "Hidden Size: 10, Learning Rate: 0.0075000250000000004, LR Decay: 0.47250000000000003, Regularization: 0.30000000000000004, Validation accuracy: 0.9334\n",
      "iteration 0 / 2000: loss 2.302595\n",
      "iteration 100 / 2000: loss 0.515016\n",
      "iteration 200 / 2000: loss 0.350698\n",
      "iteration 300 / 2000: loss 0.347167\n",
      "iteration 400 / 2000: loss 0.319010\n",
      "iteration 500 / 2000: loss 0.362650\n",
      "iteration 600 / 2000: loss 0.350847\n",
      "iteration 700 / 2000: loss 0.327686\n",
      "iteration 800 / 2000: loss 0.399768\n",
      "iteration 900 / 2000: loss 0.366809\n",
      "iteration 1000 / 2000: loss 0.229590\n",
      "iteration 1100 / 2000: loss 0.347462\n",
      "iteration 1200 / 2000: loss 0.336635\n",
      "iteration 1300 / 2000: loss 0.339678\n",
      "iteration 1400 / 2000: loss 0.398711\n",
      "iteration 1500 / 2000: loss 0.300111\n",
      "iteration 1600 / 2000: loss 0.402436\n",
      "iteration 1700 / 2000: loss 0.387346\n",
      "iteration 1800 / 2000: loss 0.337651\n",
      "iteration 1900 / 2000: loss 0.252540\n",
      "Hidden Size: 10, Learning Rate: 0.0075000250000000004, LR Decay: 0.47250000000000003, Regularization: 0.4, Validation accuracy: 0.9306\n",
      "iteration 0 / 2000: loss 2.302580\n",
      "iteration 100 / 2000: loss 0.457939\n",
      "iteration 200 / 2000: loss 0.350008\n",
      "iteration 300 / 2000: loss 0.259581\n",
      "iteration 400 / 2000: loss 0.234273\n",
      "iteration 500 / 2000: loss 0.287442\n",
      "iteration 600 / 2000: loss 0.392286\n",
      "iteration 700 / 2000: loss 0.293277\n",
      "iteration 800 / 2000: loss 0.263758\n",
      "iteration 900 / 2000: loss 0.271274\n",
      "iteration 1000 / 2000: loss 0.226596\n",
      "iteration 1100 / 2000: loss 0.203882\n",
      "iteration 1200 / 2000: loss 0.290884\n",
      "iteration 1300 / 2000: loss 0.333593\n",
      "iteration 1400 / 2000: loss 0.246238\n",
      "iteration 1500 / 2000: loss 0.271696\n",
      "iteration 1600 / 2000: loss 0.306146\n",
      "iteration 1700 / 2000: loss 0.273192\n",
      "iteration 1800 / 2000: loss 0.285332\n",
      "iteration 1900 / 2000: loss 0.321217\n",
      "Hidden Size: 10, Learning Rate: 0.0075000250000000004, LR Decay: 0.3, Regularization: 0.0, Validation accuracy: 0.9278\n",
      "iteration 0 / 2000: loss 2.302589\n",
      "iteration 100 / 2000: loss 0.473496\n",
      "iteration 200 / 2000: loss 0.261119\n",
      "iteration 300 / 2000: loss 0.502309\n",
      "iteration 400 / 2000: loss 0.291597\n",
      "iteration 500 / 2000: loss 0.386274\n",
      "iteration 600 / 2000: loss 0.293145\n",
      "iteration 700 / 2000: loss 0.296213\n",
      "iteration 800 / 2000: loss 0.292172\n",
      "iteration 900 / 2000: loss 0.334966\n",
      "iteration 1000 / 2000: loss 0.241323\n",
      "iteration 1100 / 2000: loss 0.381972\n",
      "iteration 1200 / 2000: loss 0.370572\n",
      "iteration 1300 / 2000: loss 0.277678\n",
      "iteration 1400 / 2000: loss 0.274746\n",
      "iteration 1500 / 2000: loss 0.288292\n",
      "iteration 1600 / 2000: loss 0.316866\n",
      "iteration 1700 / 2000: loss 0.311656\n",
      "iteration 1800 / 2000: loss 0.260893\n",
      "iteration 1900 / 2000: loss 0.268136\n",
      "Hidden Size: 10, Learning Rate: 0.0075000250000000004, LR Decay: 0.3, Regularization: 0.1, Validation accuracy: 0.923\n",
      "iteration 0 / 2000: loss 2.302592\n",
      "iteration 100 / 2000: loss 0.490650\n",
      "iteration 200 / 2000: loss 0.367962\n",
      "iteration 300 / 2000: loss 0.368944\n",
      "iteration 400 / 2000: loss 0.287929\n",
      "iteration 500 / 2000: loss 0.310986\n",
      "iteration 600 / 2000: loss 0.301714\n",
      "iteration 700 / 2000: loss 0.328531\n",
      "iteration 800 / 2000: loss 0.243366\n",
      "iteration 900 / 2000: loss 0.390985\n",
      "iteration 1000 / 2000: loss 0.300569\n",
      "iteration 1100 / 2000: loss 0.298785\n",
      "iteration 1200 / 2000: loss 0.350837\n",
      "iteration 1300 / 2000: loss 0.341684\n",
      "iteration 1400 / 2000: loss 0.296237\n",
      "iteration 1500 / 2000: loss 0.168641\n",
      "iteration 1600 / 2000: loss 0.287305\n",
      "iteration 1700 / 2000: loss 0.395733\n",
      "iteration 1800 / 2000: loss 0.349778\n",
      "iteration 1900 / 2000: loss 0.218488\n",
      "Hidden Size: 10, Learning Rate: 0.0075000250000000004, LR Decay: 0.3, Regularization: 0.2, Validation accuracy: 0.9212\n",
      "iteration 0 / 2000: loss 2.302599\n",
      "iteration 100 / 2000: loss 0.488828\n",
      "iteration 200 / 2000: loss 0.338553\n",
      "iteration 300 / 2000: loss 0.359326\n",
      "iteration 400 / 2000: loss 0.226265\n",
      "iteration 500 / 2000: loss 0.341168\n",
      "iteration 600 / 2000: loss 0.212964\n",
      "iteration 700 / 2000: loss 0.334298\n",
      "iteration 800 / 2000: loss 0.353108\n",
      "iteration 900 / 2000: loss 0.440898\n",
      "iteration 1000 / 2000: loss 0.297896\n",
      "iteration 1100 / 2000: loss 0.326461\n",
      "iteration 1200 / 2000: loss 0.383620\n",
      "iteration 1300 / 2000: loss 0.483225\n",
      "iteration 1400 / 2000: loss 0.373310\n",
      "iteration 1500 / 2000: loss 0.343397\n",
      "iteration 1600 / 2000: loss 0.325527\n",
      "iteration 1700 / 2000: loss 0.339282\n",
      "iteration 1800 / 2000: loss 0.381942\n",
      "iteration 1900 / 2000: loss 0.282560\n",
      "Hidden Size: 10, Learning Rate: 0.0075000250000000004, LR Decay: 0.3, Regularization: 0.30000000000000004, Validation accuracy: 0.9222\n",
      "iteration 0 / 2000: loss 2.302611\n",
      "iteration 100 / 2000: loss 0.724068\n",
      "iteration 200 / 2000: loss 0.430628\n",
      "iteration 300 / 2000: loss 0.388837\n",
      "iteration 400 / 2000: loss 0.290778\n",
      "iteration 500 / 2000: loss 0.282407\n",
      "iteration 600 / 2000: loss 0.313977\n",
      "iteration 700 / 2000: loss 0.354149\n",
      "iteration 800 / 2000: loss 0.293564\n",
      "iteration 900 / 2000: loss 0.451385\n",
      "iteration 1000 / 2000: loss 0.391612\n",
      "iteration 1100 / 2000: loss 0.505577\n",
      "iteration 1200 / 2000: loss 0.356240\n",
      "iteration 1300 / 2000: loss 0.295923\n",
      "iteration 1400 / 2000: loss 0.268884\n",
      "iteration 1500 / 2000: loss 0.362641\n",
      "iteration 1600 / 2000: loss 0.452615\n",
      "iteration 1700 / 2000: loss 0.382504\n",
      "iteration 1800 / 2000: loss 0.346919\n",
      "iteration 1900 / 2000: loss 0.411822\n",
      "Hidden Size: 10, Learning Rate: 0.0075000250000000004, LR Decay: 0.3, Regularization: 0.4, Validation accuracy: 0.9226\n",
      "iteration 0 / 2000: loss 2.302579\n",
      "iteration 100 / 2000: loss 0.356353\n",
      "iteration 200 / 2000: loss 0.224874\n",
      "iteration 300 / 2000: loss 0.202668\n",
      "iteration 400 / 2000: loss 0.190035\n",
      "iteration 500 / 2000: loss 0.364085\n",
      "iteration 600 / 2000: loss 0.142957\n",
      "iteration 700 / 2000: loss 0.201042\n",
      "iteration 800 / 2000: loss 0.300165\n",
      "iteration 900 / 2000: loss 0.186766\n",
      "iteration 1000 / 2000: loss 0.211680\n",
      "iteration 1100 / 2000: loss 0.237721\n",
      "iteration 1200 / 2000: loss 0.225614\n",
      "iteration 1300 / 2000: loss 0.218998\n",
      "iteration 1400 / 2000: loss 0.120548\n",
      "iteration 1500 / 2000: loss 0.180017\n",
      "iteration 1600 / 2000: loss 0.210685\n",
      "iteration 1700 / 2000: loss 0.342624\n",
      "iteration 1800 / 2000: loss 0.312381\n",
      "iteration 1900 / 2000: loss 0.239890\n",
      "Hidden Size: 10, Learning Rate: 0.00500005, LR Decay: 0.99, Regularization: 0.0, Validation accuracy: 0.9378\n",
      "iteration 0 / 2000: loss 2.302590\n",
      "iteration 100 / 2000: loss 0.322589\n",
      "iteration 200 / 2000: loss 0.285180\n",
      "iteration 300 / 2000: loss 0.255603\n",
      "iteration 400 / 2000: loss 0.213194\n",
      "iteration 500 / 2000: loss 0.216938\n",
      "iteration 600 / 2000: loss 0.272682\n",
      "iteration 700 / 2000: loss 0.182216\n",
      "iteration 800 / 2000: loss 0.335945\n",
      "iteration 900 / 2000: loss 0.480366\n",
      "iteration 1000 / 2000: loss 0.288850\n",
      "iteration 1100 / 2000: loss 0.248089\n",
      "iteration 1200 / 2000: loss 0.183907\n",
      "iteration 1300 / 2000: loss 0.365240\n",
      "iteration 1400 / 2000: loss 0.301530\n",
      "iteration 1500 / 2000: loss 0.273849\n",
      "iteration 1600 / 2000: loss 0.244727\n",
      "iteration 1700 / 2000: loss 0.234160\n",
      "iteration 1800 / 2000: loss 0.263863\n",
      "iteration 1900 / 2000: loss 0.185490\n",
      "Hidden Size: 10, Learning Rate: 0.00500005, LR Decay: 0.99, Regularization: 0.1, Validation accuracy: 0.9334\n",
      "iteration 0 / 2000: loss 2.302590\n",
      "iteration 100 / 2000: loss 0.315931\n",
      "iteration 200 / 2000: loss 0.275159\n",
      "iteration 300 / 2000: loss 0.276911\n",
      "iteration 400 / 2000: loss 0.516339\n",
      "iteration 500 / 2000: loss 0.237483\n",
      "iteration 600 / 2000: loss 0.223643\n",
      "iteration 700 / 2000: loss 0.340086\n",
      "iteration 800 / 2000: loss 0.372305\n",
      "iteration 900 / 2000: loss 0.288118\n",
      "iteration 1000 / 2000: loss 0.324025\n",
      "iteration 1100 / 2000: loss 0.227048\n",
      "iteration 1200 / 2000: loss 0.334421\n",
      "iteration 1300 / 2000: loss 0.305870\n",
      "iteration 1400 / 2000: loss 0.379356\n",
      "iteration 1500 / 2000: loss 0.287820\n",
      "iteration 1600 / 2000: loss 0.347593\n",
      "iteration 1700 / 2000: loss 0.326833\n",
      "iteration 1800 / 2000: loss 0.322768\n",
      "iteration 1900 / 2000: loss 0.377026\n",
      "Hidden Size: 10, Learning Rate: 0.00500005, LR Decay: 0.99, Regularization: 0.2, Validation accuracy: 0.9336\n",
      "iteration 0 / 2000: loss 2.302598\n",
      "iteration 100 / 2000: loss 0.351809\n",
      "iteration 200 / 2000: loss 0.370507\n",
      "iteration 300 / 2000: loss 0.383176\n",
      "iteration 400 / 2000: loss 0.350390\n",
      "iteration 500 / 2000: loss 0.350440\n",
      "iteration 600 / 2000: loss 0.268434\n",
      "iteration 700 / 2000: loss 0.313441\n",
      "iteration 800 / 2000: loss 0.309001\n",
      "iteration 900 / 2000: loss 0.396851\n",
      "iteration 1000 / 2000: loss 0.424157\n",
      "iteration 1100 / 2000: loss 0.315738\n",
      "iteration 1200 / 2000: loss 0.318338\n",
      "iteration 1300 / 2000: loss 0.250207\n",
      "iteration 1400 / 2000: loss 0.361296\n",
      "iteration 1500 / 2000: loss 0.304718\n",
      "iteration 1600 / 2000: loss 0.394113\n",
      "iteration 1700 / 2000: loss 0.237831\n",
      "iteration 1800 / 2000: loss 0.281353\n",
      "iteration 1900 / 2000: loss 0.349109\n",
      "Hidden Size: 10, Learning Rate: 0.00500005, LR Decay: 0.99, Regularization: 0.30000000000000004, Validation accuracy: 0.9284\n",
      "iteration 0 / 2000: loss 2.302607\n",
      "iteration 100 / 2000: loss 0.408573\n",
      "iteration 200 / 2000: loss 0.316371\n",
      "iteration 300 / 2000: loss 0.342519\n",
      "iteration 400 / 2000: loss 0.287817\n",
      "iteration 500 / 2000: loss 0.289542\n",
      "iteration 600 / 2000: loss 0.381289\n",
      "iteration 700 / 2000: loss 0.435365\n",
      "iteration 800 / 2000: loss 0.365014\n",
      "iteration 900 / 2000: loss 0.332436\n",
      "iteration 1000 / 2000: loss 0.327581\n",
      "iteration 1100 / 2000: loss 0.335233\n",
      "iteration 1200 / 2000: loss 0.297512\n",
      "iteration 1300 / 2000: loss 0.363161\n",
      "iteration 1400 / 2000: loss 0.326169\n",
      "iteration 1500 / 2000: loss 0.269366\n",
      "iteration 1600 / 2000: loss 0.239694\n",
      "iteration 1700 / 2000: loss 0.290869\n",
      "iteration 1800 / 2000: loss 0.239001\n",
      "iteration 1900 / 2000: loss 0.376545\n",
      "Hidden Size: 10, Learning Rate: 0.00500005, LR Decay: 0.99, Regularization: 0.4, Validation accuracy: 0.9356\n",
      "iteration 0 / 2000: loss 2.302591\n",
      "iteration 100 / 2000: loss 0.378833\n",
      "iteration 200 / 2000: loss 0.236226\n",
      "iteration 300 / 2000: loss 0.271899\n",
      "iteration 400 / 2000: loss 0.239069\n",
      "iteration 500 / 2000: loss 0.229561\n",
      "iteration 600 / 2000: loss 0.184622\n",
      "iteration 700 / 2000: loss 0.216845\n",
      "iteration 800 / 2000: loss 0.224583\n",
      "iteration 900 / 2000: loss 0.199553\n",
      "iteration 1000 / 2000: loss 0.296701\n",
      "iteration 1100 / 2000: loss 0.117192\n",
      "iteration 1200 / 2000: loss 0.276285\n",
      "iteration 1300 / 2000: loss 0.238596\n",
      "iteration 1400 / 2000: loss 0.125847\n",
      "iteration 1500 / 2000: loss 0.224258\n",
      "iteration 1600 / 2000: loss 0.163375\n",
      "iteration 1700 / 2000: loss 0.205858\n",
      "iteration 1800 / 2000: loss 0.196422\n",
      "iteration 1900 / 2000: loss 0.207910\n",
      "Hidden Size: 10, Learning Rate: 0.00500005, LR Decay: 0.8175, Regularization: 0.0, Validation accuracy: 0.941\n",
      "iteration 0 / 2000: loss 2.302592\n",
      "iteration 100 / 2000: loss 0.329742\n",
      "iteration 200 / 2000: loss 0.359168\n",
      "iteration 300 / 2000: loss 0.439664\n",
      "iteration 400 / 2000: loss 0.239833\n",
      "iteration 500 / 2000: loss 0.211898\n",
      "iteration 600 / 2000: loss 0.200856\n",
      "iteration 700 / 2000: loss 0.264071\n",
      "iteration 800 / 2000: loss 0.262591\n",
      "iteration 900 / 2000: loss 0.316066\n",
      "iteration 1000 / 2000: loss 0.257237\n",
      "iteration 1100 / 2000: loss 0.239660\n",
      "iteration 1200 / 2000: loss 0.297785\n",
      "iteration 1300 / 2000: loss 0.273445\n",
      "iteration 1400 / 2000: loss 0.274960\n",
      "iteration 1500 / 2000: loss 0.272938\n",
      "iteration 1600 / 2000: loss 0.241027\n",
      "iteration 1700 / 2000: loss 0.251774\n",
      "iteration 1800 / 2000: loss 0.325395\n",
      "iteration 1900 / 2000: loss 0.246293\n",
      "Hidden Size: 10, Learning Rate: 0.00500005, LR Decay: 0.8175, Regularization: 0.1, Validation accuracy: 0.9308\n",
      "iteration 0 / 2000: loss 2.302588\n",
      "iteration 100 / 2000: loss 0.534422\n",
      "iteration 200 / 2000: loss 0.281488\n",
      "iteration 300 / 2000: loss 0.368956\n",
      "iteration 400 / 2000: loss 0.338539\n",
      "iteration 500 / 2000: loss 0.272559\n",
      "iteration 600 / 2000: loss 0.264335\n",
      "iteration 700 / 2000: loss 0.224656\n",
      "iteration 800 / 2000: loss 0.211119\n",
      "iteration 900 / 2000: loss 0.284815\n",
      "iteration 1000 / 2000: loss 0.231123\n",
      "iteration 1100 / 2000: loss 0.264719\n",
      "iteration 1200 / 2000: loss 0.269403\n",
      "iteration 1300 / 2000: loss 0.311867\n",
      "iteration 1400 / 2000: loss 0.315088\n",
      "iteration 1500 / 2000: loss 0.246148\n",
      "iteration 1600 / 2000: loss 0.253792\n",
      "iteration 1700 / 2000: loss 0.222666\n",
      "iteration 1800 / 2000: loss 0.224722\n",
      "iteration 1900 / 2000: loss 0.236887\n",
      "Hidden Size: 10, Learning Rate: 0.00500005, LR Decay: 0.8175, Regularization: 0.2, Validation accuracy: 0.936\n",
      "iteration 0 / 2000: loss 2.302593\n",
      "iteration 100 / 2000: loss 0.419994\n",
      "iteration 200 / 2000: loss 0.214378\n",
      "iteration 300 / 2000: loss 0.424604\n",
      "iteration 400 / 2000: loss 0.445653\n",
      "iteration 500 / 2000: loss 0.241642\n",
      "iteration 600 / 2000: loss 0.299898\n",
      "iteration 700 / 2000: loss 0.259544\n",
      "iteration 800 / 2000: loss 0.292156\n",
      "iteration 900 / 2000: loss 0.291176\n",
      "iteration 1000 / 2000: loss 0.346576\n",
      "iteration 1100 / 2000: loss 0.273779\n",
      "iteration 1200 / 2000: loss 0.332957\n",
      "iteration 1300 / 2000: loss 0.312192\n",
      "iteration 1400 / 2000: loss 0.275747\n",
      "iteration 1500 / 2000: loss 0.423177\n",
      "iteration 1600 / 2000: loss 0.281814\n",
      "iteration 1700 / 2000: loss 0.246790\n",
      "iteration 1800 / 2000: loss 0.299320\n",
      "iteration 1900 / 2000: loss 0.339617\n",
      "Hidden Size: 10, Learning Rate: 0.00500005, LR Decay: 0.8175, Regularization: 0.30000000000000004, Validation accuracy: 0.9424\n",
      "iteration 0 / 2000: loss 2.302606\n",
      "iteration 100 / 2000: loss 0.469292\n",
      "iteration 200 / 2000: loss 0.379919\n",
      "iteration 300 / 2000: loss 0.404409\n",
      "iteration 400 / 2000: loss 0.337675\n",
      "iteration 500 / 2000: loss 0.281793\n",
      "iteration 600 / 2000: loss 0.345851\n",
      "iteration 700 / 2000: loss 0.384965\n",
      "iteration 800 / 2000: loss 0.220900\n",
      "iteration 900 / 2000: loss 0.204497\n",
      "iteration 1000 / 2000: loss 0.342941\n",
      "iteration 1100 / 2000: loss 0.338759\n",
      "iteration 1200 / 2000: loss 0.341061\n",
      "iteration 1300 / 2000: loss 0.295677\n",
      "iteration 1400 / 2000: loss 0.241282\n",
      "iteration 1500 / 2000: loss 0.334181\n",
      "iteration 1600 / 2000: loss 0.370071\n",
      "iteration 1700 / 2000: loss 0.317370\n",
      "iteration 1800 / 2000: loss 0.322160\n",
      "iteration 1900 / 2000: loss 0.199272\n",
      "Hidden Size: 10, Learning Rate: 0.00500005, LR Decay: 0.8175, Regularization: 0.4, Validation accuracy: 0.9356\n",
      "iteration 0 / 2000: loss 2.302596\n",
      "iteration 100 / 2000: loss 0.417622\n",
      "iteration 200 / 2000: loss 0.340525\n",
      "iteration 300 / 2000: loss 0.231221\n",
      "iteration 400 / 2000: loss 0.275103\n",
      "iteration 500 / 2000: loss 0.284433\n",
      "iteration 600 / 2000: loss 0.186147\n",
      "iteration 700 / 2000: loss 0.245651\n",
      "iteration 800 / 2000: loss 0.225622\n",
      "iteration 900 / 2000: loss 0.286216\n",
      "iteration 1000 / 2000: loss 0.198390\n",
      "iteration 1100 / 2000: loss 0.238102\n",
      "iteration 1200 / 2000: loss 0.250699\n",
      "iteration 1300 / 2000: loss 0.177060\n",
      "iteration 1400 / 2000: loss 0.218441\n",
      "iteration 1500 / 2000: loss 0.181937\n",
      "iteration 1600 / 2000: loss 0.220377\n",
      "iteration 1700 / 2000: loss 0.255932\n",
      "iteration 1800 / 2000: loss 0.121744\n",
      "iteration 1900 / 2000: loss 0.225298\n",
      "Hidden Size: 10, Learning Rate: 0.00500005, LR Decay: 0.645, Regularization: 0.0, Validation accuracy: 0.9356\n",
      "iteration 0 / 2000: loss 2.302592\n",
      "iteration 100 / 2000: loss 0.447531\n",
      "iteration 200 / 2000: loss 0.237674\n",
      "iteration 300 / 2000: loss 0.304903\n",
      "iteration 400 / 2000: loss 0.228897\n",
      "iteration 500 / 2000: loss 0.252200\n",
      "iteration 600 / 2000: loss 0.244942\n",
      "iteration 700 / 2000: loss 0.311598\n",
      "iteration 800 / 2000: loss 0.163436\n",
      "iteration 900 / 2000: loss 0.195432\n",
      "iteration 1000 / 2000: loss 0.284651\n",
      "iteration 1100 / 2000: loss 0.201383\n",
      "iteration 1200 / 2000: loss 0.211183\n",
      "iteration 1300 / 2000: loss 0.233349\n",
      "iteration 1400 / 2000: loss 0.205122\n",
      "iteration 1500 / 2000: loss 0.236844\n",
      "iteration 1600 / 2000: loss 0.244808\n",
      "iteration 1700 / 2000: loss 0.181380\n",
      "iteration 1800 / 2000: loss 0.178506\n",
      "iteration 1900 / 2000: loss 0.193492\n",
      "Hidden Size: 10, Learning Rate: 0.00500005, LR Decay: 0.645, Regularization: 0.1, Validation accuracy: 0.9354\n",
      "iteration 0 / 2000: loss 2.302594\n",
      "iteration 100 / 2000: loss 0.427906\n",
      "iteration 200 / 2000: loss 0.341746\n",
      "iteration 300 / 2000: loss 0.391284\n",
      "iteration 400 / 2000: loss 0.317260\n",
      "iteration 500 / 2000: loss 0.287655\n",
      "iteration 600 / 2000: loss 0.227743\n",
      "iteration 700 / 2000: loss 0.184036\n",
      "iteration 800 / 2000: loss 0.190840\n",
      "iteration 900 / 2000: loss 0.238327\n",
      "iteration 1000 / 2000: loss 0.194239\n",
      "iteration 1100 / 2000: loss 0.292351\n",
      "iteration 1200 / 2000: loss 0.325058\n",
      "iteration 1300 / 2000: loss 0.271197\n",
      "iteration 1400 / 2000: loss 0.239737\n",
      "iteration 1500 / 2000: loss 0.280929\n",
      "iteration 1600 / 2000: loss 0.296411\n",
      "iteration 1700 / 2000: loss 0.295502\n",
      "iteration 1800 / 2000: loss 0.221710\n",
      "iteration 1900 / 2000: loss 0.280537\n",
      "Hidden Size: 10, Learning Rate: 0.00500005, LR Decay: 0.645, Regularization: 0.2, Validation accuracy: 0.9368\n",
      "iteration 0 / 2000: loss 2.302606\n",
      "iteration 100 / 2000: loss 0.623495\n",
      "iteration 200 / 2000: loss 0.314617\n",
      "iteration 300 / 2000: loss 0.323917\n",
      "iteration 400 / 2000: loss 0.255673\n",
      "iteration 500 / 2000: loss 0.291468\n",
      "iteration 600 / 2000: loss 0.341003\n",
      "iteration 700 / 2000: loss 0.337186\n",
      "iteration 800 / 2000: loss 0.315932\n",
      "iteration 900 / 2000: loss 0.361306\n",
      "iteration 1000 / 2000: loss 0.253474\n",
      "iteration 1100 / 2000: loss 0.285097\n",
      "iteration 1200 / 2000: loss 0.379339\n",
      "iteration 1300 / 2000: loss 0.397528\n",
      "iteration 1400 / 2000: loss 0.383987\n",
      "iteration 1500 / 2000: loss 0.341544\n",
      "iteration 1600 / 2000: loss 0.339911\n",
      "iteration 1700 / 2000: loss 0.429496\n",
      "iteration 1800 / 2000: loss 0.240649\n",
      "iteration 1900 / 2000: loss 0.306657\n",
      "Hidden Size: 10, Learning Rate: 0.00500005, LR Decay: 0.645, Regularization: 0.30000000000000004, Validation accuracy: 0.9372\n",
      "iteration 0 / 2000: loss 2.302603\n",
      "iteration 100 / 2000: loss 0.379469\n",
      "iteration 200 / 2000: loss 0.358842\n",
      "iteration 300 / 2000: loss 0.401184\n",
      "iteration 400 / 2000: loss 0.354359\n",
      "iteration 500 / 2000: loss 0.332151\n",
      "iteration 600 / 2000: loss 0.353455\n",
      "iteration 700 / 2000: loss 0.366687\n",
      "iteration 800 / 2000: loss 0.256309\n",
      "iteration 900 / 2000: loss 0.254944\n",
      "iteration 1000 / 2000: loss 0.297548\n",
      "iteration 1100 / 2000: loss 0.325402\n",
      "iteration 1200 / 2000: loss 0.339903\n",
      "iteration 1300 / 2000: loss 0.358474\n",
      "iteration 1400 / 2000: loss 0.285769\n",
      "iteration 1500 / 2000: loss 0.404736\n",
      "iteration 1600 / 2000: loss 0.274964\n",
      "iteration 1700 / 2000: loss 0.256927\n",
      "iteration 1800 / 2000: loss 0.343731\n",
      "iteration 1900 / 2000: loss 0.352318\n",
      "Hidden Size: 10, Learning Rate: 0.00500005, LR Decay: 0.645, Regularization: 0.4, Validation accuracy: 0.9328\n",
      "iteration 0 / 2000: loss 2.302595\n",
      "iteration 100 / 2000: loss 0.408838\n",
      "iteration 200 / 2000: loss 0.244348\n",
      "iteration 300 / 2000: loss 0.366378\n",
      "iteration 400 / 2000: loss 0.293071\n",
      "iteration 500 / 2000: loss 0.224850\n",
      "iteration 600 / 2000: loss 0.207914\n",
      "iteration 700 / 2000: loss 0.296543\n",
      "iteration 800 / 2000: loss 0.175563\n",
      "iteration 900 / 2000: loss 0.321399\n",
      "iteration 1000 / 2000: loss 0.245670\n",
      "iteration 1100 / 2000: loss 0.207128\n",
      "iteration 1200 / 2000: loss 0.309482\n",
      "iteration 1300 / 2000: loss 0.227855\n",
      "iteration 1400 / 2000: loss 0.273966\n",
      "iteration 1500 / 2000: loss 0.366371\n",
      "iteration 1600 / 2000: loss 0.205173\n",
      "iteration 1700 / 2000: loss 0.274659\n",
      "iteration 1800 / 2000: loss 0.185058\n",
      "iteration 1900 / 2000: loss 0.269533\n",
      "Hidden Size: 10, Learning Rate: 0.00500005, LR Decay: 0.47250000000000003, Regularization: 0.0, Validation accuracy: 0.9308\n",
      "iteration 0 / 2000: loss 2.302581\n",
      "iteration 100 / 2000: loss 0.434648\n",
      "iteration 200 / 2000: loss 0.268485\n",
      "iteration 300 / 2000: loss 0.237411\n",
      "iteration 400 / 2000: loss 0.324731\n",
      "iteration 500 / 2000: loss 0.383913\n",
      "iteration 600 / 2000: loss 0.212023\n",
      "iteration 700 / 2000: loss 0.299692\n",
      "iteration 800 / 2000: loss 0.261024\n",
      "iteration 900 / 2000: loss 0.253893\n",
      "iteration 1000 / 2000: loss 0.207076\n",
      "iteration 1100 / 2000: loss 0.254268\n",
      "iteration 1200 / 2000: loss 0.283174\n",
      "iteration 1300 / 2000: loss 0.361758\n",
      "iteration 1400 / 2000: loss 0.290012\n",
      "iteration 1500 / 2000: loss 0.352596\n",
      "iteration 1600 / 2000: loss 0.264443\n",
      "iteration 1700 / 2000: loss 0.312066\n",
      "iteration 1800 / 2000: loss 0.311426\n",
      "iteration 1900 / 2000: loss 0.245989\n",
      "Hidden Size: 10, Learning Rate: 0.00500005, LR Decay: 0.47250000000000003, Regularization: 0.1, Validation accuracy: 0.932\n",
      "iteration 0 / 2000: loss 2.302572\n",
      "iteration 100 / 2000: loss 0.443411\n",
      "iteration 200 / 2000: loss 0.431415\n",
      "iteration 300 / 2000: loss 0.335991\n",
      "iteration 400 / 2000: loss 0.343918\n",
      "iteration 500 / 2000: loss 0.304621\n",
      "iteration 600 / 2000: loss 0.378994\n",
      "iteration 700 / 2000: loss 0.308687\n",
      "iteration 800 / 2000: loss 0.259431\n",
      "iteration 900 / 2000: loss 0.220286\n",
      "iteration 1000 / 2000: loss 0.386760\n",
      "iteration 1100 / 2000: loss 0.377502\n",
      "iteration 1200 / 2000: loss 0.300954\n",
      "iteration 1300 / 2000: loss 0.279742\n",
      "iteration 1400 / 2000: loss 0.232104\n",
      "iteration 1500 / 2000: loss 0.287071\n",
      "iteration 1600 / 2000: loss 0.258352\n",
      "iteration 1700 / 2000: loss 0.243896\n",
      "iteration 1800 / 2000: loss 0.272054\n",
      "iteration 1900 / 2000: loss 0.251040\n",
      "Hidden Size: 10, Learning Rate: 0.00500005, LR Decay: 0.47250000000000003, Regularization: 0.2, Validation accuracy: 0.9318\n",
      "iteration 0 / 2000: loss 2.302606\n",
      "iteration 100 / 2000: loss 0.614190\n",
      "iteration 200 / 2000: loss 0.421737\n",
      "iteration 300 / 2000: loss 0.439888\n",
      "iteration 400 / 2000: loss 0.242073\n",
      "iteration 500 / 2000: loss 0.280894\n",
      "iteration 600 / 2000: loss 0.357207\n",
      "iteration 700 / 2000: loss 0.288266\n",
      "iteration 800 / 2000: loss 0.316986\n",
      "iteration 900 / 2000: loss 0.299083\n",
      "iteration 1000 / 2000: loss 0.300972\n",
      "iteration 1100 / 2000: loss 0.287884\n",
      "iteration 1200 / 2000: loss 0.243975\n",
      "iteration 1300 / 2000: loss 0.254024\n",
      "iteration 1400 / 2000: loss 0.341790\n",
      "iteration 1500 / 2000: loss 0.336721\n",
      "iteration 1600 / 2000: loss 0.306915\n",
      "iteration 1700 / 2000: loss 0.359271\n",
      "iteration 1800 / 2000: loss 0.313175\n",
      "iteration 1900 / 2000: loss 0.283147\n",
      "Hidden Size: 10, Learning Rate: 0.00500005, LR Decay: 0.47250000000000003, Regularization: 0.30000000000000004, Validation accuracy: 0.9296\n",
      "iteration 0 / 2000: loss 2.302605\n",
      "iteration 100 / 2000: loss 0.613448\n",
      "iteration 200 / 2000: loss 0.484761\n",
      "iteration 300 / 2000: loss 0.376224\n",
      "iteration 400 / 2000: loss 0.336741\n",
      "iteration 500 / 2000: loss 0.305127\n",
      "iteration 600 / 2000: loss 0.382610\n",
      "iteration 700 / 2000: loss 0.264030\n",
      "iteration 800 / 2000: loss 0.337679\n",
      "iteration 900 / 2000: loss 0.364761\n",
      "iteration 1000 / 2000: loss 0.412449\n",
      "iteration 1100 / 2000: loss 0.345675\n",
      "iteration 1200 / 2000: loss 0.344729\n",
      "iteration 1300 / 2000: loss 0.334417\n",
      "iteration 1400 / 2000: loss 0.264929\n",
      "iteration 1500 / 2000: loss 0.289531\n",
      "iteration 1600 / 2000: loss 0.373168\n",
      "iteration 1700 / 2000: loss 0.306849\n",
      "iteration 1800 / 2000: loss 0.284484\n",
      "iteration 1900 / 2000: loss 0.326695\n",
      "Hidden Size: 10, Learning Rate: 0.00500005, LR Decay: 0.47250000000000003, Regularization: 0.4, Validation accuracy: 0.9304\n",
      "iteration 0 / 2000: loss 2.302582\n",
      "iteration 100 / 2000: loss 0.659454\n",
      "iteration 200 / 2000: loss 0.446309\n",
      "iteration 300 / 2000: loss 0.296962\n",
      "iteration 400 / 2000: loss 0.346855\n",
      "iteration 500 / 2000: loss 0.246106\n",
      "iteration 600 / 2000: loss 0.239188\n",
      "iteration 700 / 2000: loss 0.317502\n",
      "iteration 800 / 2000: loss 0.206314\n",
      "iteration 900 / 2000: loss 0.331323\n",
      "iteration 1000 / 2000: loss 0.422847\n",
      "iteration 1100 / 2000: loss 0.295838\n",
      "iteration 1200 / 2000: loss 0.382176\n",
      "iteration 1300 / 2000: loss 0.246831\n",
      "iteration 1400 / 2000: loss 0.237765\n",
      "iteration 1500 / 2000: loss 0.259116\n",
      "iteration 1600 / 2000: loss 0.311371\n",
      "iteration 1700 / 2000: loss 0.288774\n",
      "iteration 1800 / 2000: loss 0.383759\n",
      "iteration 1900 / 2000: loss 0.294189\n",
      "Hidden Size: 10, Learning Rate: 0.00500005, LR Decay: 0.3, Regularization: 0.0, Validation accuracy: 0.9216\n",
      "iteration 0 / 2000: loss 2.302587\n",
      "iteration 100 / 2000: loss 0.730747\n",
      "iteration 200 / 2000: loss 0.448605\n",
      "iteration 300 / 2000: loss 0.465742\n",
      "iteration 400 / 2000: loss 0.406091\n",
      "iteration 500 / 2000: loss 0.292524\n",
      "iteration 600 / 2000: loss 0.436465\n",
      "iteration 700 / 2000: loss 0.346657\n",
      "iteration 800 / 2000: loss 0.347866\n",
      "iteration 900 / 2000: loss 0.322330\n",
      "iteration 1000 / 2000: loss 0.302168\n",
      "iteration 1100 / 2000: loss 0.249879\n",
      "iteration 1200 / 2000: loss 0.253536\n",
      "iteration 1300 / 2000: loss 0.431036\n",
      "iteration 1400 / 2000: loss 0.288272\n",
      "iteration 1500 / 2000: loss 0.441478\n",
      "iteration 1600 / 2000: loss 0.317043\n",
      "iteration 1700 / 2000: loss 0.485297\n",
      "iteration 1800 / 2000: loss 0.378169\n",
      "iteration 1900 / 2000: loss 0.277702\n",
      "Hidden Size: 10, Learning Rate: 0.00500005, LR Decay: 0.3, Regularization: 0.1, Validation accuracy: 0.9118\n",
      "iteration 0 / 2000: loss 2.302595\n",
      "iteration 100 / 2000: loss 0.916996\n",
      "iteration 200 / 2000: loss 0.507818\n",
      "iteration 300 / 2000: loss 0.475394\n",
      "iteration 400 / 2000: loss 0.367644\n",
      "iteration 500 / 2000: loss 0.325657\n",
      "iteration 600 / 2000: loss 0.426598\n",
      "iteration 700 / 2000: loss 0.314300\n",
      "iteration 800 / 2000: loss 0.251084\n",
      "iteration 900 / 2000: loss 0.298739\n",
      "iteration 1000 / 2000: loss 0.406235\n",
      "iteration 1100 / 2000: loss 0.417961\n",
      "iteration 1200 / 2000: loss 0.306955\n",
      "iteration 1300 / 2000: loss 0.378853\n",
      "iteration 1400 / 2000: loss 0.325796\n",
      "iteration 1500 / 2000: loss 0.297551\n",
      "iteration 1600 / 2000: loss 0.365878\n",
      "iteration 1700 / 2000: loss 0.269997\n",
      "iteration 1800 / 2000: loss 0.318715\n",
      "iteration 1900 / 2000: loss 0.364859\n",
      "Hidden Size: 10, Learning Rate: 0.00500005, LR Decay: 0.3, Regularization: 0.2, Validation accuracy: 0.9152\n",
      "iteration 0 / 2000: loss 2.302599\n",
      "iteration 100 / 2000: loss 0.570805\n",
      "iteration 200 / 2000: loss 0.354817\n",
      "iteration 300 / 2000: loss 0.349843\n",
      "iteration 400 / 2000: loss 0.335082\n",
      "iteration 500 / 2000: loss 0.336588\n",
      "iteration 600 / 2000: loss 0.293388\n",
      "iteration 700 / 2000: loss 0.278964\n",
      "iteration 800 / 2000: loss 0.257952\n",
      "iteration 900 / 2000: loss 0.334161\n",
      "iteration 1000 / 2000: loss 0.329429\n",
      "iteration 1100 / 2000: loss 0.378908\n",
      "iteration 1200 / 2000: loss 0.384421\n",
      "iteration 1300 / 2000: loss 0.401868\n",
      "iteration 1400 / 2000: loss 0.350975\n",
      "iteration 1500 / 2000: loss 0.470939\n",
      "iteration 1600 / 2000: loss 0.348627\n",
      "iteration 1700 / 2000: loss 0.313849\n",
      "iteration 1800 / 2000: loss 0.389266\n",
      "iteration 1900 / 2000: loss 0.281919\n",
      "Hidden Size: 10, Learning Rate: 0.00500005, LR Decay: 0.3, Regularization: 0.30000000000000004, Validation accuracy: 0.9212\n",
      "iteration 0 / 2000: loss 2.302608\n",
      "iteration 100 / 2000: loss 0.941773\n",
      "iteration 200 / 2000: loss 0.750989\n",
      "iteration 300 / 2000: loss 0.308398\n",
      "iteration 400 / 2000: loss 0.335974\n",
      "iteration 500 / 2000: loss 0.356633\n",
      "iteration 600 / 2000: loss 0.345387\n",
      "iteration 700 / 2000: loss 0.422618\n",
      "iteration 800 / 2000: loss 0.295817\n",
      "iteration 900 / 2000: loss 0.350006\n",
      "iteration 1000 / 2000: loss 0.331106\n",
      "iteration 1100 / 2000: loss 0.361179\n",
      "iteration 1200 / 2000: loss 0.367191\n",
      "iteration 1300 / 2000: loss 0.435598\n",
      "iteration 1400 / 2000: loss 0.405161\n",
      "iteration 1500 / 2000: loss 0.298420\n",
      "iteration 1600 / 2000: loss 0.300152\n",
      "iteration 1700 / 2000: loss 0.423383\n",
      "iteration 1800 / 2000: loss 0.371612\n",
      "iteration 1900 / 2000: loss 0.413605\n",
      "Hidden Size: 10, Learning Rate: 0.00500005, LR Decay: 0.3, Regularization: 0.4, Validation accuracy: 0.9152\n",
      "iteration 0 / 2000: loss 2.302580\n",
      "iteration 100 / 2000: loss 0.485392\n",
      "iteration 200 / 2000: loss 0.316965\n",
      "iteration 300 / 2000: loss 0.312028\n",
      "iteration 400 / 2000: loss 0.318564\n",
      "iteration 500 / 2000: loss 0.300748\n",
      "iteration 600 / 2000: loss 0.251031\n",
      "iteration 700 / 2000: loss 0.288044\n",
      "iteration 800 / 2000: loss 0.245639\n",
      "iteration 900 / 2000: loss 0.223188\n",
      "iteration 1000 / 2000: loss 0.238223\n",
      "iteration 1100 / 2000: loss 0.256984\n",
      "iteration 1200 / 2000: loss 0.269540\n",
      "iteration 1300 / 2000: loss 0.225320\n",
      "iteration 1400 / 2000: loss 0.314914\n",
      "iteration 1500 / 2000: loss 0.290064\n",
      "iteration 1600 / 2000: loss 0.164398\n",
      "iteration 1700 / 2000: loss 0.140810\n",
      "iteration 1800 / 2000: loss 0.168504\n",
      "iteration 1900 / 2000: loss 0.192726\n",
      "Hidden Size: 10, Learning Rate: 0.002500074999999999, LR Decay: 0.99, Regularization: 0.0, Validation accuracy: 0.9332\n",
      "iteration 0 / 2000: loss 2.302584\n",
      "iteration 100 / 2000: loss 0.661698\n",
      "iteration 200 / 2000: loss 0.370897\n",
      "iteration 300 / 2000: loss 0.441867\n",
      "iteration 400 / 2000: loss 0.386765\n",
      "iteration 500 / 2000: loss 0.293334\n",
      "iteration 600 / 2000: loss 0.318087\n",
      "iteration 700 / 2000: loss 0.340644\n",
      "iteration 800 / 2000: loss 0.260650\n",
      "iteration 900 / 2000: loss 0.245427\n",
      "iteration 1000 / 2000: loss 0.259158\n",
      "iteration 1100 / 2000: loss 0.255613\n",
      "iteration 1200 / 2000: loss 0.142523\n",
      "iteration 1300 / 2000: loss 0.297965\n",
      "iteration 1400 / 2000: loss 0.297665\n",
      "iteration 1500 / 2000: loss 0.143302\n",
      "iteration 1600 / 2000: loss 0.363393\n",
      "iteration 1700 / 2000: loss 0.256881\n",
      "iteration 1800 / 2000: loss 0.178190\n",
      "iteration 1900 / 2000: loss 0.374290\n",
      "Hidden Size: 10, Learning Rate: 0.002500074999999999, LR Decay: 0.99, Regularization: 0.1, Validation accuracy: 0.937\n",
      "iteration 0 / 2000: loss 2.302603\n",
      "iteration 100 / 2000: loss 0.452436\n",
      "iteration 200 / 2000: loss 0.356686\n",
      "iteration 300 / 2000: loss 0.317975\n",
      "iteration 400 / 2000: loss 0.221454\n",
      "iteration 500 / 2000: loss 0.305014\n",
      "iteration 600 / 2000: loss 0.247235\n",
      "iteration 700 / 2000: loss 0.255894\n",
      "iteration 800 / 2000: loss 0.350823\n",
      "iteration 900 / 2000: loss 0.255914\n",
      "iteration 1000 / 2000: loss 0.301361\n",
      "iteration 1100 / 2000: loss 0.326867\n",
      "iteration 1200 / 2000: loss 0.227912\n",
      "iteration 1300 / 2000: loss 0.269195\n",
      "iteration 1400 / 2000: loss 0.312207\n",
      "iteration 1500 / 2000: loss 0.296748\n",
      "iteration 1600 / 2000: loss 0.292684\n",
      "iteration 1700 / 2000: loss 0.288727\n",
      "iteration 1800 / 2000: loss 0.257600\n",
      "iteration 1900 / 2000: loss 0.183834\n",
      "Hidden Size: 10, Learning Rate: 0.002500074999999999, LR Decay: 0.99, Regularization: 0.2, Validation accuracy: 0.9342\n",
      "iteration 0 / 2000: loss 2.302605\n",
      "iteration 100 / 2000: loss 0.511112\n",
      "iteration 200 / 2000: loss 0.327193\n",
      "iteration 300 / 2000: loss 0.361309\n",
      "iteration 400 / 2000: loss 0.418024\n",
      "iteration 500 / 2000: loss 0.390342\n",
      "iteration 600 / 2000: loss 0.255725\n",
      "iteration 700 / 2000: loss 0.222583\n",
      "iteration 800 / 2000: loss 0.463411\n",
      "iteration 900 / 2000: loss 0.396987\n",
      "iteration 1000 / 2000: loss 0.371244\n",
      "iteration 1100 / 2000: loss 0.326988\n",
      "iteration 1200 / 2000: loss 0.347148\n",
      "iteration 1300 / 2000: loss 0.212492\n",
      "iteration 1400 / 2000: loss 0.261517\n",
      "iteration 1500 / 2000: loss 0.291060\n",
      "iteration 1600 / 2000: loss 0.274148\n",
      "iteration 1700 / 2000: loss 0.225370\n",
      "iteration 1800 / 2000: loss 0.409462\n",
      "iteration 1900 / 2000: loss 0.350456\n",
      "Hidden Size: 10, Learning Rate: 0.002500074999999999, LR Decay: 0.99, Regularization: 0.30000000000000004, Validation accuracy: 0.9382\n",
      "iteration 0 / 2000: loss 2.302596\n",
      "iteration 100 / 2000: loss 0.484728\n",
      "iteration 200 / 2000: loss 0.328833\n",
      "iteration 300 / 2000: loss 0.337834\n",
      "iteration 400 / 2000: loss 0.424683\n",
      "iteration 500 / 2000: loss 0.370295\n",
      "iteration 600 / 2000: loss 0.395834\n",
      "iteration 700 / 2000: loss 0.296607\n",
      "iteration 800 / 2000: loss 0.324672\n",
      "iteration 900 / 2000: loss 0.360482\n",
      "iteration 1000 / 2000: loss 0.311533\n",
      "iteration 1100 / 2000: loss 0.353831\n",
      "iteration 1200 / 2000: loss 0.321198\n",
      "iteration 1300 / 2000: loss 0.350610\n",
      "iteration 1400 / 2000: loss 0.287431\n",
      "iteration 1500 / 2000: loss 0.262860\n",
      "iteration 1600 / 2000: loss 0.351748\n",
      "iteration 1700 / 2000: loss 0.307447\n",
      "iteration 1800 / 2000: loss 0.264277\n",
      "iteration 1900 / 2000: loss 0.237896\n",
      "Hidden Size: 10, Learning Rate: 0.002500074999999999, LR Decay: 0.99, Regularization: 0.4, Validation accuracy: 0.9358\n",
      "iteration 0 / 2000: loss 2.302585\n",
      "iteration 100 / 2000: loss 0.560451\n",
      "iteration 200 / 2000: loss 0.408527\n",
      "iteration 300 / 2000: loss 0.478044\n",
      "iteration 400 / 2000: loss 0.312480\n",
      "iteration 500 / 2000: loss 0.334232\n",
      "iteration 600 / 2000: loss 0.254117\n",
      "iteration 700 / 2000: loss 0.255262\n",
      "iteration 800 / 2000: loss 0.248922\n",
      "iteration 900 / 2000: loss 0.276258\n",
      "iteration 1000 / 2000: loss 0.334324\n",
      "iteration 1100 / 2000: loss 0.193253\n",
      "iteration 1200 / 2000: loss 0.279886\n",
      "iteration 1300 / 2000: loss 0.204771\n",
      "iteration 1400 / 2000: loss 0.215635\n",
      "iteration 1500 / 2000: loss 0.300181\n",
      "iteration 1600 / 2000: loss 0.261152\n",
      "iteration 1700 / 2000: loss 0.235398\n",
      "iteration 1800 / 2000: loss 0.324453\n",
      "iteration 1900 / 2000: loss 0.189437\n",
      "Hidden Size: 10, Learning Rate: 0.002500074999999999, LR Decay: 0.8175, Regularization: 0.0, Validation accuracy: 0.935\n",
      "iteration 0 / 2000: loss 2.302585\n",
      "iteration 100 / 2000: loss 0.523359\n",
      "iteration 200 / 2000: loss 0.387413\n",
      "iteration 300 / 2000: loss 0.434458\n",
      "iteration 400 / 2000: loss 0.245912\n",
      "iteration 500 / 2000: loss 0.296231\n",
      "iteration 600 / 2000: loss 0.419536\n",
      "iteration 700 / 2000: loss 0.342925\n",
      "iteration 800 / 2000: loss 0.219750\n",
      "iteration 900 / 2000: loss 0.235599\n",
      "iteration 1000 / 2000: loss 0.225770\n",
      "iteration 1100 / 2000: loss 0.347034\n",
      "iteration 1200 / 2000: loss 0.266815\n",
      "iteration 1300 / 2000: loss 0.236422\n",
      "iteration 1400 / 2000: loss 0.313054\n",
      "iteration 1500 / 2000: loss 0.298130\n",
      "iteration 1600 / 2000: loss 0.297455\n",
      "iteration 1700 / 2000: loss 0.240718\n",
      "iteration 1800 / 2000: loss 0.233679\n",
      "iteration 1900 / 2000: loss 0.324506\n",
      "Hidden Size: 10, Learning Rate: 0.002500074999999999, LR Decay: 0.8175, Regularization: 0.1, Validation accuracy: 0.9328\n",
      "iteration 0 / 2000: loss 2.302598\n",
      "iteration 100 / 2000: loss 0.640478\n",
      "iteration 200 / 2000: loss 0.427837\n",
      "iteration 300 / 2000: loss 0.284262\n",
      "iteration 400 / 2000: loss 0.317367\n",
      "iteration 500 / 2000: loss 0.318621\n",
      "iteration 600 / 2000: loss 0.255410\n",
      "iteration 700 / 2000: loss 0.201386\n",
      "iteration 800 / 2000: loss 0.279782\n",
      "iteration 900 / 2000: loss 0.306034\n",
      "iteration 1000 / 2000: loss 0.230202\n",
      "iteration 1100 / 2000: loss 0.318907\n",
      "iteration 1200 / 2000: loss 0.220376\n",
      "iteration 1300 / 2000: loss 0.346727\n",
      "iteration 1400 / 2000: loss 0.369864\n",
      "iteration 1500 / 2000: loss 0.245111\n",
      "iteration 1600 / 2000: loss 0.223668\n",
      "iteration 1700 / 2000: loss 0.270798\n",
      "iteration 1800 / 2000: loss 0.274816\n",
      "iteration 1900 / 2000: loss 0.183992\n",
      "Hidden Size: 10, Learning Rate: 0.002500074999999999, LR Decay: 0.8175, Regularization: 0.2, Validation accuracy: 0.9324\n",
      "iteration 0 / 2000: loss 2.302593\n",
      "iteration 100 / 2000: loss 0.560648\n",
      "iteration 200 / 2000: loss 0.523511\n",
      "iteration 300 / 2000: loss 0.539312\n",
      "iteration 400 / 2000: loss 0.348620\n",
      "iteration 500 / 2000: loss 0.334157\n",
      "iteration 600 / 2000: loss 0.220558\n",
      "iteration 700 / 2000: loss 0.294864\n",
      "iteration 800 / 2000: loss 0.256398\n",
      "iteration 900 / 2000: loss 0.294753\n",
      "iteration 1000 / 2000: loss 0.343046\n",
      "iteration 1100 / 2000: loss 0.270390\n",
      "iteration 1200 / 2000: loss 0.333388\n",
      "iteration 1300 / 2000: loss 0.358937\n",
      "iteration 1400 / 2000: loss 0.317207\n",
      "iteration 1500 / 2000: loss 0.239611\n",
      "iteration 1600 / 2000: loss 0.380826\n",
      "iteration 1700 / 2000: loss 0.371334\n",
      "iteration 1800 / 2000: loss 0.346096\n",
      "iteration 1900 / 2000: loss 0.256413\n",
      "Hidden Size: 10, Learning Rate: 0.002500074999999999, LR Decay: 0.8175, Regularization: 0.30000000000000004, Validation accuracy: 0.9338\n",
      "iteration 0 / 2000: loss 2.302590\n",
      "iteration 100 / 2000: loss 0.560599\n",
      "iteration 200 / 2000: loss 0.454839\n",
      "iteration 300 / 2000: loss 0.293870\n",
      "iteration 400 / 2000: loss 0.285181\n",
      "iteration 500 / 2000: loss 0.273991\n",
      "iteration 600 / 2000: loss 0.299808\n",
      "iteration 700 / 2000: loss 0.303457\n",
      "iteration 800 / 2000: loss 0.261944\n",
      "iteration 900 / 2000: loss 0.297974\n",
      "iteration 1000 / 2000: loss 0.189618\n",
      "iteration 1100 / 2000: loss 0.307887\n",
      "iteration 1200 / 2000: loss 0.269522\n",
      "iteration 1300 / 2000: loss 0.277884\n",
      "iteration 1400 / 2000: loss 0.312958\n",
      "iteration 1500 / 2000: loss 0.374407\n",
      "iteration 1600 / 2000: loss 0.381296\n",
      "iteration 1700 / 2000: loss 0.212444\n",
      "iteration 1800 / 2000: loss 0.429173\n",
      "iteration 1900 / 2000: loss 0.355901\n",
      "Hidden Size: 10, Learning Rate: 0.002500074999999999, LR Decay: 0.8175, Regularization: 0.4, Validation accuracy: 0.935\n",
      "iteration 0 / 2000: loss 2.302587\n",
      "iteration 100 / 2000: loss 0.962996\n",
      "iteration 200 / 2000: loss 0.284883\n",
      "iteration 300 / 2000: loss 0.317718\n",
      "iteration 400 / 2000: loss 0.238282\n",
      "iteration 500 / 2000: loss 0.310474\n",
      "iteration 600 / 2000: loss 0.243306\n",
      "iteration 700 / 2000: loss 0.224856\n",
      "iteration 800 / 2000: loss 0.197075\n",
      "iteration 900 / 2000: loss 0.280682\n",
      "iteration 1000 / 2000: loss 0.307057\n",
      "iteration 1100 / 2000: loss 0.178222\n",
      "iteration 1200 / 2000: loss 0.229104\n",
      "iteration 1300 / 2000: loss 0.245285\n",
      "iteration 1400 / 2000: loss 0.281538\n",
      "iteration 1500 / 2000: loss 0.183377\n",
      "iteration 1600 / 2000: loss 0.212180\n",
      "iteration 1700 / 2000: loss 0.190734\n",
      "iteration 1800 / 2000: loss 0.306295\n",
      "iteration 1900 / 2000: loss 0.255898\n",
      "Hidden Size: 10, Learning Rate: 0.002500074999999999, LR Decay: 0.645, Regularization: 0.0, Validation accuracy: 0.9316\n",
      "iteration 0 / 2000: loss 2.302588\n",
      "iteration 100 / 2000: loss 0.877849\n",
      "iteration 200 / 2000: loss 0.439849\n",
      "iteration 300 / 2000: loss 0.461166\n",
      "iteration 400 / 2000: loss 0.233581\n",
      "iteration 500 / 2000: loss 0.338796\n",
      "iteration 600 / 2000: loss 0.410048\n",
      "iteration 700 / 2000: loss 0.306968\n",
      "iteration 800 / 2000: loss 0.277311\n",
      "iteration 900 / 2000: loss 0.343987\n",
      "iteration 1000 / 2000: loss 0.315994\n",
      "iteration 1100 / 2000: loss 0.348294\n",
      "iteration 1200 / 2000: loss 0.288242\n",
      "iteration 1300 / 2000: loss 0.247368\n",
      "iteration 1400 / 2000: loss 0.374031\n",
      "iteration 1500 / 2000: loss 0.234847\n",
      "iteration 1600 / 2000: loss 0.358203\n",
      "iteration 1700 / 2000: loss 0.362181\n",
      "iteration 1800 / 2000: loss 0.302706\n",
      "iteration 1900 / 2000: loss 0.209490\n",
      "Hidden Size: 10, Learning Rate: 0.002500074999999999, LR Decay: 0.645, Regularization: 0.1, Validation accuracy: 0.9284\n",
      "iteration 0 / 2000: loss 2.302594\n",
      "iteration 100 / 2000: loss 0.855746\n",
      "iteration 200 / 2000: loss 0.375039\n",
      "iteration 300 / 2000: loss 0.354801\n",
      "iteration 400 / 2000: loss 0.267188\n",
      "iteration 500 / 2000: loss 0.329462\n",
      "iteration 600 / 2000: loss 0.301860\n",
      "iteration 700 / 2000: loss 0.288612\n",
      "iteration 800 / 2000: loss 0.321929\n",
      "iteration 900 / 2000: loss 0.367377\n",
      "iteration 1000 / 2000: loss 0.378391\n",
      "iteration 1100 / 2000: loss 0.302352\n",
      "iteration 1200 / 2000: loss 0.305045\n",
      "iteration 1300 / 2000: loss 0.326956\n",
      "iteration 1400 / 2000: loss 0.186658\n",
      "iteration 1500 / 2000: loss 0.381686\n",
      "iteration 1600 / 2000: loss 0.408455\n",
      "iteration 1700 / 2000: loss 0.347831\n",
      "iteration 1800 / 2000: loss 0.280498\n",
      "iteration 1900 / 2000: loss 0.318742\n",
      "Hidden Size: 10, Learning Rate: 0.002500074999999999, LR Decay: 0.645, Regularization: 0.2, Validation accuracy: 0.9278\n",
      "iteration 0 / 2000: loss 2.302593\n",
      "iteration 100 / 2000: loss 1.023093\n",
      "iteration 200 / 2000: loss 0.422717\n",
      "iteration 300 / 2000: loss 0.304750\n",
      "iteration 400 / 2000: loss 0.361387\n",
      "iteration 500 / 2000: loss 0.446800\n",
      "iteration 600 / 2000: loss 0.266722\n",
      "iteration 700 / 2000: loss 0.263519\n",
      "iteration 800 / 2000: loss 0.381583\n",
      "iteration 900 / 2000: loss 0.333054\n",
      "iteration 1000 / 2000: loss 0.301275\n",
      "iteration 1100 / 2000: loss 0.372337\n",
      "iteration 1200 / 2000: loss 0.372559\n",
      "iteration 1300 / 2000: loss 0.296816\n",
      "iteration 1400 / 2000: loss 0.297989\n",
      "iteration 1500 / 2000: loss 0.421804\n",
      "iteration 1600 / 2000: loss 0.408985\n",
      "iteration 1700 / 2000: loss 0.315114\n",
      "iteration 1800 / 2000: loss 0.390651\n",
      "iteration 1900 / 2000: loss 0.381474\n",
      "Hidden Size: 10, Learning Rate: 0.002500074999999999, LR Decay: 0.645, Regularization: 0.30000000000000004, Validation accuracy: 0.9236\n",
      "iteration 0 / 2000: loss 2.302598\n",
      "iteration 100 / 2000: loss 0.629309\n",
      "iteration 200 / 2000: loss 0.378467\n",
      "iteration 300 / 2000: loss 0.360469\n",
      "iteration 400 / 2000: loss 0.367780\n",
      "iteration 500 / 2000: loss 0.453812\n",
      "iteration 600 / 2000: loss 0.404278\n",
      "iteration 700 / 2000: loss 0.372720\n",
      "iteration 800 / 2000: loss 0.434766\n",
      "iteration 900 / 2000: loss 0.293395\n",
      "iteration 1000 / 2000: loss 0.256944\n",
      "iteration 1100 / 2000: loss 0.247004\n",
      "iteration 1200 / 2000: loss 0.285140\n",
      "iteration 1300 / 2000: loss 0.388865\n",
      "iteration 1400 / 2000: loss 0.305405\n",
      "iteration 1500 / 2000: loss 0.347431\n",
      "iteration 1600 / 2000: loss 0.372205\n",
      "iteration 1700 / 2000: loss 0.308960\n",
      "iteration 1800 / 2000: loss 0.351455\n",
      "iteration 1900 / 2000: loss 0.262178\n",
      "Hidden Size: 10, Learning Rate: 0.002500074999999999, LR Decay: 0.645, Regularization: 0.4, Validation accuracy: 0.929\n",
      "iteration 0 / 2000: loss 2.302583\n",
      "iteration 100 / 2000: loss 0.915810\n",
      "iteration 200 / 2000: loss 0.341723\n",
      "iteration 300 / 2000: loss 0.340181\n",
      "iteration 400 / 2000: loss 0.315138\n",
      "iteration 500 / 2000: loss 0.347202\n",
      "iteration 600 / 2000: loss 0.338895\n",
      "iteration 700 / 2000: loss 0.326946\n",
      "iteration 800 / 2000: loss 0.342565\n",
      "iteration 900 / 2000: loss 0.239500\n",
      "iteration 1000 / 2000: loss 0.233525\n",
      "iteration 1100 / 2000: loss 0.251167\n",
      "iteration 1200 / 2000: loss 0.319838\n",
      "iteration 1300 / 2000: loss 0.307507\n",
      "iteration 1400 / 2000: loss 0.232309\n",
      "iteration 1500 / 2000: loss 0.308938\n",
      "iteration 1600 / 2000: loss 0.258711\n",
      "iteration 1700 / 2000: loss 0.325160\n",
      "iteration 1800 / 2000: loss 0.231694\n",
      "iteration 1900 / 2000: loss 0.301344\n",
      "Hidden Size: 10, Learning Rate: 0.002500074999999999, LR Decay: 0.47250000000000003, Regularization: 0.0, Validation accuracy: 0.9178\n",
      "iteration 0 / 2000: loss 2.302588\n",
      "iteration 100 / 2000: loss 0.900438\n",
      "iteration 200 / 2000: loss 0.430107\n",
      "iteration 300 / 2000: loss 0.408124\n",
      "iteration 400 / 2000: loss 0.496914\n",
      "iteration 500 / 2000: loss 0.344114\n",
      "iteration 600 / 2000: loss 0.399249\n",
      "iteration 700 / 2000: loss 0.375008\n",
      "iteration 800 / 2000: loss 0.359111\n",
      "iteration 900 / 2000: loss 0.330848\n",
      "iteration 1000 / 2000: loss 0.279704\n",
      "iteration 1100 / 2000: loss 0.270323\n",
      "iteration 1200 / 2000: loss 0.411026\n",
      "iteration 1300 / 2000: loss 0.304226\n",
      "iteration 1400 / 2000: loss 0.338745\n",
      "iteration 1500 / 2000: loss 0.350999\n",
      "iteration 1600 / 2000: loss 0.260261\n",
      "iteration 1700 / 2000: loss 0.360937\n",
      "iteration 1800 / 2000: loss 0.344545\n",
      "iteration 1900 / 2000: loss 0.271343\n",
      "Hidden Size: 10, Learning Rate: 0.002500074999999999, LR Decay: 0.47250000000000003, Regularization: 0.1, Validation accuracy: 0.9136\n",
      "iteration 0 / 2000: loss 2.302588\n",
      "iteration 100 / 2000: loss 0.925767\n",
      "iteration 200 / 2000: loss 0.457243\n",
      "iteration 300 / 2000: loss 0.371522\n",
      "iteration 400 / 2000: loss 0.405630\n",
      "iteration 500 / 2000: loss 0.397718\n",
      "iteration 600 / 2000: loss 0.314524\n",
      "iteration 700 / 2000: loss 0.352999\n",
      "iteration 800 / 2000: loss 0.447995\n",
      "iteration 900 / 2000: loss 0.361282\n",
      "iteration 1000 / 2000: loss 0.276686\n",
      "iteration 1100 / 2000: loss 0.278021\n",
      "iteration 1200 / 2000: loss 0.425321\n",
      "iteration 1300 / 2000: loss 0.357925\n",
      "iteration 1400 / 2000: loss 0.387810\n",
      "iteration 1500 / 2000: loss 0.341733\n",
      "iteration 1600 / 2000: loss 0.348187\n",
      "iteration 1700 / 2000: loss 0.350448\n",
      "iteration 1800 / 2000: loss 0.330658\n",
      "iteration 1900 / 2000: loss 0.337160\n",
      "Hidden Size: 10, Learning Rate: 0.002500074999999999, LR Decay: 0.47250000000000003, Regularization: 0.2, Validation accuracy: 0.9212\n",
      "iteration 0 / 2000: loss 2.302582\n",
      "iteration 100 / 2000: loss 1.042439\n",
      "iteration 200 / 2000: loss 0.513677\n",
      "iteration 300 / 2000: loss 0.341245\n",
      "iteration 400 / 2000: loss 0.314699\n",
      "iteration 500 / 2000: loss 0.325056\n",
      "iteration 600 / 2000: loss 0.315584\n",
      "iteration 700 / 2000: loss 0.347143\n",
      "iteration 800 / 2000: loss 0.319279\n",
      "iteration 900 / 2000: loss 0.426262\n",
      "iteration 1000 / 2000: loss 0.376009\n",
      "iteration 1100 / 2000: loss 0.296905\n",
      "iteration 1200 / 2000: loss 0.363355\n",
      "iteration 1300 / 2000: loss 0.422995\n",
      "iteration 1400 / 2000: loss 0.430863\n",
      "iteration 1500 / 2000: loss 0.388935\n",
      "iteration 1600 / 2000: loss 0.365385\n",
      "iteration 1700 / 2000: loss 0.318503\n",
      "iteration 1800 / 2000: loss 0.310321\n",
      "iteration 1900 / 2000: loss 0.307475\n",
      "Hidden Size: 10, Learning Rate: 0.002500074999999999, LR Decay: 0.47250000000000003, Regularization: 0.30000000000000004, Validation accuracy: 0.9142\n",
      "iteration 0 / 2000: loss 2.302601\n",
      "iteration 100 / 2000: loss 1.348485\n",
      "iteration 200 / 2000: loss 0.524523\n",
      "iteration 300 / 2000: loss 0.398573\n",
      "iteration 400 / 2000: loss 0.363160\n",
      "iteration 500 / 2000: loss 0.342284\n",
      "iteration 600 / 2000: loss 0.381137\n",
      "iteration 700 / 2000: loss 0.348393\n",
      "iteration 800 / 2000: loss 0.407020\n",
      "iteration 900 / 2000: loss 0.425434\n",
      "iteration 1000 / 2000: loss 0.356445\n",
      "iteration 1100 / 2000: loss 0.327913\n",
      "iteration 1200 / 2000: loss 0.338815\n",
      "iteration 1300 / 2000: loss 0.377240\n",
      "iteration 1400 / 2000: loss 0.367095\n",
      "iteration 1500 / 2000: loss 0.359663\n",
      "iteration 1600 / 2000: loss 0.308974\n",
      "iteration 1700 / 2000: loss 0.386797\n",
      "iteration 1800 / 2000: loss 0.441770\n",
      "iteration 1900 / 2000: loss 0.326125\n",
      "Hidden Size: 10, Learning Rate: 0.002500074999999999, LR Decay: 0.47250000000000003, Regularization: 0.4, Validation accuracy: 0.916\n",
      "iteration 0 / 2000: loss 2.302582\n",
      "iteration 100 / 2000: loss 1.812394\n",
      "iteration 200 / 2000: loss 0.675984\n",
      "iteration 300 / 2000: loss 0.505304\n",
      "iteration 400 / 2000: loss 0.471986\n",
      "iteration 500 / 2000: loss 0.351554\n",
      "iteration 600 / 2000: loss 0.422081\n",
      "iteration 700 / 2000: loss 0.427904\n",
      "iteration 800 / 2000: loss 0.400283\n",
      "iteration 900 / 2000: loss 0.269821\n",
      "iteration 1000 / 2000: loss 0.414400\n",
      "iteration 1100 / 2000: loss 0.383614\n",
      "iteration 1200 / 2000: loss 0.401269\n",
      "iteration 1300 / 2000: loss 0.248725\n",
      "iteration 1400 / 2000: loss 0.394892\n",
      "iteration 1500 / 2000: loss 0.358622\n",
      "iteration 1600 / 2000: loss 0.305207\n",
      "iteration 1700 / 2000: loss 0.366126\n",
      "iteration 1800 / 2000: loss 0.441387\n",
      "iteration 1900 / 2000: loss 0.292995\n",
      "Hidden Size: 10, Learning Rate: 0.002500074999999999, LR Decay: 0.3, Regularization: 0.0, Validation accuracy: 0.8892\n",
      "iteration 0 / 2000: loss 2.302579\n",
      "iteration 100 / 2000: loss 1.791335\n",
      "iteration 200 / 2000: loss 0.726912\n",
      "iteration 300 / 2000: loss 0.536417\n",
      "iteration 400 / 2000: loss 0.546123\n",
      "iteration 500 / 2000: loss 0.344544\n",
      "iteration 600 / 2000: loss 0.487900\n",
      "iteration 700 / 2000: loss 0.422978\n",
      "iteration 800 / 2000: loss 0.434910\n",
      "iteration 900 / 2000: loss 0.409824\n",
      "iteration 1000 / 2000: loss 0.388248\n",
      "iteration 1100 / 2000: loss 0.476046\n",
      "iteration 1200 / 2000: loss 0.452141\n",
      "iteration 1300 / 2000: loss 0.428894\n",
      "iteration 1400 / 2000: loss 0.497696\n",
      "iteration 1500 / 2000: loss 0.479067\n",
      "iteration 1600 / 2000: loss 0.517699\n",
      "iteration 1700 / 2000: loss 0.394086\n",
      "iteration 1800 / 2000: loss 0.440991\n",
      "iteration 1900 / 2000: loss 0.536201\n",
      "Hidden Size: 10, Learning Rate: 0.002500074999999999, LR Decay: 0.3, Regularization: 0.1, Validation accuracy: 0.8806\n",
      "iteration 0 / 2000: loss 2.302584\n",
      "iteration 100 / 2000: loss 1.593547\n",
      "iteration 200 / 2000: loss 1.138575\n",
      "iteration 300 / 2000: loss 0.635698\n",
      "iteration 400 / 2000: loss 0.819787\n",
      "iteration 500 / 2000: loss 0.623528\n",
      "iteration 600 / 2000: loss 0.499467\n",
      "iteration 700 / 2000: loss 0.479478\n",
      "iteration 800 / 2000: loss 0.510976\n",
      "iteration 900 / 2000: loss 0.505012\n",
      "iteration 1000 / 2000: loss 0.574996\n",
      "iteration 1100 / 2000: loss 0.506275\n",
      "iteration 1200 / 2000: loss 0.560129\n",
      "iteration 1300 / 2000: loss 0.500485\n",
      "iteration 1400 / 2000: loss 0.477925\n",
      "iteration 1500 / 2000: loss 0.582250\n",
      "iteration 1600 / 2000: loss 0.494039\n",
      "iteration 1700 / 2000: loss 0.455462\n",
      "iteration 1800 / 2000: loss 0.582807\n",
      "iteration 1900 / 2000: loss 0.554733\n",
      "Hidden Size: 10, Learning Rate: 0.002500074999999999, LR Decay: 0.3, Regularization: 0.2, Validation accuracy: 0.8558\n",
      "iteration 0 / 2000: loss 2.302604\n",
      "iteration 100 / 2000: loss 1.593668\n",
      "iteration 200 / 2000: loss 0.758354\n",
      "iteration 300 / 2000: loss 0.524513\n",
      "iteration 400 / 2000: loss 0.531579\n",
      "iteration 500 / 2000: loss 0.407169\n",
      "iteration 600 / 2000: loss 0.451266\n",
      "iteration 700 / 2000: loss 0.383832\n",
      "iteration 800 / 2000: loss 0.434737\n",
      "iteration 900 / 2000: loss 0.348071\n",
      "iteration 1000 / 2000: loss 0.349438\n",
      "iteration 1100 / 2000: loss 0.477516\n",
      "iteration 1200 / 2000: loss 0.331634\n",
      "iteration 1300 / 2000: loss 0.375902\n",
      "iteration 1400 / 2000: loss 0.373175\n",
      "iteration 1500 / 2000: loss 0.389738\n",
      "iteration 1600 / 2000: loss 0.442254\n",
      "iteration 1700 / 2000: loss 0.309165\n",
      "iteration 1800 / 2000: loss 0.424688\n",
      "iteration 1900 / 2000: loss 0.368668\n",
      "Hidden Size: 10, Learning Rate: 0.002500074999999999, LR Decay: 0.3, Regularization: 0.30000000000000004, Validation accuracy: 0.8916\n",
      "iteration 0 / 2000: loss 2.302596\n",
      "iteration 100 / 2000: loss 1.690162\n",
      "iteration 200 / 2000: loss 0.773855\n",
      "iteration 300 / 2000: loss 0.571149\n",
      "iteration 400 / 2000: loss 0.453928\n",
      "iteration 500 / 2000: loss 0.613963\n",
      "iteration 600 / 2000: loss 0.447214\n",
      "iteration 700 / 2000: loss 0.484961\n",
      "iteration 800 / 2000: loss 0.423727\n",
      "iteration 900 / 2000: loss 0.524284\n",
      "iteration 1000 / 2000: loss 0.501977\n",
      "iteration 1100 / 2000: loss 0.346393\n",
      "iteration 1200 / 2000: loss 0.377016\n",
      "iteration 1300 / 2000: loss 0.443618\n",
      "iteration 1400 / 2000: loss 0.514609\n",
      "iteration 1500 / 2000: loss 0.344190\n",
      "iteration 1600 / 2000: loss 0.418633\n",
      "iteration 1700 / 2000: loss 0.475265\n",
      "iteration 1800 / 2000: loss 0.464570\n",
      "iteration 1900 / 2000: loss 0.431707\n",
      "Hidden Size: 10, Learning Rate: 0.002500074999999999, LR Decay: 0.3, Regularization: 0.4, Validation accuracy: 0.9002\n",
      "iteration 0 / 2000: loss 2.302587\n",
      "iteration 100 / 2000: loss 2.302581\n",
      "iteration 200 / 2000: loss 2.302576\n",
      "iteration 300 / 2000: loss 2.302584\n",
      "iteration 400 / 2000: loss 2.302579\n",
      "iteration 500 / 2000: loss 2.302584\n",
      "iteration 600 / 2000: loss 2.302585\n",
      "iteration 700 / 2000: loss 2.302580\n",
      "iteration 800 / 2000: loss 2.302582\n",
      "iteration 900 / 2000: loss 2.302578\n",
      "iteration 1000 / 2000: loss 2.302578\n",
      "iteration 1100 / 2000: loss 2.302585\n",
      "iteration 1200 / 2000: loss 2.302581\n",
      "iteration 1300 / 2000: loss 2.302584\n",
      "iteration 1400 / 2000: loss 2.302582\n",
      "iteration 1500 / 2000: loss 2.302582\n",
      "iteration 1600 / 2000: loss 2.302581\n",
      "iteration 1700 / 2000: loss 2.302576\n",
      "iteration 1800 / 2000: loss 2.302575\n",
      "iteration 1900 / 2000: loss 2.302583\n",
      "Hidden Size: 10, Learning Rate: 1e-07, LR Decay: 0.99, Regularization: 0.0, Validation accuracy: 0.0812\n",
      "iteration 0 / 2000: loss 2.302590\n",
      "iteration 100 / 2000: loss 2.302586\n",
      "iteration 200 / 2000: loss 2.302589\n",
      "iteration 300 / 2000: loss 2.302591\n",
      "iteration 400 / 2000: loss 2.302589\n",
      "iteration 500 / 2000: loss 2.302591\n",
      "iteration 600 / 2000: loss 2.302588\n",
      "iteration 700 / 2000: loss 2.302590\n",
      "iteration 800 / 2000: loss 2.302587\n",
      "iteration 900 / 2000: loss 2.302583\n",
      "iteration 1000 / 2000: loss 2.302588\n",
      "iteration 1100 / 2000: loss 2.302590\n",
      "iteration 1200 / 2000: loss 2.302585\n",
      "iteration 1300 / 2000: loss 2.302589\n",
      "iteration 1400 / 2000: loss 2.302589\n",
      "iteration 1500 / 2000: loss 2.302589\n",
      "iteration 1600 / 2000: loss 2.302592\n",
      "iteration 1700 / 2000: loss 2.302586\n",
      "iteration 1800 / 2000: loss 2.302586\n",
      "iteration 1900 / 2000: loss 2.302585\n",
      "Hidden Size: 10, Learning Rate: 1e-07, LR Decay: 0.99, Regularization: 0.1, Validation accuracy: 0.0772\n",
      "iteration 0 / 2000: loss 2.302593\n",
      "iteration 100 / 2000: loss 2.302588\n",
      "iteration 200 / 2000: loss 2.302590\n",
      "iteration 300 / 2000: loss 2.302584\n",
      "iteration 400 / 2000: loss 2.302589\n",
      "iteration 500 / 2000: loss 2.302592\n",
      "iteration 600 / 2000: loss 2.302583\n",
      "iteration 700 / 2000: loss 2.302586\n",
      "iteration 800 / 2000: loss 2.302586\n",
      "iteration 900 / 2000: loss 2.302588\n",
      "iteration 1000 / 2000: loss 2.302585\n",
      "iteration 1100 / 2000: loss 2.302587\n",
      "iteration 1200 / 2000: loss 2.302591\n",
      "iteration 1300 / 2000: loss 2.302583\n",
      "iteration 1400 / 2000: loss 2.302592\n",
      "iteration 1500 / 2000: loss 2.302587\n",
      "iteration 1600 / 2000: loss 2.302588\n",
      "iteration 1700 / 2000: loss 2.302584\n",
      "iteration 1800 / 2000: loss 2.302590\n",
      "iteration 1900 / 2000: loss 2.302585\n",
      "Hidden Size: 10, Learning Rate: 1e-07, LR Decay: 0.99, Regularization: 0.2, Validation accuracy: 0.1326\n",
      "iteration 0 / 2000: loss 2.302590\n",
      "iteration 100 / 2000: loss 2.302592\n",
      "iteration 200 / 2000: loss 2.302589\n",
      "iteration 300 / 2000: loss 2.302590\n",
      "iteration 400 / 2000: loss 2.302588\n",
      "iteration 500 / 2000: loss 2.302592\n",
      "iteration 600 / 2000: loss 2.302597\n",
      "iteration 700 / 2000: loss 2.302590\n",
      "iteration 800 / 2000: loss 2.302588\n",
      "iteration 900 / 2000: loss 2.302592\n",
      "iteration 1000 / 2000: loss 2.302593\n",
      "iteration 1100 / 2000: loss 2.302590\n",
      "iteration 1200 / 2000: loss 2.302594\n",
      "iteration 1300 / 2000: loss 2.302591\n",
      "iteration 1400 / 2000: loss 2.302588\n",
      "iteration 1500 / 2000: loss 2.302593\n",
      "iteration 1600 / 2000: loss 2.302593\n",
      "iteration 1700 / 2000: loss 2.302594\n",
      "iteration 1800 / 2000: loss 2.302592\n",
      "iteration 1900 / 2000: loss 2.302589\n",
      "Hidden Size: 10, Learning Rate: 1e-07, LR Decay: 0.99, Regularization: 0.30000000000000004, Validation accuracy: 0.1072\n",
      "iteration 0 / 2000: loss 2.302608\n",
      "iteration 100 / 2000: loss 2.302606\n",
      "iteration 200 / 2000: loss 2.302611\n",
      "iteration 300 / 2000: loss 2.302610\n",
      "iteration 400 / 2000: loss 2.302603\n",
      "iteration 500 / 2000: loss 2.302600\n",
      "iteration 600 / 2000: loss 2.302607\n",
      "iteration 700 / 2000: loss 2.302605\n",
      "iteration 800 / 2000: loss 2.302606\n",
      "iteration 900 / 2000: loss 2.302608\n",
      "iteration 1000 / 2000: loss 2.302602\n",
      "iteration 1100 / 2000: loss 2.302609\n",
      "iteration 1200 / 2000: loss 2.302608\n",
      "iteration 1300 / 2000: loss 2.302603\n",
      "iteration 1400 / 2000: loss 2.302602\n",
      "iteration 1500 / 2000: loss 2.302601\n",
      "iteration 1600 / 2000: loss 2.302604\n",
      "iteration 1700 / 2000: loss 2.302600\n",
      "iteration 1800 / 2000: loss 2.302604\n",
      "iteration 1900 / 2000: loss 2.302608\n",
      "Hidden Size: 10, Learning Rate: 1e-07, LR Decay: 0.99, Regularization: 0.4, Validation accuracy: 0.1102\n",
      "iteration 0 / 2000: loss 2.302587\n",
      "iteration 100 / 2000: loss 2.302585\n",
      "iteration 200 / 2000: loss 2.302588\n",
      "iteration 300 / 2000: loss 2.302591\n",
      "iteration 400 / 2000: loss 2.302593\n",
      "iteration 500 / 2000: loss 2.302586\n",
      "iteration 600 / 2000: loss 2.302588\n",
      "iteration 700 / 2000: loss 2.302589\n",
      "iteration 800 / 2000: loss 2.302588\n",
      "iteration 900 / 2000: loss 2.302585\n",
      "iteration 1000 / 2000: loss 2.302588\n",
      "iteration 1100 / 2000: loss 2.302586\n",
      "iteration 1200 / 2000: loss 2.302589\n",
      "iteration 1300 / 2000: loss 2.302590\n",
      "iteration 1400 / 2000: loss 2.302586\n",
      "iteration 1500 / 2000: loss 2.302589\n",
      "iteration 1600 / 2000: loss 2.302588\n",
      "iteration 1700 / 2000: loss 2.302586\n",
      "iteration 1800 / 2000: loss 2.302583\n",
      "iteration 1900 / 2000: loss 2.302584\n",
      "Hidden Size: 10, Learning Rate: 1e-07, LR Decay: 0.8175, Regularization: 0.0, Validation accuracy: 0.1104\n",
      "iteration 0 / 2000: loss 2.302588\n",
      "iteration 100 / 2000: loss 2.302591\n",
      "iteration 200 / 2000: loss 2.302592\n",
      "iteration 300 / 2000: loss 2.302588\n",
      "iteration 400 / 2000: loss 2.302591\n",
      "iteration 500 / 2000: loss 2.302592\n",
      "iteration 600 / 2000: loss 2.302591\n",
      "iteration 700 / 2000: loss 2.302587\n",
      "iteration 800 / 2000: loss 2.302589\n",
      "iteration 900 / 2000: loss 2.302588\n",
      "iteration 1000 / 2000: loss 2.302588\n",
      "iteration 1100 / 2000: loss 2.302587\n",
      "iteration 1200 / 2000: loss 2.302588\n",
      "iteration 1300 / 2000: loss 2.302589\n",
      "iteration 1400 / 2000: loss 2.302591\n",
      "iteration 1500 / 2000: loss 2.302594\n",
      "iteration 1600 / 2000: loss 2.302592\n",
      "iteration 1700 / 2000: loss 2.302594\n",
      "iteration 1800 / 2000: loss 2.302589\n",
      "iteration 1900 / 2000: loss 2.302590\n",
      "Hidden Size: 10, Learning Rate: 1e-07, LR Decay: 0.8175, Regularization: 0.1, Validation accuracy: 0.0972\n",
      "iteration 0 / 2000: loss 2.302593\n",
      "iteration 100 / 2000: loss 2.302594\n",
      "iteration 200 / 2000: loss 2.302594\n",
      "iteration 300 / 2000: loss 2.302594\n",
      "iteration 400 / 2000: loss 2.302600\n",
      "iteration 500 / 2000: loss 2.302599\n",
      "iteration 600 / 2000: loss 2.302597\n",
      "iteration 700 / 2000: loss 2.302595\n",
      "iteration 800 / 2000: loss 2.302595\n",
      "iteration 900 / 2000: loss 2.302596\n",
      "iteration 1000 / 2000: loss 2.302597\n",
      "iteration 1100 / 2000: loss 2.302594\n",
      "iteration 1200 / 2000: loss 2.302596\n",
      "iteration 1300 / 2000: loss 2.302597\n",
      "iteration 1400 / 2000: loss 2.302597\n",
      "iteration 1500 / 2000: loss 2.302594\n",
      "iteration 1600 / 2000: loss 2.302597\n",
      "iteration 1700 / 2000: loss 2.302596\n",
      "iteration 1800 / 2000: loss 2.302594\n",
      "iteration 1900 / 2000: loss 2.302597\n",
      "Hidden Size: 10, Learning Rate: 1e-07, LR Decay: 0.8175, Regularization: 0.2, Validation accuracy: 0.0686\n",
      "iteration 0 / 2000: loss 2.302596\n",
      "iteration 100 / 2000: loss 2.302598\n",
      "iteration 200 / 2000: loss 2.302600\n",
      "iteration 300 / 2000: loss 2.302597\n",
      "iteration 400 / 2000: loss 2.302598\n",
      "iteration 500 / 2000: loss 2.302600\n",
      "iteration 600 / 2000: loss 2.302600\n",
      "iteration 700 / 2000: loss 2.302601\n",
      "iteration 800 / 2000: loss 2.302598\n",
      "iteration 900 / 2000: loss 2.302598\n",
      "iteration 1000 / 2000: loss 2.302597\n",
      "iteration 1100 / 2000: loss 2.302598\n",
      "iteration 1200 / 2000: loss 2.302599\n",
      "iteration 1300 / 2000: loss 2.302602\n",
      "iteration 1400 / 2000: loss 2.302599\n",
      "iteration 1500 / 2000: loss 2.302597\n",
      "iteration 1600 / 2000: loss 2.302600\n",
      "iteration 1700 / 2000: loss 2.302598\n",
      "iteration 1800 / 2000: loss 2.302599\n",
      "iteration 1900 / 2000: loss 2.302596\n",
      "Hidden Size: 10, Learning Rate: 1e-07, LR Decay: 0.8175, Regularization: 0.30000000000000004, Validation accuracy: 0.0534\n",
      "iteration 0 / 2000: loss 2.302606\n",
      "iteration 100 / 2000: loss 2.302607\n",
      "iteration 200 / 2000: loss 2.302603\n",
      "iteration 300 / 2000: loss 2.302608\n",
      "iteration 400 / 2000: loss 2.302605\n",
      "iteration 500 / 2000: loss 2.302605\n",
      "iteration 600 / 2000: loss 2.302608\n",
      "iteration 700 / 2000: loss 2.302605\n",
      "iteration 800 / 2000: loss 2.302606\n",
      "iteration 900 / 2000: loss 2.302600\n",
      "iteration 1000 / 2000: loss 2.302604\n",
      "iteration 1100 / 2000: loss 2.302604\n",
      "iteration 1200 / 2000: loss 2.302605\n",
      "iteration 1300 / 2000: loss 2.302604\n",
      "iteration 1400 / 2000: loss 2.302607\n",
      "iteration 1500 / 2000: loss 2.302605\n",
      "iteration 1600 / 2000: loss 2.302606\n",
      "iteration 1700 / 2000: loss 2.302604\n",
      "iteration 1800 / 2000: loss 2.302601\n",
      "iteration 1900 / 2000: loss 2.302605\n",
      "Hidden Size: 10, Learning Rate: 1e-07, LR Decay: 0.8175, Regularization: 0.4, Validation accuracy: 0.0498\n",
      "iteration 0 / 2000: loss 2.302589\n",
      "iteration 100 / 2000: loss 2.302588\n",
      "iteration 200 / 2000: loss 2.302589\n",
      "iteration 300 / 2000: loss 2.302585\n",
      "iteration 400 / 2000: loss 2.302589\n",
      "iteration 500 / 2000: loss 2.302590\n",
      "iteration 600 / 2000: loss 2.302586\n",
      "iteration 700 / 2000: loss 2.302589\n",
      "iteration 800 / 2000: loss 2.302589\n",
      "iteration 900 / 2000: loss 2.302581\n",
      "iteration 1000 / 2000: loss 2.302589\n",
      "iteration 1100 / 2000: loss 2.302585\n",
      "iteration 1200 / 2000: loss 2.302584\n",
      "iteration 1300 / 2000: loss 2.302589\n",
      "iteration 1400 / 2000: loss 2.302585\n",
      "iteration 1500 / 2000: loss 2.302588\n",
      "iteration 1600 / 2000: loss 2.302590\n",
      "iteration 1700 / 2000: loss 2.302591\n",
      "iteration 1800 / 2000: loss 2.302587\n",
      "iteration 1900 / 2000: loss 2.302587\n",
      "Hidden Size: 10, Learning Rate: 1e-07, LR Decay: 0.645, Regularization: 0.0, Validation accuracy: 0.0898\n",
      "iteration 0 / 2000: loss 2.302584\n",
      "iteration 100 / 2000: loss 2.302585\n",
      "iteration 200 / 2000: loss 2.302588\n",
      "iteration 300 / 2000: loss 2.302584\n",
      "iteration 400 / 2000: loss 2.302587\n",
      "iteration 500 / 2000: loss 2.302585\n",
      "iteration 600 / 2000: loss 2.302585\n",
      "iteration 700 / 2000: loss 2.302584\n",
      "iteration 800 / 2000: loss 2.302591\n",
      "iteration 900 / 2000: loss 2.302584\n",
      "iteration 1000 / 2000: loss 2.302593\n",
      "iteration 1100 / 2000: loss 2.302590\n",
      "iteration 1200 / 2000: loss 2.302589\n",
      "iteration 1300 / 2000: loss 2.302588\n",
      "iteration 1400 / 2000: loss 2.302586\n",
      "iteration 1500 / 2000: loss 2.302588\n",
      "iteration 1600 / 2000: loss 2.302585\n",
      "iteration 1700 / 2000: loss 2.302582\n",
      "iteration 1800 / 2000: loss 2.302582\n",
      "iteration 1900 / 2000: loss 2.302582\n",
      "Hidden Size: 10, Learning Rate: 1e-07, LR Decay: 0.645, Regularization: 0.1, Validation accuracy: 0.1192\n",
      "iteration 0 / 2000: loss 2.302594\n",
      "iteration 100 / 2000: loss 2.302595\n",
      "iteration 200 / 2000: loss 2.302597\n",
      "iteration 300 / 2000: loss 2.302601\n",
      "iteration 400 / 2000: loss 2.302596\n",
      "iteration 500 / 2000: loss 2.302597\n",
      "iteration 600 / 2000: loss 2.302598\n",
      "iteration 700 / 2000: loss 2.302600\n",
      "iteration 800 / 2000: loss 2.302596\n",
      "iteration 900 / 2000: loss 2.302591\n",
      "iteration 1000 / 2000: loss 2.302599\n",
      "iteration 1100 / 2000: loss 2.302591\n",
      "iteration 1200 / 2000: loss 2.302597\n",
      "iteration 1300 / 2000: loss 2.302598\n",
      "iteration 1400 / 2000: loss 2.302591\n",
      "iteration 1500 / 2000: loss 2.302594\n",
      "iteration 1600 / 2000: loss 2.302598\n",
      "iteration 1700 / 2000: loss 2.302597\n",
      "iteration 1800 / 2000: loss 2.302597\n",
      "iteration 1900 / 2000: loss 2.302598\n",
      "Hidden Size: 10, Learning Rate: 1e-07, LR Decay: 0.645, Regularization: 0.2, Validation accuracy: 0.099\n",
      "iteration 0 / 2000: loss 2.302595\n",
      "iteration 100 / 2000: loss 2.302590\n",
      "iteration 200 / 2000: loss 2.302588\n",
      "iteration 300 / 2000: loss 2.302594\n",
      "iteration 400 / 2000: loss 2.302593\n",
      "iteration 500 / 2000: loss 2.302594\n",
      "iteration 600 / 2000: loss 2.302594\n",
      "iteration 700 / 2000: loss 2.302594\n",
      "iteration 800 / 2000: loss 2.302594\n",
      "iteration 900 / 2000: loss 2.302591\n",
      "iteration 1000 / 2000: loss 2.302591\n",
      "iteration 1100 / 2000: loss 2.302592\n",
      "iteration 1200 / 2000: loss 2.302591\n",
      "iteration 1300 / 2000: loss 2.302593\n",
      "iteration 1400 / 2000: loss 2.302594\n",
      "iteration 1500 / 2000: loss 2.302595\n",
      "iteration 1600 / 2000: loss 2.302591\n",
      "iteration 1700 / 2000: loss 2.302592\n",
      "iteration 1800 / 2000: loss 2.302595\n",
      "iteration 1900 / 2000: loss 2.302597\n",
      "Hidden Size: 10, Learning Rate: 1e-07, LR Decay: 0.645, Regularization: 0.30000000000000004, Validation accuracy: 0.1328\n",
      "iteration 0 / 2000: loss 2.302607\n",
      "iteration 100 / 2000: loss 2.302599\n",
      "iteration 200 / 2000: loss 2.302599\n",
      "iteration 300 / 2000: loss 2.302606\n",
      "iteration 400 / 2000: loss 2.302605\n",
      "iteration 500 / 2000: loss 2.302604\n",
      "iteration 600 / 2000: loss 2.302603\n",
      "iteration 700 / 2000: loss 2.302610\n",
      "iteration 800 / 2000: loss 2.302605\n",
      "iteration 900 / 2000: loss 2.302606\n",
      "iteration 1000 / 2000: loss 2.302604\n",
      "iteration 1100 / 2000: loss 2.302605\n",
      "iteration 1200 / 2000: loss 2.302605\n",
      "iteration 1300 / 2000: loss 2.302609\n",
      "iteration 1400 / 2000: loss 2.302605\n",
      "iteration 1500 / 2000: loss 2.302605\n",
      "iteration 1600 / 2000: loss 2.302605\n",
      "iteration 1700 / 2000: loss 2.302608\n",
      "iteration 1800 / 2000: loss 2.302604\n",
      "iteration 1900 / 2000: loss 2.302606\n",
      "Hidden Size: 10, Learning Rate: 1e-07, LR Decay: 0.645, Regularization: 0.4, Validation accuracy: 0.1178\n",
      "iteration 0 / 2000: loss 2.302581\n",
      "iteration 100 / 2000: loss 2.302582\n",
      "iteration 200 / 2000: loss 2.302576\n",
      "iteration 300 / 2000: loss 2.302583\n",
      "iteration 400 / 2000: loss 2.302584\n",
      "iteration 500 / 2000: loss 2.302592\n",
      "iteration 600 / 2000: loss 2.302585\n",
      "iteration 700 / 2000: loss 2.302585\n",
      "iteration 800 / 2000: loss 2.302583\n",
      "iteration 900 / 2000: loss 2.302583\n",
      "iteration 1000 / 2000: loss 2.302587\n",
      "iteration 1100 / 2000: loss 2.302583\n",
      "iteration 1200 / 2000: loss 2.302586\n",
      "iteration 1300 / 2000: loss 2.302585\n",
      "iteration 1400 / 2000: loss 2.302584\n",
      "iteration 1500 / 2000: loss 2.302584\n",
      "iteration 1600 / 2000: loss 2.302588\n",
      "iteration 1700 / 2000: loss 2.302586\n",
      "iteration 1800 / 2000: loss 2.302580\n",
      "iteration 1900 / 2000: loss 2.302583\n",
      "Hidden Size: 10, Learning Rate: 1e-07, LR Decay: 0.47250000000000003, Regularization: 0.0, Validation accuracy: 0.128\n",
      "iteration 0 / 2000: loss 2.302591\n",
      "iteration 100 / 2000: loss 2.302591\n",
      "iteration 200 / 2000: loss 2.302589\n",
      "iteration 300 / 2000: loss 2.302595\n",
      "iteration 400 / 2000: loss 2.302593\n",
      "iteration 500 / 2000: loss 2.302593\n",
      "iteration 600 / 2000: loss 2.302585\n",
      "iteration 700 / 2000: loss 2.302588\n",
      "iteration 800 / 2000: loss 2.302590\n",
      "iteration 900 / 2000: loss 2.302590\n",
      "iteration 1000 / 2000: loss 2.302592\n",
      "iteration 1100 / 2000: loss 2.302591\n",
      "iteration 1200 / 2000: loss 2.302587\n",
      "iteration 1300 / 2000: loss 2.302588\n",
      "iteration 1400 / 2000: loss 2.302593\n",
      "iteration 1500 / 2000: loss 2.302593\n",
      "iteration 1600 / 2000: loss 2.302591\n",
      "iteration 1700 / 2000: loss 2.302590\n",
      "iteration 1800 / 2000: loss 2.302595\n",
      "iteration 1900 / 2000: loss 2.302589\n",
      "Hidden Size: 10, Learning Rate: 1e-07, LR Decay: 0.47250000000000003, Regularization: 0.1, Validation accuracy: 0.1226\n",
      "iteration 0 / 2000: loss 2.302585\n",
      "iteration 100 / 2000: loss 2.302583\n",
      "iteration 200 / 2000: loss 2.302585\n",
      "iteration 300 / 2000: loss 2.302586\n",
      "iteration 400 / 2000: loss 2.302585\n",
      "iteration 500 / 2000: loss 2.302586\n",
      "iteration 600 / 2000: loss 2.302587\n",
      "iteration 700 / 2000: loss 2.302589\n",
      "iteration 800 / 2000: loss 2.302589\n",
      "iteration 900 / 2000: loss 2.302588\n",
      "iteration 1000 / 2000: loss 2.302584\n",
      "iteration 1100 / 2000: loss 2.302580\n",
      "iteration 1200 / 2000: loss 2.302590\n",
      "iteration 1300 / 2000: loss 2.302587\n",
      "iteration 1400 / 2000: loss 2.302585\n",
      "iteration 1500 / 2000: loss 2.302587\n",
      "iteration 1600 / 2000: loss 2.302586\n",
      "iteration 1700 / 2000: loss 2.302588\n",
      "iteration 1800 / 2000: loss 2.302586\n",
      "iteration 1900 / 2000: loss 2.302589\n",
      "Hidden Size: 10, Learning Rate: 1e-07, LR Decay: 0.47250000000000003, Regularization: 0.2, Validation accuracy: 0.0996\n",
      "iteration 0 / 2000: loss 2.302606\n",
      "iteration 100 / 2000: loss 2.302605\n",
      "iteration 200 / 2000: loss 2.302607\n",
      "iteration 300 / 2000: loss 2.302603\n",
      "iteration 400 / 2000: loss 2.302608\n",
      "iteration 500 / 2000: loss 2.302606\n",
      "iteration 600 / 2000: loss 2.302606\n",
      "iteration 700 / 2000: loss 2.302601\n",
      "iteration 800 / 2000: loss 2.302604\n",
      "iteration 900 / 2000: loss 2.302603\n",
      "iteration 1000 / 2000: loss 2.302607\n",
      "iteration 1100 / 2000: loss 2.302605\n",
      "iteration 1200 / 2000: loss 2.302604\n",
      "iteration 1300 / 2000: loss 2.302607\n",
      "iteration 1400 / 2000: loss 2.302603\n",
      "iteration 1500 / 2000: loss 2.302607\n",
      "iteration 1600 / 2000: loss 2.302604\n",
      "iteration 1700 / 2000: loss 2.302605\n",
      "iteration 1800 / 2000: loss 2.302602\n",
      "iteration 1900 / 2000: loss 2.302602\n",
      "Hidden Size: 10, Learning Rate: 1e-07, LR Decay: 0.47250000000000003, Regularization: 0.30000000000000004, Validation accuracy: 0.0726\n",
      "iteration 0 / 2000: loss 2.302602\n",
      "iteration 100 / 2000: loss 2.302602\n",
      "iteration 200 / 2000: loss 2.302600\n",
      "iteration 300 / 2000: loss 2.302605\n",
      "iteration 400 / 2000: loss 2.302604\n",
      "iteration 500 / 2000: loss 2.302605\n",
      "iteration 600 / 2000: loss 2.302598\n",
      "iteration 700 / 2000: loss 2.302602\n",
      "iteration 800 / 2000: loss 2.302605\n",
      "iteration 900 / 2000: loss 2.302596\n",
      "iteration 1000 / 2000: loss 2.302602\n",
      "iteration 1100 / 2000: loss 2.302606\n",
      "iteration 1200 / 2000: loss 2.302606\n",
      "iteration 1300 / 2000: loss 2.302603\n",
      "iteration 1400 / 2000: loss 2.302607\n",
      "iteration 1500 / 2000: loss 2.302604\n",
      "iteration 1600 / 2000: loss 2.302603\n",
      "iteration 1700 / 2000: loss 2.302604\n",
      "iteration 1800 / 2000: loss 2.302606\n",
      "iteration 1900 / 2000: loss 2.302599\n",
      "Hidden Size: 10, Learning Rate: 1e-07, LR Decay: 0.47250000000000003, Regularization: 0.4, Validation accuracy: 0.089\n",
      "iteration 0 / 2000: loss 2.302587\n",
      "iteration 100 / 2000: loss 2.302587\n",
      "iteration 200 / 2000: loss 2.302590\n",
      "iteration 300 / 2000: loss 2.302590\n",
      "iteration 400 / 2000: loss 2.302587\n",
      "iteration 500 / 2000: loss 2.302586\n",
      "iteration 600 / 2000: loss 2.302588\n",
      "iteration 700 / 2000: loss 2.302588\n",
      "iteration 800 / 2000: loss 2.302586\n",
      "iteration 900 / 2000: loss 2.302588\n",
      "iteration 1000 / 2000: loss 2.302591\n",
      "iteration 1100 / 2000: loss 2.302589\n",
      "iteration 1200 / 2000: loss 2.302585\n",
      "iteration 1300 / 2000: loss 2.302587\n",
      "iteration 1400 / 2000: loss 2.302590\n",
      "iteration 1500 / 2000: loss 2.302589\n",
      "iteration 1600 / 2000: loss 2.302589\n",
      "iteration 1700 / 2000: loss 2.302588\n",
      "iteration 1800 / 2000: loss 2.302594\n",
      "iteration 1900 / 2000: loss 2.302588\n",
      "Hidden Size: 10, Learning Rate: 1e-07, LR Decay: 0.3, Regularization: 0.0, Validation accuracy: 0.0762\n",
      "iteration 0 / 2000: loss 2.302601\n",
      "iteration 100 / 2000: loss 2.302596\n",
      "iteration 200 / 2000: loss 2.302596\n",
      "iteration 300 / 2000: loss 2.302598\n",
      "iteration 400 / 2000: loss 2.302603\n",
      "iteration 500 / 2000: loss 2.302595\n",
      "iteration 600 / 2000: loss 2.302601\n",
      "iteration 700 / 2000: loss 2.302592\n",
      "iteration 800 / 2000: loss 2.302599\n",
      "iteration 900 / 2000: loss 2.302595\n",
      "iteration 1000 / 2000: loss 2.302597\n",
      "iteration 1100 / 2000: loss 2.302606\n",
      "iteration 1200 / 2000: loss 2.302603\n",
      "iteration 1300 / 2000: loss 2.302597\n",
      "iteration 1400 / 2000: loss 2.302600\n",
      "iteration 1500 / 2000: loss 2.302597\n",
      "iteration 1600 / 2000: loss 2.302593\n",
      "iteration 1700 / 2000: loss 2.302602\n",
      "iteration 1800 / 2000: loss 2.302598\n",
      "iteration 1900 / 2000: loss 2.302602\n",
      "Hidden Size: 10, Learning Rate: 1e-07, LR Decay: 0.3, Regularization: 0.1, Validation accuracy: 0.0822\n",
      "iteration 0 / 2000: loss 2.302589\n",
      "iteration 100 / 2000: loss 2.302592\n",
      "iteration 200 / 2000: loss 2.302593\n",
      "iteration 300 / 2000: loss 2.302591\n",
      "iteration 400 / 2000: loss 2.302591\n",
      "iteration 500 / 2000: loss 2.302594\n",
      "iteration 600 / 2000: loss 2.302591\n",
      "iteration 700 / 2000: loss 2.302589\n",
      "iteration 800 / 2000: loss 2.302588\n",
      "iteration 900 / 2000: loss 2.302595\n",
      "iteration 1000 / 2000: loss 2.302589\n",
      "iteration 1100 / 2000: loss 2.302592\n",
      "iteration 1200 / 2000: loss 2.302588\n",
      "iteration 1300 / 2000: loss 2.302591\n",
      "iteration 1400 / 2000: loss 2.302589\n",
      "iteration 1500 / 2000: loss 2.302589\n",
      "iteration 1600 / 2000: loss 2.302591\n",
      "iteration 1700 / 2000: loss 2.302590\n",
      "iteration 1800 / 2000: loss 2.302589\n",
      "iteration 1900 / 2000: loss 2.302587\n",
      "Hidden Size: 10, Learning Rate: 1e-07, LR Decay: 0.3, Regularization: 0.2, Validation accuracy: 0.0728\n",
      "iteration 0 / 2000: loss 2.302604\n",
      "iteration 100 / 2000: loss 2.302601\n",
      "iteration 200 / 2000: loss 2.302603\n",
      "iteration 300 / 2000: loss 2.302603\n",
      "iteration 400 / 2000: loss 2.302602\n",
      "iteration 500 / 2000: loss 2.302602\n",
      "iteration 600 / 2000: loss 2.302601\n",
      "iteration 700 / 2000: loss 2.302597\n",
      "iteration 800 / 2000: loss 2.302600\n",
      "iteration 900 / 2000: loss 2.302598\n",
      "iteration 1000 / 2000: loss 2.302605\n",
      "iteration 1100 / 2000: loss 2.302600\n",
      "iteration 1200 / 2000: loss 2.302603\n",
      "iteration 1300 / 2000: loss 2.302602\n",
      "iteration 1400 / 2000: loss 2.302599\n",
      "iteration 1500 / 2000: loss 2.302602\n",
      "iteration 1600 / 2000: loss 2.302603\n",
      "iteration 1700 / 2000: loss 2.302599\n",
      "iteration 1800 / 2000: loss 2.302600\n",
      "iteration 1900 / 2000: loss 2.302601\n",
      "Hidden Size: 10, Learning Rate: 1e-07, LR Decay: 0.3, Regularization: 0.30000000000000004, Validation accuracy: 0.0808\n",
      "iteration 0 / 2000: loss 2.302604\n",
      "iteration 100 / 2000: loss 2.302602\n",
      "iteration 200 / 2000: loss 2.302603\n",
      "iteration 300 / 2000: loss 2.302601\n",
      "iteration 400 / 2000: loss 2.302606\n",
      "iteration 500 / 2000: loss 2.302603\n",
      "iteration 600 / 2000: loss 2.302600\n",
      "iteration 700 / 2000: loss 2.302601\n",
      "iteration 800 / 2000: loss 2.302600\n",
      "iteration 900 / 2000: loss 2.302604\n",
      "iteration 1000 / 2000: loss 2.302602\n",
      "iteration 1100 / 2000: loss 2.302601\n",
      "iteration 1200 / 2000: loss 2.302603\n",
      "iteration 1300 / 2000: loss 2.302599\n",
      "iteration 1400 / 2000: loss 2.302600\n",
      "iteration 1500 / 2000: loss 2.302602\n",
      "iteration 1600 / 2000: loss 2.302594\n",
      "iteration 1700 / 2000: loss 2.302599\n",
      "iteration 1800 / 2000: loss 2.302604\n",
      "iteration 1900 / 2000: loss 2.302601\n",
      "Hidden Size: 10, Learning Rate: 1e-07, LR Decay: 0.3, Regularization: 0.4, Validation accuracy: 0.0992\n",
      "iteration 0 / 2000: loss 2.302590\n",
      "iteration 100 / 2000: loss 0.306185\n",
      "iteration 200 / 2000: loss 0.306613\n",
      "iteration 300 / 2000: loss 0.265229\n",
      "iteration 400 / 2000: loss 0.124113\n",
      "iteration 500 / 2000: loss 0.276928\n",
      "iteration 600 / 2000: loss 0.358888\n",
      "iteration 700 / 2000: loss 0.217173\n",
      "iteration 800 / 2000: loss 0.192195\n",
      "iteration 900 / 2000: loss 0.129689\n",
      "iteration 1000 / 2000: loss 0.134649\n",
      "iteration 1100 / 2000: loss 0.122964\n",
      "iteration 1200 / 2000: loss 0.175724\n",
      "iteration 1300 / 2000: loss 0.172537\n",
      "iteration 1400 / 2000: loss 0.162414\n",
      "iteration 1500 / 2000: loss 0.150901\n",
      "iteration 1600 / 2000: loss 0.138909\n",
      "iteration 1700 / 2000: loss 0.171030\n",
      "iteration 1800 / 2000: loss 0.224098\n",
      "iteration 1900 / 2000: loss 0.130119\n",
      "Hidden Size: 20, Learning Rate: 0.01, LR Decay: 0.99, Regularization: 0.0, Validation accuracy: 0.9488\n",
      "iteration 0 / 2000: loss 2.302594\n",
      "iteration 100 / 2000: loss 0.358524\n",
      "iteration 200 / 2000: loss 0.183452\n",
      "iteration 300 / 2000: loss 0.289038\n",
      "iteration 400 / 2000: loss 0.180555\n",
      "iteration 500 / 2000: loss 0.264166\n",
      "iteration 600 / 2000: loss 0.266748\n",
      "iteration 700 / 2000: loss 0.236241\n",
      "iteration 800 / 2000: loss 0.205478\n",
      "iteration 900 / 2000: loss 0.221418\n",
      "iteration 1000 / 2000: loss 0.262341\n",
      "iteration 1100 / 2000: loss 0.185236\n",
      "iteration 1200 / 2000: loss 0.146096\n",
      "iteration 1300 / 2000: loss 0.187820\n",
      "iteration 1400 / 2000: loss 0.204642\n",
      "iteration 1500 / 2000: loss 0.199948\n",
      "iteration 1600 / 2000: loss 0.197357\n",
      "iteration 1700 / 2000: loss 0.237713\n",
      "iteration 1800 / 2000: loss 0.225527\n",
      "iteration 1900 / 2000: loss 0.202777\n",
      "Hidden Size: 20, Learning Rate: 0.01, LR Decay: 0.99, Regularization: 0.1, Validation accuracy: 0.951\n",
      "iteration 0 / 2000: loss 2.302599\n",
      "iteration 100 / 2000: loss 0.320640\n",
      "iteration 200 / 2000: loss 0.360933\n",
      "iteration 300 / 2000: loss 0.280748\n",
      "iteration 400 / 2000: loss 0.185601\n",
      "iteration 500 / 2000: loss 0.267475\n",
      "iteration 600 / 2000: loss 0.251109\n",
      "iteration 700 / 2000: loss 0.154839\n",
      "iteration 800 / 2000: loss 0.338667\n",
      "iteration 900 / 2000: loss 0.264746\n",
      "iteration 1000 / 2000: loss 0.255855\n",
      "iteration 1100 / 2000: loss 0.248318\n",
      "iteration 1200 / 2000: loss 0.244519\n",
      "iteration 1300 / 2000: loss 0.225631\n",
      "iteration 1400 / 2000: loss 0.169122\n",
      "iteration 1500 / 2000: loss 0.240183\n",
      "iteration 1600 / 2000: loss 0.186277\n",
      "iteration 1700 / 2000: loss 0.183563\n",
      "iteration 1800 / 2000: loss 0.223727\n",
      "iteration 1900 / 2000: loss 0.238230\n",
      "Hidden Size: 20, Learning Rate: 0.01, LR Decay: 0.99, Regularization: 0.2, Validation accuracy: 0.9492\n",
      "iteration 0 / 2000: loss 2.302611\n",
      "iteration 100 / 2000: loss 0.298185\n",
      "iteration 200 / 2000: loss 0.187825\n",
      "iteration 300 / 2000: loss 0.347139\n",
      "iteration 400 / 2000: loss 0.267439\n",
      "iteration 500 / 2000: loss 0.640084\n",
      "iteration 600 / 2000: loss 0.263575\n",
      "iteration 700 / 2000: loss 0.289009\n",
      "iteration 800 / 2000: loss 0.252696\n",
      "iteration 900 / 2000: loss 0.297762\n",
      "iteration 1000 / 2000: loss 0.228162\n",
      "iteration 1100 / 2000: loss 0.211778\n",
      "iteration 1200 / 2000: loss 0.254177\n",
      "iteration 1300 / 2000: loss 0.229122\n",
      "iteration 1400 / 2000: loss 0.324949\n",
      "iteration 1500 / 2000: loss 0.214449\n",
      "iteration 1600 / 2000: loss 0.369505\n",
      "iteration 1700 / 2000: loss 0.256119\n",
      "iteration 1800 / 2000: loss 0.172896\n",
      "iteration 1900 / 2000: loss 0.339101\n",
      "Hidden Size: 20, Learning Rate: 0.01, LR Decay: 0.99, Regularization: 0.30000000000000004, Validation accuracy: 0.9484\n",
      "iteration 0 / 2000: loss 2.302624\n",
      "iteration 100 / 2000: loss 0.490355\n",
      "iteration 200 / 2000: loss 0.294042\n",
      "iteration 300 / 2000: loss 0.231980\n",
      "iteration 400 / 2000: loss 0.309718\n",
      "iteration 500 / 2000: loss 0.298045\n",
      "iteration 600 / 2000: loss 0.433085\n",
      "iteration 700 / 2000: loss 0.291049\n",
      "iteration 800 / 2000: loss 0.227606\n",
      "iteration 900 / 2000: loss 0.238959\n",
      "iteration 1000 / 2000: loss 0.316465\n",
      "iteration 1100 / 2000: loss 0.282558\n",
      "iteration 1200 / 2000: loss 0.348868\n",
      "iteration 1300 / 2000: loss 0.341972\n",
      "iteration 1400 / 2000: loss 0.298615\n",
      "iteration 1500 / 2000: loss 0.317427\n",
      "iteration 1600 / 2000: loss 0.271347\n",
      "iteration 1700 / 2000: loss 0.356037\n",
      "iteration 1800 / 2000: loss 0.225678\n",
      "iteration 1900 / 2000: loss 0.269230\n",
      "Hidden Size: 20, Learning Rate: 0.01, LR Decay: 0.99, Regularization: 0.4, Validation accuracy: 0.9472\n",
      "iteration 0 / 2000: loss 2.302582\n",
      "iteration 100 / 2000: loss 0.313569\n",
      "iteration 200 / 2000: loss 0.158887\n",
      "iteration 300 / 2000: loss 0.168445\n",
      "iteration 400 / 2000: loss 0.153543\n",
      "iteration 500 / 2000: loss 0.173319\n",
      "iteration 600 / 2000: loss 0.077849\n",
      "iteration 700 / 2000: loss 0.097982\n",
      "iteration 800 / 2000: loss 0.097966\n",
      "iteration 900 / 2000: loss 0.099111\n",
      "iteration 1000 / 2000: loss 0.124598\n",
      "iteration 1100 / 2000: loss 0.093913\n",
      "iteration 1200 / 2000: loss 0.084685\n",
      "iteration 1300 / 2000: loss 0.093206\n",
      "iteration 1400 / 2000: loss 0.111242\n",
      "iteration 1500 / 2000: loss 0.072894\n",
      "iteration 1600 / 2000: loss 0.097388\n",
      "iteration 1700 / 2000: loss 0.150178\n",
      "iteration 1800 / 2000: loss 0.112577\n",
      "iteration 1900 / 2000: loss 0.039454\n",
      "Hidden Size: 20, Learning Rate: 0.01, LR Decay: 0.8175, Regularization: 0.0, Validation accuracy: 0.9626\n",
      "iteration 0 / 2000: loss 2.302587\n",
      "iteration 100 / 2000: loss 0.257284\n",
      "iteration 200 / 2000: loss 0.257301\n",
      "iteration 300 / 2000: loss 0.241033\n",
      "iteration 400 / 2000: loss 0.172925\n",
      "iteration 500 / 2000: loss 0.175791\n",
      "iteration 600 / 2000: loss 0.228233\n",
      "iteration 700 / 2000: loss 0.158912\n",
      "iteration 800 / 2000: loss 0.205435\n",
      "iteration 900 / 2000: loss 0.168831\n",
      "iteration 1000 / 2000: loss 0.186422\n",
      "iteration 1100 / 2000: loss 0.152449\n",
      "iteration 1200 / 2000: loss 0.129525\n",
      "iteration 1300 / 2000: loss 0.185478\n",
      "iteration 1400 / 2000: loss 0.153860\n",
      "iteration 1500 / 2000: loss 0.211656\n",
      "iteration 1600 / 2000: loss 0.111738\n",
      "iteration 1700 / 2000: loss 0.117791\n",
      "iteration 1800 / 2000: loss 0.134032\n",
      "iteration 1900 / 2000: loss 0.093589\n",
      "Hidden Size: 20, Learning Rate: 0.01, LR Decay: 0.8175, Regularization: 0.1, Validation accuracy: 0.964\n",
      "iteration 0 / 2000: loss 2.302599\n",
      "iteration 100 / 2000: loss 0.385341\n",
      "iteration 200 / 2000: loss 0.343535\n",
      "iteration 300 / 2000: loss 0.265351\n",
      "iteration 400 / 2000: loss 0.315775\n",
      "iteration 500 / 2000: loss 0.348821\n",
      "iteration 600 / 2000: loss 0.322225\n",
      "iteration 700 / 2000: loss 0.221544\n",
      "iteration 800 / 2000: loss 0.276658\n",
      "iteration 900 / 2000: loss 0.209804\n",
      "iteration 1000 / 2000: loss 0.209862\n",
      "iteration 1100 / 2000: loss 0.175929\n",
      "iteration 1200 / 2000: loss 0.240790\n",
      "iteration 1300 / 2000: loss 0.244970\n",
      "iteration 1400 / 2000: loss 0.154841\n",
      "iteration 1500 / 2000: loss 0.185119\n",
      "iteration 1600 / 2000: loss 0.216954\n",
      "iteration 1700 / 2000: loss 0.170218\n",
      "iteration 1800 / 2000: loss 0.176190\n",
      "iteration 1900 / 2000: loss 0.175700\n",
      "Hidden Size: 20, Learning Rate: 0.01, LR Decay: 0.8175, Regularization: 0.2, Validation accuracy: 0.9632\n",
      "iteration 0 / 2000: loss 2.302614\n",
      "iteration 100 / 2000: loss 0.261369\n",
      "iteration 200 / 2000: loss 0.260278\n",
      "iteration 300 / 2000: loss 0.284309\n",
      "iteration 400 / 2000: loss 0.224450\n",
      "iteration 500 / 2000: loss 0.246756\n",
      "iteration 600 / 2000: loss 0.281740\n",
      "iteration 700 / 2000: loss 0.277363\n",
      "iteration 800 / 2000: loss 0.212773\n",
      "iteration 900 / 2000: loss 0.266503\n",
      "iteration 1000 / 2000: loss 0.229309\n",
      "iteration 1100 / 2000: loss 0.246468\n",
      "iteration 1200 / 2000: loss 0.290712\n",
      "iteration 1300 / 2000: loss 0.292307\n",
      "iteration 1400 / 2000: loss 0.261317\n",
      "iteration 1500 / 2000: loss 0.223987\n",
      "iteration 1600 / 2000: loss 0.250105\n",
      "iteration 1700 / 2000: loss 0.232221\n",
      "iteration 1800 / 2000: loss 0.327231\n",
      "iteration 1900 / 2000: loss 0.210011\n",
      "Hidden Size: 20, Learning Rate: 0.01, LR Decay: 0.8175, Regularization: 0.30000000000000004, Validation accuracy: 0.9598\n",
      "iteration 0 / 2000: loss 2.302629\n",
      "iteration 100 / 2000: loss 0.336541\n",
      "iteration 200 / 2000: loss 0.279826\n",
      "iteration 300 / 2000: loss 0.267695\n",
      "iteration 400 / 2000: loss 0.268717\n",
      "iteration 500 / 2000: loss 0.343004\n",
      "iteration 600 / 2000: loss 0.250966\n",
      "iteration 700 / 2000: loss 0.284231\n",
      "iteration 800 / 2000: loss 0.230947\n",
      "iteration 900 / 2000: loss 0.246991\n",
      "iteration 1000 / 2000: loss 0.237172\n",
      "iteration 1100 / 2000: loss 0.215424\n",
      "iteration 1200 / 2000: loss 0.215736\n",
      "iteration 1300 / 2000: loss 0.254692\n",
      "iteration 1400 / 2000: loss 0.195769\n",
      "iteration 1500 / 2000: loss 0.279533\n",
      "iteration 1600 / 2000: loss 0.253564\n",
      "iteration 1700 / 2000: loss 0.240617\n",
      "iteration 1800 / 2000: loss 0.286733\n",
      "iteration 1900 / 2000: loss 0.224074\n",
      "Hidden Size: 20, Learning Rate: 0.01, LR Decay: 0.8175, Regularization: 0.4, Validation accuracy: 0.9608\n",
      "iteration 0 / 2000: loss 2.302581\n",
      "iteration 100 / 2000: loss 0.305985\n",
      "iteration 200 / 2000: loss 0.237910\n",
      "iteration 300 / 2000: loss 0.077352\n",
      "iteration 400 / 2000: loss 0.204443\n",
      "iteration 500 / 2000: loss 0.218046\n",
      "iteration 600 / 2000: loss 0.190099\n",
      "iteration 700 / 2000: loss 0.158176\n",
      "iteration 800 / 2000: loss 0.105829\n",
      "iteration 900 / 2000: loss 0.130741\n",
      "iteration 1000 / 2000: loss 0.197003\n",
      "iteration 1100 / 2000: loss 0.094518\n",
      "iteration 1200 / 2000: loss 0.115850\n",
      "iteration 1300 / 2000: loss 0.133741\n",
      "iteration 1400 / 2000: loss 0.134547\n",
      "iteration 1500 / 2000: loss 0.110224\n",
      "iteration 1600 / 2000: loss 0.084544\n",
      "iteration 1700 / 2000: loss 0.186375\n",
      "iteration 1800 / 2000: loss 0.100540\n",
      "iteration 1900 / 2000: loss 0.147651\n",
      "Hidden Size: 20, Learning Rate: 0.01, LR Decay: 0.645, Regularization: 0.0, Validation accuracy: 0.9626\n",
      "iteration 0 / 2000: loss 2.302584\n",
      "iteration 100 / 2000: loss 0.309497\n",
      "iteration 200 / 2000: loss 0.302658\n",
      "iteration 300 / 2000: loss 0.141588\n",
      "iteration 400 / 2000: loss 0.132690\n",
      "iteration 500 / 2000: loss 0.238186\n",
      "iteration 600 / 2000: loss 0.172841\n",
      "iteration 700 / 2000: loss 0.165060\n",
      "iteration 800 / 2000: loss 0.176975\n",
      "iteration 900 / 2000: loss 0.167904\n",
      "iteration 1000 / 2000: loss 0.093122\n",
      "iteration 1100 / 2000: loss 0.189763\n",
      "iteration 1200 / 2000: loss 0.077033\n",
      "iteration 1300 / 2000: loss 0.186505\n",
      "iteration 1400 / 2000: loss 0.112785\n",
      "iteration 1500 / 2000: loss 0.137911\n",
      "iteration 1600 / 2000: loss 0.133902\n",
      "iteration 1700 / 2000: loss 0.152000\n",
      "iteration 1800 / 2000: loss 0.095290\n",
      "iteration 1900 / 2000: loss 0.113605\n",
      "Hidden Size: 20, Learning Rate: 0.01, LR Decay: 0.645, Regularization: 0.1, Validation accuracy: 0.9594\n",
      "iteration 0 / 2000: loss 2.302597\n",
      "iteration 100 / 2000: loss 0.292381\n",
      "iteration 200 / 2000: loss 0.253424\n",
      "iteration 300 / 2000: loss 0.231895\n",
      "iteration 400 / 2000: loss 0.204367\n",
      "iteration 500 / 2000: loss 0.214042\n",
      "iteration 600 / 2000: loss 0.228373\n",
      "iteration 700 / 2000: loss 0.158766\n",
      "iteration 800 / 2000: loss 0.167535\n",
      "iteration 900 / 2000: loss 0.200339\n",
      "iteration 1000 / 2000: loss 0.211721\n",
      "iteration 1100 / 2000: loss 0.242314\n",
      "iteration 1200 / 2000: loss 0.142361\n",
      "iteration 1300 / 2000: loss 0.152769\n",
      "iteration 1400 / 2000: loss 0.175534\n",
      "iteration 1500 / 2000: loss 0.151253\n",
      "iteration 1600 / 2000: loss 0.196889\n",
      "iteration 1700 / 2000: loss 0.158522\n",
      "iteration 1800 / 2000: loss 0.148479\n",
      "iteration 1900 / 2000: loss 0.219131\n",
      "Hidden Size: 20, Learning Rate: 0.01, LR Decay: 0.645, Regularization: 0.2, Validation accuracy: 0.9608\n",
      "iteration 0 / 2000: loss 2.302610\n",
      "iteration 100 / 2000: loss 0.275208\n",
      "iteration 200 / 2000: loss 0.225698\n",
      "iteration 300 / 2000: loss 0.231882\n",
      "iteration 400 / 2000: loss 0.315151\n",
      "iteration 500 / 2000: loss 0.243019\n",
      "iteration 600 / 2000: loss 0.306221\n",
      "iteration 700 / 2000: loss 0.213177\n",
      "iteration 800 / 2000: loss 0.230134\n",
      "iteration 900 / 2000: loss 0.215593\n",
      "iteration 1000 / 2000: loss 0.172499\n",
      "iteration 1100 / 2000: loss 0.250492\n",
      "iteration 1200 / 2000: loss 0.304427\n",
      "iteration 1300 / 2000: loss 0.219728\n",
      "iteration 1400 / 2000: loss 0.183796\n",
      "iteration 1500 / 2000: loss 0.201638\n",
      "iteration 1600 / 2000: loss 0.263697\n",
      "iteration 1700 / 2000: loss 0.268410\n",
      "iteration 1800 / 2000: loss 0.213990\n",
      "iteration 1900 / 2000: loss 0.178671\n",
      "Hidden Size: 20, Learning Rate: 0.01, LR Decay: 0.645, Regularization: 0.30000000000000004, Validation accuracy: 0.9566\n",
      "iteration 0 / 2000: loss 2.302611\n",
      "iteration 100 / 2000: loss 0.343331\n",
      "iteration 200 / 2000: loss 0.291538\n",
      "iteration 300 / 2000: loss 0.280516\n",
      "iteration 400 / 2000: loss 0.361591\n",
      "iteration 500 / 2000: loss 0.275173\n",
      "iteration 600 / 2000: loss 0.293144\n",
      "iteration 700 / 2000: loss 0.174076\n",
      "iteration 800 / 2000: loss 0.269593\n",
      "iteration 900 / 2000: loss 0.174788\n",
      "iteration 1000 / 2000: loss 0.224049\n",
      "iteration 1100 / 2000: loss 0.265707\n",
      "iteration 1200 / 2000: loss 0.262785\n",
      "iteration 1300 / 2000: loss 0.251641\n",
      "iteration 1400 / 2000: loss 0.323774\n",
      "iteration 1500 / 2000: loss 0.249374\n",
      "iteration 1600 / 2000: loss 0.196231\n",
      "iteration 1700 / 2000: loss 0.287476\n",
      "iteration 1800 / 2000: loss 0.263840\n",
      "iteration 1900 / 2000: loss 0.257734\n",
      "Hidden Size: 20, Learning Rate: 0.01, LR Decay: 0.645, Regularization: 0.4, Validation accuracy: 0.9596\n",
      "iteration 0 / 2000: loss 2.302591\n",
      "iteration 100 / 2000: loss 0.408888\n",
      "iteration 200 / 2000: loss 0.259328\n",
      "iteration 300 / 2000: loss 0.178914\n",
      "iteration 400 / 2000: loss 0.219618\n",
      "iteration 500 / 2000: loss 0.206354\n",
      "iteration 600 / 2000: loss 0.103714\n",
      "iteration 700 / 2000: loss 0.170945\n",
      "iteration 800 / 2000: loss 0.183321\n",
      "iteration 900 / 2000: loss 0.216787\n",
      "iteration 1000 / 2000: loss 0.194696\n",
      "iteration 1100 / 2000: loss 0.108641\n",
      "iteration 1200 / 2000: loss 0.140642\n",
      "iteration 1300 / 2000: loss 0.146950\n",
      "iteration 1400 / 2000: loss 0.107225\n",
      "iteration 1500 / 2000: loss 0.145616\n",
      "iteration 1600 / 2000: loss 0.141929\n",
      "iteration 1700 / 2000: loss 0.226914\n",
      "iteration 1800 / 2000: loss 0.161134\n",
      "iteration 1900 / 2000: loss 0.097807\n",
      "Hidden Size: 20, Learning Rate: 0.01, LR Decay: 0.47250000000000003, Regularization: 0.0, Validation accuracy: 0.9574\n",
      "iteration 0 / 2000: loss 2.302604\n",
      "iteration 100 / 2000: loss 0.369771\n",
      "iteration 200 / 2000: loss 0.239333\n",
      "iteration 300 / 2000: loss 0.206390\n",
      "iteration 400 / 2000: loss 0.271737\n",
      "iteration 500 / 2000: loss 0.252617\n",
      "iteration 600 / 2000: loss 0.172401\n",
      "iteration 700 / 2000: loss 0.181601\n",
      "iteration 800 / 2000: loss 0.225924\n",
      "iteration 900 / 2000: loss 0.171745\n",
      "iteration 1000 / 2000: loss 0.102381\n",
      "iteration 1100 / 2000: loss 0.223842\n",
      "iteration 1200 / 2000: loss 0.201878\n",
      "iteration 1300 / 2000: loss 0.158734\n",
      "iteration 1400 / 2000: loss 0.164644\n",
      "iteration 1500 / 2000: loss 0.190137\n",
      "iteration 1600 / 2000: loss 0.092648\n",
      "iteration 1700 / 2000: loss 0.121170\n",
      "iteration 1800 / 2000: loss 0.177992\n",
      "iteration 1900 / 2000: loss 0.180880\n",
      "Hidden Size: 20, Learning Rate: 0.01, LR Decay: 0.47250000000000003, Regularization: 0.1, Validation accuracy: 0.9522\n",
      "iteration 0 / 2000: loss 2.302615\n",
      "iteration 100 / 2000: loss 0.289556\n",
      "iteration 200 / 2000: loss 0.213674\n",
      "iteration 300 / 2000: loss 0.261788\n",
      "iteration 400 / 2000: loss 0.271720\n",
      "iteration 500 / 2000: loss 0.205935\n",
      "iteration 600 / 2000: loss 0.214572\n",
      "iteration 700 / 2000: loss 0.240541\n",
      "iteration 800 / 2000: loss 0.237914\n",
      "iteration 900 / 2000: loss 0.239487\n",
      "iteration 1000 / 2000: loss 0.181451\n",
      "iteration 1100 / 2000: loss 0.262905\n",
      "iteration 1200 / 2000: loss 0.232773\n",
      "iteration 1300 / 2000: loss 0.165557\n",
      "iteration 1400 / 2000: loss 0.165717\n",
      "iteration 1500 / 2000: loss 0.229171\n",
      "iteration 1600 / 2000: loss 0.267335\n",
      "iteration 1700 / 2000: loss 0.201074\n",
      "iteration 1800 / 2000: loss 0.154280\n",
      "iteration 1900 / 2000: loss 0.175879\n",
      "Hidden Size: 20, Learning Rate: 0.01, LR Decay: 0.47250000000000003, Regularization: 0.2, Validation accuracy: 0.9568\n",
      "iteration 0 / 2000: loss 2.302606\n",
      "iteration 100 / 2000: loss 0.382487\n",
      "iteration 200 / 2000: loss 0.261817\n",
      "iteration 300 / 2000: loss 0.222725\n",
      "iteration 400 / 2000: loss 0.237127\n",
      "iteration 500 / 2000: loss 0.229502\n",
      "iteration 600 / 2000: loss 0.249088\n",
      "iteration 700 / 2000: loss 0.236826\n",
      "iteration 800 / 2000: loss 0.236278\n",
      "iteration 900 / 2000: loss 0.251347\n",
      "iteration 1000 / 2000: loss 0.280776\n",
      "iteration 1100 / 2000: loss 0.198276\n",
      "iteration 1200 / 2000: loss 0.218618\n",
      "iteration 1300 / 2000: loss 0.270061\n",
      "iteration 1400 / 2000: loss 0.179716\n",
      "iteration 1500 / 2000: loss 0.248872\n",
      "iteration 1600 / 2000: loss 0.234760\n",
      "iteration 1700 / 2000: loss 0.216245\n",
      "iteration 1800 / 2000: loss 0.276710\n",
      "iteration 1900 / 2000: loss 0.203688\n",
      "Hidden Size: 20, Learning Rate: 0.01, LR Decay: 0.47250000000000003, Regularization: 0.30000000000000004, Validation accuracy: 0.9562\n",
      "iteration 0 / 2000: loss 2.302616\n",
      "iteration 100 / 2000: loss 0.319349\n",
      "iteration 200 / 2000: loss 0.344779\n",
      "iteration 300 / 2000: loss 0.364752\n",
      "iteration 400 / 2000: loss 0.254240\n",
      "iteration 500 / 2000: loss 0.329377\n",
      "iteration 600 / 2000: loss 0.252375\n",
      "iteration 700 / 2000: loss 0.296974\n",
      "iteration 800 / 2000: loss 0.243806\n",
      "iteration 900 / 2000: loss 0.244248\n",
      "iteration 1000 / 2000: loss 0.296393\n",
      "iteration 1100 / 2000: loss 0.298170\n",
      "iteration 1200 / 2000: loss 0.211048\n",
      "iteration 1300 / 2000: loss 0.313497\n",
      "iteration 1400 / 2000: loss 0.228751\n",
      "iteration 1500 / 2000: loss 0.209867\n",
      "iteration 1600 / 2000: loss 0.225378\n",
      "iteration 1700 / 2000: loss 0.156786\n",
      "iteration 1800 / 2000: loss 0.222356\n",
      "iteration 1900 / 2000: loss 0.242126\n",
      "Hidden Size: 20, Learning Rate: 0.01, LR Decay: 0.47250000000000003, Regularization: 0.4, Validation accuracy: 0.9542\n",
      "iteration 0 / 2000: loss 2.302595\n",
      "iteration 100 / 2000: loss 0.245787\n",
      "iteration 200 / 2000: loss 0.320369\n",
      "iteration 300 / 2000: loss 0.200792\n",
      "iteration 400 / 2000: loss 0.198491\n",
      "iteration 500 / 2000: loss 0.220895\n",
      "iteration 600 / 2000: loss 0.180713\n",
      "iteration 700 / 2000: loss 0.269963\n",
      "iteration 800 / 2000: loss 0.192604\n",
      "iteration 900 / 2000: loss 0.125871\n",
      "iteration 1000 / 2000: loss 0.288406\n",
      "iteration 1100 / 2000: loss 0.214309\n",
      "iteration 1200 / 2000: loss 0.185757\n",
      "iteration 1300 / 2000: loss 0.211875\n",
      "iteration 1400 / 2000: loss 0.243386\n",
      "iteration 1500 / 2000: loss 0.161290\n",
      "iteration 1600 / 2000: loss 0.218444\n",
      "iteration 1700 / 2000: loss 0.143436\n",
      "iteration 1800 / 2000: loss 0.209723\n",
      "iteration 1900 / 2000: loss 0.230529\n",
      "Hidden Size: 20, Learning Rate: 0.01, LR Decay: 0.3, Regularization: 0.0, Validation accuracy: 0.9412\n",
      "iteration 0 / 2000: loss 2.302596\n",
      "iteration 100 / 2000: loss 0.247321\n",
      "iteration 200 / 2000: loss 0.193397\n",
      "iteration 300 / 2000: loss 0.263228\n",
      "iteration 400 / 2000: loss 0.296994\n",
      "iteration 500 / 2000: loss 0.199751\n",
      "iteration 600 / 2000: loss 0.262067\n",
      "iteration 700 / 2000: loss 0.194560\n",
      "iteration 800 / 2000: loss 0.190799\n",
      "iteration 900 / 2000: loss 0.211598\n",
      "iteration 1000 / 2000: loss 0.266014\n",
      "iteration 1100 / 2000: loss 0.326335\n",
      "iteration 1200 / 2000: loss 0.183655\n",
      "iteration 1300 / 2000: loss 0.254837\n",
      "iteration 1400 / 2000: loss 0.136107\n",
      "iteration 1500 / 2000: loss 0.191682\n",
      "iteration 1600 / 2000: loss 0.164985\n",
      "iteration 1700 / 2000: loss 0.312392\n",
      "iteration 1800 / 2000: loss 0.220223\n",
      "iteration 1900 / 2000: loss 0.209511\n",
      "Hidden Size: 20, Learning Rate: 0.01, LR Decay: 0.3, Regularization: 0.1, Validation accuracy: 0.9446\n",
      "iteration 0 / 2000: loss 2.302602\n",
      "iteration 100 / 2000: loss 0.421375\n",
      "iteration 200 / 2000: loss 0.277425\n",
      "iteration 300 / 2000: loss 0.250948\n",
      "iteration 400 / 2000: loss 0.257307\n",
      "iteration 500 / 2000: loss 0.200537\n",
      "iteration 600 / 2000: loss 0.328281\n",
      "iteration 700 / 2000: loss 0.214024\n",
      "iteration 800 / 2000: loss 0.252338\n",
      "iteration 900 / 2000: loss 0.167401\n",
      "iteration 1000 / 2000: loss 0.265016\n",
      "iteration 1100 / 2000: loss 0.275651\n",
      "iteration 1200 / 2000: loss 0.275453\n",
      "iteration 1300 / 2000: loss 0.219485\n",
      "iteration 1400 / 2000: loss 0.283354\n",
      "iteration 1500 / 2000: loss 0.237043\n",
      "iteration 1600 / 2000: loss 0.290784\n",
      "iteration 1700 / 2000: loss 0.250009\n",
      "iteration 1800 / 2000: loss 0.256842\n",
      "iteration 1900 / 2000: loss 0.237439\n",
      "Hidden Size: 20, Learning Rate: 0.01, LR Decay: 0.3, Regularization: 0.2, Validation accuracy: 0.9436\n",
      "iteration 0 / 2000: loss 2.302627\n",
      "iteration 100 / 2000: loss 0.390323\n",
      "iteration 200 / 2000: loss 0.267343\n",
      "iteration 300 / 2000: loss 0.364026\n",
      "iteration 400 / 2000: loss 0.250572\n",
      "iteration 500 / 2000: loss 0.283265\n",
      "iteration 600 / 2000: loss 0.267749\n",
      "iteration 700 / 2000: loss 0.312179\n",
      "iteration 800 / 2000: loss 0.325439\n",
      "iteration 900 / 2000: loss 0.343981\n",
      "iteration 1000 / 2000: loss 0.244298\n",
      "iteration 1100 / 2000: loss 0.254186\n",
      "iteration 1200 / 2000: loss 0.266284\n",
      "iteration 1300 / 2000: loss 0.304289\n",
      "iteration 1400 / 2000: loss 0.241180\n",
      "iteration 1500 / 2000: loss 0.278240\n",
      "iteration 1600 / 2000: loss 0.308575\n",
      "iteration 1700 / 2000: loss 0.321107\n",
      "iteration 1800 / 2000: loss 0.265441\n",
      "iteration 1900 / 2000: loss 0.284903\n",
      "Hidden Size: 20, Learning Rate: 0.01, LR Decay: 0.3, Regularization: 0.30000000000000004, Validation accuracy: 0.9418\n",
      "iteration 0 / 2000: loss 2.302626\n",
      "iteration 100 / 2000: loss 0.496521\n",
      "iteration 200 / 2000: loss 0.296736\n",
      "iteration 300 / 2000: loss 0.397604\n",
      "iteration 400 / 2000: loss 0.399920\n",
      "iteration 500 / 2000: loss 0.286932\n",
      "iteration 600 / 2000: loss 0.281886\n",
      "iteration 700 / 2000: loss 0.265198\n",
      "iteration 800 / 2000: loss 0.316974\n",
      "iteration 900 / 2000: loss 0.253625\n",
      "iteration 1000 / 2000: loss 0.250906\n",
      "iteration 1100 / 2000: loss 0.235565\n",
      "iteration 1200 / 2000: loss 0.256702\n",
      "iteration 1300 / 2000: loss 0.286051\n",
      "iteration 1400 / 2000: loss 0.246536\n",
      "iteration 1500 / 2000: loss 0.218632\n",
      "iteration 1600 / 2000: loss 0.386626\n",
      "iteration 1700 / 2000: loss 0.234099\n",
      "iteration 1800 / 2000: loss 0.311465\n",
      "iteration 1900 / 2000: loss 0.219844\n",
      "Hidden Size: 20, Learning Rate: 0.01, LR Decay: 0.3, Regularization: 0.4, Validation accuracy: 0.9444\n",
      "iteration 0 / 2000: loss 2.302587\n",
      "iteration 100 / 2000: loss 0.234492\n",
      "iteration 200 / 2000: loss 0.226246\n",
      "iteration 300 / 2000: loss 0.264303\n",
      "iteration 400 / 2000: loss 0.101612\n",
      "iteration 500 / 2000: loss 0.231187\n",
      "iteration 600 / 2000: loss 0.238417\n",
      "iteration 700 / 2000: loss 0.105008\n",
      "iteration 800 / 2000: loss 0.154577\n",
      "iteration 900 / 2000: loss 0.134918\n",
      "iteration 1000 / 2000: loss 0.203675\n",
      "iteration 1100 / 2000: loss 0.124829\n",
      "iteration 1200 / 2000: loss 0.081638\n",
      "iteration 1300 / 2000: loss 0.091165\n",
      "iteration 1400 / 2000: loss 0.145965\n",
      "iteration 1500 / 2000: loss 0.176434\n",
      "iteration 1600 / 2000: loss 0.076560\n",
      "iteration 1700 / 2000: loss 0.074659\n",
      "iteration 1800 / 2000: loss 0.125806\n",
      "iteration 1900 / 2000: loss 0.047910\n",
      "Hidden Size: 20, Learning Rate: 0.0075000250000000004, LR Decay: 0.99, Regularization: 0.0, Validation accuracy: 0.9538\n",
      "iteration 0 / 2000: loss 2.302586\n",
      "iteration 100 / 2000: loss 0.365072\n",
      "iteration 200 / 2000: loss 0.200366\n",
      "iteration 300 / 2000: loss 0.302870\n",
      "iteration 400 / 2000: loss 0.156591\n",
      "iteration 500 / 2000: loss 0.169094\n",
      "iteration 600 / 2000: loss 0.189413\n",
      "iteration 700 / 2000: loss 0.204130\n",
      "iteration 800 / 2000: loss 0.333709\n",
      "iteration 900 / 2000: loss 0.282249\n",
      "iteration 1000 / 2000: loss 0.217160\n",
      "iteration 1100 / 2000: loss 0.196548\n",
      "iteration 1200 / 2000: loss 0.215120\n",
      "iteration 1300 / 2000: loss 0.226045\n",
      "iteration 1400 / 2000: loss 0.217622\n",
      "iteration 1500 / 2000: loss 0.164957\n",
      "iteration 1600 / 2000: loss 0.240640\n",
      "iteration 1700 / 2000: loss 0.100953\n",
      "iteration 1800 / 2000: loss 0.172720\n",
      "iteration 1900 / 2000: loss 0.196589\n",
      "Hidden Size: 20, Learning Rate: 0.0075000250000000004, LR Decay: 0.99, Regularization: 0.1, Validation accuracy: 0.9612\n",
      "iteration 0 / 2000: loss 2.302587\n",
      "iteration 100 / 2000: loss 0.362520\n",
      "iteration 200 / 2000: loss 0.288483\n",
      "iteration 300 / 2000: loss 0.284501\n",
      "iteration 400 / 2000: loss 0.255942\n",
      "iteration 500 / 2000: loss 0.235572\n",
      "iteration 600 / 2000: loss 0.208257\n",
      "iteration 700 / 2000: loss 0.260698\n",
      "iteration 800 / 2000: loss 0.249416\n",
      "iteration 900 / 2000: loss 0.245113\n",
      "iteration 1000 / 2000: loss 0.287553\n",
      "iteration 1100 / 2000: loss 0.253665\n",
      "iteration 1200 / 2000: loss 0.248710\n",
      "iteration 1300 / 2000: loss 0.221901\n",
      "iteration 1400 / 2000: loss 0.206674\n",
      "iteration 1500 / 2000: loss 0.222030\n",
      "iteration 1600 / 2000: loss 0.217688\n",
      "iteration 1700 / 2000: loss 0.208065\n",
      "iteration 1800 / 2000: loss 0.261402\n",
      "iteration 1900 / 2000: loss 0.251857\n",
      "Hidden Size: 20, Learning Rate: 0.0075000250000000004, LR Decay: 0.99, Regularization: 0.2, Validation accuracy: 0.9542\n",
      "iteration 0 / 2000: loss 2.302620\n",
      "iteration 100 / 2000: loss 0.383107\n",
      "iteration 200 / 2000: loss 0.294160\n",
      "iteration 300 / 2000: loss 0.246700\n",
      "iteration 400 / 2000: loss 0.275492\n",
      "iteration 500 / 2000: loss 0.194040\n",
      "iteration 600 / 2000: loss 0.172547\n",
      "iteration 700 / 2000: loss 0.189758\n",
      "iteration 800 / 2000: loss 0.275609\n",
      "iteration 900 / 2000: loss 0.269600\n",
      "iteration 1000 / 2000: loss 0.293038\n",
      "iteration 1100 / 2000: loss 0.236236\n",
      "iteration 1200 / 2000: loss 0.309313\n",
      "iteration 1300 / 2000: loss 0.184436\n",
      "iteration 1400 / 2000: loss 0.185897\n",
      "iteration 1500 / 2000: loss 0.316404\n",
      "iteration 1600 / 2000: loss 0.330306\n",
      "iteration 1700 / 2000: loss 0.183938\n",
      "iteration 1800 / 2000: loss 0.169247\n",
      "iteration 1900 / 2000: loss 0.224677\n",
      "Hidden Size: 20, Learning Rate: 0.0075000250000000004, LR Decay: 0.99, Regularization: 0.30000000000000004, Validation accuracy: 0.9546\n",
      "iteration 0 / 2000: loss 2.302623\n",
      "iteration 100 / 2000: loss 0.360228\n",
      "iteration 200 / 2000: loss 0.304162\n",
      "iteration 300 / 2000: loss 0.319662\n",
      "iteration 400 / 2000: loss 0.290063\n",
      "iteration 500 / 2000: loss 0.244553\n",
      "iteration 600 / 2000: loss 0.198837\n",
      "iteration 700 / 2000: loss 0.289431\n",
      "iteration 800 / 2000: loss 0.243796\n",
      "iteration 900 / 2000: loss 0.238660\n",
      "iteration 1000 / 2000: loss 0.244676\n",
      "iteration 1100 / 2000: loss 0.253490\n",
      "iteration 1200 / 2000: loss 0.295077\n",
      "iteration 1300 / 2000: loss 0.273847\n",
      "iteration 1400 / 2000: loss 0.250137\n",
      "iteration 1500 / 2000: loss 0.270993\n",
      "iteration 1600 / 2000: loss 0.209891\n",
      "iteration 1700 / 2000: loss 0.338042\n",
      "iteration 1800 / 2000: loss 0.289728\n",
      "iteration 1900 / 2000: loss 0.264597\n",
      "Hidden Size: 20, Learning Rate: 0.0075000250000000004, LR Decay: 0.99, Regularization: 0.4, Validation accuracy: 0.9456\n",
      "iteration 0 / 2000: loss 2.302590\n",
      "iteration 100 / 2000: loss 0.252383\n",
      "iteration 200 / 2000: loss 0.221084\n",
      "iteration 300 / 2000: loss 0.188376\n",
      "iteration 400 / 2000: loss 0.154109\n",
      "iteration 500 / 2000: loss 0.131614\n",
      "iteration 600 / 2000: loss 0.093423\n",
      "iteration 700 / 2000: loss 0.097101\n",
      "iteration 800 / 2000: loss 0.209155\n",
      "iteration 900 / 2000: loss 0.130659\n",
      "iteration 1000 / 2000: loss 0.089410\n",
      "iteration 1100 / 2000: loss 0.201162\n",
      "iteration 1200 / 2000: loss 0.143289\n",
      "iteration 1300 / 2000: loss 0.097282\n",
      "iteration 1400 / 2000: loss 0.092351\n",
      "iteration 1500 / 2000: loss 0.092295\n",
      "iteration 1600 / 2000: loss 0.075310\n",
      "iteration 1700 / 2000: loss 0.110767\n",
      "iteration 1800 / 2000: loss 0.125506\n",
      "iteration 1900 / 2000: loss 0.100212\n",
      "Hidden Size: 20, Learning Rate: 0.0075000250000000004, LR Decay: 0.8175, Regularization: 0.0, Validation accuracy: 0.9636\n",
      "iteration 0 / 2000: loss 2.302599\n",
      "iteration 100 / 2000: loss 0.383106\n",
      "iteration 200 / 2000: loss 0.212775\n",
      "iteration 300 / 2000: loss 0.290713\n",
      "iteration 400 / 2000: loss 0.267782\n",
      "iteration 500 / 2000: loss 0.218999\n",
      "iteration 600 / 2000: loss 0.128006\n",
      "iteration 700 / 2000: loss 0.181503\n",
      "iteration 800 / 2000: loss 0.284446\n",
      "iteration 900 / 2000: loss 0.147681\n",
      "iteration 1000 / 2000: loss 0.213739\n",
      "iteration 1100 / 2000: loss 0.120470\n",
      "iteration 1200 / 2000: loss 0.216256\n",
      "iteration 1300 / 2000: loss 0.159972\n",
      "iteration 1400 / 2000: loss 0.179966\n",
      "iteration 1500 / 2000: loss 0.149350\n",
      "iteration 1600 / 2000: loss 0.179612\n",
      "iteration 1700 / 2000: loss 0.107692\n",
      "iteration 1800 / 2000: loss 0.125655\n",
      "iteration 1900 / 2000: loss 0.153202\n",
      "Hidden Size: 20, Learning Rate: 0.0075000250000000004, LR Decay: 0.8175, Regularization: 0.1, Validation accuracy: 0.9614\n",
      "iteration 0 / 2000: loss 2.302601\n",
      "iteration 100 / 2000: loss 0.248134\n",
      "iteration 200 / 2000: loss 0.298369\n",
      "iteration 300 / 2000: loss 0.334736\n",
      "iteration 400 / 2000: loss 0.259851\n",
      "iteration 500 / 2000: loss 0.307451\n",
      "iteration 600 / 2000: loss 0.292814\n",
      "iteration 700 / 2000: loss 0.201372\n",
      "iteration 800 / 2000: loss 0.166559\n",
      "iteration 900 / 2000: loss 0.143521\n",
      "iteration 1000 / 2000: loss 0.189949\n",
      "iteration 1100 / 2000: loss 0.225644\n",
      "iteration 1200 / 2000: loss 0.216347\n",
      "iteration 1300 / 2000: loss 0.192797\n",
      "iteration 1400 / 2000: loss 0.252069\n",
      "iteration 1500 / 2000: loss 0.156986\n",
      "iteration 1600 / 2000: loss 0.245352\n",
      "iteration 1700 / 2000: loss 0.195538\n",
      "iteration 1800 / 2000: loss 0.219388\n",
      "iteration 1900 / 2000: loss 0.202172\n",
      "Hidden Size: 20, Learning Rate: 0.0075000250000000004, LR Decay: 0.8175, Regularization: 0.2, Validation accuracy: 0.9578\n",
      "iteration 0 / 2000: loss 2.302609\n",
      "iteration 100 / 2000: loss 0.362027\n",
      "iteration 200 / 2000: loss 0.290504\n",
      "iteration 300 / 2000: loss 0.308962\n",
      "iteration 400 / 2000: loss 0.302516\n",
      "iteration 500 / 2000: loss 0.200901\n",
      "iteration 600 / 2000: loss 0.232152\n",
      "iteration 700 / 2000: loss 0.218763\n",
      "iteration 800 / 2000: loss 0.249770\n",
      "iteration 900 / 2000: loss 0.219083\n",
      "iteration 1000 / 2000: loss 0.286848\n",
      "iteration 1100 / 2000: loss 0.204076\n",
      "iteration 1200 / 2000: loss 0.274720\n",
      "iteration 1300 / 2000: loss 0.190206\n",
      "iteration 1400 / 2000: loss 0.187207\n",
      "iteration 1500 / 2000: loss 0.239890\n",
      "iteration 1600 / 2000: loss 0.201416\n",
      "iteration 1700 / 2000: loss 0.187630\n",
      "iteration 1800 / 2000: loss 0.201474\n",
      "iteration 1900 / 2000: loss 0.253220\n",
      "Hidden Size: 20, Learning Rate: 0.0075000250000000004, LR Decay: 0.8175, Regularization: 0.30000000000000004, Validation accuracy: 0.96\n",
      "iteration 0 / 2000: loss 2.302606\n",
      "iteration 100 / 2000: loss 0.309133\n",
      "iteration 200 / 2000: loss 0.418087\n",
      "iteration 300 / 2000: loss 0.284660\n",
      "iteration 400 / 2000: loss 0.307785\n",
      "iteration 500 / 2000: loss 0.293515\n",
      "iteration 600 / 2000: loss 0.230601\n",
      "iteration 700 / 2000: loss 0.256992\n",
      "iteration 800 / 2000: loss 0.266648\n",
      "iteration 900 / 2000: loss 0.241140\n",
      "iteration 1000 / 2000: loss 0.301661\n",
      "iteration 1100 / 2000: loss 0.306114\n",
      "iteration 1200 / 2000: loss 0.226230\n",
      "iteration 1300 / 2000: loss 0.248061\n",
      "iteration 1400 / 2000: loss 0.290017\n",
      "iteration 1500 / 2000: loss 0.206671\n",
      "iteration 1600 / 2000: loss 0.164998\n",
      "iteration 1700 / 2000: loss 0.243631\n",
      "iteration 1800 / 2000: loss 0.194464\n",
      "iteration 1900 / 2000: loss 0.214024\n",
      "Hidden Size: 20, Learning Rate: 0.0075000250000000004, LR Decay: 0.8175, Regularization: 0.4, Validation accuracy: 0.9564\n",
      "iteration 0 / 2000: loss 2.302582\n",
      "iteration 100 / 2000: loss 0.241322\n",
      "iteration 200 / 2000: loss 0.258799\n",
      "iteration 300 / 2000: loss 0.216720\n",
      "iteration 400 / 2000: loss 0.174962\n",
      "iteration 500 / 2000: loss 0.188304\n",
      "iteration 600 / 2000: loss 0.196652\n",
      "iteration 700 / 2000: loss 0.166057\n",
      "iteration 800 / 2000: loss 0.119137\n",
      "iteration 900 / 2000: loss 0.107787\n",
      "iteration 1000 / 2000: loss 0.085701\n",
      "iteration 1100 / 2000: loss 0.226592\n",
      "iteration 1200 / 2000: loss 0.129460\n",
      "iteration 1300 / 2000: loss 0.157138\n",
      "iteration 1400 / 2000: loss 0.089576\n",
      "iteration 1500 / 2000: loss 0.143735\n",
      "iteration 1600 / 2000: loss 0.123392\n",
      "iteration 1700 / 2000: loss 0.121167\n",
      "iteration 1800 / 2000: loss 0.182183\n",
      "iteration 1900 / 2000: loss 0.092135\n",
      "Hidden Size: 20, Learning Rate: 0.0075000250000000004, LR Decay: 0.645, Regularization: 0.0, Validation accuracy: 0.9578\n",
      "iteration 0 / 2000: loss 2.302596\n",
      "iteration 100 / 2000: loss 0.333141\n",
      "iteration 200 / 2000: loss 0.327473\n",
      "iteration 300 / 2000: loss 0.176807\n",
      "iteration 400 / 2000: loss 0.152130\n",
      "iteration 500 / 2000: loss 0.200711\n",
      "iteration 600 / 2000: loss 0.133213\n",
      "iteration 700 / 2000: loss 0.187055\n",
      "iteration 800 / 2000: loss 0.165913\n",
      "iteration 900 / 2000: loss 0.129253\n",
      "iteration 1000 / 2000: loss 0.307114\n",
      "iteration 1100 / 2000: loss 0.127463\n",
      "iteration 1200 / 2000: loss 0.219980\n",
      "iteration 1300 / 2000: loss 0.186501\n",
      "iteration 1400 / 2000: loss 0.180470\n",
      "iteration 1500 / 2000: loss 0.102810\n",
      "iteration 1600 / 2000: loss 0.204205\n",
      "iteration 1700 / 2000: loss 0.145937\n",
      "iteration 1800 / 2000: loss 0.120329\n",
      "iteration 1900 / 2000: loss 0.205773\n",
      "Hidden Size: 20, Learning Rate: 0.0075000250000000004, LR Decay: 0.645, Regularization: 0.1, Validation accuracy: 0.961\n",
      "iteration 0 / 2000: loss 2.302599\n",
      "iteration 100 / 2000: loss 0.407253\n",
      "iteration 200 / 2000: loss 0.386117\n",
      "iteration 300 / 2000: loss 0.259984\n",
      "iteration 400 / 2000: loss 0.227577\n",
      "iteration 500 / 2000: loss 0.224618\n",
      "iteration 600 / 2000: loss 0.188118\n",
      "iteration 700 / 2000: loss 0.203786\n",
      "iteration 800 / 2000: loss 0.264195\n",
      "iteration 900 / 2000: loss 0.227120\n",
      "iteration 1000 / 2000: loss 0.193930\n",
      "iteration 1100 / 2000: loss 0.178410\n",
      "iteration 1200 / 2000: loss 0.257498\n",
      "iteration 1300 / 2000: loss 0.155072\n",
      "iteration 1400 / 2000: loss 0.181960\n",
      "iteration 1500 / 2000: loss 0.184370\n",
      "iteration 1600 / 2000: loss 0.149507\n",
      "iteration 1700 / 2000: loss 0.240505\n",
      "iteration 1800 / 2000: loss 0.150991\n",
      "iteration 1900 / 2000: loss 0.183736\n",
      "Hidden Size: 20, Learning Rate: 0.0075000250000000004, LR Decay: 0.645, Regularization: 0.2, Validation accuracy: 0.9578\n",
      "iteration 0 / 2000: loss 2.302616\n",
      "iteration 100 / 2000: loss 0.363627\n",
      "iteration 200 / 2000: loss 0.301760\n",
      "iteration 300 / 2000: loss 0.290864\n",
      "iteration 400 / 2000: loss 0.243604\n",
      "iteration 500 / 2000: loss 0.212307\n",
      "iteration 600 / 2000: loss 0.261133\n",
      "iteration 700 / 2000: loss 0.245679\n",
      "iteration 800 / 2000: loss 0.241762\n",
      "iteration 900 / 2000: loss 0.240231\n",
      "iteration 1000 / 2000: loss 0.228358\n",
      "iteration 1100 / 2000: loss 0.242504\n",
      "iteration 1200 / 2000: loss 0.246693\n",
      "iteration 1300 / 2000: loss 0.182627\n",
      "iteration 1400 / 2000: loss 0.209220\n",
      "iteration 1500 / 2000: loss 0.199178\n",
      "iteration 1600 / 2000: loss 0.215105\n",
      "iteration 1700 / 2000: loss 0.154950\n",
      "iteration 1800 / 2000: loss 0.237564\n",
      "iteration 1900 / 2000: loss 0.191263\n",
      "Hidden Size: 20, Learning Rate: 0.0075000250000000004, LR Decay: 0.645, Regularization: 0.30000000000000004, Validation accuracy: 0.9586\n",
      "iteration 0 / 2000: loss 2.302629\n",
      "iteration 100 / 2000: loss 0.372211\n",
      "iteration 200 / 2000: loss 0.284761\n",
      "iteration 300 / 2000: loss 0.302958\n",
      "iteration 400 / 2000: loss 0.353802\n",
      "iteration 500 / 2000: loss 0.272470\n",
      "iteration 600 / 2000: loss 0.241956\n",
      "iteration 700 / 2000: loss 0.322491\n",
      "iteration 800 / 2000: loss 0.185780\n",
      "iteration 900 / 2000: loss 0.265792\n",
      "iteration 1000 / 2000: loss 0.205203\n",
      "iteration 1100 / 2000: loss 0.296783\n",
      "iteration 1200 / 2000: loss 0.196007\n",
      "iteration 1300 / 2000: loss 0.231142\n",
      "iteration 1400 / 2000: loss 0.236654\n",
      "iteration 1500 / 2000: loss 0.265797\n",
      "iteration 1600 / 2000: loss 0.207094\n",
      "iteration 1700 / 2000: loss 0.210885\n",
      "iteration 1800 / 2000: loss 0.276640\n",
      "iteration 1900 / 2000: loss 0.313187\n",
      "Hidden Size: 20, Learning Rate: 0.0075000250000000004, LR Decay: 0.645, Regularization: 0.4, Validation accuracy: 0.9534\n",
      "iteration 0 / 2000: loss 2.302587\n",
      "iteration 100 / 2000: loss 0.382857\n",
      "iteration 200 / 2000: loss 0.306425\n",
      "iteration 300 / 2000: loss 0.261105\n",
      "iteration 400 / 2000: loss 0.152492\n",
      "iteration 500 / 2000: loss 0.174538\n",
      "iteration 600 / 2000: loss 0.261584\n",
      "iteration 700 / 2000: loss 0.134555\n",
      "iteration 800 / 2000: loss 0.187628\n",
      "iteration 900 / 2000: loss 0.119795\n",
      "iteration 1000 / 2000: loss 0.154730\n",
      "iteration 1100 / 2000: loss 0.134627\n",
      "iteration 1200 / 2000: loss 0.083101\n",
      "iteration 1300 / 2000: loss 0.164164\n",
      "iteration 1400 / 2000: loss 0.118705\n",
      "iteration 1500 / 2000: loss 0.111095\n",
      "iteration 1600 / 2000: loss 0.133363\n",
      "iteration 1700 / 2000: loss 0.081434\n",
      "iteration 1800 / 2000: loss 0.100222\n",
      "iteration 1900 / 2000: loss 0.151558\n",
      "Hidden Size: 20, Learning Rate: 0.0075000250000000004, LR Decay: 0.47250000000000003, Regularization: 0.0, Validation accuracy: 0.954\n",
      "iteration 0 / 2000: loss 2.302587\n",
      "iteration 100 / 2000: loss 0.330059\n",
      "iteration 200 / 2000: loss 0.241468\n",
      "iteration 300 / 2000: loss 0.253582\n",
      "iteration 400 / 2000: loss 0.115842\n",
      "iteration 500 / 2000: loss 0.153861\n",
      "iteration 600 / 2000: loss 0.156839\n",
      "iteration 700 / 2000: loss 0.167438\n",
      "iteration 800 / 2000: loss 0.198638\n",
      "iteration 900 / 2000: loss 0.176711\n",
      "iteration 1000 / 2000: loss 0.337359\n",
      "iteration 1100 / 2000: loss 0.196885\n",
      "iteration 1200 / 2000: loss 0.231543\n",
      "iteration 1300 / 2000: loss 0.118105\n",
      "iteration 1400 / 2000: loss 0.198903\n",
      "iteration 1500 / 2000: loss 0.157188\n",
      "iteration 1600 / 2000: loss 0.166464\n",
      "iteration 1700 / 2000: loss 0.256791\n",
      "iteration 1800 / 2000: loss 0.200302\n",
      "iteration 1900 / 2000: loss 0.139185\n",
      "Hidden Size: 20, Learning Rate: 0.0075000250000000004, LR Decay: 0.47250000000000003, Regularization: 0.1, Validation accuracy: 0.9542\n",
      "iteration 0 / 2000: loss 2.302614\n",
      "iteration 100 / 2000: loss 0.380558\n",
      "iteration 200 / 2000: loss 0.253250\n",
      "iteration 300 / 2000: loss 0.298987\n",
      "iteration 400 / 2000: loss 0.260347\n",
      "iteration 500 / 2000: loss 0.214616\n",
      "iteration 600 / 2000: loss 0.227261\n",
      "iteration 700 / 2000: loss 0.169882\n",
      "iteration 800 / 2000: loss 0.186548\n",
      "iteration 900 / 2000: loss 0.176609\n",
      "iteration 1000 / 2000: loss 0.259594\n",
      "iteration 1100 / 2000: loss 0.264036\n",
      "iteration 1200 / 2000: loss 0.222631\n",
      "iteration 1300 / 2000: loss 0.258167\n",
      "iteration 1400 / 2000: loss 0.259302\n",
      "iteration 1500 / 2000: loss 0.157400\n",
      "iteration 1600 / 2000: loss 0.262905\n",
      "iteration 1700 / 2000: loss 0.221448\n",
      "iteration 1800 / 2000: loss 0.200908\n",
      "iteration 1900 / 2000: loss 0.256104\n",
      "Hidden Size: 20, Learning Rate: 0.0075000250000000004, LR Decay: 0.47250000000000003, Regularization: 0.2, Validation accuracy: 0.9554\n",
      "iteration 0 / 2000: loss 2.302616\n",
      "iteration 100 / 2000: loss 0.343452\n",
      "iteration 200 / 2000: loss 0.314065\n",
      "iteration 300 / 2000: loss 0.164769\n",
      "iteration 400 / 2000: loss 0.242581\n",
      "iteration 500 / 2000: loss 0.283299\n",
      "iteration 600 / 2000: loss 0.269587\n",
      "iteration 700 / 2000: loss 0.205491\n",
      "iteration 800 / 2000: loss 0.238807\n",
      "iteration 900 / 2000: loss 0.248399\n",
      "iteration 1000 / 2000: loss 0.208789\n",
      "iteration 1100 / 2000: loss 0.272399\n",
      "iteration 1200 / 2000: loss 0.327886\n",
      "iteration 1300 / 2000: loss 0.225629\n",
      "iteration 1400 / 2000: loss 0.222603\n",
      "iteration 1500 / 2000: loss 0.246838\n",
      "iteration 1600 / 2000: loss 0.260031\n",
      "iteration 1700 / 2000: loss 0.210573\n",
      "iteration 1800 / 2000: loss 0.221427\n",
      "iteration 1900 / 2000: loss 0.307415\n",
      "Hidden Size: 20, Learning Rate: 0.0075000250000000004, LR Decay: 0.47250000000000003, Regularization: 0.30000000000000004, Validation accuracy: 0.9514\n",
      "iteration 0 / 2000: loss 2.302624\n",
      "iteration 100 / 2000: loss 0.387820\n",
      "iteration 200 / 2000: loss 0.361722\n",
      "iteration 300 / 2000: loss 0.282270\n",
      "iteration 400 / 2000: loss 0.266649\n",
      "iteration 500 / 2000: loss 0.253812\n",
      "iteration 600 / 2000: loss 0.188934\n",
      "iteration 700 / 2000: loss 0.278877\n",
      "iteration 800 / 2000: loss 0.280892\n",
      "iteration 900 / 2000: loss 0.282567\n",
      "iteration 1000 / 2000: loss 0.295341\n",
      "iteration 1100 / 2000: loss 0.296491\n",
      "iteration 1200 / 2000: loss 0.299747\n",
      "iteration 1300 / 2000: loss 0.319027\n",
      "iteration 1400 / 2000: loss 0.270827\n",
      "iteration 1500 / 2000: loss 0.224937\n",
      "iteration 1600 / 2000: loss 0.271165\n",
      "iteration 1700 / 2000: loss 0.246728\n",
      "iteration 1800 / 2000: loss 0.208476\n",
      "iteration 1900 / 2000: loss 0.345212\n",
      "Hidden Size: 20, Learning Rate: 0.0075000250000000004, LR Decay: 0.47250000000000003, Regularization: 0.4, Validation accuracy: 0.9496\n",
      "iteration 0 / 2000: loss 2.302591\n",
      "iteration 100 / 2000: loss 0.354790\n",
      "iteration 200 / 2000: loss 0.353161\n",
      "iteration 300 / 2000: loss 0.225272\n",
      "iteration 400 / 2000: loss 0.223177\n",
      "iteration 500 / 2000: loss 0.227073\n",
      "iteration 600 / 2000: loss 0.199891\n",
      "iteration 700 / 2000: loss 0.179168\n",
      "iteration 800 / 2000: loss 0.302659\n",
      "iteration 900 / 2000: loss 0.274442\n",
      "iteration 1000 / 2000: loss 0.234033\n",
      "iteration 1100 / 2000: loss 0.315608\n",
      "iteration 1200 / 2000: loss 0.261156\n",
      "iteration 1300 / 2000: loss 0.212155\n",
      "iteration 1400 / 2000: loss 0.239257\n",
      "iteration 1500 / 2000: loss 0.169788\n",
      "iteration 1600 / 2000: loss 0.205151\n",
      "iteration 1700 / 2000: loss 0.250299\n",
      "iteration 1800 / 2000: loss 0.164629\n",
      "iteration 1900 / 2000: loss 0.218835\n",
      "Hidden Size: 20, Learning Rate: 0.0075000250000000004, LR Decay: 0.3, Regularization: 0.0, Validation accuracy: 0.9426\n",
      "iteration 0 / 2000: loss 2.302592\n",
      "iteration 100 / 2000: loss 0.382702\n",
      "iteration 200 / 2000: loss 0.433063\n",
      "iteration 300 / 2000: loss 0.364818\n",
      "iteration 400 / 2000: loss 0.292660\n",
      "iteration 500 / 2000: loss 0.278541\n",
      "iteration 600 / 2000: loss 0.202189\n",
      "iteration 700 / 2000: loss 0.247923\n",
      "iteration 800 / 2000: loss 0.200255\n",
      "iteration 900 / 2000: loss 0.325546\n",
      "iteration 1000 / 2000: loss 0.215419\n",
      "iteration 1100 / 2000: loss 0.251523\n",
      "iteration 1200 / 2000: loss 0.208723\n",
      "iteration 1300 / 2000: loss 0.185027\n",
      "iteration 1400 / 2000: loss 0.172323\n",
      "iteration 1500 / 2000: loss 0.276733\n",
      "iteration 1600 / 2000: loss 0.244838\n",
      "iteration 1700 / 2000: loss 0.190197\n",
      "iteration 1800 / 2000: loss 0.299701\n",
      "iteration 1900 / 2000: loss 0.256937\n",
      "Hidden Size: 20, Learning Rate: 0.0075000250000000004, LR Decay: 0.3, Regularization: 0.1, Validation accuracy: 0.937\n",
      "iteration 0 / 2000: loss 2.302609\n",
      "iteration 100 / 2000: loss 0.423228\n",
      "iteration 200 / 2000: loss 0.298452\n",
      "iteration 300 / 2000: loss 0.274532\n",
      "iteration 400 / 2000: loss 0.217222\n",
      "iteration 500 / 2000: loss 0.367900\n",
      "iteration 600 / 2000: loss 0.417929\n",
      "iteration 700 / 2000: loss 0.227394\n",
      "iteration 800 / 2000: loss 0.256005\n",
      "iteration 900 / 2000: loss 0.209574\n",
      "iteration 1000 / 2000: loss 0.264859\n",
      "iteration 1100 / 2000: loss 0.251013\n",
      "iteration 1200 / 2000: loss 0.232849\n",
      "iteration 1300 / 2000: loss 0.229447\n",
      "iteration 1400 / 2000: loss 0.299206\n",
      "iteration 1500 / 2000: loss 0.272915\n",
      "iteration 1600 / 2000: loss 0.249161\n",
      "iteration 1700 / 2000: loss 0.440390\n",
      "iteration 1800 / 2000: loss 0.259345\n",
      "iteration 1900 / 2000: loss 0.280709\n",
      "Hidden Size: 20, Learning Rate: 0.0075000250000000004, LR Decay: 0.3, Regularization: 0.2, Validation accuracy: 0.9358\n",
      "iteration 0 / 2000: loss 2.302616\n",
      "iteration 100 / 2000: loss 0.409155\n",
      "iteration 200 / 2000: loss 0.234605\n",
      "iteration 300 / 2000: loss 0.304221\n",
      "iteration 400 / 2000: loss 0.303984\n",
      "iteration 500 / 2000: loss 0.221304\n",
      "iteration 600 / 2000: loss 0.258093\n",
      "iteration 700 / 2000: loss 0.322711\n",
      "iteration 800 / 2000: loss 0.275373\n",
      "iteration 900 / 2000: loss 0.296917\n",
      "iteration 1000 / 2000: loss 0.248832\n",
      "iteration 1100 / 2000: loss 0.237702\n",
      "iteration 1200 / 2000: loss 0.325783\n",
      "iteration 1300 / 2000: loss 0.269442\n",
      "iteration 1400 / 2000: loss 0.210853\n",
      "iteration 1500 / 2000: loss 0.227535\n",
      "iteration 1600 / 2000: loss 0.245697\n",
      "iteration 1700 / 2000: loss 0.319504\n",
      "iteration 1800 / 2000: loss 0.259884\n",
      "iteration 1900 / 2000: loss 0.234481\n",
      "Hidden Size: 20, Learning Rate: 0.0075000250000000004, LR Decay: 0.3, Regularization: 0.30000000000000004, Validation accuracy: 0.936\n",
      "iteration 0 / 2000: loss 2.302617\n",
      "iteration 100 / 2000: loss 0.393924\n",
      "iteration 200 / 2000: loss 0.294718\n",
      "iteration 300 / 2000: loss 0.299670\n",
      "iteration 400 / 2000: loss 0.294051\n",
      "iteration 500 / 2000: loss 0.309232\n",
      "iteration 600 / 2000: loss 0.206456\n",
      "iteration 700 / 2000: loss 0.314246\n",
      "iteration 800 / 2000: loss 0.232561\n",
      "iteration 900 / 2000: loss 0.318355\n",
      "iteration 1000 / 2000: loss 0.323937\n",
      "iteration 1100 / 2000: loss 0.305836\n",
      "iteration 1200 / 2000: loss 0.197493\n",
      "iteration 1300 / 2000: loss 0.328332\n",
      "iteration 1400 / 2000: loss 0.283388\n",
      "iteration 1500 / 2000: loss 0.282703\n",
      "iteration 1600 / 2000: loss 0.321373\n",
      "iteration 1700 / 2000: loss 0.271277\n",
      "iteration 1800 / 2000: loss 0.352939\n",
      "iteration 1900 / 2000: loss 0.323143\n",
      "Hidden Size: 20, Learning Rate: 0.0075000250000000004, LR Decay: 0.3, Regularization: 0.4, Validation accuracy: 0.9332\n",
      "iteration 0 / 2000: loss 2.302588\n",
      "iteration 100 / 2000: loss 0.239924\n",
      "iteration 200 / 2000: loss 0.264602\n",
      "iteration 300 / 2000: loss 0.161970\n",
      "iteration 400 / 2000: loss 0.202218\n",
      "iteration 500 / 2000: loss 0.169412\n",
      "iteration 600 / 2000: loss 0.109752\n",
      "iteration 700 / 2000: loss 0.183148\n",
      "iteration 800 / 2000: loss 0.242344\n",
      "iteration 900 / 2000: loss 0.086617\n",
      "iteration 1000 / 2000: loss 0.124026\n",
      "iteration 1100 / 2000: loss 0.150728\n",
      "iteration 1200 / 2000: loss 0.097387\n",
      "iteration 1300 / 2000: loss 0.105945\n",
      "iteration 1400 / 2000: loss 0.184759\n",
      "iteration 1500 / 2000: loss 0.091654\n",
      "iteration 1600 / 2000: loss 0.051859\n",
      "iteration 1700 / 2000: loss 0.083978\n",
      "iteration 1800 / 2000: loss 0.117976\n",
      "iteration 1900 / 2000: loss 0.075476\n",
      "Hidden Size: 20, Learning Rate: 0.00500005, LR Decay: 0.99, Regularization: 0.0, Validation accuracy: 0.96\n",
      "iteration 0 / 2000: loss 2.302602\n",
      "iteration 100 / 2000: loss 0.367602\n",
      "iteration 200 / 2000: loss 0.229655\n",
      "iteration 300 / 2000: loss 0.287129\n",
      "iteration 400 / 2000: loss 0.187310\n",
      "iteration 500 / 2000: loss 0.194645\n",
      "iteration 600 / 2000: loss 0.147779\n",
      "iteration 700 / 2000: loss 0.186245\n",
      "iteration 800 / 2000: loss 0.196903\n",
      "iteration 900 / 2000: loss 0.140998\n",
      "iteration 1000 / 2000: loss 0.163752\n",
      "iteration 1100 / 2000: loss 0.111153\n",
      "iteration 1200 / 2000: loss 0.134757\n",
      "iteration 1300 / 2000: loss 0.231337\n",
      "iteration 1400 / 2000: loss 0.195421\n",
      "iteration 1500 / 2000: loss 0.165761\n",
      "iteration 1600 / 2000: loss 0.121761\n",
      "iteration 1700 / 2000: loss 0.138389\n",
      "iteration 1800 / 2000: loss 0.178034\n",
      "iteration 1900 / 2000: loss 0.150521\n",
      "Hidden Size: 20, Learning Rate: 0.00500005, LR Decay: 0.99, Regularization: 0.1, Validation accuracy: 0.9568\n",
      "iteration 0 / 2000: loss 2.302592\n",
      "iteration 100 / 2000: loss 0.393933\n",
      "iteration 200 / 2000: loss 0.293197\n",
      "iteration 300 / 2000: loss 0.204104\n",
      "iteration 400 / 2000: loss 0.240824\n",
      "iteration 500 / 2000: loss 0.335530\n",
      "iteration 600 / 2000: loss 0.181272\n",
      "iteration 700 / 2000: loss 0.206702\n",
      "iteration 800 / 2000: loss 0.226291\n",
      "iteration 900 / 2000: loss 0.245675\n",
      "iteration 1000 / 2000: loss 0.137973\n",
      "iteration 1100 / 2000: loss 0.270207\n",
      "iteration 1200 / 2000: loss 0.161811\n",
      "iteration 1300 / 2000: loss 0.178284\n",
      "iteration 1400 / 2000: loss 0.210735\n",
      "iteration 1500 / 2000: loss 0.209010\n",
      "iteration 1600 / 2000: loss 0.191547\n",
      "iteration 1700 / 2000: loss 0.182983\n",
      "iteration 1800 / 2000: loss 0.218161\n",
      "iteration 1900 / 2000: loss 0.202959\n",
      "Hidden Size: 20, Learning Rate: 0.00500005, LR Decay: 0.99, Regularization: 0.2, Validation accuracy: 0.9556\n",
      "iteration 0 / 2000: loss 2.302598\n",
      "iteration 100 / 2000: loss 0.464817\n",
      "iteration 200 / 2000: loss 0.308004\n",
      "iteration 300 / 2000: loss 0.322493\n",
      "iteration 400 / 2000: loss 0.222801\n",
      "iteration 500 / 2000: loss 0.214191\n",
      "iteration 600 / 2000: loss 0.197152\n",
      "iteration 700 / 2000: loss 0.189082\n",
      "iteration 800 / 2000: loss 0.253811\n",
      "iteration 900 / 2000: loss 0.272350\n",
      "iteration 1000 / 2000: loss 0.251413\n",
      "iteration 1100 / 2000: loss 0.264600\n",
      "iteration 1200 / 2000: loss 0.277888\n",
      "iteration 1300 / 2000: loss 0.231295\n",
      "iteration 1400 / 2000: loss 0.207069\n",
      "iteration 1500 / 2000: loss 0.200489\n",
      "iteration 1600 / 2000: loss 0.262428\n",
      "iteration 1700 / 2000: loss 0.235076\n",
      "iteration 1800 / 2000: loss 0.184531\n",
      "iteration 1900 / 2000: loss 0.283576\n",
      "Hidden Size: 20, Learning Rate: 0.00500005, LR Decay: 0.99, Regularization: 0.30000000000000004, Validation accuracy: 0.956\n",
      "iteration 0 / 2000: loss 2.302614\n",
      "iteration 100 / 2000: loss 0.409527\n",
      "iteration 200 / 2000: loss 0.320920\n",
      "iteration 300 / 2000: loss 0.255978\n",
      "iteration 400 / 2000: loss 0.310535\n",
      "iteration 500 / 2000: loss 0.328254\n",
      "iteration 600 / 2000: loss 0.268199\n",
      "iteration 700 / 2000: loss 0.202033\n",
      "iteration 800 / 2000: loss 0.304704\n",
      "iteration 900 / 2000: loss 0.227222\n",
      "iteration 1000 / 2000: loss 0.296602\n",
      "iteration 1100 / 2000: loss 0.189770\n",
      "iteration 1200 / 2000: loss 0.242631\n",
      "iteration 1300 / 2000: loss 0.228890\n",
      "iteration 1400 / 2000: loss 0.299065\n",
      "iteration 1500 / 2000: loss 0.220925\n",
      "iteration 1600 / 2000: loss 0.209542\n",
      "iteration 1700 / 2000: loss 0.237707\n",
      "iteration 1800 / 2000: loss 0.258884\n",
      "iteration 1900 / 2000: loss 0.267970\n",
      "Hidden Size: 20, Learning Rate: 0.00500005, LR Decay: 0.99, Regularization: 0.4, Validation accuracy: 0.9578\n",
      "iteration 0 / 2000: loss 2.302581\n",
      "iteration 100 / 2000: loss 0.467584\n",
      "iteration 200 / 2000: loss 0.189345\n",
      "iteration 300 / 2000: loss 0.186673\n",
      "iteration 400 / 2000: loss 0.158849\n",
      "iteration 500 / 2000: loss 0.201845\n",
      "iteration 600 / 2000: loss 0.146832\n",
      "iteration 700 / 2000: loss 0.111030\n",
      "iteration 800 / 2000: loss 0.118433\n",
      "iteration 900 / 2000: loss 0.112103\n",
      "iteration 1000 / 2000: loss 0.087885\n",
      "iteration 1100 / 2000: loss 0.211812\n",
      "iteration 1200 / 2000: loss 0.122637\n",
      "iteration 1300 / 2000: loss 0.100261\n",
      "iteration 1400 / 2000: loss 0.150041\n",
      "iteration 1500 / 2000: loss 0.130745\n",
      "iteration 1600 / 2000: loss 0.105966\n",
      "iteration 1700 / 2000: loss 0.118774\n",
      "iteration 1800 / 2000: loss 0.122563\n",
      "iteration 1900 / 2000: loss 0.094089\n",
      "Hidden Size: 20, Learning Rate: 0.00500005, LR Decay: 0.8175, Regularization: 0.0, Validation accuracy: 0.9576\n",
      "iteration 0 / 2000: loss 2.302599\n",
      "iteration 100 / 2000: loss 0.416918\n",
      "iteration 200 / 2000: loss 0.307735\n",
      "iteration 300 / 2000: loss 0.280219\n",
      "iteration 400 / 2000: loss 0.172609\n",
      "iteration 500 / 2000: loss 0.269449\n",
      "iteration 600 / 2000: loss 0.210081\n",
      "iteration 700 / 2000: loss 0.306577\n",
      "iteration 800 / 2000: loss 0.135863\n",
      "iteration 900 / 2000: loss 0.174119\n",
      "iteration 1000 / 2000: loss 0.214791\n",
      "iteration 1100 / 2000: loss 0.188215\n",
      "iteration 1200 / 2000: loss 0.191371\n",
      "iteration 1300 / 2000: loss 0.167224\n",
      "iteration 1400 / 2000: loss 0.155814\n",
      "iteration 1500 / 2000: loss 0.195136\n",
      "iteration 1600 / 2000: loss 0.192087\n",
      "iteration 1700 / 2000: loss 0.128443\n",
      "iteration 1800 / 2000: loss 0.090779\n",
      "iteration 1900 / 2000: loss 0.204615\n",
      "Hidden Size: 20, Learning Rate: 0.00500005, LR Decay: 0.8175, Regularization: 0.1, Validation accuracy: 0.9618\n",
      "iteration 0 / 2000: loss 2.302596\n",
      "iteration 100 / 2000: loss 0.380767\n",
      "iteration 200 / 2000: loss 0.315668\n",
      "iteration 300 / 2000: loss 0.173081\n",
      "iteration 400 / 2000: loss 0.218158\n",
      "iteration 500 / 2000: loss 0.151135\n",
      "iteration 600 / 2000: loss 0.197861\n",
      "iteration 700 / 2000: loss 0.178892\n",
      "iteration 800 / 2000: loss 0.149314\n",
      "iteration 900 / 2000: loss 0.133566\n",
      "iteration 1000 / 2000: loss 0.209433\n",
      "iteration 1100 / 2000: loss 0.217182\n",
      "iteration 1200 / 2000: loss 0.151396\n",
      "iteration 1300 / 2000: loss 0.167376\n",
      "iteration 1400 / 2000: loss 0.173938\n",
      "iteration 1500 / 2000: loss 0.190834\n",
      "iteration 1600 / 2000: loss 0.184529\n",
      "iteration 1700 / 2000: loss 0.164997\n",
      "iteration 1800 / 2000: loss 0.158117\n",
      "iteration 1900 / 2000: loss 0.177585\n",
      "Hidden Size: 20, Learning Rate: 0.00500005, LR Decay: 0.8175, Regularization: 0.2, Validation accuracy: 0.962\n",
      "iteration 0 / 2000: loss 2.302607\n",
      "iteration 100 / 2000: loss 0.427667\n",
      "iteration 200 / 2000: loss 0.427080\n",
      "iteration 300 / 2000: loss 0.246610\n",
      "iteration 400 / 2000: loss 0.201951\n",
      "iteration 500 / 2000: loss 0.482365\n",
      "iteration 600 / 2000: loss 0.278842\n",
      "iteration 700 / 2000: loss 0.226977\n",
      "iteration 800 / 2000: loss 0.176728\n",
      "iteration 900 / 2000: loss 0.234781\n",
      "iteration 1000 / 2000: loss 0.182036\n",
      "iteration 1100 / 2000: loss 0.289319\n",
      "iteration 1200 / 2000: loss 0.302637\n",
      "iteration 1300 / 2000: loss 0.248184\n",
      "iteration 1400 / 2000: loss 0.196362\n",
      "iteration 1500 / 2000: loss 0.291489\n",
      "iteration 1600 / 2000: loss 0.245299\n",
      "iteration 1700 / 2000: loss 0.220031\n",
      "iteration 1800 / 2000: loss 0.248045\n",
      "iteration 1900 / 2000: loss 0.172332\n",
      "Hidden Size: 20, Learning Rate: 0.00500005, LR Decay: 0.8175, Regularization: 0.30000000000000004, Validation accuracy: 0.9586\n",
      "iteration 0 / 2000: loss 2.302616\n",
      "iteration 100 / 2000: loss 0.397063\n",
      "iteration 200 / 2000: loss 0.357204\n",
      "iteration 300 / 2000: loss 0.234743\n",
      "iteration 400 / 2000: loss 0.390425\n",
      "iteration 500 / 2000: loss 0.282520\n",
      "iteration 600 / 2000: loss 0.249759\n",
      "iteration 700 / 2000: loss 0.348102\n",
      "iteration 800 / 2000: loss 0.220641\n",
      "iteration 900 / 2000: loss 0.265554\n",
      "iteration 1000 / 2000: loss 0.360531\n",
      "iteration 1100 / 2000: loss 0.271386\n",
      "iteration 1200 / 2000: loss 0.378094\n",
      "iteration 1300 / 2000: loss 0.242278\n",
      "iteration 1400 / 2000: loss 0.246930\n",
      "iteration 1500 / 2000: loss 0.372570\n",
      "iteration 1600 / 2000: loss 0.243756\n",
      "iteration 1700 / 2000: loss 0.279424\n",
      "iteration 1800 / 2000: loss 0.270153\n",
      "iteration 1900 / 2000: loss 0.239287\n",
      "Hidden Size: 20, Learning Rate: 0.00500005, LR Decay: 0.8175, Regularization: 0.4, Validation accuracy: 0.96\n",
      "iteration 0 / 2000: loss 2.302585\n",
      "iteration 100 / 2000: loss 0.293586\n",
      "iteration 200 / 2000: loss 0.289283\n",
      "iteration 300 / 2000: loss 0.240323\n",
      "iteration 400 / 2000: loss 0.232392\n",
      "iteration 500 / 2000: loss 0.167959\n",
      "iteration 600 / 2000: loss 0.096689\n",
      "iteration 700 / 2000: loss 0.225864\n",
      "iteration 800 / 2000: loss 0.152321\n",
      "iteration 900 / 2000: loss 0.221584\n",
      "iteration 1000 / 2000: loss 0.130326\n",
      "iteration 1100 / 2000: loss 0.108559\n",
      "iteration 1200 / 2000: loss 0.185465\n",
      "iteration 1300 / 2000: loss 0.184419\n",
      "iteration 1400 / 2000: loss 0.106281\n",
      "iteration 1500 / 2000: loss 0.137126\n",
      "iteration 1600 / 2000: loss 0.178020\n",
      "iteration 1700 / 2000: loss 0.221207\n",
      "iteration 1800 / 2000: loss 0.119959\n",
      "iteration 1900 / 2000: loss 0.169415\n",
      "Hidden Size: 20, Learning Rate: 0.00500005, LR Decay: 0.645, Regularization: 0.0, Validation accuracy: 0.9564\n",
      "iteration 0 / 2000: loss 2.302593\n",
      "iteration 100 / 2000: loss 0.349027\n",
      "iteration 200 / 2000: loss 0.226131\n",
      "iteration 300 / 2000: loss 0.264607\n",
      "iteration 400 / 2000: loss 0.212927\n",
      "iteration 500 / 2000: loss 0.202528\n",
      "iteration 600 / 2000: loss 0.233791\n",
      "iteration 700 / 2000: loss 0.171854\n",
      "iteration 800 / 2000: loss 0.218366\n",
      "iteration 900 / 2000: loss 0.172230\n",
      "iteration 1000 / 2000: loss 0.201888\n",
      "iteration 1100 / 2000: loss 0.199721\n",
      "iteration 1200 / 2000: loss 0.126151\n",
      "iteration 1300 / 2000: loss 0.148917\n",
      "iteration 1400 / 2000: loss 0.199451\n",
      "iteration 1500 / 2000: loss 0.146464\n",
      "iteration 1600 / 2000: loss 0.163068\n",
      "iteration 1700 / 2000: loss 0.161680\n",
      "iteration 1800 / 2000: loss 0.134088\n",
      "iteration 1900 / 2000: loss 0.187438\n",
      "Hidden Size: 20, Learning Rate: 0.00500005, LR Decay: 0.645, Regularization: 0.1, Validation accuracy: 0.9542\n",
      "iteration 0 / 2000: loss 2.302596\n",
      "iteration 100 / 2000: loss 0.406710\n",
      "iteration 200 / 2000: loss 0.298675\n",
      "iteration 300 / 2000: loss 0.229191\n",
      "iteration 400 / 2000: loss 0.250661\n",
      "iteration 500 / 2000: loss 0.257755\n",
      "iteration 600 / 2000: loss 0.252741\n",
      "iteration 700 / 2000: loss 0.180414\n",
      "iteration 800 / 2000: loss 0.350597\n",
      "iteration 900 / 2000: loss 0.263045\n",
      "iteration 1000 / 2000: loss 0.250402\n",
      "iteration 1100 / 2000: loss 0.195550\n",
      "iteration 1200 / 2000: loss 0.155418\n",
      "iteration 1300 / 2000: loss 0.154687\n",
      "iteration 1400 / 2000: loss 0.183579\n",
      "iteration 1500 / 2000: loss 0.192086\n",
      "iteration 1600 / 2000: loss 0.193871\n",
      "iteration 1700 / 2000: loss 0.238300\n",
      "iteration 1800 / 2000: loss 0.213182\n",
      "iteration 1900 / 2000: loss 0.179365\n",
      "Hidden Size: 20, Learning Rate: 0.00500005, LR Decay: 0.645, Regularization: 0.2, Validation accuracy: 0.9552\n",
      "iteration 0 / 2000: loss 2.302602\n",
      "iteration 100 / 2000: loss 0.334255\n",
      "iteration 200 / 2000: loss 0.277345\n",
      "iteration 300 / 2000: loss 0.416894\n",
      "iteration 400 / 2000: loss 0.229145\n",
      "iteration 500 / 2000: loss 0.273680\n",
      "iteration 600 / 2000: loss 0.272405\n",
      "iteration 700 / 2000: loss 0.188772\n",
      "iteration 800 / 2000: loss 0.220939\n",
      "iteration 900 / 2000: loss 0.314251\n",
      "iteration 1000 / 2000: loss 0.306212\n",
      "iteration 1100 / 2000: loss 0.290353\n",
      "iteration 1200 / 2000: loss 0.227442\n",
      "iteration 1300 / 2000: loss 0.249438\n",
      "iteration 1400 / 2000: loss 0.246418\n",
      "iteration 1500 / 2000: loss 0.290305\n",
      "iteration 1600 / 2000: loss 0.189101\n",
      "iteration 1700 / 2000: loss 0.200448\n",
      "iteration 1800 / 2000: loss 0.190238\n",
      "iteration 1900 / 2000: loss 0.212608\n",
      "Hidden Size: 20, Learning Rate: 0.00500005, LR Decay: 0.645, Regularization: 0.30000000000000004, Validation accuracy: 0.9506\n",
      "iteration 0 / 2000: loss 2.302629\n",
      "iteration 100 / 2000: loss 0.338472\n",
      "iteration 200 / 2000: loss 0.361903\n",
      "iteration 300 / 2000: loss 0.227127\n",
      "iteration 400 / 2000: loss 0.274155\n",
      "iteration 500 / 2000: loss 0.319278\n",
      "iteration 600 / 2000: loss 0.291381\n",
      "iteration 700 / 2000: loss 0.374944\n",
      "iteration 800 / 2000: loss 0.295851\n",
      "iteration 900 / 2000: loss 0.256673\n",
      "iteration 1000 / 2000: loss 0.271341\n",
      "iteration 1100 / 2000: loss 0.283147\n",
      "iteration 1200 / 2000: loss 0.223972\n",
      "iteration 1300 / 2000: loss 0.394198\n",
      "iteration 1400 / 2000: loss 0.381399\n",
      "iteration 1500 / 2000: loss 0.238674\n",
      "iteration 1600 / 2000: loss 0.275545\n",
      "iteration 1700 / 2000: loss 0.242151\n",
      "iteration 1800 / 2000: loss 0.192860\n",
      "iteration 1900 / 2000: loss 0.196919\n",
      "Hidden Size: 20, Learning Rate: 0.00500005, LR Decay: 0.645, Regularization: 0.4, Validation accuracy: 0.9484\n",
      "iteration 0 / 2000: loss 2.302582\n",
      "iteration 100 / 2000: loss 0.396357\n",
      "iteration 200 / 2000: loss 0.227873\n",
      "iteration 300 / 2000: loss 0.328181\n",
      "iteration 400 / 2000: loss 0.234862\n",
      "iteration 500 / 2000: loss 0.195495\n",
      "iteration 600 / 2000: loss 0.189667\n",
      "iteration 700 / 2000: loss 0.257098\n",
      "iteration 800 / 2000: loss 0.187275\n",
      "iteration 900 / 2000: loss 0.233861\n",
      "iteration 1000 / 2000: loss 0.137412\n",
      "iteration 1100 / 2000: loss 0.143246\n",
      "iteration 1200 / 2000: loss 0.170411\n",
      "iteration 1300 / 2000: loss 0.169064\n",
      "iteration 1400 / 2000: loss 0.159111\n",
      "iteration 1500 / 2000: loss 0.144993\n",
      "iteration 1600 / 2000: loss 0.117280\n",
      "iteration 1700 / 2000: loss 0.190800\n",
      "iteration 1800 / 2000: loss 0.215117\n",
      "iteration 1900 / 2000: loss 0.200182\n",
      "Hidden Size: 20, Learning Rate: 0.00500005, LR Decay: 0.47250000000000003, Regularization: 0.0, Validation accuracy: 0.9476\n",
      "iteration 0 / 2000: loss 2.302595\n",
      "iteration 100 / 2000: loss 0.347575\n",
      "iteration 200 / 2000: loss 0.365668\n",
      "iteration 300 / 2000: loss 0.255669\n",
      "iteration 400 / 2000: loss 0.260248\n",
      "iteration 500 / 2000: loss 0.307485\n",
      "iteration 600 / 2000: loss 0.176458\n",
      "iteration 700 / 2000: loss 0.206564\n",
      "iteration 800 / 2000: loss 0.231151\n",
      "iteration 900 / 2000: loss 0.271373\n",
      "iteration 1000 / 2000: loss 0.209354\n",
      "iteration 1100 / 2000: loss 0.179113\n",
      "iteration 1200 / 2000: loss 0.161232\n",
      "iteration 1300 / 2000: loss 0.298382\n",
      "iteration 1400 / 2000: loss 0.160883\n",
      "iteration 1500 / 2000: loss 0.139266\n",
      "iteration 1600 / 2000: loss 0.218629\n",
      "iteration 1700 / 2000: loss 0.187678\n",
      "iteration 1800 / 2000: loss 0.241717\n",
      "iteration 1900 / 2000: loss 0.190600\n",
      "Hidden Size: 20, Learning Rate: 0.00500005, LR Decay: 0.47250000000000003, Regularization: 0.1, Validation accuracy: 0.9464\n",
      "iteration 0 / 2000: loss 2.302610\n",
      "iteration 100 / 2000: loss 0.417182\n",
      "iteration 200 / 2000: loss 0.274038\n",
      "iteration 300 / 2000: loss 0.357053\n",
      "iteration 400 / 2000: loss 0.298990\n",
      "iteration 500 / 2000: loss 0.229288\n",
      "iteration 600 / 2000: loss 0.236311\n",
      "iteration 700 / 2000: loss 0.185838\n",
      "iteration 800 / 2000: loss 0.249717\n",
      "iteration 900 / 2000: loss 0.278697\n",
      "iteration 1000 / 2000: loss 0.213546\n",
      "iteration 1100 / 2000: loss 0.200880\n",
      "iteration 1200 / 2000: loss 0.260262\n",
      "iteration 1300 / 2000: loss 0.278097\n",
      "iteration 1400 / 2000: loss 0.175581\n",
      "iteration 1500 / 2000: loss 0.213583\n",
      "iteration 1600 / 2000: loss 0.325257\n",
      "iteration 1700 / 2000: loss 0.169006\n",
      "iteration 1800 / 2000: loss 0.264777\n",
      "iteration 1900 / 2000: loss 0.170603\n",
      "Hidden Size: 20, Learning Rate: 0.00500005, LR Decay: 0.47250000000000003, Regularization: 0.2, Validation accuracy: 0.9402\n",
      "iteration 0 / 2000: loss 2.302615\n",
      "iteration 100 / 2000: loss 0.464239\n",
      "iteration 200 / 2000: loss 0.292821\n",
      "iteration 300 / 2000: loss 0.351652\n",
      "iteration 400 / 2000: loss 0.304566\n",
      "iteration 500 / 2000: loss 0.289148\n",
      "iteration 600 / 2000: loss 0.224028\n",
      "iteration 700 / 2000: loss 0.283193\n",
      "iteration 800 / 2000: loss 0.302374\n",
      "iteration 900 / 2000: loss 0.250576\n",
      "iteration 1000 / 2000: loss 0.201009\n",
      "iteration 1100 / 2000: loss 0.190911\n",
      "iteration 1200 / 2000: loss 0.248847\n",
      "iteration 1300 / 2000: loss 0.322067\n",
      "iteration 1400 / 2000: loss 0.249152\n",
      "iteration 1500 / 2000: loss 0.292425\n",
      "iteration 1600 / 2000: loss 0.296308\n",
      "iteration 1700 / 2000: loss 0.257402\n",
      "iteration 1800 / 2000: loss 0.247648\n",
      "iteration 1900 / 2000: loss 0.230959\n",
      "Hidden Size: 20, Learning Rate: 0.00500005, LR Decay: 0.47250000000000003, Regularization: 0.30000000000000004, Validation accuracy: 0.942\n",
      "iteration 0 / 2000: loss 2.302628\n",
      "iteration 100 / 2000: loss 0.483612\n",
      "iteration 200 / 2000: loss 0.405744\n",
      "iteration 300 / 2000: loss 0.382440\n",
      "iteration 400 / 2000: loss 0.311478\n",
      "iteration 500 / 2000: loss 0.253585\n",
      "iteration 600 / 2000: loss 0.268792\n",
      "iteration 700 / 2000: loss 0.269037\n",
      "iteration 800 / 2000: loss 0.347825\n",
      "iteration 900 / 2000: loss 0.257964\n",
      "iteration 1000 / 2000: loss 0.281685\n",
      "iteration 1100 / 2000: loss 0.316352\n",
      "iteration 1200 / 2000: loss 0.260751\n",
      "iteration 1300 / 2000: loss 0.289478\n",
      "iteration 1400 / 2000: loss 0.264264\n",
      "iteration 1500 / 2000: loss 0.333572\n",
      "iteration 1600 / 2000: loss 0.283178\n",
      "iteration 1700 / 2000: loss 0.246385\n",
      "iteration 1800 / 2000: loss 0.317015\n",
      "iteration 1900 / 2000: loss 0.370785\n",
      "Hidden Size: 20, Learning Rate: 0.00500005, LR Decay: 0.47250000000000003, Regularization: 0.4, Validation accuracy: 0.9388\n",
      "iteration 0 / 2000: loss 2.302575\n",
      "iteration 100 / 2000: loss 0.472950\n",
      "iteration 200 / 2000: loss 0.408065\n",
      "iteration 300 / 2000: loss 0.448608\n",
      "iteration 400 / 2000: loss 0.283111\n",
      "iteration 500 / 2000: loss 0.344968\n",
      "iteration 600 / 2000: loss 0.195761\n",
      "iteration 700 / 2000: loss 0.206706\n",
      "iteration 800 / 2000: loss 0.274936\n",
      "iteration 900 / 2000: loss 0.229842\n",
      "iteration 1000 / 2000: loss 0.274008\n",
      "iteration 1100 / 2000: loss 0.256942\n",
      "iteration 1200 / 2000: loss 0.251300\n",
      "iteration 1300 / 2000: loss 0.134677\n",
      "iteration 1400 / 2000: loss 0.359971\n",
      "iteration 1500 / 2000: loss 0.263724\n",
      "iteration 1600 / 2000: loss 0.263661\n",
      "iteration 1700 / 2000: loss 0.270065\n",
      "iteration 1800 / 2000: loss 0.213777\n",
      "iteration 1900 / 2000: loss 0.212624\n",
      "Hidden Size: 20, Learning Rate: 0.00500005, LR Decay: 0.3, Regularization: 0.0, Validation accuracy: 0.929\n",
      "iteration 0 / 2000: loss 2.302594\n",
      "iteration 100 / 2000: loss 0.494502\n",
      "iteration 200 / 2000: loss 0.298119\n",
      "iteration 300 / 2000: loss 0.291432\n",
      "iteration 400 / 2000: loss 0.288482\n",
      "iteration 500 / 2000: loss 0.343636\n",
      "iteration 600 / 2000: loss 0.308677\n",
      "iteration 700 / 2000: loss 0.231671\n",
      "iteration 800 / 2000: loss 0.280474\n",
      "iteration 900 / 2000: loss 0.322273\n",
      "iteration 1000 / 2000: loss 0.248880\n",
      "iteration 1100 / 2000: loss 0.272490\n",
      "iteration 1200 / 2000: loss 0.217179\n",
      "iteration 1300 / 2000: loss 0.351277\n",
      "iteration 1400 / 2000: loss 0.282284\n",
      "iteration 1500 / 2000: loss 0.324263\n",
      "iteration 1600 / 2000: loss 0.298792\n",
      "iteration 1700 / 2000: loss 0.227553\n",
      "iteration 1800 / 2000: loss 0.349636\n",
      "iteration 1900 / 2000: loss 0.227852\n",
      "Hidden Size: 20, Learning Rate: 0.00500005, LR Decay: 0.3, Regularization: 0.1, Validation accuracy: 0.9258\n",
      "iteration 0 / 2000: loss 2.302613\n",
      "iteration 100 / 2000: loss 0.667762\n",
      "iteration 200 / 2000: loss 0.207244\n",
      "iteration 300 / 2000: loss 0.284636\n",
      "iteration 400 / 2000: loss 0.306623\n",
      "iteration 500 / 2000: loss 0.363331\n",
      "iteration 600 / 2000: loss 0.323635\n",
      "iteration 700 / 2000: loss 0.239460\n",
      "iteration 800 / 2000: loss 0.271546\n",
      "iteration 900 / 2000: loss 0.207855\n",
      "iteration 1000 / 2000: loss 0.264491\n",
      "iteration 1100 / 2000: loss 0.324321\n",
      "iteration 1200 / 2000: loss 0.226618\n",
      "iteration 1300 / 2000: loss 0.322619\n",
      "iteration 1400 / 2000: loss 0.307032\n",
      "iteration 1500 / 2000: loss 0.270929\n",
      "iteration 1600 / 2000: loss 0.307991\n",
      "iteration 1700 / 2000: loss 0.365448\n",
      "iteration 1800 / 2000: loss 0.274331\n",
      "iteration 1900 / 2000: loss 0.210846\n",
      "Hidden Size: 20, Learning Rate: 0.00500005, LR Decay: 0.3, Regularization: 0.2, Validation accuracy: 0.9298\n",
      "iteration 0 / 2000: loss 2.302614\n",
      "iteration 100 / 2000: loss 0.658234\n",
      "iteration 200 / 2000: loss 0.385905\n",
      "iteration 300 / 2000: loss 0.322418\n",
      "iteration 400 / 2000: loss 0.330789\n",
      "iteration 500 / 2000: loss 0.381723\n",
      "iteration 600 / 2000: loss 0.253874\n",
      "iteration 700 / 2000: loss 0.398543\n",
      "iteration 800 / 2000: loss 0.296970\n",
      "iteration 900 / 2000: loss 0.285792\n",
      "iteration 1000 / 2000: loss 0.267064\n",
      "iteration 1100 / 2000: loss 0.266463\n",
      "iteration 1200 / 2000: loss 0.294832\n",
      "iteration 1300 / 2000: loss 0.284722\n",
      "iteration 1400 / 2000: loss 0.243683\n",
      "iteration 1500 / 2000: loss 0.323644\n",
      "iteration 1600 / 2000: loss 0.280194\n",
      "iteration 1700 / 2000: loss 0.303622\n",
      "iteration 1800 / 2000: loss 0.317536\n",
      "iteration 1900 / 2000: loss 0.340457\n",
      "Hidden Size: 20, Learning Rate: 0.00500005, LR Decay: 0.3, Regularization: 0.30000000000000004, Validation accuracy: 0.9286\n",
      "iteration 0 / 2000: loss 2.302622\n",
      "iteration 100 / 2000: loss 0.588976\n",
      "iteration 200 / 2000: loss 0.404079\n",
      "iteration 300 / 2000: loss 0.368403\n",
      "iteration 400 / 2000: loss 0.347123\n",
      "iteration 500 / 2000: loss 0.348715\n",
      "iteration 600 / 2000: loss 0.312459\n",
      "iteration 700 / 2000: loss 0.334239\n",
      "iteration 800 / 2000: loss 0.331864\n",
      "iteration 900 / 2000: loss 0.227941\n",
      "iteration 1000 / 2000: loss 0.313637\n",
      "iteration 1100 / 2000: loss 0.252078\n",
      "iteration 1200 / 2000: loss 0.314334\n",
      "iteration 1300 / 2000: loss 0.246817\n",
      "iteration 1400 / 2000: loss 0.253405\n",
      "iteration 1500 / 2000: loss 0.359037\n",
      "iteration 1600 / 2000: loss 0.268171\n",
      "iteration 1700 / 2000: loss 0.309741\n",
      "iteration 1800 / 2000: loss 0.336576\n",
      "iteration 1900 / 2000: loss 0.281892\n",
      "Hidden Size: 20, Learning Rate: 0.00500005, LR Decay: 0.3, Regularization: 0.4, Validation accuracy: 0.9276\n",
      "iteration 0 / 2000: loss 2.302576\n",
      "iteration 100 / 2000: loss 0.464673\n",
      "iteration 200 / 2000: loss 0.253136\n",
      "iteration 300 / 2000: loss 0.289559\n",
      "iteration 400 / 2000: loss 0.195499\n",
      "iteration 500 / 2000: loss 0.133942\n",
      "iteration 600 / 2000: loss 0.209401\n",
      "iteration 700 / 2000: loss 0.183692\n",
      "iteration 800 / 2000: loss 0.163643\n",
      "iteration 900 / 2000: loss 0.180083\n",
      "iteration 1000 / 2000: loss 0.155678\n",
      "iteration 1100 / 2000: loss 0.162426\n",
      "iteration 1200 / 2000: loss 0.160711\n",
      "iteration 1300 / 2000: loss 0.137042\n",
      "iteration 1400 / 2000: loss 0.158471\n",
      "iteration 1500 / 2000: loss 0.137645\n",
      "iteration 1600 / 2000: loss 0.096287\n",
      "iteration 1700 / 2000: loss 0.154025\n",
      "iteration 1800 / 2000: loss 0.062660\n",
      "iteration 1900 / 2000: loss 0.137515\n",
      "Hidden Size: 20, Learning Rate: 0.002500074999999999, LR Decay: 0.99, Regularization: 0.0, Validation accuracy: 0.9594\n",
      "iteration 0 / 2000: loss 2.302584\n",
      "iteration 100 / 2000: loss 0.420968\n",
      "iteration 200 / 2000: loss 0.251194\n",
      "iteration 300 / 2000: loss 0.322786\n",
      "iteration 400 / 2000: loss 0.258795\n",
      "iteration 500 / 2000: loss 0.286770\n",
      "iteration 600 / 2000: loss 0.203498\n",
      "iteration 700 / 2000: loss 0.131963\n",
      "iteration 800 / 2000: loss 0.181949\n",
      "iteration 900 / 2000: loss 0.208709\n",
      "iteration 1000 / 2000: loss 0.145911\n",
      "iteration 1100 / 2000: loss 0.182639\n",
      "iteration 1200 / 2000: loss 0.122981\n",
      "iteration 1300 / 2000: loss 0.189636\n",
      "iteration 1400 / 2000: loss 0.167280\n",
      "iteration 1500 / 2000: loss 0.134907\n",
      "iteration 1600 / 2000: loss 0.151008\n",
      "iteration 1700 / 2000: loss 0.141948\n",
      "iteration 1800 / 2000: loss 0.106437\n",
      "iteration 1900 / 2000: loss 0.211577\n",
      "Hidden Size: 20, Learning Rate: 0.002500074999999999, LR Decay: 0.99, Regularization: 0.1, Validation accuracy: 0.958\n",
      "iteration 0 / 2000: loss 2.302595\n",
      "iteration 100 / 2000: loss 0.429124\n",
      "iteration 200 / 2000: loss 0.319538\n",
      "iteration 300 / 2000: loss 0.285822\n",
      "iteration 400 / 2000: loss 0.294296\n",
      "iteration 500 / 2000: loss 0.251903\n",
      "iteration 600 / 2000: loss 0.157224\n",
      "iteration 700 / 2000: loss 0.268686\n",
      "iteration 800 / 2000: loss 0.223350\n",
      "iteration 900 / 2000: loss 0.243876\n",
      "iteration 1000 / 2000: loss 0.207857\n",
      "iteration 1100 / 2000: loss 0.220170\n",
      "iteration 1200 / 2000: loss 0.190082\n",
      "iteration 1300 / 2000: loss 0.193096\n",
      "iteration 1400 / 2000: loss 0.224877\n",
      "iteration 1500 / 2000: loss 0.138184\n",
      "iteration 1600 / 2000: loss 0.195197\n",
      "iteration 1700 / 2000: loss 0.223919\n",
      "iteration 1800 / 2000: loss 0.166767\n",
      "iteration 1900 / 2000: loss 0.127950\n",
      "Hidden Size: 20, Learning Rate: 0.002500074999999999, LR Decay: 0.99, Regularization: 0.2, Validation accuracy: 0.9608\n",
      "iteration 0 / 2000: loss 2.302602\n",
      "iteration 100 / 2000: loss 0.423138\n",
      "iteration 200 / 2000: loss 0.470136\n",
      "iteration 300 / 2000: loss 0.312307\n",
      "iteration 400 / 2000: loss 0.221423\n",
      "iteration 500 / 2000: loss 0.222306\n",
      "iteration 600 / 2000: loss 0.233925\n",
      "iteration 700 / 2000: loss 0.236689\n",
      "iteration 800 / 2000: loss 0.293403\n",
      "iteration 900 / 2000: loss 0.191474\n",
      "iteration 1000 / 2000: loss 0.214396\n",
      "iteration 1100 / 2000: loss 0.244140\n",
      "iteration 1200 / 2000: loss 0.244016\n",
      "iteration 1300 / 2000: loss 0.239942\n",
      "iteration 1400 / 2000: loss 0.271819\n",
      "iteration 1500 / 2000: loss 0.225286\n",
      "iteration 1600 / 2000: loss 0.184445\n",
      "iteration 1700 / 2000: loss 0.193604\n",
      "iteration 1800 / 2000: loss 0.185527\n",
      "iteration 1900 / 2000: loss 0.184753\n",
      "Hidden Size: 20, Learning Rate: 0.002500074999999999, LR Decay: 0.99, Regularization: 0.30000000000000004, Validation accuracy: 0.9558\n",
      "iteration 0 / 2000: loss 2.302611\n",
      "iteration 100 / 2000: loss 0.476131\n",
      "iteration 200 / 2000: loss 0.396726\n",
      "iteration 300 / 2000: loss 0.256399\n",
      "iteration 400 / 2000: loss 0.229014\n",
      "iteration 500 / 2000: loss 0.325566\n",
      "iteration 600 / 2000: loss 0.241066\n",
      "iteration 700 / 2000: loss 0.216806\n",
      "iteration 800 / 2000: loss 0.233411\n",
      "iteration 900 / 2000: loss 0.212233\n",
      "iteration 1000 / 2000: loss 0.239184\n",
      "iteration 1100 / 2000: loss 0.242404\n",
      "iteration 1200 / 2000: loss 0.181286\n",
      "iteration 1300 / 2000: loss 0.260423\n",
      "iteration 1400 / 2000: loss 0.177585\n",
      "iteration 1500 / 2000: loss 0.264758\n",
      "iteration 1600 / 2000: loss 0.247267\n",
      "iteration 1700 / 2000: loss 0.254243\n",
      "iteration 1800 / 2000: loss 0.222545\n",
      "iteration 1900 / 2000: loss 0.286703\n",
      "Hidden Size: 20, Learning Rate: 0.002500074999999999, LR Decay: 0.99, Regularization: 0.4, Validation accuracy: 0.9544\n",
      "iteration 0 / 2000: loss 2.302586\n",
      "iteration 100 / 2000: loss 0.363995\n",
      "iteration 200 / 2000: loss 0.336490\n",
      "iteration 300 / 2000: loss 0.279153\n",
      "iteration 400 / 2000: loss 0.164707\n",
      "iteration 500 / 2000: loss 0.152599\n",
      "iteration 600 / 2000: loss 0.205672\n",
      "iteration 700 / 2000: loss 0.259486\n",
      "iteration 800 / 2000: loss 0.323188\n",
      "iteration 900 / 2000: loss 0.269533\n",
      "iteration 1000 / 2000: loss 0.202527\n",
      "iteration 1100 / 2000: loss 0.135996\n",
      "iteration 1200 / 2000: loss 0.134138\n",
      "iteration 1300 / 2000: loss 0.260102\n",
      "iteration 1400 / 2000: loss 0.119794\n",
      "iteration 1500 / 2000: loss 0.181152\n",
      "iteration 1600 / 2000: loss 0.167932\n",
      "iteration 1700 / 2000: loss 0.112529\n",
      "iteration 1800 / 2000: loss 0.086726\n",
      "iteration 1900 / 2000: loss 0.141099\n",
      "Hidden Size: 20, Learning Rate: 0.002500074999999999, LR Decay: 0.8175, Regularization: 0.0, Validation accuracy: 0.953\n",
      "iteration 0 / 2000: loss 2.302583\n",
      "iteration 100 / 2000: loss 0.452321\n",
      "iteration 200 / 2000: loss 0.304549\n",
      "iteration 300 / 2000: loss 0.277818\n",
      "iteration 400 / 2000: loss 0.196790\n",
      "iteration 500 / 2000: loss 0.184921\n",
      "iteration 600 / 2000: loss 0.160922\n",
      "iteration 700 / 2000: loss 0.233782\n",
      "iteration 800 / 2000: loss 0.187706\n",
      "iteration 900 / 2000: loss 0.183157\n",
      "iteration 1000 / 2000: loss 0.131419\n",
      "iteration 1100 / 2000: loss 0.199684\n",
      "iteration 1200 / 2000: loss 0.147217\n",
      "iteration 1300 / 2000: loss 0.194965\n",
      "iteration 1400 / 2000: loss 0.254140\n",
      "iteration 1500 / 2000: loss 0.255648\n",
      "iteration 1600 / 2000: loss 0.177368\n",
      "iteration 1700 / 2000: loss 0.160021\n",
      "iteration 1800 / 2000: loss 0.173725\n",
      "iteration 1900 / 2000: loss 0.160400\n",
      "Hidden Size: 20, Learning Rate: 0.002500074999999999, LR Decay: 0.8175, Regularization: 0.1, Validation accuracy: 0.9548\n",
      "iteration 0 / 2000: loss 2.302602\n",
      "iteration 100 / 2000: loss 0.418958\n",
      "iteration 200 / 2000: loss 0.356529\n",
      "iteration 300 / 2000: loss 0.358654\n",
      "iteration 400 / 2000: loss 0.340458\n",
      "iteration 500 / 2000: loss 0.368030\n",
      "iteration 600 / 2000: loss 0.232074\n",
      "iteration 700 / 2000: loss 0.260625\n",
      "iteration 800 / 2000: loss 0.188905\n",
      "iteration 900 / 2000: loss 0.153478\n",
      "iteration 1000 / 2000: loss 0.254398\n",
      "iteration 1100 / 2000: loss 0.265704\n",
      "iteration 1200 / 2000: loss 0.196411\n",
      "iteration 1300 / 2000: loss 0.186675\n",
      "iteration 1400 / 2000: loss 0.178694\n",
      "iteration 1500 / 2000: loss 0.221017\n",
      "iteration 1600 / 2000: loss 0.298412\n",
      "iteration 1700 / 2000: loss 0.232802\n",
      "iteration 1800 / 2000: loss 0.160692\n",
      "iteration 1900 / 2000: loss 0.210325\n",
      "Hidden Size: 20, Learning Rate: 0.002500074999999999, LR Decay: 0.8175, Regularization: 0.2, Validation accuracy: 0.9556\n",
      "iteration 0 / 2000: loss 2.302607\n",
      "iteration 100 / 2000: loss 0.373804\n",
      "iteration 200 / 2000: loss 0.205346\n",
      "iteration 300 / 2000: loss 0.290477\n",
      "iteration 400 / 2000: loss 0.253678\n",
      "iteration 500 / 2000: loss 0.297088\n",
      "iteration 600 / 2000: loss 0.307231\n",
      "iteration 700 / 2000: loss 0.267063\n",
      "iteration 800 / 2000: loss 0.279635\n",
      "iteration 900 / 2000: loss 0.277502\n",
      "iteration 1000 / 2000: loss 0.205470\n",
      "iteration 1100 / 2000: loss 0.275442\n",
      "iteration 1200 / 2000: loss 0.274530\n",
      "iteration 1300 / 2000: loss 0.245551\n",
      "iteration 1400 / 2000: loss 0.247899\n",
      "iteration 1500 / 2000: loss 0.181696\n",
      "iteration 1600 / 2000: loss 0.244427\n",
      "iteration 1700 / 2000: loss 0.243977\n",
      "iteration 1800 / 2000: loss 0.265671\n",
      "iteration 1900 / 2000: loss 0.209354\n",
      "Hidden Size: 20, Learning Rate: 0.002500074999999999, LR Decay: 0.8175, Regularization: 0.30000000000000004, Validation accuracy: 0.9506\n",
      "iteration 0 / 2000: loss 2.302612\n",
      "iteration 100 / 2000: loss 0.470790\n",
      "iteration 200 / 2000: loss 0.345846\n",
      "iteration 300 / 2000: loss 0.331263\n",
      "iteration 400 / 2000: loss 0.348751\n",
      "iteration 500 / 2000: loss 0.263616\n",
      "iteration 600 / 2000: loss 0.258511\n",
      "iteration 700 / 2000: loss 0.231674\n",
      "iteration 800 / 2000: loss 0.238224\n",
      "iteration 900 / 2000: loss 0.282738\n",
      "iteration 1000 / 2000: loss 0.330295\n",
      "iteration 1100 / 2000: loss 0.241256\n",
      "iteration 1200 / 2000: loss 0.320478\n",
      "iteration 1300 / 2000: loss 0.280340\n",
      "iteration 1400 / 2000: loss 0.291576\n",
      "iteration 1500 / 2000: loss 0.248973\n",
      "iteration 1600 / 2000: loss 0.220067\n",
      "iteration 1700 / 2000: loss 0.235843\n",
      "iteration 1800 / 2000: loss 0.212485\n",
      "iteration 1900 / 2000: loss 0.265417\n",
      "Hidden Size: 20, Learning Rate: 0.002500074999999999, LR Decay: 0.8175, Regularization: 0.4, Validation accuracy: 0.9486\n",
      "iteration 0 / 2000: loss 2.302585\n",
      "iteration 100 / 2000: loss 0.425352\n",
      "iteration 200 / 2000: loss 0.305339\n",
      "iteration 300 / 2000: loss 0.218410\n",
      "iteration 400 / 2000: loss 0.329056\n",
      "iteration 500 / 2000: loss 0.220909\n",
      "iteration 600 / 2000: loss 0.219737\n",
      "iteration 700 / 2000: loss 0.203514\n",
      "iteration 800 / 2000: loss 0.237355\n",
      "iteration 900 / 2000: loss 0.206491\n",
      "iteration 1000 / 2000: loss 0.138530\n",
      "iteration 1100 / 2000: loss 0.213501\n",
      "iteration 1200 / 2000: loss 0.298608\n",
      "iteration 1300 / 2000: loss 0.113085\n",
      "iteration 1400 / 2000: loss 0.147441\n",
      "iteration 1500 / 2000: loss 0.128722\n",
      "iteration 1600 / 2000: loss 0.259911\n",
      "iteration 1700 / 2000: loss 0.218459\n",
      "iteration 1800 / 2000: loss 0.141403\n",
      "iteration 1900 / 2000: loss 0.182456\n",
      "Hidden Size: 20, Learning Rate: 0.002500074999999999, LR Decay: 0.645, Regularization: 0.0, Validation accuracy: 0.9428\n",
      "iteration 0 / 2000: loss 2.302595\n",
      "iteration 100 / 2000: loss 0.500334\n",
      "iteration 200 / 2000: loss 0.329797\n",
      "iteration 300 / 2000: loss 0.395403\n",
      "iteration 400 / 2000: loss 0.313811\n",
      "iteration 500 / 2000: loss 0.244838\n",
      "iteration 600 / 2000: loss 0.316860\n",
      "iteration 700 / 2000: loss 0.249266\n",
      "iteration 800 / 2000: loss 0.174092\n",
      "iteration 900 / 2000: loss 0.267863\n",
      "iteration 1000 / 2000: loss 0.275666\n",
      "iteration 1100 / 2000: loss 0.284026\n",
      "iteration 1200 / 2000: loss 0.244306\n",
      "iteration 1300 / 2000: loss 0.254813\n",
      "iteration 1400 / 2000: loss 0.189711\n",
      "iteration 1500 / 2000: loss 0.217914\n",
      "iteration 1600 / 2000: loss 0.144319\n",
      "iteration 1700 / 2000: loss 0.259345\n",
      "iteration 1800 / 2000: loss 0.180154\n",
      "iteration 1900 / 2000: loss 0.243985\n",
      "Hidden Size: 20, Learning Rate: 0.002500074999999999, LR Decay: 0.645, Regularization: 0.1, Validation accuracy: 0.943\n",
      "iteration 0 / 2000: loss 2.302604\n",
      "iteration 100 / 2000: loss 0.539926\n",
      "iteration 200 / 2000: loss 0.276290\n",
      "iteration 300 / 2000: loss 0.318004\n",
      "iteration 400 / 2000: loss 0.203451\n",
      "iteration 500 / 2000: loss 0.340748\n",
      "iteration 600 / 2000: loss 0.235259\n",
      "iteration 700 / 2000: loss 0.222007\n",
      "iteration 800 / 2000: loss 0.214795\n",
      "iteration 900 / 2000: loss 0.311641\n",
      "iteration 1000 / 2000: loss 0.300883\n",
      "iteration 1100 / 2000: loss 0.232852\n",
      "iteration 1200 / 2000: loss 0.299875\n",
      "iteration 1300 / 2000: loss 0.251703\n",
      "iteration 1400 / 2000: loss 0.229491\n",
      "iteration 1500 / 2000: loss 0.259927\n",
      "iteration 1600 / 2000: loss 0.190359\n",
      "iteration 1700 / 2000: loss 0.200161\n",
      "iteration 1800 / 2000: loss 0.202914\n",
      "iteration 1900 / 2000: loss 0.282906\n",
      "Hidden Size: 20, Learning Rate: 0.002500074999999999, LR Decay: 0.645, Regularization: 0.2, Validation accuracy: 0.9438\n",
      "iteration 0 / 2000: loss 2.302605\n",
      "iteration 100 / 2000: loss 0.616361\n",
      "iteration 200 / 2000: loss 0.434789\n",
      "iteration 300 / 2000: loss 0.346978\n",
      "iteration 400 / 2000: loss 0.214969\n",
      "iteration 500 / 2000: loss 0.305528\n",
      "iteration 600 / 2000: loss 0.292531\n",
      "iteration 700 / 2000: loss 0.392859\n",
      "iteration 800 / 2000: loss 0.270036\n",
      "iteration 900 / 2000: loss 0.289453\n",
      "iteration 1000 / 2000: loss 0.298158\n",
      "iteration 1100 / 2000: loss 0.354931\n",
      "iteration 1200 / 2000: loss 0.254342\n",
      "iteration 1300 / 2000: loss 0.215570\n",
      "iteration 1400 / 2000: loss 0.266922\n",
      "iteration 1500 / 2000: loss 0.276793\n",
      "iteration 1600 / 2000: loss 0.249912\n",
      "iteration 1700 / 2000: loss 0.266520\n",
      "iteration 1800 / 2000: loss 0.249332\n",
      "iteration 1900 / 2000: loss 0.274073\n",
      "Hidden Size: 20, Learning Rate: 0.002500074999999999, LR Decay: 0.645, Regularization: 0.30000000000000004, Validation accuracy: 0.9412\n",
      "iteration 0 / 2000: loss 2.302621\n",
      "iteration 100 / 2000: loss 0.489292\n",
      "iteration 200 / 2000: loss 0.345186\n",
      "iteration 300 / 2000: loss 0.276760\n",
      "iteration 400 / 2000: loss 0.285173\n",
      "iteration 500 / 2000: loss 0.301474\n",
      "iteration 600 / 2000: loss 0.251060\n",
      "iteration 700 / 2000: loss 0.283295\n",
      "iteration 800 / 2000: loss 0.328171\n",
      "iteration 900 / 2000: loss 0.397051\n",
      "iteration 1000 / 2000: loss 0.239284\n",
      "iteration 1100 / 2000: loss 0.277208\n",
      "iteration 1200 / 2000: loss 0.336981\n",
      "iteration 1300 / 2000: loss 0.231131\n",
      "iteration 1400 / 2000: loss 0.321703\n",
      "iteration 1500 / 2000: loss 0.251057\n",
      "iteration 1600 / 2000: loss 0.208928\n",
      "iteration 1700 / 2000: loss 0.311500\n",
      "iteration 1800 / 2000: loss 0.303476\n",
      "iteration 1900 / 2000: loss 0.319531\n",
      "Hidden Size: 20, Learning Rate: 0.002500074999999999, LR Decay: 0.645, Regularization: 0.4, Validation accuracy: 0.9444\n",
      "iteration 0 / 2000: loss 2.302594\n",
      "iteration 100 / 2000: loss 1.027355\n",
      "iteration 200 / 2000: loss 0.411782\n",
      "iteration 300 / 2000: loss 0.289146\n",
      "iteration 400 / 2000: loss 0.294970\n",
      "iteration 500 / 2000: loss 0.303872\n",
      "iteration 600 / 2000: loss 0.347328\n",
      "iteration 700 / 2000: loss 0.271190\n",
      "iteration 800 / 2000: loss 0.290028\n",
      "iteration 900 / 2000: loss 0.318984\n",
      "iteration 1000 / 2000: loss 0.289052\n",
      "iteration 1100 / 2000: loss 0.339617\n",
      "iteration 1200 / 2000: loss 0.280801\n",
      "iteration 1300 / 2000: loss 0.321451\n",
      "iteration 1400 / 2000: loss 0.286427\n",
      "iteration 1500 / 2000: loss 0.311371\n",
      "iteration 1600 / 2000: loss 0.306147\n",
      "iteration 1700 / 2000: loss 0.213422\n",
      "iteration 1800 / 2000: loss 0.277042\n",
      "iteration 1900 / 2000: loss 0.274488\n",
      "Hidden Size: 20, Learning Rate: 0.002500074999999999, LR Decay: 0.47250000000000003, Regularization: 0.0, Validation accuracy: 0.9322\n",
      "iteration 0 / 2000: loss 2.302590\n",
      "iteration 100 / 2000: loss 0.830512\n",
      "iteration 200 / 2000: loss 0.471857\n",
      "iteration 300 / 2000: loss 0.359523\n",
      "iteration 400 / 2000: loss 0.306511\n",
      "iteration 500 / 2000: loss 0.218376\n",
      "iteration 600 / 2000: loss 0.339828\n",
      "iteration 700 / 2000: loss 0.213918\n",
      "iteration 800 / 2000: loss 0.238151\n",
      "iteration 900 / 2000: loss 0.227512\n",
      "iteration 1000 / 2000: loss 0.287309\n",
      "iteration 1100 / 2000: loss 0.194350\n",
      "iteration 1200 / 2000: loss 0.174865\n",
      "iteration 1300 / 2000: loss 0.278768\n",
      "iteration 1400 / 2000: loss 0.226320\n",
      "iteration 1500 / 2000: loss 0.289403\n",
      "iteration 1600 / 2000: loss 0.184712\n",
      "iteration 1700 / 2000: loss 0.290943\n",
      "iteration 1800 / 2000: loss 0.251573\n",
      "iteration 1900 / 2000: loss 0.293638\n",
      "Hidden Size: 20, Learning Rate: 0.002500074999999999, LR Decay: 0.47250000000000003, Regularization: 0.1, Validation accuracy: 0.9298\n",
      "iteration 0 / 2000: loss 2.302578\n",
      "iteration 100 / 2000: loss 0.737291\n",
      "iteration 200 / 2000: loss 0.528739\n",
      "iteration 300 / 2000: loss 0.318328\n",
      "iteration 400 / 2000: loss 0.243977\n",
      "iteration 500 / 2000: loss 0.344916\n",
      "iteration 600 / 2000: loss 0.276249\n",
      "iteration 700 / 2000: loss 0.296777\n",
      "iteration 800 / 2000: loss 0.280509\n",
      "iteration 900 / 2000: loss 0.318709\n",
      "iteration 1000 / 2000: loss 0.237165\n",
      "iteration 1100 / 2000: loss 0.320964\n",
      "iteration 1200 / 2000: loss 0.330641\n",
      "iteration 1300 / 2000: loss 0.298161\n",
      "iteration 1400 / 2000: loss 0.273104\n",
      "iteration 1500 / 2000: loss 0.222141\n",
      "iteration 1600 / 2000: loss 0.255430\n",
      "iteration 1700 / 2000: loss 0.315302\n",
      "iteration 1800 / 2000: loss 0.316508\n",
      "iteration 1900 / 2000: loss 0.286738\n",
      "Hidden Size: 20, Learning Rate: 0.002500074999999999, LR Decay: 0.47250000000000003, Regularization: 0.2, Validation accuracy: 0.928\n",
      "iteration 0 / 2000: loss 2.302611\n",
      "iteration 100 / 2000: loss 0.801852\n",
      "iteration 200 / 2000: loss 0.383336\n",
      "iteration 300 / 2000: loss 0.325188\n",
      "iteration 400 / 2000: loss 0.401546\n",
      "iteration 500 / 2000: loss 0.289034\n",
      "iteration 600 / 2000: loss 0.333023\n",
      "iteration 700 / 2000: loss 0.299547\n",
      "iteration 800 / 2000: loss 0.307128\n",
      "iteration 900 / 2000: loss 0.348718\n",
      "iteration 1000 / 2000: loss 0.290407\n",
      "iteration 1100 / 2000: loss 0.402414\n",
      "iteration 1200 / 2000: loss 0.348186\n",
      "iteration 1300 / 2000: loss 0.238219\n",
      "iteration 1400 / 2000: loss 0.323676\n",
      "iteration 1500 / 2000: loss 0.304745\n",
      "iteration 1600 / 2000: loss 0.262768\n",
      "iteration 1700 / 2000: loss 0.264424\n",
      "iteration 1800 / 2000: loss 0.231310\n",
      "iteration 1900 / 2000: loss 0.376849\n",
      "Hidden Size: 20, Learning Rate: 0.002500074999999999, LR Decay: 0.47250000000000003, Regularization: 0.30000000000000004, Validation accuracy: 0.9274\n",
      "iteration 0 / 2000: loss 2.302615\n",
      "iteration 100 / 2000: loss 0.855848\n",
      "iteration 200 / 2000: loss 0.388123\n",
      "iteration 300 / 2000: loss 0.349087\n",
      "iteration 400 / 2000: loss 0.405609\n",
      "iteration 500 / 2000: loss 0.396447\n",
      "iteration 600 / 2000: loss 0.461507\n",
      "iteration 700 / 2000: loss 0.316320\n",
      "iteration 800 / 2000: loss 0.206565\n",
      "iteration 900 / 2000: loss 0.327156\n",
      "iteration 1000 / 2000: loss 0.312830\n",
      "iteration 1100 / 2000: loss 0.301738\n",
      "iteration 1200 / 2000: loss 0.375170\n",
      "iteration 1300 / 2000: loss 0.247731\n",
      "iteration 1400 / 2000: loss 0.265621\n",
      "iteration 1500 / 2000: loss 0.267202\n",
      "iteration 1600 / 2000: loss 0.310526\n",
      "iteration 1700 / 2000: loss 0.323045\n",
      "iteration 1800 / 2000: loss 0.266395\n",
      "iteration 1900 / 2000: loss 0.247691\n",
      "Hidden Size: 20, Learning Rate: 0.002500074999999999, LR Decay: 0.47250000000000003, Regularization: 0.4, Validation accuracy: 0.9294\n",
      "iteration 0 / 2000: loss 2.302591\n",
      "iteration 100 / 2000: loss 1.587430\n",
      "iteration 200 / 2000: loss 0.672962\n",
      "iteration 300 / 2000: loss 0.498046\n",
      "iteration 400 / 2000: loss 0.341063\n",
      "iteration 500 / 2000: loss 0.272630\n",
      "iteration 600 / 2000: loss 0.247168\n",
      "iteration 700 / 2000: loss 0.282407\n",
      "iteration 800 / 2000: loss 0.254391\n",
      "iteration 900 / 2000: loss 0.231451\n",
      "iteration 1000 / 2000: loss 0.346933\n",
      "iteration 1100 / 2000: loss 0.299699\n",
      "iteration 1200 / 2000: loss 0.359496\n",
      "iteration 1300 / 2000: loss 0.307476\n",
      "iteration 1400 / 2000: loss 0.302204\n",
      "iteration 1500 / 2000: loss 0.282819\n",
      "iteration 1600 / 2000: loss 0.238552\n",
      "iteration 1700 / 2000: loss 0.331222\n",
      "iteration 1800 / 2000: loss 0.258725\n",
      "iteration 1900 / 2000: loss 0.260075\n",
      "Hidden Size: 20, Learning Rate: 0.002500074999999999, LR Decay: 0.3, Regularization: 0.0, Validation accuracy: 0.912\n",
      "iteration 0 / 2000: loss 2.302587\n",
      "iteration 100 / 2000: loss 1.527907\n",
      "iteration 200 / 2000: loss 0.596441\n",
      "iteration 300 / 2000: loss 0.331669\n",
      "iteration 400 / 2000: loss 0.299158\n",
      "iteration 500 / 2000: loss 0.376087\n",
      "iteration 600 / 2000: loss 0.298517\n",
      "iteration 700 / 2000: loss 0.311403\n",
      "iteration 800 / 2000: loss 0.306891\n",
      "iteration 900 / 2000: loss 0.382904\n",
      "iteration 1000 / 2000: loss 0.278455\n",
      "iteration 1100 / 2000: loss 0.318875\n",
      "iteration 1200 / 2000: loss 0.266328\n",
      "iteration 1300 / 2000: loss 0.283670\n",
      "iteration 1400 / 2000: loss 0.323926\n",
      "iteration 1500 / 2000: loss 0.262303\n",
      "iteration 1600 / 2000: loss 0.413582\n",
      "iteration 1700 / 2000: loss 0.290208\n",
      "iteration 1800 / 2000: loss 0.329101\n",
      "iteration 1900 / 2000: loss 0.345829\n",
      "Hidden Size: 20, Learning Rate: 0.002500074999999999, LR Decay: 0.3, Regularization: 0.1, Validation accuracy: 0.9084\n",
      "iteration 0 / 2000: loss 2.302597\n",
      "iteration 100 / 2000: loss 1.521923\n",
      "iteration 200 / 2000: loss 0.572556\n",
      "iteration 300 / 2000: loss 0.350115\n",
      "iteration 400 / 2000: loss 0.344261\n",
      "iteration 500 / 2000: loss 0.380619\n",
      "iteration 600 / 2000: loss 0.370709\n",
      "iteration 700 / 2000: loss 0.326614\n",
      "iteration 800 / 2000: loss 0.263722\n",
      "iteration 900 / 2000: loss 0.370963\n",
      "iteration 1000 / 2000: loss 0.329366\n",
      "iteration 1100 / 2000: loss 0.413092\n",
      "iteration 1200 / 2000: loss 0.342256\n",
      "iteration 1300 / 2000: loss 0.383578\n",
      "iteration 1400 / 2000: loss 0.317615\n",
      "iteration 1500 / 2000: loss 0.383303\n",
      "iteration 1600 / 2000: loss 0.433569\n",
      "iteration 1700 / 2000: loss 0.314398\n",
      "iteration 1800 / 2000: loss 0.233720\n",
      "iteration 1900 / 2000: loss 0.363686\n",
      "Hidden Size: 20, Learning Rate: 0.002500074999999999, LR Decay: 0.3, Regularization: 0.2, Validation accuracy: 0.9104\n",
      "iteration 0 / 2000: loss 2.302614\n",
      "iteration 100 / 2000: loss 1.782488\n",
      "iteration 200 / 2000: loss 0.543990\n",
      "iteration 300 / 2000: loss 0.366055\n",
      "iteration 400 / 2000: loss 0.413624\n",
      "iteration 500 / 2000: loss 0.345373\n",
      "iteration 600 / 2000: loss 0.384420\n",
      "iteration 700 / 2000: loss 0.474839\n",
      "iteration 800 / 2000: loss 0.401658\n",
      "iteration 900 / 2000: loss 0.336489\n",
      "iteration 1000 / 2000: loss 0.398754\n",
      "iteration 1100 / 2000: loss 0.352445\n",
      "iteration 1200 / 2000: loss 0.310472\n",
      "iteration 1300 / 2000: loss 0.351478\n",
      "iteration 1400 / 2000: loss 0.393003\n",
      "iteration 1500 / 2000: loss 0.385910\n",
      "iteration 1600 / 2000: loss 0.375346\n",
      "iteration 1700 / 2000: loss 0.361838\n",
      "iteration 1800 / 2000: loss 0.395591\n",
      "iteration 1900 / 2000: loss 0.277436\n",
      "Hidden Size: 20, Learning Rate: 0.002500074999999999, LR Decay: 0.3, Regularization: 0.30000000000000004, Validation accuracy: 0.9056\n",
      "iteration 0 / 2000: loss 2.302618\n",
      "iteration 100 / 2000: loss 1.426876\n",
      "iteration 200 / 2000: loss 0.722629\n",
      "iteration 300 / 2000: loss 0.400508\n",
      "iteration 400 / 2000: loss 0.435621\n",
      "iteration 500 / 2000: loss 0.373039\n",
      "iteration 600 / 2000: loss 0.437339\n",
      "iteration 700 / 2000: loss 0.482082\n",
      "iteration 800 / 2000: loss 0.419304\n",
      "iteration 900 / 2000: loss 0.422158\n",
      "iteration 1000 / 2000: loss 0.426759\n",
      "iteration 1100 / 2000: loss 0.445139\n",
      "iteration 1200 / 2000: loss 0.387434\n",
      "iteration 1300 / 2000: loss 0.345270\n",
      "iteration 1400 / 2000: loss 0.397162\n",
      "iteration 1500 / 2000: loss 0.327500\n",
      "iteration 1600 / 2000: loss 0.521151\n",
      "iteration 1700 / 2000: loss 0.399633\n",
      "iteration 1800 / 2000: loss 0.318982\n",
      "iteration 1900 / 2000: loss 0.422595\n",
      "Hidden Size: 20, Learning Rate: 0.002500074999999999, LR Decay: 0.3, Regularization: 0.4, Validation accuracy: 0.9056\n",
      "iteration 0 / 2000: loss 2.302577\n",
      "iteration 100 / 2000: loss 2.302573\n",
      "iteration 200 / 2000: loss 2.302577\n",
      "iteration 300 / 2000: loss 2.302575\n",
      "iteration 400 / 2000: loss 2.302573\n",
      "iteration 500 / 2000: loss 2.302581\n",
      "iteration 600 / 2000: loss 2.302578\n",
      "iteration 700 / 2000: loss 2.302575\n",
      "iteration 800 / 2000: loss 2.302574\n",
      "iteration 900 / 2000: loss 2.302574\n",
      "iteration 1000 / 2000: loss 2.302580\n",
      "iteration 1100 / 2000: loss 2.302573\n",
      "iteration 1200 / 2000: loss 2.302569\n",
      "iteration 1300 / 2000: loss 2.302572\n",
      "iteration 1400 / 2000: loss 2.302565\n",
      "iteration 1500 / 2000: loss 2.302582\n",
      "iteration 1600 / 2000: loss 2.302571\n",
      "iteration 1700 / 2000: loss 2.302572\n",
      "iteration 1800 / 2000: loss 2.302576\n",
      "iteration 1900 / 2000: loss 2.302566\n",
      "Hidden Size: 20, Learning Rate: 1e-07, LR Decay: 0.99, Regularization: 0.0, Validation accuracy: 0.1978\n",
      "iteration 0 / 2000: loss 2.302590\n",
      "iteration 100 / 2000: loss 2.302596\n",
      "iteration 200 / 2000: loss 2.302594\n",
      "iteration 300 / 2000: loss 2.302593\n",
      "iteration 400 / 2000: loss 2.302597\n",
      "iteration 500 / 2000: loss 2.302593\n",
      "iteration 600 / 2000: loss 2.302588\n",
      "iteration 700 / 2000: loss 2.302588\n",
      "iteration 800 / 2000: loss 2.302597\n",
      "iteration 900 / 2000: loss 2.302598\n",
      "iteration 1000 / 2000: loss 2.302589\n",
      "iteration 1100 / 2000: loss 2.302591\n",
      "iteration 1200 / 2000: loss 2.302595\n",
      "iteration 1300 / 2000: loss 2.302595\n",
      "iteration 1400 / 2000: loss 2.302591\n",
      "iteration 1500 / 2000: loss 2.302599\n",
      "iteration 1600 / 2000: loss 2.302589\n",
      "iteration 1700 / 2000: loss 2.302593\n",
      "iteration 1800 / 2000: loss 2.302595\n",
      "iteration 1900 / 2000: loss 2.302594\n",
      "Hidden Size: 20, Learning Rate: 1e-07, LR Decay: 0.99, Regularization: 0.1, Validation accuracy: 0.0606\n",
      "iteration 0 / 2000: loss 2.302600\n",
      "iteration 100 / 2000: loss 2.302600\n",
      "iteration 200 / 2000: loss 2.302606\n",
      "iteration 300 / 2000: loss 2.302598\n",
      "iteration 400 / 2000: loss 2.302600\n",
      "iteration 500 / 2000: loss 2.302597\n",
      "iteration 600 / 2000: loss 2.302595\n",
      "iteration 700 / 2000: loss 2.302607\n",
      "iteration 800 / 2000: loss 2.302596\n",
      "iteration 900 / 2000: loss 2.302606\n",
      "iteration 1000 / 2000: loss 2.302590\n",
      "iteration 1100 / 2000: loss 2.302598\n",
      "iteration 1200 / 2000: loss 2.302595\n",
      "iteration 1300 / 2000: loss 2.302603\n",
      "iteration 1400 / 2000: loss 2.302605\n",
      "iteration 1500 / 2000: loss 2.302600\n",
      "iteration 1600 / 2000: loss 2.302597\n",
      "iteration 1700 / 2000: loss 2.302599\n",
      "iteration 1800 / 2000: loss 2.302608\n",
      "iteration 1900 / 2000: loss 2.302601\n",
      "Hidden Size: 20, Learning Rate: 1e-07, LR Decay: 0.99, Regularization: 0.2, Validation accuracy: 0.1194\n",
      "iteration 0 / 2000: loss 2.302623\n",
      "iteration 100 / 2000: loss 2.302631\n",
      "iteration 200 / 2000: loss 2.302624\n",
      "iteration 300 / 2000: loss 2.302624\n",
      "iteration 400 / 2000: loss 2.302627\n",
      "iteration 500 / 2000: loss 2.302611\n",
      "iteration 600 / 2000: loss 2.302625\n",
      "iteration 700 / 2000: loss 2.302633\n",
      "iteration 800 / 2000: loss 2.302613\n",
      "iteration 900 / 2000: loss 2.302624\n",
      "iteration 1000 / 2000: loss 2.302623\n",
      "iteration 1100 / 2000: loss 2.302626\n",
      "iteration 1200 / 2000: loss 2.302629\n",
      "iteration 1300 / 2000: loss 2.302622\n",
      "iteration 1400 / 2000: loss 2.302623\n",
      "iteration 1500 / 2000: loss 2.302628\n",
      "iteration 1600 / 2000: loss 2.302623\n",
      "iteration 1700 / 2000: loss 2.302617\n",
      "iteration 1800 / 2000: loss 2.302621\n",
      "iteration 1900 / 2000: loss 2.302623\n",
      "Hidden Size: 20, Learning Rate: 1e-07, LR Decay: 0.99, Regularization: 0.30000000000000004, Validation accuracy: 0.0872\n",
      "iteration 0 / 2000: loss 2.302605\n",
      "iteration 100 / 2000: loss 2.302610\n",
      "iteration 200 / 2000: loss 2.302612\n",
      "iteration 300 / 2000: loss 2.302611\n",
      "iteration 400 / 2000: loss 2.302611\n",
      "iteration 500 / 2000: loss 2.302613\n",
      "iteration 600 / 2000: loss 2.302607\n",
      "iteration 700 / 2000: loss 2.302614\n",
      "iteration 800 / 2000: loss 2.302615\n",
      "iteration 900 / 2000: loss 2.302613\n",
      "iteration 1000 / 2000: loss 2.302617\n",
      "iteration 1100 / 2000: loss 2.302608\n",
      "iteration 1200 / 2000: loss 2.302604\n",
      "iteration 1300 / 2000: loss 2.302609\n",
      "iteration 1400 / 2000: loss 2.302610\n",
      "iteration 1500 / 2000: loss 2.302613\n",
      "iteration 1600 / 2000: loss 2.302610\n",
      "iteration 1700 / 2000: loss 2.302614\n",
      "iteration 1800 / 2000: loss 2.302608\n",
      "iteration 1900 / 2000: loss 2.302612\n",
      "Hidden Size: 20, Learning Rate: 1e-07, LR Decay: 0.99, Regularization: 0.4, Validation accuracy: 0.1012\n",
      "iteration 0 / 2000: loss 2.302590\n",
      "iteration 100 / 2000: loss 2.302601\n",
      "iteration 200 / 2000: loss 2.302590\n",
      "iteration 300 / 2000: loss 2.302587\n",
      "iteration 400 / 2000: loss 2.302595\n",
      "iteration 500 / 2000: loss 2.302600\n",
      "iteration 600 / 2000: loss 2.302590\n",
      "iteration 700 / 2000: loss 2.302597\n",
      "iteration 800 / 2000: loss 2.302589\n",
      "iteration 900 / 2000: loss 2.302589\n",
      "iteration 1000 / 2000: loss 2.302603\n",
      "iteration 1100 / 2000: loss 2.302590\n",
      "iteration 1200 / 2000: loss 2.302599\n",
      "iteration 1300 / 2000: loss 2.302587\n",
      "iteration 1400 / 2000: loss 2.302587\n",
      "iteration 1500 / 2000: loss 2.302589\n",
      "iteration 1600 / 2000: loss 2.302584\n",
      "iteration 1700 / 2000: loss 2.302592\n",
      "iteration 1800 / 2000: loss 2.302600\n",
      "iteration 1900 / 2000: loss 2.302594\n",
      "Hidden Size: 20, Learning Rate: 1e-07, LR Decay: 0.8175, Regularization: 0.0, Validation accuracy: 0.106\n",
      "iteration 0 / 2000: loss 2.302597\n",
      "iteration 100 / 2000: loss 2.302600\n",
      "iteration 200 / 2000: loss 2.302590\n",
      "iteration 300 / 2000: loss 2.302599\n",
      "iteration 400 / 2000: loss 2.302591\n",
      "iteration 500 / 2000: loss 2.302594\n",
      "iteration 600 / 2000: loss 2.302596\n",
      "iteration 700 / 2000: loss 2.302594\n",
      "iteration 800 / 2000: loss 2.302592\n",
      "iteration 900 / 2000: loss 2.302594\n",
      "iteration 1000 / 2000: loss 2.302596\n",
      "iteration 1100 / 2000: loss 2.302594\n",
      "iteration 1200 / 2000: loss 2.302597\n",
      "iteration 1300 / 2000: loss 2.302593\n",
      "iteration 1400 / 2000: loss 2.302594\n",
      "iteration 1500 / 2000: loss 2.302593\n",
      "iteration 1600 / 2000: loss 2.302598\n",
      "iteration 1700 / 2000: loss 2.302592\n",
      "iteration 1800 / 2000: loss 2.302596\n",
      "iteration 1900 / 2000: loss 2.302596\n",
      "Hidden Size: 20, Learning Rate: 1e-07, LR Decay: 0.8175, Regularization: 0.1, Validation accuracy: 0.0546\n",
      "iteration 0 / 2000: loss 2.302596\n",
      "iteration 100 / 2000: loss 2.302596\n",
      "iteration 200 / 2000: loss 2.302601\n",
      "iteration 300 / 2000: loss 2.302596\n",
      "iteration 400 / 2000: loss 2.302595\n",
      "iteration 500 / 2000: loss 2.302599\n",
      "iteration 600 / 2000: loss 2.302594\n",
      "iteration 700 / 2000: loss 2.302598\n",
      "iteration 800 / 2000: loss 2.302597\n",
      "iteration 900 / 2000: loss 2.302600\n",
      "iteration 1000 / 2000: loss 2.302602\n",
      "iteration 1100 / 2000: loss 2.302590\n",
      "iteration 1200 / 2000: loss 2.302596\n",
      "iteration 1300 / 2000: loss 2.302594\n",
      "iteration 1400 / 2000: loss 2.302595\n",
      "iteration 1500 / 2000: loss 2.302593\n",
      "iteration 1600 / 2000: loss 2.302593\n",
      "iteration 1700 / 2000: loss 2.302596\n",
      "iteration 1800 / 2000: loss 2.302601\n",
      "iteration 1900 / 2000: loss 2.302597\n",
      "Hidden Size: 20, Learning Rate: 1e-07, LR Decay: 0.8175, Regularization: 0.2, Validation accuracy: 0.1018\n",
      "iteration 0 / 2000: loss 2.302603\n",
      "iteration 100 / 2000: loss 2.302625\n",
      "iteration 200 / 2000: loss 2.302600\n",
      "iteration 300 / 2000: loss 2.302614\n",
      "iteration 400 / 2000: loss 2.302608\n",
      "iteration 500 / 2000: loss 2.302612\n",
      "iteration 600 / 2000: loss 2.302608\n",
      "iteration 700 / 2000: loss 2.302605\n",
      "iteration 800 / 2000: loss 2.302606\n",
      "iteration 900 / 2000: loss 2.302605\n",
      "iteration 1000 / 2000: loss 2.302610\n",
      "iteration 1100 / 2000: loss 2.302608\n",
      "iteration 1200 / 2000: loss 2.302609\n",
      "iteration 1300 / 2000: loss 2.302614\n",
      "iteration 1400 / 2000: loss 2.302606\n",
      "iteration 1500 / 2000: loss 2.302605\n",
      "iteration 1600 / 2000: loss 2.302615\n",
      "iteration 1700 / 2000: loss 2.302612\n",
      "iteration 1800 / 2000: loss 2.302614\n",
      "iteration 1900 / 2000: loss 2.302608\n",
      "Hidden Size: 20, Learning Rate: 1e-07, LR Decay: 0.8175, Regularization: 0.30000000000000004, Validation accuracy: 0.1292\n",
      "iteration 0 / 2000: loss 2.302629\n",
      "iteration 100 / 2000: loss 2.302631\n",
      "iteration 200 / 2000: loss 2.302641\n",
      "iteration 300 / 2000: loss 2.302625\n",
      "iteration 400 / 2000: loss 2.302642\n",
      "iteration 500 / 2000: loss 2.302637\n",
      "iteration 600 / 2000: loss 2.302632\n",
      "iteration 700 / 2000: loss 2.302635\n",
      "iteration 800 / 2000: loss 2.302627\n",
      "iteration 900 / 2000: loss 2.302627\n",
      "iteration 1000 / 2000: loss 2.302633\n",
      "iteration 1100 / 2000: loss 2.302633\n",
      "iteration 1200 / 2000: loss 2.302642\n",
      "iteration 1300 / 2000: loss 2.302626\n",
      "iteration 1400 / 2000: loss 2.302635\n",
      "iteration 1500 / 2000: loss 2.302632\n",
      "iteration 1600 / 2000: loss 2.302630\n",
      "iteration 1700 / 2000: loss 2.302633\n",
      "iteration 1800 / 2000: loss 2.302632\n",
      "iteration 1900 / 2000: loss 2.302635\n",
      "Hidden Size: 20, Learning Rate: 1e-07, LR Decay: 0.8175, Regularization: 0.4, Validation accuracy: 0.0852\n",
      "iteration 0 / 2000: loss 2.302577\n",
      "iteration 100 / 2000: loss 2.302572\n",
      "iteration 200 / 2000: loss 2.302579\n",
      "iteration 300 / 2000: loss 2.302580\n",
      "iteration 400 / 2000: loss 2.302573\n",
      "iteration 500 / 2000: loss 2.302582\n",
      "iteration 600 / 2000: loss 2.302576\n",
      "iteration 700 / 2000: loss 2.302582\n",
      "iteration 800 / 2000: loss 2.302580\n",
      "iteration 900 / 2000: loss 2.302575\n",
      "iteration 1000 / 2000: loss 2.302582\n",
      "iteration 1100 / 2000: loss 2.302583\n",
      "iteration 1200 / 2000: loss 2.302575\n",
      "iteration 1300 / 2000: loss 2.302579\n",
      "iteration 1400 / 2000: loss 2.302575\n",
      "iteration 1500 / 2000: loss 2.302583\n",
      "iteration 1600 / 2000: loss 2.302570\n",
      "iteration 1700 / 2000: loss 2.302577\n",
      "iteration 1800 / 2000: loss 2.302577\n",
      "iteration 1900 / 2000: loss 2.302568\n",
      "Hidden Size: 20, Learning Rate: 1e-07, LR Decay: 0.645, Regularization: 0.0, Validation accuracy: 0.1304\n",
      "iteration 0 / 2000: loss 2.302592\n",
      "iteration 100 / 2000: loss 2.302594\n",
      "iteration 200 / 2000: loss 2.302599\n",
      "iteration 300 / 2000: loss 2.302599\n",
      "iteration 400 / 2000: loss 2.302595\n",
      "iteration 500 / 2000: loss 2.302607\n",
      "iteration 600 / 2000: loss 2.302604\n",
      "iteration 700 / 2000: loss 2.302595\n",
      "iteration 800 / 2000: loss 2.302597\n",
      "iteration 900 / 2000: loss 2.302598\n",
      "iteration 1000 / 2000: loss 2.302587\n",
      "iteration 1100 / 2000: loss 2.302601\n",
      "iteration 1200 / 2000: loss 2.302595\n",
      "iteration 1300 / 2000: loss 2.302592\n",
      "iteration 1400 / 2000: loss 2.302601\n",
      "iteration 1500 / 2000: loss 2.302603\n",
      "iteration 1600 / 2000: loss 2.302594\n",
      "iteration 1700 / 2000: loss 2.302603\n",
      "iteration 1800 / 2000: loss 2.302595\n",
      "iteration 1900 / 2000: loss 2.302595\n",
      "Hidden Size: 20, Learning Rate: 1e-07, LR Decay: 0.645, Regularization: 0.1, Validation accuracy: 0.103\n",
      "iteration 0 / 2000: loss 2.302591\n",
      "iteration 100 / 2000: loss 2.302595\n",
      "iteration 200 / 2000: loss 2.302597\n",
      "iteration 300 / 2000: loss 2.302589\n",
      "iteration 400 / 2000: loss 2.302592\n",
      "iteration 500 / 2000: loss 2.302589\n",
      "iteration 600 / 2000: loss 2.302596\n",
      "iteration 700 / 2000: loss 2.302588\n",
      "iteration 800 / 2000: loss 2.302594\n",
      "iteration 900 / 2000: loss 2.302591\n",
      "iteration 1000 / 2000: loss 2.302603\n",
      "iteration 1100 / 2000: loss 2.302591\n",
      "iteration 1200 / 2000: loss 2.302588\n",
      "iteration 1300 / 2000: loss 2.302589\n",
      "iteration 1400 / 2000: loss 2.302594\n",
      "iteration 1500 / 2000: loss 2.302590\n",
      "iteration 1600 / 2000: loss 2.302586\n",
      "iteration 1700 / 2000: loss 2.302592\n",
      "iteration 1800 / 2000: loss 2.302585\n",
      "iteration 1900 / 2000: loss 2.302590\n",
      "Hidden Size: 20, Learning Rate: 1e-07, LR Decay: 0.645, Regularization: 0.2, Validation accuracy: 0.1594\n",
      "iteration 0 / 2000: loss 2.302616\n",
      "iteration 100 / 2000: loss 2.302613\n",
      "iteration 200 / 2000: loss 2.302624\n",
      "iteration 300 / 2000: loss 2.302623\n",
      "iteration 400 / 2000: loss 2.302615\n",
      "iteration 500 / 2000: loss 2.302619\n",
      "iteration 600 / 2000: loss 2.302612\n",
      "iteration 700 / 2000: loss 2.302613\n",
      "iteration 800 / 2000: loss 2.302613\n",
      "iteration 900 / 2000: loss 2.302612\n",
      "iteration 1000 / 2000: loss 2.302613\n",
      "iteration 1100 / 2000: loss 2.302618\n",
      "iteration 1200 / 2000: loss 2.302618\n",
      "iteration 1300 / 2000: loss 2.302617\n",
      "iteration 1400 / 2000: loss 2.302618\n",
      "iteration 1500 / 2000: loss 2.302614\n",
      "iteration 1600 / 2000: loss 2.302620\n",
      "iteration 1700 / 2000: loss 2.302618\n",
      "iteration 1800 / 2000: loss 2.302612\n",
      "iteration 1900 / 2000: loss 2.302618\n",
      "Hidden Size: 20, Learning Rate: 1e-07, LR Decay: 0.645, Regularization: 0.30000000000000004, Validation accuracy: 0.0794\n",
      "iteration 0 / 2000: loss 2.302616\n",
      "iteration 100 / 2000: loss 2.302612\n",
      "iteration 200 / 2000: loss 2.302614\n",
      "iteration 300 / 2000: loss 2.302615\n",
      "iteration 400 / 2000: loss 2.302607\n",
      "iteration 500 / 2000: loss 2.302616\n",
      "iteration 600 / 2000: loss 2.302613\n",
      "iteration 700 / 2000: loss 2.302616\n",
      "iteration 800 / 2000: loss 2.302616\n",
      "iteration 900 / 2000: loss 2.302613\n",
      "iteration 1000 / 2000: loss 2.302614\n",
      "iteration 1100 / 2000: loss 2.302612\n",
      "iteration 1200 / 2000: loss 2.302615\n",
      "iteration 1300 / 2000: loss 2.302612\n",
      "iteration 1400 / 2000: loss 2.302609\n",
      "iteration 1500 / 2000: loss 2.302608\n",
      "iteration 1600 / 2000: loss 2.302615\n",
      "iteration 1700 / 2000: loss 2.302609\n",
      "iteration 1800 / 2000: loss 2.302612\n",
      "iteration 1900 / 2000: loss 2.302617\n",
      "Hidden Size: 20, Learning Rate: 1e-07, LR Decay: 0.645, Regularization: 0.4, Validation accuracy: 0.0796\n",
      "iteration 0 / 2000: loss 2.302592\n",
      "iteration 100 / 2000: loss 2.302587\n",
      "iteration 200 / 2000: loss 2.302592\n",
      "iteration 300 / 2000: loss 2.302593\n",
      "iteration 400 / 2000: loss 2.302586\n",
      "iteration 500 / 2000: loss 2.302591\n",
      "iteration 600 / 2000: loss 2.302595\n",
      "iteration 700 / 2000: loss 2.302592\n",
      "iteration 800 / 2000: loss 2.302594\n",
      "iteration 900 / 2000: loss 2.302589\n",
      "iteration 1000 / 2000: loss 2.302592\n",
      "iteration 1100 / 2000: loss 2.302590\n",
      "iteration 1200 / 2000: loss 2.302585\n",
      "iteration 1300 / 2000: loss 2.302587\n",
      "iteration 1400 / 2000: loss 2.302592\n",
      "iteration 1500 / 2000: loss 2.302590\n",
      "iteration 1600 / 2000: loss 2.302588\n",
      "iteration 1700 / 2000: loss 2.302586\n",
      "iteration 1800 / 2000: loss 2.302589\n",
      "iteration 1900 / 2000: loss 2.302588\n",
      "Hidden Size: 20, Learning Rate: 1e-07, LR Decay: 0.47250000000000003, Regularization: 0.0, Validation accuracy: 0.066\n",
      "iteration 0 / 2000: loss 2.302588\n",
      "iteration 100 / 2000: loss 2.302593\n",
      "iteration 200 / 2000: loss 2.302594\n",
      "iteration 300 / 2000: loss 2.302595\n",
      "iteration 400 / 2000: loss 2.302584\n",
      "iteration 500 / 2000: loss 2.302593\n",
      "iteration 600 / 2000: loss 2.302593\n",
      "iteration 700 / 2000: loss 2.302593\n",
      "iteration 800 / 2000: loss 2.302591\n",
      "iteration 900 / 2000: loss 2.302593\n",
      "iteration 1000 / 2000: loss 2.302592\n",
      "iteration 1100 / 2000: loss 2.302595\n",
      "iteration 1200 / 2000: loss 2.302594\n",
      "iteration 1300 / 2000: loss 2.302591\n",
      "iteration 1400 / 2000: loss 2.302591\n",
      "iteration 1500 / 2000: loss 2.302592\n",
      "iteration 1600 / 2000: loss 2.302595\n",
      "iteration 1700 / 2000: loss 2.302596\n",
      "iteration 1800 / 2000: loss 2.302589\n",
      "iteration 1900 / 2000: loss 2.302586\n",
      "Hidden Size: 20, Learning Rate: 1e-07, LR Decay: 0.47250000000000003, Regularization: 0.1, Validation accuracy: 0.1012\n",
      "iteration 0 / 2000: loss 2.302598\n",
      "iteration 100 / 2000: loss 2.302601\n",
      "iteration 200 / 2000: loss 2.302600\n",
      "iteration 300 / 2000: loss 2.302599\n",
      "iteration 400 / 2000: loss 2.302595\n",
      "iteration 500 / 2000: loss 2.302603\n",
      "iteration 600 / 2000: loss 2.302606\n",
      "iteration 700 / 2000: loss 2.302596\n",
      "iteration 800 / 2000: loss 2.302596\n",
      "iteration 900 / 2000: loss 2.302599\n",
      "iteration 1000 / 2000: loss 2.302594\n",
      "iteration 1100 / 2000: loss 2.302609\n",
      "iteration 1200 / 2000: loss 2.302602\n",
      "iteration 1300 / 2000: loss 2.302602\n",
      "iteration 1400 / 2000: loss 2.302594\n",
      "iteration 1500 / 2000: loss 2.302594\n",
      "iteration 1600 / 2000: loss 2.302601\n",
      "iteration 1700 / 2000: loss 2.302604\n",
      "iteration 1800 / 2000: loss 2.302602\n",
      "iteration 1900 / 2000: loss 2.302595\n",
      "Hidden Size: 20, Learning Rate: 1e-07, LR Decay: 0.47250000000000003, Regularization: 0.2, Validation accuracy: 0.103\n",
      "iteration 0 / 2000: loss 2.302606\n",
      "iteration 100 / 2000: loss 2.302603\n",
      "iteration 200 / 2000: loss 2.302606\n",
      "iteration 300 / 2000: loss 2.302605\n",
      "iteration 400 / 2000: loss 2.302608\n",
      "iteration 500 / 2000: loss 2.302597\n",
      "iteration 600 / 2000: loss 2.302596\n",
      "iteration 700 / 2000: loss 2.302615\n",
      "iteration 800 / 2000: loss 2.302606\n",
      "iteration 900 / 2000: loss 2.302605\n",
      "iteration 1000 / 2000: loss 2.302605\n",
      "iteration 1100 / 2000: loss 2.302607\n",
      "iteration 1200 / 2000: loss 2.302599\n",
      "iteration 1300 / 2000: loss 2.302605\n",
      "iteration 1400 / 2000: loss 2.302604\n",
      "iteration 1500 / 2000: loss 2.302608\n",
      "iteration 1600 / 2000: loss 2.302604\n",
      "iteration 1700 / 2000: loss 2.302596\n",
      "iteration 1800 / 2000: loss 2.302609\n",
      "iteration 1900 / 2000: loss 2.302605\n",
      "Hidden Size: 20, Learning Rate: 1e-07, LR Decay: 0.47250000000000003, Regularization: 0.30000000000000004, Validation accuracy: 0.0854\n",
      "iteration 0 / 2000: loss 2.302630\n",
      "iteration 100 / 2000: loss 2.302632\n",
      "iteration 200 / 2000: loss 2.302624\n",
      "iteration 300 / 2000: loss 2.302622\n",
      "iteration 400 / 2000: loss 2.302622\n",
      "iteration 500 / 2000: loss 2.302619\n",
      "iteration 600 / 2000: loss 2.302626\n",
      "iteration 700 / 2000: loss 2.302628\n",
      "iteration 800 / 2000: loss 2.302623\n",
      "iteration 900 / 2000: loss 2.302622\n",
      "iteration 1000 / 2000: loss 2.302622\n",
      "iteration 1100 / 2000: loss 2.302627\n",
      "iteration 1200 / 2000: loss 2.302627\n",
      "iteration 1300 / 2000: loss 2.302623\n",
      "iteration 1400 / 2000: loss 2.302629\n",
      "iteration 1500 / 2000: loss 2.302627\n",
      "iteration 1600 / 2000: loss 2.302629\n",
      "iteration 1700 / 2000: loss 2.302628\n",
      "iteration 1800 / 2000: loss 2.302622\n",
      "iteration 1900 / 2000: loss 2.302632\n",
      "Hidden Size: 20, Learning Rate: 1e-07, LR Decay: 0.47250000000000003, Regularization: 0.4, Validation accuracy: 0.078\n",
      "iteration 0 / 2000: loss 2.302592\n",
      "iteration 100 / 2000: loss 2.302584\n",
      "iteration 200 / 2000: loss 2.302581\n",
      "iteration 300 / 2000: loss 2.302583\n",
      "iteration 400 / 2000: loss 2.302586\n",
      "iteration 500 / 2000: loss 2.302588\n",
      "iteration 600 / 2000: loss 2.302581\n",
      "iteration 700 / 2000: loss 2.302587\n",
      "iteration 800 / 2000: loss 2.302588\n",
      "iteration 900 / 2000: loss 2.302582\n",
      "iteration 1000 / 2000: loss 2.302577\n",
      "iteration 1100 / 2000: loss 2.302589\n",
      "iteration 1200 / 2000: loss 2.302579\n",
      "iteration 1300 / 2000: loss 2.302590\n",
      "iteration 1400 / 2000: loss 2.302583\n",
      "iteration 1500 / 2000: loss 2.302587\n",
      "iteration 1600 / 2000: loss 2.302581\n",
      "iteration 1700 / 2000: loss 2.302581\n",
      "iteration 1800 / 2000: loss 2.302585\n",
      "iteration 1900 / 2000: loss 2.302583\n",
      "Hidden Size: 20, Learning Rate: 1e-07, LR Decay: 0.3, Regularization: 0.0, Validation accuracy: 0.1276\n",
      "iteration 0 / 2000: loss 2.302599\n",
      "iteration 100 / 2000: loss 2.302595\n",
      "iteration 200 / 2000: loss 2.302595\n",
      "iteration 300 / 2000: loss 2.302599\n",
      "iteration 400 / 2000: loss 2.302593\n",
      "iteration 500 / 2000: loss 2.302597\n",
      "iteration 600 / 2000: loss 2.302599\n",
      "iteration 700 / 2000: loss 2.302597\n",
      "iteration 800 / 2000: loss 2.302596\n",
      "iteration 900 / 2000: loss 2.302601\n",
      "iteration 1000 / 2000: loss 2.302594\n",
      "iteration 1100 / 2000: loss 2.302599\n",
      "iteration 1200 / 2000: loss 2.302598\n",
      "iteration 1300 / 2000: loss 2.302591\n",
      "iteration 1400 / 2000: loss 2.302597\n",
      "iteration 1500 / 2000: loss 2.302595\n",
      "iteration 1600 / 2000: loss 2.302594\n",
      "iteration 1700 / 2000: loss 2.302595\n",
      "iteration 1800 / 2000: loss 2.302597\n",
      "iteration 1900 / 2000: loss 2.302600\n",
      "Hidden Size: 20, Learning Rate: 1e-07, LR Decay: 0.3, Regularization: 0.1, Validation accuracy: 0.07\n",
      "iteration 0 / 2000: loss 2.302600\n",
      "iteration 100 / 2000: loss 2.302614\n",
      "iteration 200 / 2000: loss 2.302609\n",
      "iteration 300 / 2000: loss 2.302601\n",
      "iteration 400 / 2000: loss 2.302609\n",
      "iteration 500 / 2000: loss 2.302603\n",
      "iteration 600 / 2000: loss 2.302608\n",
      "iteration 700 / 2000: loss 2.302608\n",
      "iteration 800 / 2000: loss 2.302608\n",
      "iteration 900 / 2000: loss 2.302608\n",
      "iteration 1000 / 2000: loss 2.302614\n",
      "iteration 1100 / 2000: loss 2.302610\n",
      "iteration 1200 / 2000: loss 2.302601\n",
      "iteration 1300 / 2000: loss 2.302596\n",
      "iteration 1400 / 2000: loss 2.302603\n",
      "iteration 1500 / 2000: loss 2.302606\n",
      "iteration 1600 / 2000: loss 2.302613\n",
      "iteration 1700 / 2000: loss 2.302614\n",
      "iteration 1800 / 2000: loss 2.302606\n",
      "iteration 1900 / 2000: loss 2.302607\n",
      "Hidden Size: 20, Learning Rate: 1e-07, LR Decay: 0.3, Regularization: 0.2, Validation accuracy: 0.0836\n",
      "iteration 0 / 2000: loss 2.302603\n",
      "iteration 100 / 2000: loss 2.302605\n",
      "iteration 200 / 2000: loss 2.302605\n",
      "iteration 300 / 2000: loss 2.302605\n",
      "iteration 400 / 2000: loss 2.302609\n",
      "iteration 500 / 2000: loss 2.302602\n",
      "iteration 600 / 2000: loss 2.302606\n",
      "iteration 700 / 2000: loss 2.302600\n",
      "iteration 800 / 2000: loss 2.302606\n",
      "iteration 900 / 2000: loss 2.302602\n",
      "iteration 1000 / 2000: loss 2.302600\n",
      "iteration 1100 / 2000: loss 2.302605\n",
      "iteration 1200 / 2000: loss 2.302604\n",
      "iteration 1300 / 2000: loss 2.302605\n",
      "iteration 1400 / 2000: loss 2.302609\n",
      "iteration 1500 / 2000: loss 2.302606\n",
      "iteration 1600 / 2000: loss 2.302609\n",
      "iteration 1700 / 2000: loss 2.302604\n",
      "iteration 1800 / 2000: loss 2.302603\n",
      "iteration 1900 / 2000: loss 2.302607\n",
      "Hidden Size: 20, Learning Rate: 1e-07, LR Decay: 0.3, Regularization: 0.30000000000000004, Validation accuracy: 0.083\n",
      "iteration 0 / 2000: loss 2.302612\n",
      "iteration 100 / 2000: loss 2.302608\n",
      "iteration 200 / 2000: loss 2.302610\n",
      "iteration 300 / 2000: loss 2.302608\n",
      "iteration 400 / 2000: loss 2.302607\n",
      "iteration 500 / 2000: loss 2.302603\n",
      "iteration 600 / 2000: loss 2.302609\n",
      "iteration 700 / 2000: loss 2.302610\n",
      "iteration 800 / 2000: loss 2.302605\n",
      "iteration 900 / 2000: loss 2.302609\n",
      "iteration 1000 / 2000: loss 2.302606\n",
      "iteration 1100 / 2000: loss 2.302606\n",
      "iteration 1200 / 2000: loss 2.302606\n",
      "iteration 1300 / 2000: loss 2.302609\n",
      "iteration 1400 / 2000: loss 2.302610\n",
      "iteration 1500 / 2000: loss 2.302609\n",
      "iteration 1600 / 2000: loss 2.302606\n",
      "iteration 1700 / 2000: loss 2.302610\n",
      "iteration 1800 / 2000: loss 2.302605\n",
      "iteration 1900 / 2000: loss 2.302609\n",
      "Hidden Size: 20, Learning Rate: 1e-07, LR Decay: 0.3, Regularization: 0.4, Validation accuracy: 0.092\n",
      "iteration 0 / 2000: loss 2.302590\n",
      "iteration 100 / 2000: loss 0.232163\n",
      "iteration 200 / 2000: loss 0.164845\n",
      "iteration 300 / 2000: loss 0.192873\n",
      "iteration 400 / 2000: loss 0.150289\n",
      "iteration 500 / 2000: loss 0.165145\n",
      "iteration 600 / 2000: loss 0.219512\n",
      "iteration 700 / 2000: loss 0.140232\n",
      "iteration 800 / 2000: loss 0.182706\n",
      "iteration 900 / 2000: loss 0.059272\n",
      "iteration 1000 / 2000: loss 0.242011\n",
      "iteration 1100 / 2000: loss 0.098567\n",
      "iteration 1200 / 2000: loss 0.083070\n",
      "iteration 1300 / 2000: loss 0.077674\n",
      "iteration 1400 / 2000: loss 0.046572\n",
      "iteration 1500 / 2000: loss 0.068601\n",
      "iteration 1600 / 2000: loss 0.135662\n",
      "iteration 1700 / 2000: loss 0.049166\n",
      "iteration 1800 / 2000: loss 0.317855\n",
      "iteration 1900 / 2000: loss 0.088411\n",
      "Hidden Size: 30, Learning Rate: 0.01, LR Decay: 0.99, Regularization: 0.0, Validation accuracy: 0.9614\n",
      "iteration 0 / 2000: loss 2.302607\n",
      "iteration 100 / 2000: loss 0.298770\n",
      "iteration 200 / 2000: loss 0.200739\n",
      "iteration 300 / 2000: loss 0.118314\n",
      "iteration 400 / 2000: loss 0.158178\n",
      "iteration 500 / 2000: loss 0.147125\n",
      "iteration 600 / 2000: loss 0.196578\n",
      "iteration 700 / 2000: loss 0.274118\n",
      "iteration 800 / 2000: loss 0.165643\n",
      "iteration 900 / 2000: loss 0.164291\n",
      "iteration 1000 / 2000: loss 0.247117\n",
      "iteration 1100 / 2000: loss 0.173664\n",
      "iteration 1200 / 2000: loss 0.181285\n",
      "iteration 1300 / 2000: loss 0.171635\n",
      "iteration 1400 / 2000: loss 0.146226\n",
      "iteration 1500 / 2000: loss 0.187689\n",
      "iteration 1600 / 2000: loss 0.166994\n",
      "iteration 1700 / 2000: loss 0.104340\n",
      "iteration 1800 / 2000: loss 0.122071\n",
      "iteration 1900 / 2000: loss 0.189181\n",
      "Hidden Size: 30, Learning Rate: 0.01, LR Decay: 0.99, Regularization: 0.1, Validation accuracy: 0.9598\n",
      "iteration 0 / 2000: loss 2.302608\n",
      "iteration 100 / 2000: loss 0.406103\n",
      "iteration 200 / 2000: loss 0.300142\n",
      "iteration 300 / 2000: loss 0.178990\n",
      "iteration 400 / 2000: loss 0.202569\n",
      "iteration 500 / 2000: loss 0.246199\n",
      "iteration 600 / 2000: loss 0.241822\n",
      "iteration 700 / 2000: loss 0.198185\n",
      "iteration 800 / 2000: loss 0.210900\n",
      "iteration 900 / 2000: loss 0.237896\n",
      "iteration 1000 / 2000: loss 0.250739\n",
      "iteration 1100 / 2000: loss 0.213540\n",
      "iteration 1200 / 2000: loss 0.175460\n",
      "iteration 1300 / 2000: loss 0.186502\n",
      "iteration 1400 / 2000: loss 0.241997\n",
      "iteration 1500 / 2000: loss 0.217178\n",
      "iteration 1600 / 2000: loss 0.207055\n",
      "iteration 1700 / 2000: loss 0.213358\n",
      "iteration 1800 / 2000: loss 0.197036\n",
      "iteration 1900 / 2000: loss 0.206878\n",
      "Hidden Size: 30, Learning Rate: 0.01, LR Decay: 0.99, Regularization: 0.2, Validation accuracy: 0.9642\n",
      "iteration 0 / 2000: loss 2.302613\n",
      "iteration 100 / 2000: loss 0.360337\n",
      "iteration 200 / 2000: loss 0.212888\n",
      "iteration 300 / 2000: loss 0.231171\n",
      "iteration 400 / 2000: loss 0.269472\n",
      "iteration 500 / 2000: loss 0.250170\n",
      "iteration 600 / 2000: loss 0.297999\n",
      "iteration 700 / 2000: loss 0.296478\n",
      "iteration 800 / 2000: loss 0.221814\n",
      "iteration 900 / 2000: loss 0.301905\n",
      "iteration 1000 / 2000: loss 0.262813\n",
      "iteration 1100 / 2000: loss 0.185360\n",
      "iteration 1200 / 2000: loss 0.222855\n",
      "iteration 1300 / 2000: loss 0.230888\n",
      "iteration 1400 / 2000: loss 0.230853\n",
      "iteration 1500 / 2000: loss 0.224275\n",
      "iteration 1600 / 2000: loss 0.224257\n",
      "iteration 1700 / 2000: loss 0.221852\n",
      "iteration 1800 / 2000: loss 0.230560\n",
      "iteration 1900 / 2000: loss 0.230446\n",
      "Hidden Size: 30, Learning Rate: 0.01, LR Decay: 0.99, Regularization: 0.30000000000000004, Validation accuracy: 0.9594\n",
      "iteration 0 / 2000: loss 2.302625\n",
      "iteration 100 / 2000: loss 0.365111\n",
      "iteration 200 / 2000: loss 0.349609\n",
      "iteration 300 / 2000: loss 0.293242\n",
      "iteration 400 / 2000: loss 0.331841\n",
      "iteration 500 / 2000: loss 0.345215\n",
      "iteration 600 / 2000: loss 0.285707\n",
      "iteration 700 / 2000: loss 0.247546\n",
      "iteration 800 / 2000: loss 0.239670\n",
      "iteration 900 / 2000: loss 0.229582\n",
      "iteration 1000 / 2000: loss 0.270716\n",
      "iteration 1100 / 2000: loss 0.276497\n",
      "iteration 1200 / 2000: loss 0.358289\n",
      "iteration 1300 / 2000: loss 0.253427\n",
      "iteration 1400 / 2000: loss 0.305211\n",
      "iteration 1500 / 2000: loss 0.222547\n",
      "iteration 1600 / 2000: loss 0.241741\n",
      "iteration 1700 / 2000: loss 0.293488\n",
      "iteration 1800 / 2000: loss 0.210245\n",
      "iteration 1900 / 2000: loss 0.288688\n",
      "Hidden Size: 30, Learning Rate: 0.01, LR Decay: 0.99, Regularization: 0.4, Validation accuracy: 0.9586\n",
      "iteration 0 / 2000: loss 2.302587\n",
      "iteration 100 / 2000: loss 0.367705\n",
      "iteration 200 / 2000: loss 0.130882\n",
      "iteration 300 / 2000: loss 0.093237\n",
      "iteration 400 / 2000: loss 0.079144\n",
      "iteration 500 / 2000: loss 0.090742\n",
      "iteration 600 / 2000: loss 0.115314\n",
      "iteration 700 / 2000: loss 0.110326\n",
      "iteration 800 / 2000: loss 0.067144\n",
      "iteration 900 / 2000: loss 0.074689\n",
      "iteration 1000 / 2000: loss 0.074494\n",
      "iteration 1100 / 2000: loss 0.052265\n",
      "iteration 1200 / 2000: loss 0.097700\n",
      "iteration 1300 / 2000: loss 0.035934\n",
      "iteration 1400 / 2000: loss 0.143317\n",
      "iteration 1500 / 2000: loss 0.096580\n",
      "iteration 1600 / 2000: loss 0.044891\n",
      "iteration 1700 / 2000: loss 0.048247\n",
      "iteration 1800 / 2000: loss 0.039201\n",
      "iteration 1900 / 2000: loss 0.154600\n",
      "Hidden Size: 30, Learning Rate: 0.01, LR Decay: 0.8175, Regularization: 0.0, Validation accuracy: 0.9708\n",
      "iteration 0 / 2000: loss 2.302590\n",
      "iteration 100 / 2000: loss 0.403235\n",
      "iteration 200 / 2000: loss 0.241000\n",
      "iteration 300 / 2000: loss 0.138468\n",
      "iteration 400 / 2000: loss 0.223362\n",
      "iteration 500 / 2000: loss 0.276180\n",
      "iteration 600 / 2000: loss 0.161108\n",
      "iteration 700 / 2000: loss 0.127364\n",
      "iteration 800 / 2000: loss 0.182524\n",
      "iteration 900 / 2000: loss 0.126944\n",
      "iteration 1000 / 2000: loss 0.116538\n",
      "iteration 1100 / 2000: loss 0.121670\n",
      "iteration 1200 / 2000: loss 0.120712\n",
      "iteration 1300 / 2000: loss 0.142177\n",
      "iteration 1400 / 2000: loss 0.199225\n",
      "iteration 1500 / 2000: loss 0.111100\n",
      "iteration 1600 / 2000: loss 0.109628\n",
      "iteration 1700 / 2000: loss 0.130077\n",
      "iteration 1800 / 2000: loss 0.086588\n",
      "iteration 1900 / 2000: loss 0.127326\n",
      "Hidden Size: 30, Learning Rate: 0.01, LR Decay: 0.8175, Regularization: 0.1, Validation accuracy: 0.97\n",
      "iteration 0 / 2000: loss 2.302603\n",
      "iteration 100 / 2000: loss 0.274642\n",
      "iteration 200 / 2000: loss 0.242592\n",
      "iteration 300 / 2000: loss 0.176639\n",
      "iteration 400 / 2000: loss 0.207937\n",
      "iteration 500 / 2000: loss 0.181970\n",
      "iteration 600 / 2000: loss 0.168989\n",
      "iteration 700 / 2000: loss 0.176408\n",
      "iteration 800 / 2000: loss 0.170833\n",
      "iteration 900 / 2000: loss 0.148552\n",
      "iteration 1000 / 2000: loss 0.154197\n",
      "iteration 1100 / 2000: loss 0.149724\n",
      "iteration 1200 / 2000: loss 0.149745\n",
      "iteration 1300 / 2000: loss 0.181206\n",
      "iteration 1400 / 2000: loss 0.170135\n",
      "iteration 1500 / 2000: loss 0.225296\n",
      "iteration 1600 / 2000: loss 0.219095\n",
      "iteration 1700 / 2000: loss 0.174375\n",
      "iteration 1800 / 2000: loss 0.148746\n",
      "iteration 1900 / 2000: loss 0.155834\n",
      "Hidden Size: 30, Learning Rate: 0.01, LR Decay: 0.8175, Regularization: 0.2, Validation accuracy: 0.9688\n",
      "iteration 0 / 2000: loss 2.302620\n",
      "iteration 100 / 2000: loss 0.382874\n",
      "iteration 200 / 2000: loss 0.214437\n",
      "iteration 300 / 2000: loss 0.324230\n",
      "iteration 400 / 2000: loss 0.228724\n",
      "iteration 500 / 2000: loss 0.243636\n",
      "iteration 600 / 2000: loss 0.222512\n",
      "iteration 700 / 2000: loss 0.247114\n",
      "iteration 800 / 2000: loss 0.219385\n",
      "iteration 900 / 2000: loss 0.252416\n",
      "iteration 1000 / 2000: loss 0.215020\n",
      "iteration 1100 / 2000: loss 0.232830\n",
      "iteration 1200 / 2000: loss 0.226290\n",
      "iteration 1300 / 2000: loss 0.188291\n",
      "iteration 1400 / 2000: loss 0.186655\n",
      "iteration 1500 / 2000: loss 0.216039\n",
      "iteration 1600 / 2000: loss 0.184678\n",
      "iteration 1700 / 2000: loss 0.177356\n",
      "iteration 1800 / 2000: loss 0.195491\n",
      "iteration 1900 / 2000: loss 0.188886\n",
      "Hidden Size: 30, Learning Rate: 0.01, LR Decay: 0.8175, Regularization: 0.30000000000000004, Validation accuracy: 0.9684\n",
      "iteration 0 / 2000: loss 2.302626\n",
      "iteration 100 / 2000: loss 0.333961\n",
      "iteration 200 / 2000: loss 0.265708\n",
      "iteration 300 / 2000: loss 0.254057\n",
      "iteration 400 / 2000: loss 0.221709\n",
      "iteration 500 / 2000: loss 0.339983\n",
      "iteration 600 / 2000: loss 0.319790\n",
      "iteration 700 / 2000: loss 0.277973\n",
      "iteration 800 / 2000: loss 0.233590\n",
      "iteration 900 / 2000: loss 0.260067\n",
      "iteration 1000 / 2000: loss 0.221388\n",
      "iteration 1100 / 2000: loss 0.281829\n",
      "iteration 1200 / 2000: loss 0.234757\n",
      "iteration 1300 / 2000: loss 0.227356\n",
      "iteration 1400 / 2000: loss 0.216302\n",
      "iteration 1500 / 2000: loss 0.250172\n",
      "iteration 1600 / 2000: loss 0.198156\n",
      "iteration 1700 / 2000: loss 0.234521\n",
      "iteration 1800 / 2000: loss 0.249014\n",
      "iteration 1900 / 2000: loss 0.242149\n",
      "Hidden Size: 30, Learning Rate: 0.01, LR Decay: 0.8175, Regularization: 0.4, Validation accuracy: 0.9676\n",
      "iteration 0 / 2000: loss 2.302576\n",
      "iteration 100 / 2000: loss 0.285527\n",
      "iteration 200 / 2000: loss 0.191966\n",
      "iteration 300 / 2000: loss 0.197431\n",
      "iteration 400 / 2000: loss 0.184432\n",
      "iteration 500 / 2000: loss 0.072882\n",
      "iteration 600 / 2000: loss 0.149036\n",
      "iteration 700 / 2000: loss 0.119079\n",
      "iteration 800 / 2000: loss 0.228256\n",
      "iteration 900 / 2000: loss 0.178346\n",
      "iteration 1000 / 2000: loss 0.091938\n",
      "iteration 1100 / 2000: loss 0.043534\n",
      "iteration 1200 / 2000: loss 0.052963\n",
      "iteration 1300 / 2000: loss 0.056613\n",
      "iteration 1400 / 2000: loss 0.064365\n",
      "iteration 1500 / 2000: loss 0.065111\n",
      "iteration 1600 / 2000: loss 0.106246\n",
      "iteration 1700 / 2000: loss 0.065323\n",
      "iteration 1800 / 2000: loss 0.116860\n",
      "iteration 1900 / 2000: loss 0.093160\n",
      "Hidden Size: 30, Learning Rate: 0.01, LR Decay: 0.645, Regularization: 0.0, Validation accuracy: 0.9684\n",
      "iteration 0 / 2000: loss 2.302587\n",
      "iteration 100 / 2000: loss 0.375625\n",
      "iteration 200 / 2000: loss 0.299050\n",
      "iteration 300 / 2000: loss 0.164526\n",
      "iteration 400 / 2000: loss 0.262138\n",
      "iteration 500 / 2000: loss 0.210108\n",
      "iteration 600 / 2000: loss 0.179206\n",
      "iteration 700 / 2000: loss 0.159632\n",
      "iteration 800 / 2000: loss 0.124099\n",
      "iteration 900 / 2000: loss 0.119059\n",
      "iteration 1000 / 2000: loss 0.118802\n",
      "iteration 1100 / 2000: loss 0.134366\n",
      "iteration 1200 / 2000: loss 0.115384\n",
      "iteration 1300 / 2000: loss 0.106965\n",
      "iteration 1400 / 2000: loss 0.141692\n",
      "iteration 1500 / 2000: loss 0.159561\n",
      "iteration 1600 / 2000: loss 0.145224\n",
      "iteration 1700 / 2000: loss 0.147742\n",
      "iteration 1800 / 2000: loss 0.111251\n",
      "iteration 1900 / 2000: loss 0.169695\n",
      "Hidden Size: 30, Learning Rate: 0.01, LR Decay: 0.645, Regularization: 0.1, Validation accuracy: 0.9676\n",
      "iteration 0 / 2000: loss 2.302597\n",
      "iteration 100 / 2000: loss 0.269297\n",
      "iteration 200 / 2000: loss 0.244253\n",
      "iteration 300 / 2000: loss 0.229080\n",
      "iteration 400 / 2000: loss 0.199965\n",
      "iteration 500 / 2000: loss 0.203288\n",
      "iteration 600 / 2000: loss 0.149156\n",
      "iteration 700 / 2000: loss 0.163896\n",
      "iteration 800 / 2000: loss 0.224660\n",
      "iteration 900 / 2000: loss 0.167812\n",
      "iteration 1000 / 2000: loss 0.167176\n",
      "iteration 1100 / 2000: loss 0.184060\n",
      "iteration 1200 / 2000: loss 0.133566\n",
      "iteration 1300 / 2000: loss 0.179562\n",
      "iteration 1400 / 2000: loss 0.166473\n",
      "iteration 1500 / 2000: loss 0.144800\n",
      "iteration 1600 / 2000: loss 0.123830\n",
      "iteration 1700 / 2000: loss 0.245834\n",
      "iteration 1800 / 2000: loss 0.166045\n",
      "iteration 1900 / 2000: loss 0.214215\n",
      "Hidden Size: 30, Learning Rate: 0.01, LR Decay: 0.645, Regularization: 0.2, Validation accuracy: 0.9682\n",
      "iteration 0 / 2000: loss 2.302611\n",
      "iteration 100 / 2000: loss 0.369968\n",
      "iteration 200 / 2000: loss 0.266656\n",
      "iteration 300 / 2000: loss 0.228814\n",
      "iteration 400 / 2000: loss 0.242061\n",
      "iteration 500 / 2000: loss 0.194346\n",
      "iteration 600 / 2000: loss 0.274166\n",
      "iteration 700 / 2000: loss 0.172254\n",
      "iteration 800 / 2000: loss 0.203284\n",
      "iteration 900 / 2000: loss 0.181049\n",
      "iteration 1000 / 2000: loss 0.194117\n",
      "iteration 1100 / 2000: loss 0.225438\n",
      "iteration 1200 / 2000: loss 0.157351\n",
      "iteration 1300 / 2000: loss 0.200729\n",
      "iteration 1400 / 2000: loss 0.191143\n",
      "iteration 1500 / 2000: loss 0.230223\n",
      "iteration 1600 / 2000: loss 0.228802\n",
      "iteration 1700 / 2000: loss 0.200322\n",
      "iteration 1800 / 2000: loss 0.160537\n",
      "iteration 1900 / 2000: loss 0.223795\n",
      "Hidden Size: 30, Learning Rate: 0.01, LR Decay: 0.645, Regularization: 0.30000000000000004, Validation accuracy: 0.9638\n",
      "iteration 0 / 2000: loss 2.302632\n",
      "iteration 100 / 2000: loss 0.339414\n",
      "iteration 200 / 2000: loss 0.221537\n",
      "iteration 300 / 2000: loss 0.247638\n",
      "iteration 400 / 2000: loss 0.246795\n",
      "iteration 500 / 2000: loss 0.239652\n",
      "iteration 600 / 2000: loss 0.255856\n",
      "iteration 700 / 2000: loss 0.221527\n",
      "iteration 800 / 2000: loss 0.255528\n",
      "iteration 900 / 2000: loss 0.238098\n",
      "iteration 1000 / 2000: loss 0.253798\n",
      "iteration 1100 / 2000: loss 0.205636\n",
      "iteration 1200 / 2000: loss 0.173640\n",
      "iteration 1300 / 2000: loss 0.218629\n",
      "iteration 1400 / 2000: loss 0.271880\n",
      "iteration 1500 / 2000: loss 0.208834\n",
      "iteration 1600 / 2000: loss 0.199644\n",
      "iteration 1700 / 2000: loss 0.215444\n",
      "iteration 1800 / 2000: loss 0.189244\n",
      "iteration 1900 / 2000: loss 0.199539\n",
      "Hidden Size: 30, Learning Rate: 0.01, LR Decay: 0.645, Regularization: 0.4, Validation accuracy: 0.9642\n",
      "iteration 0 / 2000: loss 2.302599\n",
      "iteration 100 / 2000: loss 0.253502\n",
      "iteration 200 / 2000: loss 0.254474\n",
      "iteration 300 / 2000: loss 0.243555\n",
      "iteration 400 / 2000: loss 0.168606\n",
      "iteration 500 / 2000: loss 0.073693\n",
      "iteration 600 / 2000: loss 0.178636\n",
      "iteration 700 / 2000: loss 0.134976\n",
      "iteration 800 / 2000: loss 0.160741\n",
      "iteration 900 / 2000: loss 0.195553\n",
      "iteration 1000 / 2000: loss 0.183105\n",
      "iteration 1100 / 2000: loss 0.090411\n",
      "iteration 1200 / 2000: loss 0.144527\n",
      "iteration 1300 / 2000: loss 0.098195\n",
      "iteration 1400 / 2000: loss 0.132788\n",
      "iteration 1500 / 2000: loss 0.201297\n",
      "iteration 1600 / 2000: loss 0.095783\n",
      "iteration 1700 / 2000: loss 0.084716\n",
      "iteration 1800 / 2000: loss 0.098293\n",
      "iteration 1900 / 2000: loss 0.047391\n",
      "Hidden Size: 30, Learning Rate: 0.01, LR Decay: 0.47250000000000003, Regularization: 0.0, Validation accuracy: 0.959\n",
      "iteration 0 / 2000: loss 2.302586\n",
      "iteration 100 / 2000: loss 0.282402\n",
      "iteration 200 / 2000: loss 0.227757\n",
      "iteration 300 / 2000: loss 0.175263\n",
      "iteration 400 / 2000: loss 0.176015\n",
      "iteration 500 / 2000: loss 0.345906\n",
      "iteration 600 / 2000: loss 0.209325\n",
      "iteration 700 / 2000: loss 0.102059\n",
      "iteration 800 / 2000: loss 0.180590\n",
      "iteration 900 / 2000: loss 0.187435\n",
      "iteration 1000 / 2000: loss 0.155060\n",
      "iteration 1100 / 2000: loss 0.119288\n",
      "iteration 1200 / 2000: loss 0.140051\n",
      "iteration 1300 / 2000: loss 0.281592\n",
      "iteration 1400 / 2000: loss 0.143823\n",
      "iteration 1500 / 2000: loss 0.211596\n",
      "iteration 1600 / 2000: loss 0.139109\n",
      "iteration 1700 / 2000: loss 0.104621\n",
      "iteration 1800 / 2000: loss 0.176496\n",
      "iteration 1900 / 2000: loss 0.175341\n",
      "Hidden Size: 30, Learning Rate: 0.01, LR Decay: 0.47250000000000003, Regularization: 0.1, Validation accuracy: 0.9608\n",
      "iteration 0 / 2000: loss 2.302616\n",
      "iteration 100 / 2000: loss 0.342799\n",
      "iteration 200 / 2000: loss 0.266792\n",
      "iteration 300 / 2000: loss 0.289191\n",
      "iteration 400 / 2000: loss 0.217333\n",
      "iteration 500 / 2000: loss 0.181945\n",
      "iteration 600 / 2000: loss 0.178593\n",
      "iteration 700 / 2000: loss 0.148073\n",
      "iteration 800 / 2000: loss 0.186949\n",
      "iteration 900 / 2000: loss 0.205794\n",
      "iteration 1000 / 2000: loss 0.165092\n",
      "iteration 1100 / 2000: loss 0.169389\n",
      "iteration 1200 / 2000: loss 0.169686\n",
      "iteration 1300 / 2000: loss 0.169777\n",
      "iteration 1400 / 2000: loss 0.197292\n",
      "iteration 1500 / 2000: loss 0.159583\n",
      "iteration 1600 / 2000: loss 0.215722\n",
      "iteration 1700 / 2000: loss 0.172300\n",
      "iteration 1800 / 2000: loss 0.265859\n",
      "iteration 1900 / 2000: loss 0.192762\n",
      "Hidden Size: 30, Learning Rate: 0.01, LR Decay: 0.47250000000000003, Regularization: 0.2, Validation accuracy: 0.9622\n",
      "iteration 0 / 2000: loss 2.302617\n",
      "iteration 100 / 2000: loss 0.309403\n",
      "iteration 200 / 2000: loss 0.219753\n",
      "iteration 300 / 2000: loss 0.243804\n",
      "iteration 400 / 2000: loss 0.266388\n",
      "iteration 500 / 2000: loss 0.246759\n",
      "iteration 600 / 2000: loss 0.217505\n",
      "iteration 700 / 2000: loss 0.252744\n",
      "iteration 800 / 2000: loss 0.212815\n",
      "iteration 900 / 2000: loss 0.226743\n",
      "iteration 1000 / 2000: loss 0.237070\n",
      "iteration 1100 / 2000: loss 0.172341\n",
      "iteration 1200 / 2000: loss 0.251877\n",
      "iteration 1300 / 2000: loss 0.317806\n",
      "iteration 1400 / 2000: loss 0.227428\n",
      "iteration 1500 / 2000: loss 0.179452\n",
      "iteration 1600 / 2000: loss 0.176255\n",
      "iteration 1700 / 2000: loss 0.255481\n",
      "iteration 1800 / 2000: loss 0.163480\n",
      "iteration 1900 / 2000: loss 0.215740\n",
      "Hidden Size: 30, Learning Rate: 0.01, LR Decay: 0.47250000000000003, Regularization: 0.30000000000000004, Validation accuracy: 0.9634\n",
      "iteration 0 / 2000: loss 2.302646\n",
      "iteration 100 / 2000: loss 0.433848\n",
      "iteration 200 / 2000: loss 0.369647\n",
      "iteration 300 / 2000: loss 0.232040\n",
      "iteration 400 / 2000: loss 0.169527\n",
      "iteration 500 / 2000: loss 0.206234\n",
      "iteration 600 / 2000: loss 0.221789\n",
      "iteration 700 / 2000: loss 0.169272\n",
      "iteration 800 / 2000: loss 0.210438\n",
      "iteration 900 / 2000: loss 0.269220\n",
      "iteration 1000 / 2000: loss 0.233054\n",
      "iteration 1100 / 2000: loss 0.245523\n",
      "iteration 1200 / 2000: loss 0.210131\n",
      "iteration 1300 / 2000: loss 0.229297\n",
      "iteration 1400 / 2000: loss 0.221063\n",
      "iteration 1500 / 2000: loss 0.261556\n",
      "iteration 1600 / 2000: loss 0.216441\n",
      "iteration 1700 / 2000: loss 0.211497\n",
      "iteration 1800 / 2000: loss 0.227328\n",
      "iteration 1900 / 2000: loss 0.306192\n",
      "Hidden Size: 30, Learning Rate: 0.01, LR Decay: 0.47250000000000003, Regularization: 0.4, Validation accuracy: 0.961\n",
      "iteration 0 / 2000: loss 2.302595\n",
      "iteration 100 / 2000: loss 0.276255\n",
      "iteration 200 / 2000: loss 0.205914\n",
      "iteration 300 / 2000: loss 0.148291\n",
      "iteration 400 / 2000: loss 0.144884\n",
      "iteration 500 / 2000: loss 0.216277\n",
      "iteration 600 / 2000: loss 0.157737\n",
      "iteration 700 / 2000: loss 0.257821\n",
      "iteration 800 / 2000: loss 0.130761\n",
      "iteration 900 / 2000: loss 0.139138\n",
      "iteration 1000 / 2000: loss 0.257870\n",
      "iteration 1100 / 2000: loss 0.155666\n",
      "iteration 1200 / 2000: loss 0.157176\n",
      "iteration 1300 / 2000: loss 0.218462\n",
      "iteration 1400 / 2000: loss 0.268473\n",
      "iteration 1500 / 2000: loss 0.185707\n",
      "iteration 1600 / 2000: loss 0.220380\n",
      "iteration 1700 / 2000: loss 0.242485\n",
      "iteration 1800 / 2000: loss 0.122132\n",
      "iteration 1900 / 2000: loss 0.143251\n",
      "Hidden Size: 30, Learning Rate: 0.01, LR Decay: 0.3, Regularization: 0.0, Validation accuracy: 0.9464\n",
      "iteration 0 / 2000: loss 2.302589\n",
      "iteration 100 / 2000: loss 0.382668\n",
      "iteration 200 / 2000: loss 0.351471\n",
      "iteration 300 / 2000: loss 0.179984\n",
      "iteration 400 / 2000: loss 0.199474\n",
      "iteration 500 / 2000: loss 0.275269\n",
      "iteration 600 / 2000: loss 0.179718\n",
      "iteration 700 / 2000: loss 0.151227\n",
      "iteration 800 / 2000: loss 0.187025\n",
      "iteration 900 / 2000: loss 0.158642\n",
      "iteration 1000 / 2000: loss 0.156249\n",
      "iteration 1100 / 2000: loss 0.180437\n",
      "iteration 1200 / 2000: loss 0.181151\n",
      "iteration 1300 / 2000: loss 0.179093\n",
      "iteration 1400 / 2000: loss 0.236541\n",
      "iteration 1500 / 2000: loss 0.207830\n",
      "iteration 1600 / 2000: loss 0.142001\n",
      "iteration 1700 / 2000: loss 0.260432\n",
      "iteration 1800 / 2000: loss 0.173352\n",
      "iteration 1900 / 2000: loss 0.156614\n",
      "Hidden Size: 30, Learning Rate: 0.01, LR Decay: 0.3, Regularization: 0.1, Validation accuracy: 0.9544\n",
      "iteration 0 / 2000: loss 2.302607\n",
      "iteration 100 / 2000: loss 0.374906\n",
      "iteration 200 / 2000: loss 0.220165\n",
      "iteration 300 / 2000: loss 0.273471\n",
      "iteration 400 / 2000: loss 0.229131\n",
      "iteration 500 / 2000: loss 0.213682\n",
      "iteration 600 / 2000: loss 0.232398\n",
      "iteration 700 / 2000: loss 0.219876\n",
      "iteration 800 / 2000: loss 0.136421\n",
      "iteration 900 / 2000: loss 0.251467\n",
      "iteration 1000 / 2000: loss 0.202957\n",
      "iteration 1100 / 2000: loss 0.239256\n",
      "iteration 1200 / 2000: loss 0.229958\n",
      "iteration 1300 / 2000: loss 0.284087\n",
      "iteration 1400 / 2000: loss 0.312749\n",
      "iteration 1500 / 2000: loss 0.251654\n",
      "iteration 1600 / 2000: loss 0.247472\n",
      "iteration 1700 / 2000: loss 0.184337\n",
      "iteration 1800 / 2000: loss 0.271781\n",
      "iteration 1900 / 2000: loss 0.245178\n",
      "Hidden Size: 30, Learning Rate: 0.01, LR Decay: 0.3, Regularization: 0.2, Validation accuracy: 0.9474\n",
      "iteration 0 / 2000: loss 2.302622\n",
      "iteration 100 / 2000: loss 0.432033\n",
      "iteration 200 / 2000: loss 0.322856\n",
      "iteration 300 / 2000: loss 0.237791\n",
      "iteration 400 / 2000: loss 0.345168\n",
      "iteration 500 / 2000: loss 0.209209\n",
      "iteration 600 / 2000: loss 0.278996\n",
      "iteration 700 / 2000: loss 0.210122\n",
      "iteration 800 / 2000: loss 0.290543\n",
      "iteration 900 / 2000: loss 0.264372\n",
      "iteration 1000 / 2000: loss 0.282139\n",
      "iteration 1100 / 2000: loss 0.181579\n",
      "iteration 1200 / 2000: loss 0.242025\n",
      "iteration 1300 / 2000: loss 0.310877\n",
      "iteration 1400 / 2000: loss 0.175731\n",
      "iteration 1500 / 2000: loss 0.263014\n",
      "iteration 1600 / 2000: loss 0.215377\n",
      "iteration 1700 / 2000: loss 0.264788\n",
      "iteration 1800 / 2000: loss 0.260685\n",
      "iteration 1900 / 2000: loss 0.207922\n",
      "Hidden Size: 30, Learning Rate: 0.01, LR Decay: 0.3, Regularization: 0.30000000000000004, Validation accuracy: 0.9448\n",
      "iteration 0 / 2000: loss 2.302634\n",
      "iteration 100 / 2000: loss 0.378865\n",
      "iteration 200 / 2000: loss 0.252678\n",
      "iteration 300 / 2000: loss 0.385787\n",
      "iteration 400 / 2000: loss 0.280317\n",
      "iteration 500 / 2000: loss 0.307119\n",
      "iteration 600 / 2000: loss 0.243054\n",
      "iteration 700 / 2000: loss 0.197706\n",
      "iteration 800 / 2000: loss 0.238754\n",
      "iteration 900 / 2000: loss 0.347657\n",
      "iteration 1000 / 2000: loss 0.246055\n",
      "iteration 1100 / 2000: loss 0.210640\n",
      "iteration 1200 / 2000: loss 0.290904\n",
      "iteration 1300 / 2000: loss 0.198155\n",
      "iteration 1400 / 2000: loss 0.210092\n",
      "iteration 1500 / 2000: loss 0.242392\n",
      "iteration 1600 / 2000: loss 0.252753\n",
      "iteration 1700 / 2000: loss 0.199759\n",
      "iteration 1800 / 2000: loss 0.356968\n",
      "iteration 1900 / 2000: loss 0.340242\n",
      "Hidden Size: 30, Learning Rate: 0.01, LR Decay: 0.3, Regularization: 0.4, Validation accuracy: 0.9486\n",
      "iteration 0 / 2000: loss 2.302579\n",
      "iteration 100 / 2000: loss 0.286705\n",
      "iteration 200 / 2000: loss 0.220255\n",
      "iteration 300 / 2000: loss 0.234952\n",
      "iteration 400 / 2000: loss 0.189147\n",
      "iteration 500 / 2000: loss 0.102248\n",
      "iteration 600 / 2000: loss 0.089497\n",
      "iteration 700 / 2000: loss 0.069307\n",
      "iteration 800 / 2000: loss 0.101004\n",
      "iteration 900 / 2000: loss 0.115622\n",
      "iteration 1000 / 2000: loss 0.101341\n",
      "iteration 1100 / 2000: loss 0.038452\n",
      "iteration 1200 / 2000: loss 0.036018\n",
      "iteration 1300 / 2000: loss 0.101107\n",
      "iteration 1400 / 2000: loss 0.075930\n",
      "iteration 1500 / 2000: loss 0.136289\n",
      "iteration 1600 / 2000: loss 0.051992\n",
      "iteration 1700 / 2000: loss 0.047921\n",
      "iteration 1800 / 2000: loss 0.018347\n",
      "iteration 1900 / 2000: loss 0.072895\n",
      "Hidden Size: 30, Learning Rate: 0.0075000250000000004, LR Decay: 0.99, Regularization: 0.0, Validation accuracy: 0.9646\n",
      "iteration 0 / 2000: loss 2.302584\n",
      "iteration 100 / 2000: loss 0.273737\n",
      "iteration 200 / 2000: loss 0.219597\n",
      "iteration 300 / 2000: loss 0.163745\n",
      "iteration 400 / 2000: loss 0.163783\n",
      "iteration 500 / 2000: loss 0.220635\n",
      "iteration 600 / 2000: loss 0.218727\n",
      "iteration 700 / 2000: loss 0.132841\n",
      "iteration 800 / 2000: loss 0.151899\n",
      "iteration 900 / 2000: loss 0.138580\n",
      "iteration 1000 / 2000: loss 0.136135\n",
      "iteration 1100 / 2000: loss 0.145680\n",
      "iteration 1200 / 2000: loss 0.134943\n",
      "iteration 1300 / 2000: loss 0.165883\n",
      "iteration 1400 / 2000: loss 0.123254\n",
      "iteration 1500 / 2000: loss 0.216849\n",
      "iteration 1600 / 2000: loss 0.139901\n",
      "iteration 1700 / 2000: loss 0.123808\n",
      "iteration 1800 / 2000: loss 0.172442\n",
      "iteration 1900 / 2000: loss 0.121171\n",
      "Hidden Size: 30, Learning Rate: 0.0075000250000000004, LR Decay: 0.99, Regularization: 0.1, Validation accuracy: 0.9652\n",
      "iteration 0 / 2000: loss 2.302604\n",
      "iteration 100 / 2000: loss 0.280763\n",
      "iteration 200 / 2000: loss 0.160865\n",
      "iteration 300 / 2000: loss 0.159891\n",
      "iteration 400 / 2000: loss 0.148835\n",
      "iteration 500 / 2000: loss 0.263885\n",
      "iteration 600 / 2000: loss 0.149424\n",
      "iteration 700 / 2000: loss 0.194590\n",
      "iteration 800 / 2000: loss 0.211218\n",
      "iteration 900 / 2000: loss 0.175324\n",
      "iteration 1000 / 2000: loss 0.178932\n",
      "iteration 1100 / 2000: loss 0.236215\n",
      "iteration 1200 / 2000: loss 0.242632\n",
      "iteration 1300 / 2000: loss 0.185004\n",
      "iteration 1400 / 2000: loss 0.187773\n",
      "iteration 1500 / 2000: loss 0.169300\n",
      "iteration 1600 / 2000: loss 0.181034\n",
      "iteration 1700 / 2000: loss 0.148948\n",
      "iteration 1800 / 2000: loss 0.149095\n",
      "iteration 1900 / 2000: loss 0.164924\n",
      "Hidden Size: 30, Learning Rate: 0.0075000250000000004, LR Decay: 0.99, Regularization: 0.2, Validation accuracy: 0.966\n",
      "iteration 0 / 2000: loss 2.302629\n",
      "iteration 100 / 2000: loss 0.233521\n",
      "iteration 200 / 2000: loss 0.232826\n",
      "iteration 300 / 2000: loss 0.339105\n",
      "iteration 400 / 2000: loss 0.186447\n",
      "iteration 500 / 2000: loss 0.232512\n",
      "iteration 600 / 2000: loss 0.269887\n",
      "iteration 700 / 2000: loss 0.266162\n",
      "iteration 800 / 2000: loss 0.196003\n",
      "iteration 900 / 2000: loss 0.216985\n",
      "iteration 1000 / 2000: loss 0.259446\n",
      "iteration 1100 / 2000: loss 0.194349\n",
      "iteration 1200 / 2000: loss 0.245794\n",
      "iteration 1300 / 2000: loss 0.209921\n",
      "iteration 1400 / 2000: loss 0.219581\n",
      "iteration 1500 / 2000: loss 0.170975\n",
      "iteration 1600 / 2000: loss 0.225349\n",
      "iteration 1700 / 2000: loss 0.216001\n",
      "iteration 1800 / 2000: loss 0.204167\n",
      "iteration 1900 / 2000: loss 0.184653\n",
      "Hidden Size: 30, Learning Rate: 0.0075000250000000004, LR Decay: 0.99, Regularization: 0.30000000000000004, Validation accuracy: 0.9668\n",
      "iteration 0 / 2000: loss 2.302608\n",
      "iteration 100 / 2000: loss 0.296628\n",
      "iteration 200 / 2000: loss 0.382880\n",
      "iteration 300 / 2000: loss 0.239493\n",
      "iteration 400 / 2000: loss 0.299285\n",
      "iteration 500 / 2000: loss 0.308638\n",
      "iteration 600 / 2000: loss 0.276592\n",
      "iteration 700 / 2000: loss 0.273523\n",
      "iteration 800 / 2000: loss 0.237885\n",
      "iteration 900 / 2000: loss 0.251287\n",
      "iteration 1000 / 2000: loss 0.221849\n",
      "iteration 1100 / 2000: loss 0.242630\n",
      "iteration 1200 / 2000: loss 0.255536\n",
      "iteration 1300 / 2000: loss 0.243984\n",
      "iteration 1400 / 2000: loss 0.287127\n",
      "iteration 1500 / 2000: loss 0.273809\n",
      "iteration 1600 / 2000: loss 0.222962\n",
      "iteration 1700 / 2000: loss 0.193031\n",
      "iteration 1800 / 2000: loss 0.206243\n",
      "iteration 1900 / 2000: loss 0.233249\n",
      "Hidden Size: 30, Learning Rate: 0.0075000250000000004, LR Decay: 0.99, Regularization: 0.4, Validation accuracy: 0.9574\n",
      "iteration 0 / 2000: loss 2.302590\n",
      "iteration 100 / 2000: loss 0.284702\n",
      "iteration 200 / 2000: loss 0.143295\n",
      "iteration 300 / 2000: loss 0.190195\n",
      "iteration 400 / 2000: loss 0.178442\n",
      "iteration 500 / 2000: loss 0.151014\n",
      "iteration 600 / 2000: loss 0.151971\n",
      "iteration 700 / 2000: loss 0.085208\n",
      "iteration 800 / 2000: loss 0.107110\n",
      "iteration 900 / 2000: loss 0.092551\n",
      "iteration 1000 / 2000: loss 0.035455\n",
      "iteration 1100 / 2000: loss 0.071543\n",
      "iteration 1200 / 2000: loss 0.056385\n",
      "iteration 1300 / 2000: loss 0.089142\n",
      "iteration 1400 / 2000: loss 0.110787\n",
      "iteration 1500 / 2000: loss 0.040656\n",
      "iteration 1600 / 2000: loss 0.117797\n",
      "iteration 1700 / 2000: loss 0.072865\n",
      "iteration 1800 / 2000: loss 0.118175\n",
      "iteration 1900 / 2000: loss 0.125047\n",
      "Hidden Size: 30, Learning Rate: 0.0075000250000000004, LR Decay: 0.8175, Regularization: 0.0, Validation accuracy: 0.9676\n",
      "iteration 0 / 2000: loss 2.302603\n",
      "iteration 100 / 2000: loss 0.288369\n",
      "iteration 200 / 2000: loss 0.222722\n",
      "iteration 300 / 2000: loss 0.131120\n",
      "iteration 400 / 2000: loss 0.226372\n",
      "iteration 500 / 2000: loss 0.195026\n",
      "iteration 600 / 2000: loss 0.204940\n",
      "iteration 700 / 2000: loss 0.131608\n",
      "iteration 800 / 2000: loss 0.152671\n",
      "iteration 900 / 2000: loss 0.126685\n",
      "iteration 1000 / 2000: loss 0.150452\n",
      "iteration 1100 / 2000: loss 0.184239\n",
      "iteration 1200 / 2000: loss 0.123526\n",
      "iteration 1300 / 2000: loss 0.179670\n",
      "iteration 1400 / 2000: loss 0.119701\n",
      "iteration 1500 / 2000: loss 0.123416\n",
      "iteration 1600 / 2000: loss 0.087361\n",
      "iteration 1700 / 2000: loss 0.120356\n",
      "iteration 1800 / 2000: loss 0.130408\n",
      "iteration 1900 / 2000: loss 0.099281\n",
      "Hidden Size: 30, Learning Rate: 0.0075000250000000004, LR Decay: 0.8175, Regularization: 0.1, Validation accuracy: 0.9688\n",
      "iteration 0 / 2000: loss 2.302611\n",
      "iteration 100 / 2000: loss 0.306284\n",
      "iteration 200 / 2000: loss 0.291389\n",
      "iteration 300 / 2000: loss 0.224893\n",
      "iteration 400 / 2000: loss 0.182944\n",
      "iteration 500 / 2000: loss 0.196702\n",
      "iteration 600 / 2000: loss 0.197365\n",
      "iteration 700 / 2000: loss 0.203969\n",
      "iteration 800 / 2000: loss 0.191212\n",
      "iteration 900 / 2000: loss 0.196406\n",
      "iteration 1000 / 2000: loss 0.199073\n",
      "iteration 1100 / 2000: loss 0.182378\n",
      "iteration 1200 / 2000: loss 0.159750\n",
      "iteration 1300 / 2000: loss 0.160600\n",
      "iteration 1400 / 2000: loss 0.157022\n",
      "iteration 1500 / 2000: loss 0.206940\n",
      "iteration 1600 / 2000: loss 0.155526\n",
      "iteration 1700 / 2000: loss 0.134761\n",
      "iteration 1800 / 2000: loss 0.138640\n",
      "iteration 1900 / 2000: loss 0.157644\n",
      "Hidden Size: 30, Learning Rate: 0.0075000250000000004, LR Decay: 0.8175, Regularization: 0.2, Validation accuracy: 0.9694\n",
      "iteration 0 / 2000: loss 2.302614\n",
      "iteration 100 / 2000: loss 0.406475\n",
      "iteration 200 / 2000: loss 0.236531\n",
      "iteration 300 / 2000: loss 0.251865\n",
      "iteration 400 / 2000: loss 0.197799\n",
      "iteration 500 / 2000: loss 0.267030\n",
      "iteration 600 / 2000: loss 0.303523\n",
      "iteration 700 / 2000: loss 0.171989\n",
      "iteration 800 / 2000: loss 0.189909\n",
      "iteration 900 / 2000: loss 0.242173\n",
      "iteration 1000 / 2000: loss 0.206945\n",
      "iteration 1100 / 2000: loss 0.189662\n",
      "iteration 1200 / 2000: loss 0.250183\n",
      "iteration 1300 / 2000: loss 0.188304\n",
      "iteration 1400 / 2000: loss 0.207882\n",
      "iteration 1500 / 2000: loss 0.214192\n",
      "iteration 1600 / 2000: loss 0.204940\n",
      "iteration 1700 / 2000: loss 0.198096\n",
      "iteration 1800 / 2000: loss 0.214733\n",
      "iteration 1900 / 2000: loss 0.212451\n",
      "Hidden Size: 30, Learning Rate: 0.0075000250000000004, LR Decay: 0.8175, Regularization: 0.30000000000000004, Validation accuracy: 0.9692\n",
      "iteration 0 / 2000: loss 2.302618\n",
      "iteration 100 / 2000: loss 0.285045\n",
      "iteration 200 / 2000: loss 0.301396\n",
      "iteration 300 / 2000: loss 0.180129\n",
      "iteration 400 / 2000: loss 0.223037\n",
      "iteration 500 / 2000: loss 0.219815\n",
      "iteration 600 / 2000: loss 0.260131\n",
      "iteration 700 / 2000: loss 0.316999\n",
      "iteration 800 / 2000: loss 0.265857\n",
      "iteration 900 / 2000: loss 0.266007\n",
      "iteration 1000 / 2000: loss 0.203344\n",
      "iteration 1100 / 2000: loss 0.175584\n",
      "iteration 1200 / 2000: loss 0.196048\n",
      "iteration 1300 / 2000: loss 0.285248\n",
      "iteration 1400 / 2000: loss 0.235695\n",
      "iteration 1500 / 2000: loss 0.232310\n",
      "iteration 1600 / 2000: loss 0.217424\n",
      "iteration 1700 / 2000: loss 0.195304\n",
      "iteration 1800 / 2000: loss 0.192325\n",
      "iteration 1900 / 2000: loss 0.221065\n",
      "Hidden Size: 30, Learning Rate: 0.0075000250000000004, LR Decay: 0.8175, Regularization: 0.4, Validation accuracy: 0.9668\n",
      "iteration 0 / 2000: loss 2.302588\n",
      "iteration 100 / 2000: loss 0.441385\n",
      "iteration 200 / 2000: loss 0.206856\n",
      "iteration 300 / 2000: loss 0.172939\n",
      "iteration 400 / 2000: loss 0.183085\n",
      "iteration 500 / 2000: loss 0.215004\n",
      "iteration 600 / 2000: loss 0.116528\n",
      "iteration 700 / 2000: loss 0.095761\n",
      "iteration 800 / 2000: loss 0.163194\n",
      "iteration 900 / 2000: loss 0.207295\n",
      "iteration 1000 / 2000: loss 0.130416\n",
      "iteration 1100 / 2000: loss 0.135689\n",
      "iteration 1200 / 2000: loss 0.119249\n",
      "iteration 1300 / 2000: loss 0.160859\n",
      "iteration 1400 / 2000: loss 0.062309\n",
      "iteration 1500 / 2000: loss 0.137954\n",
      "iteration 1600 / 2000: loss 0.072122\n",
      "iteration 1700 / 2000: loss 0.108685\n",
      "iteration 1800 / 2000: loss 0.136438\n",
      "iteration 1900 / 2000: loss 0.103881\n",
      "Hidden Size: 30, Learning Rate: 0.0075000250000000004, LR Decay: 0.645, Regularization: 0.0, Validation accuracy: 0.9666\n",
      "iteration 0 / 2000: loss 2.302600\n",
      "iteration 100 / 2000: loss 0.276411\n",
      "iteration 200 / 2000: loss 0.196418\n",
      "iteration 300 / 2000: loss 0.178530\n",
      "iteration 400 / 2000: loss 0.201952\n",
      "iteration 500 / 2000: loss 0.178474\n",
      "iteration 600 / 2000: loss 0.148283\n",
      "iteration 700 / 2000: loss 0.277364\n",
      "iteration 800 / 2000: loss 0.153680\n",
      "iteration 900 / 2000: loss 0.111428\n",
      "iteration 1000 / 2000: loss 0.123793\n",
      "iteration 1100 / 2000: loss 0.136932\n",
      "iteration 1200 / 2000: loss 0.144878\n",
      "iteration 1300 / 2000: loss 0.126669\n",
      "iteration 1400 / 2000: loss 0.164178\n",
      "iteration 1500 / 2000: loss 0.255097\n",
      "iteration 1600 / 2000: loss 0.147593\n",
      "iteration 1700 / 2000: loss 0.124810\n",
      "iteration 1800 / 2000: loss 0.092703\n",
      "iteration 1900 / 2000: loss 0.116735\n",
      "Hidden Size: 30, Learning Rate: 0.0075000250000000004, LR Decay: 0.645, Regularization: 0.1, Validation accuracy: 0.9656\n",
      "iteration 0 / 2000: loss 2.302606\n",
      "iteration 100 / 2000: loss 0.351397\n",
      "iteration 200 / 2000: loss 0.195741\n",
      "iteration 300 / 2000: loss 0.226635\n",
      "iteration 400 / 2000: loss 0.219754\n",
      "iteration 500 / 2000: loss 0.210636\n",
      "iteration 600 / 2000: loss 0.130254\n",
      "iteration 700 / 2000: loss 0.230648\n",
      "iteration 800 / 2000: loss 0.160558\n",
      "iteration 900 / 2000: loss 0.228096\n",
      "iteration 1000 / 2000: loss 0.192853\n",
      "iteration 1100 / 2000: loss 0.209880\n",
      "iteration 1200 / 2000: loss 0.205988\n",
      "iteration 1300 / 2000: loss 0.218431\n",
      "iteration 1400 / 2000: loss 0.191096\n",
      "iteration 1500 / 2000: loss 0.183860\n",
      "iteration 1600 / 2000: loss 0.152673\n",
      "iteration 1700 / 2000: loss 0.137870\n",
      "iteration 1800 / 2000: loss 0.217420\n",
      "iteration 1900 / 2000: loss 0.211348\n",
      "Hidden Size: 30, Learning Rate: 0.0075000250000000004, LR Decay: 0.645, Regularization: 0.2, Validation accuracy: 0.9642\n",
      "iteration 0 / 2000: loss 2.302632\n",
      "iteration 100 / 2000: loss 0.314683\n",
      "iteration 200 / 2000: loss 0.331156\n",
      "iteration 300 / 2000: loss 0.275566\n",
      "iteration 400 / 2000: loss 0.214304\n",
      "iteration 500 / 2000: loss 0.234173\n",
      "iteration 600 / 2000: loss 0.232385\n",
      "iteration 700 / 2000: loss 0.221034\n",
      "iteration 800 / 2000: loss 0.200525\n",
      "iteration 900 / 2000: loss 0.181435\n",
      "iteration 1000 / 2000: loss 0.211102\n",
      "iteration 1100 / 2000: loss 0.189640\n",
      "iteration 1200 / 2000: loss 0.184284\n",
      "iteration 1300 / 2000: loss 0.249377\n",
      "iteration 1400 / 2000: loss 0.199024\n",
      "iteration 1500 / 2000: loss 0.207241\n",
      "iteration 1600 / 2000: loss 0.204004\n",
      "iteration 1700 / 2000: loss 0.184448\n",
      "iteration 1800 / 2000: loss 0.181145\n",
      "iteration 1900 / 2000: loss 0.235056\n",
      "Hidden Size: 30, Learning Rate: 0.0075000250000000004, LR Decay: 0.645, Regularization: 0.30000000000000004, Validation accuracy: 0.9638\n",
      "iteration 0 / 2000: loss 2.302617\n",
      "iteration 100 / 2000: loss 0.310243\n",
      "iteration 200 / 2000: loss 0.285766\n",
      "iteration 300 / 2000: loss 0.290640\n",
      "iteration 400 / 2000: loss 0.403107\n",
      "iteration 500 / 2000: loss 0.230804\n",
      "iteration 600 / 2000: loss 0.204128\n",
      "iteration 700 / 2000: loss 0.269504\n",
      "iteration 800 / 2000: loss 0.207953\n",
      "iteration 900 / 2000: loss 0.184484\n",
      "iteration 1000 / 2000: loss 0.261095\n",
      "iteration 1100 / 2000: loss 0.225627\n",
      "iteration 1200 / 2000: loss 0.189078\n",
      "iteration 1300 / 2000: loss 0.297489\n",
      "iteration 1400 / 2000: loss 0.251154\n",
      "iteration 1500 / 2000: loss 0.201571\n",
      "iteration 1600 / 2000: loss 0.209341\n",
      "iteration 1700 / 2000: loss 0.286605\n",
      "iteration 1800 / 2000: loss 0.218517\n",
      "iteration 1900 / 2000: loss 0.238599\n",
      "Hidden Size: 30, Learning Rate: 0.0075000250000000004, LR Decay: 0.645, Regularization: 0.4, Validation accuracy: 0.9614\n",
      "iteration 0 / 2000: loss 2.302583\n",
      "iteration 100 / 2000: loss 0.280171\n",
      "iteration 200 / 2000: loss 0.161210\n",
      "iteration 300 / 2000: loss 0.141330\n",
      "iteration 400 / 2000: loss 0.093283\n",
      "iteration 500 / 2000: loss 0.161345\n",
      "iteration 600 / 2000: loss 0.155813\n",
      "iteration 700 / 2000: loss 0.084581\n",
      "iteration 800 / 2000: loss 0.107958\n",
      "iteration 900 / 2000: loss 0.132011\n",
      "iteration 1000 / 2000: loss 0.142633\n",
      "iteration 1100 / 2000: loss 0.148150\n",
      "iteration 1200 / 2000: loss 0.066543\n",
      "iteration 1300 / 2000: loss 0.107234\n",
      "iteration 1400 / 2000: loss 0.108890\n",
      "iteration 1500 / 2000: loss 0.145623\n",
      "iteration 1600 / 2000: loss 0.153956\n",
      "iteration 1700 / 2000: loss 0.235463\n",
      "iteration 1800 / 2000: loss 0.117099\n",
      "iteration 1900 / 2000: loss 0.108652\n",
      "Hidden Size: 30, Learning Rate: 0.0075000250000000004, LR Decay: 0.47250000000000003, Regularization: 0.0, Validation accuracy: 0.9608\n",
      "iteration 0 / 2000: loss 2.302597\n",
      "iteration 100 / 2000: loss 0.457463\n",
      "iteration 200 / 2000: loss 0.201197\n",
      "iteration 300 / 2000: loss 0.236158\n",
      "iteration 400 / 2000: loss 0.197143\n",
      "iteration 500 / 2000: loss 0.146207\n",
      "iteration 600 / 2000: loss 0.202773\n",
      "iteration 700 / 2000: loss 0.219650\n",
      "iteration 800 / 2000: loss 0.182692\n",
      "iteration 900 / 2000: loss 0.249940\n",
      "iteration 1000 / 2000: loss 0.109675\n",
      "iteration 1100 / 2000: loss 0.171239\n",
      "iteration 1200 / 2000: loss 0.188327\n",
      "iteration 1300 / 2000: loss 0.148232\n",
      "iteration 1400 / 2000: loss 0.218971\n",
      "iteration 1500 / 2000: loss 0.180652\n",
      "iteration 1600 / 2000: loss 0.201622\n",
      "iteration 1700 / 2000: loss 0.196419\n",
      "iteration 1800 / 2000: loss 0.215302\n",
      "iteration 1900 / 2000: loss 0.186070\n",
      "Hidden Size: 30, Learning Rate: 0.0075000250000000004, LR Decay: 0.47250000000000003, Regularization: 0.1, Validation accuracy: 0.9574\n",
      "iteration 0 / 2000: loss 2.302612\n",
      "iteration 100 / 2000: loss 0.332471\n",
      "iteration 200 / 2000: loss 0.295628\n",
      "iteration 300 / 2000: loss 0.216223\n",
      "iteration 400 / 2000: loss 0.291840\n",
      "iteration 500 / 2000: loss 0.248614\n",
      "iteration 600 / 2000: loss 0.219614\n",
      "iteration 700 / 2000: loss 0.145574\n",
      "iteration 800 / 2000: loss 0.194661\n",
      "iteration 900 / 2000: loss 0.234113\n",
      "iteration 1000 / 2000: loss 0.129816\n",
      "iteration 1100 / 2000: loss 0.205576\n",
      "iteration 1200 / 2000: loss 0.153268\n",
      "iteration 1300 / 2000: loss 0.201186\n",
      "iteration 1400 / 2000: loss 0.160512\n",
      "iteration 1500 / 2000: loss 0.211957\n",
      "iteration 1600 / 2000: loss 0.171162\n",
      "iteration 1700 / 2000: loss 0.208102\n",
      "iteration 1800 / 2000: loss 0.201026\n",
      "iteration 1900 / 2000: loss 0.261104\n",
      "Hidden Size: 30, Learning Rate: 0.0075000250000000004, LR Decay: 0.47250000000000003, Regularization: 0.2, Validation accuracy: 0.9516\n",
      "iteration 0 / 2000: loss 2.302618\n",
      "iteration 100 / 2000: loss 0.331137\n",
      "iteration 200 / 2000: loss 0.287347\n",
      "iteration 300 / 2000: loss 0.286838\n",
      "iteration 400 / 2000: loss 0.264443\n",
      "iteration 500 / 2000: loss 0.232607\n",
      "iteration 600 / 2000: loss 0.199581\n",
      "iteration 700 / 2000: loss 0.262600\n",
      "iteration 800 / 2000: loss 0.215259\n",
      "iteration 900 / 2000: loss 0.178705\n",
      "iteration 1000 / 2000: loss 0.238270\n",
      "iteration 1100 / 2000: loss 0.220018\n",
      "iteration 1200 / 2000: loss 0.265184\n",
      "iteration 1300 / 2000: loss 0.275726\n",
      "iteration 1400 / 2000: loss 0.193482\n",
      "iteration 1500 / 2000: loss 0.286304\n",
      "iteration 1600 / 2000: loss 0.217815\n",
      "iteration 1700 / 2000: loss 0.212648\n",
      "iteration 1800 / 2000: loss 0.298269\n",
      "iteration 1900 / 2000: loss 0.185756\n",
      "Hidden Size: 30, Learning Rate: 0.0075000250000000004, LR Decay: 0.47250000000000003, Regularization: 0.30000000000000004, Validation accuracy: 0.955\n",
      "iteration 0 / 2000: loss 2.302635\n",
      "iteration 100 / 2000: loss 0.341058\n",
      "iteration 200 / 2000: loss 0.335998\n",
      "iteration 300 / 2000: loss 0.271263\n",
      "iteration 400 / 2000: loss 0.207601\n",
      "iteration 500 / 2000: loss 0.238128\n",
      "iteration 600 / 2000: loss 0.254571\n",
      "iteration 700 / 2000: loss 0.230474\n",
      "iteration 800 / 2000: loss 0.279741\n",
      "iteration 900 / 2000: loss 0.254879\n",
      "iteration 1000 / 2000: loss 0.254315\n",
      "iteration 1100 / 2000: loss 0.261466\n",
      "iteration 1200 / 2000: loss 0.182383\n",
      "iteration 1300 / 2000: loss 0.270959\n",
      "iteration 1400 / 2000: loss 0.266903\n",
      "iteration 1500 / 2000: loss 0.253567\n",
      "iteration 1600 / 2000: loss 0.289059\n",
      "iteration 1700 / 2000: loss 0.231315\n",
      "iteration 1800 / 2000: loss 0.182532\n",
      "iteration 1900 / 2000: loss 0.273518\n",
      "Hidden Size: 30, Learning Rate: 0.0075000250000000004, LR Decay: 0.47250000000000003, Regularization: 0.4, Validation accuracy: 0.958\n",
      "iteration 0 / 2000: loss 2.302584\n",
      "iteration 100 / 2000: loss 0.399514\n",
      "iteration 200 / 2000: loss 0.335852\n",
      "iteration 300 / 2000: loss 0.175285\n",
      "iteration 400 / 2000: loss 0.186378\n",
      "iteration 500 / 2000: loss 0.204990\n",
      "iteration 600 / 2000: loss 0.183548\n",
      "iteration 700 / 2000: loss 0.157985\n",
      "iteration 800 / 2000: loss 0.157041\n",
      "iteration 900 / 2000: loss 0.169343\n",
      "iteration 1000 / 2000: loss 0.257559\n",
      "iteration 1100 / 2000: loss 0.187437\n",
      "iteration 1200 / 2000: loss 0.215540\n",
      "iteration 1300 / 2000: loss 0.237116\n",
      "iteration 1400 / 2000: loss 0.185838\n",
      "iteration 1500 / 2000: loss 0.138441\n",
      "iteration 1600 / 2000: loss 0.295564\n",
      "iteration 1700 / 2000: loss 0.329628\n",
      "iteration 1800 / 2000: loss 0.219560\n",
      "iteration 1900 / 2000: loss 0.194527\n",
      "Hidden Size: 30, Learning Rate: 0.0075000250000000004, LR Decay: 0.3, Regularization: 0.0, Validation accuracy: 0.943\n",
      "iteration 0 / 2000: loss 2.302596\n",
      "iteration 100 / 2000: loss 0.337249\n",
      "iteration 200 / 2000: loss 0.239278\n",
      "iteration 300 / 2000: loss 0.216508\n",
      "iteration 400 / 2000: loss 0.279270\n",
      "iteration 500 / 2000: loss 0.229771\n",
      "iteration 600 / 2000: loss 0.221308\n",
      "iteration 700 / 2000: loss 0.265731\n",
      "iteration 800 / 2000: loss 0.275811\n",
      "iteration 900 / 2000: loss 0.164840\n",
      "iteration 1000 / 2000: loss 0.262760\n",
      "iteration 1100 / 2000: loss 0.301580\n",
      "iteration 1200 / 2000: loss 0.203262\n",
      "iteration 1300 / 2000: loss 0.163655\n",
      "iteration 1400 / 2000: loss 0.131597\n",
      "iteration 1500 / 2000: loss 0.235674\n",
      "iteration 1600 / 2000: loss 0.152886\n",
      "iteration 1700 / 2000: loss 0.192758\n",
      "iteration 1800 / 2000: loss 0.137927\n",
      "iteration 1900 / 2000: loss 0.252542\n",
      "Hidden Size: 30, Learning Rate: 0.0075000250000000004, LR Decay: 0.3, Regularization: 0.1, Validation accuracy: 0.9396\n",
      "iteration 0 / 2000: loss 2.302609\n",
      "iteration 100 / 2000: loss 0.342730\n",
      "iteration 200 / 2000: loss 0.273806\n",
      "iteration 300 / 2000: loss 0.262575\n",
      "iteration 400 / 2000: loss 0.241995\n",
      "iteration 500 / 2000: loss 0.272191\n",
      "iteration 600 / 2000: loss 0.239398\n",
      "iteration 700 / 2000: loss 0.255901\n",
      "iteration 800 / 2000: loss 0.195891\n",
      "iteration 900 / 2000: loss 0.249841\n",
      "iteration 1000 / 2000: loss 0.357630\n",
      "iteration 1100 / 2000: loss 0.284847\n",
      "iteration 1200 / 2000: loss 0.300110\n",
      "iteration 1300 / 2000: loss 0.165538\n",
      "iteration 1400 / 2000: loss 0.265145\n",
      "iteration 1500 / 2000: loss 0.305004\n",
      "iteration 1600 / 2000: loss 0.424283\n",
      "iteration 1700 / 2000: loss 0.172091\n",
      "iteration 1800 / 2000: loss 0.227688\n",
      "iteration 1900 / 2000: loss 0.238427\n",
      "Hidden Size: 30, Learning Rate: 0.0075000250000000004, LR Decay: 0.3, Regularization: 0.2, Validation accuracy: 0.9388\n",
      "iteration 0 / 2000: loss 2.302620\n",
      "iteration 100 / 2000: loss 0.377905\n",
      "iteration 200 / 2000: loss 0.472086\n",
      "iteration 300 / 2000: loss 0.331235\n",
      "iteration 400 / 2000: loss 0.193066\n",
      "iteration 500 / 2000: loss 0.324356\n",
      "iteration 600 / 2000: loss 0.227357\n",
      "iteration 700 / 2000: loss 0.224109\n",
      "iteration 800 / 2000: loss 0.253423\n",
      "iteration 900 / 2000: loss 0.237602\n",
      "iteration 1000 / 2000: loss 0.225490\n",
      "iteration 1100 / 2000: loss 0.290273\n",
      "iteration 1200 / 2000: loss 0.305609\n",
      "iteration 1300 / 2000: loss 0.279637\n",
      "iteration 1400 / 2000: loss 0.373057\n",
      "iteration 1500 / 2000: loss 0.263793\n",
      "iteration 1600 / 2000: loss 0.369602\n",
      "iteration 1700 / 2000: loss 0.257440\n",
      "iteration 1800 / 2000: loss 0.210389\n",
      "iteration 1900 / 2000: loss 0.290009\n",
      "Hidden Size: 30, Learning Rate: 0.0075000250000000004, LR Decay: 0.3, Regularization: 0.30000000000000004, Validation accuracy: 0.9398\n",
      "iteration 0 / 2000: loss 2.302619\n",
      "iteration 100 / 2000: loss 0.466488\n",
      "iteration 200 / 2000: loss 0.373655\n",
      "iteration 300 / 2000: loss 0.320024\n",
      "iteration 400 / 2000: loss 0.420483\n",
      "iteration 500 / 2000: loss 0.297443\n",
      "iteration 600 / 2000: loss 0.335383\n",
      "iteration 700 / 2000: loss 0.321801\n",
      "iteration 800 / 2000: loss 0.266415\n",
      "iteration 900 / 2000: loss 0.215109\n",
      "iteration 1000 / 2000: loss 0.349394\n",
      "iteration 1100 / 2000: loss 0.232280\n",
      "iteration 1200 / 2000: loss 0.341517\n",
      "iteration 1300 / 2000: loss 0.303381\n",
      "iteration 1400 / 2000: loss 0.249840\n",
      "iteration 1500 / 2000: loss 0.294447\n",
      "iteration 1600 / 2000: loss 0.276262\n",
      "iteration 1700 / 2000: loss 0.229304\n",
      "iteration 1800 / 2000: loss 0.260484\n",
      "iteration 1900 / 2000: loss 0.235309\n",
      "Hidden Size: 30, Learning Rate: 0.0075000250000000004, LR Decay: 0.3, Regularization: 0.4, Validation accuracy: 0.9378\n",
      "iteration 0 / 2000: loss 2.302584\n",
      "iteration 100 / 2000: loss 0.284924\n",
      "iteration 200 / 2000: loss 0.212408\n",
      "iteration 300 / 2000: loss 0.180293\n",
      "iteration 400 / 2000: loss 0.232056\n",
      "iteration 500 / 2000: loss 0.127634\n",
      "iteration 600 / 2000: loss 0.185599\n",
      "iteration 700 / 2000: loss 0.113921\n",
      "iteration 800 / 2000: loss 0.147159\n",
      "iteration 900 / 2000: loss 0.100950\n",
      "iteration 1000 / 2000: loss 0.123529\n",
      "iteration 1100 / 2000: loss 0.166097\n",
      "iteration 1200 / 2000: loss 0.157680\n",
      "iteration 1300 / 2000: loss 0.103838\n",
      "iteration 1400 / 2000: loss 0.085807\n",
      "iteration 1500 / 2000: loss 0.051685\n",
      "iteration 1600 / 2000: loss 0.052167\n",
      "iteration 1700 / 2000: loss 0.080054\n",
      "iteration 1800 / 2000: loss 0.106145\n",
      "iteration 1900 / 2000: loss 0.066120\n",
      "Hidden Size: 30, Learning Rate: 0.00500005, LR Decay: 0.99, Regularization: 0.0, Validation accuracy: 0.9638\n",
      "iteration 0 / 2000: loss 2.302602\n",
      "iteration 100 / 2000: loss 0.245340\n",
      "iteration 200 / 2000: loss 0.180703\n",
      "iteration 300 / 2000: loss 0.144382\n",
      "iteration 400 / 2000: loss 0.130622\n",
      "iteration 500 / 2000: loss 0.200118\n",
      "iteration 600 / 2000: loss 0.168649\n",
      "iteration 700 / 2000: loss 0.121220\n",
      "iteration 800 / 2000: loss 0.206517\n",
      "iteration 900 / 2000: loss 0.194517\n",
      "iteration 1000 / 2000: loss 0.107736\n",
      "iteration 1100 / 2000: loss 0.118157\n",
      "iteration 1200 / 2000: loss 0.180047\n",
      "iteration 1300 / 2000: loss 0.108472\n",
      "iteration 1400 / 2000: loss 0.125142\n",
      "iteration 1500 / 2000: loss 0.137607\n",
      "iteration 1600 / 2000: loss 0.130578\n",
      "iteration 1700 / 2000: loss 0.118802\n",
      "iteration 1800 / 2000: loss 0.142010\n",
      "iteration 1900 / 2000: loss 0.101942\n",
      "Hidden Size: 30, Learning Rate: 0.00500005, LR Decay: 0.99, Regularization: 0.1, Validation accuracy: 0.9664\n",
      "iteration 0 / 2000: loss 2.302619\n",
      "iteration 100 / 2000: loss 0.344897\n",
      "iteration 200 / 2000: loss 0.313400\n",
      "iteration 300 / 2000: loss 0.220031\n",
      "iteration 400 / 2000: loss 0.232545\n",
      "iteration 500 / 2000: loss 0.209114\n",
      "iteration 600 / 2000: loss 0.200010\n",
      "iteration 700 / 2000: loss 0.247436\n",
      "iteration 800 / 2000: loss 0.262297\n",
      "iteration 900 / 2000: loss 0.201065\n",
      "iteration 1000 / 2000: loss 0.257388\n",
      "iteration 1100 / 2000: loss 0.181605\n",
      "iteration 1200 / 2000: loss 0.125476\n",
      "iteration 1300 / 2000: loss 0.153563\n",
      "iteration 1400 / 2000: loss 0.178124\n",
      "iteration 1500 / 2000: loss 0.131063\n",
      "iteration 1600 / 2000: loss 0.228544\n",
      "iteration 1700 / 2000: loss 0.166416\n",
      "iteration 1800 / 2000: loss 0.168523\n",
      "iteration 1900 / 2000: loss 0.163063\n",
      "Hidden Size: 30, Learning Rate: 0.00500005, LR Decay: 0.99, Regularization: 0.2, Validation accuracy: 0.9662\n",
      "iteration 0 / 2000: loss 2.302617\n",
      "iteration 100 / 2000: loss 0.393586\n",
      "iteration 200 / 2000: loss 0.318429\n",
      "iteration 300 / 2000: loss 0.304868\n",
      "iteration 400 / 2000: loss 0.192594\n",
      "iteration 500 / 2000: loss 0.274817\n",
      "iteration 600 / 2000: loss 0.262560\n",
      "iteration 700 / 2000: loss 0.205721\n",
      "iteration 800 / 2000: loss 0.217556\n",
      "iteration 900 / 2000: loss 0.244987\n",
      "iteration 1000 / 2000: loss 0.178316\n",
      "iteration 1100 / 2000: loss 0.175544\n",
      "iteration 1200 / 2000: loss 0.289101\n",
      "iteration 1300 / 2000: loss 0.173020\n",
      "iteration 1400 / 2000: loss 0.204433\n",
      "iteration 1500 / 2000: loss 0.195513\n",
      "iteration 1600 / 2000: loss 0.166840\n",
      "iteration 1700 / 2000: loss 0.182101\n",
      "iteration 1800 / 2000: loss 0.183088\n",
      "iteration 1900 / 2000: loss 0.326149\n",
      "Hidden Size: 30, Learning Rate: 0.00500005, LR Decay: 0.99, Regularization: 0.30000000000000004, Validation accuracy: 0.9642\n",
      "iteration 0 / 2000: loss 2.302639\n",
      "iteration 100 / 2000: loss 0.359542\n",
      "iteration 200 / 2000: loss 0.387002\n",
      "iteration 300 / 2000: loss 0.274712\n",
      "iteration 400 / 2000: loss 0.216143\n",
      "iteration 500 / 2000: loss 0.268755\n",
      "iteration 600 / 2000: loss 0.212678\n",
      "iteration 700 / 2000: loss 0.259252\n",
      "iteration 800 / 2000: loss 0.212465\n",
      "iteration 900 / 2000: loss 0.287340\n",
      "iteration 1000 / 2000: loss 0.241345\n",
      "iteration 1100 / 2000: loss 0.274307\n",
      "iteration 1200 / 2000: loss 0.279373\n",
      "iteration 1300 / 2000: loss 0.211648\n",
      "iteration 1400 / 2000: loss 0.243316\n",
      "iteration 1500 / 2000: loss 0.220331\n",
      "iteration 1600 / 2000: loss 0.249627\n",
      "iteration 1700 / 2000: loss 0.218717\n",
      "iteration 1800 / 2000: loss 0.261511\n",
      "iteration 1900 / 2000: loss 0.214764\n",
      "Hidden Size: 30, Learning Rate: 0.00500005, LR Decay: 0.99, Regularization: 0.4, Validation accuracy: 0.963\n",
      "iteration 0 / 2000: loss 2.302583\n",
      "iteration 100 / 2000: loss 0.388501\n",
      "iteration 200 / 2000: loss 0.200063\n",
      "iteration 300 / 2000: loss 0.284871\n",
      "iteration 400 / 2000: loss 0.228824\n",
      "iteration 500 / 2000: loss 0.154561\n",
      "iteration 600 / 2000: loss 0.127398\n",
      "iteration 700 / 2000: loss 0.122147\n",
      "iteration 800 / 2000: loss 0.137082\n",
      "iteration 900 / 2000: loss 0.067497\n",
      "iteration 1000 / 2000: loss 0.184737\n",
      "iteration 1100 / 2000: loss 0.120935\n",
      "iteration 1200 / 2000: loss 0.163906\n",
      "iteration 1300 / 2000: loss 0.134711\n",
      "iteration 1400 / 2000: loss 0.050741\n",
      "iteration 1500 / 2000: loss 0.075206\n",
      "iteration 1600 / 2000: loss 0.057380\n",
      "iteration 1700 / 2000: loss 0.077613\n",
      "iteration 1800 / 2000: loss 0.063457\n",
      "iteration 1900 / 2000: loss 0.089064\n",
      "Hidden Size: 30, Learning Rate: 0.00500005, LR Decay: 0.8175, Regularization: 0.0, Validation accuracy: 0.9682\n",
      "iteration 0 / 2000: loss 2.302601\n",
      "iteration 100 / 2000: loss 0.309792\n",
      "iteration 200 / 2000: loss 0.246841\n",
      "iteration 300 / 2000: loss 0.249074\n",
      "iteration 400 / 2000: loss 0.146120\n",
      "iteration 500 / 2000: loss 0.257148\n",
      "iteration 600 / 2000: loss 0.184748\n",
      "iteration 700 / 2000: loss 0.156260\n",
      "iteration 800 / 2000: loss 0.163125\n",
      "iteration 900 / 2000: loss 0.092006\n",
      "iteration 1000 / 2000: loss 0.151710\n",
      "iteration 1100 / 2000: loss 0.148876\n",
      "iteration 1200 / 2000: loss 0.129794\n",
      "iteration 1300 / 2000: loss 0.081218\n",
      "iteration 1400 / 2000: loss 0.122775\n",
      "iteration 1500 / 2000: loss 0.113440\n",
      "iteration 1600 / 2000: loss 0.110025\n",
      "iteration 1700 / 2000: loss 0.085273\n",
      "iteration 1800 / 2000: loss 0.084192\n",
      "iteration 1900 / 2000: loss 0.153602\n",
      "Hidden Size: 30, Learning Rate: 0.00500005, LR Decay: 0.8175, Regularization: 0.1, Validation accuracy: 0.9712\n",
      "iteration 0 / 2000: loss 2.302616\n",
      "iteration 100 / 2000: loss 0.333255\n",
      "iteration 200 / 2000: loss 0.312103\n",
      "iteration 300 / 2000: loss 0.218947\n",
      "iteration 400 / 2000: loss 0.241753\n",
      "iteration 500 / 2000: loss 0.172277\n",
      "iteration 600 / 2000: loss 0.177135\n",
      "iteration 700 / 2000: loss 0.233284\n",
      "iteration 800 / 2000: loss 0.141356\n",
      "iteration 900 / 2000: loss 0.238614\n",
      "iteration 1000 / 2000: loss 0.190539\n",
      "iteration 1100 / 2000: loss 0.226113\n",
      "iteration 1200 / 2000: loss 0.153112\n",
      "iteration 1300 / 2000: loss 0.144705\n",
      "iteration 1400 / 2000: loss 0.179899\n",
      "iteration 1500 / 2000: loss 0.186473\n",
      "iteration 1600 / 2000: loss 0.175074\n",
      "iteration 1700 / 2000: loss 0.140426\n",
      "iteration 1800 / 2000: loss 0.172618\n",
      "iteration 1900 / 2000: loss 0.172590\n",
      "Hidden Size: 30, Learning Rate: 0.00500005, LR Decay: 0.8175, Regularization: 0.2, Validation accuracy: 0.9678\n",
      "iteration 0 / 2000: loss 2.302627\n",
      "iteration 100 / 2000: loss 0.372265\n",
      "iteration 200 / 2000: loss 0.356493\n",
      "iteration 300 / 2000: loss 0.250567\n",
      "iteration 400 / 2000: loss 0.319655\n",
      "iteration 500 / 2000: loss 0.237540\n",
      "iteration 600 / 2000: loss 0.228498\n",
      "iteration 700 / 2000: loss 0.278276\n",
      "iteration 800 / 2000: loss 0.216692\n",
      "iteration 900 / 2000: loss 0.224894\n",
      "iteration 1000 / 2000: loss 0.200886\n",
      "iteration 1100 / 2000: loss 0.225761\n",
      "iteration 1200 / 2000: loss 0.218994\n",
      "iteration 1300 / 2000: loss 0.170544\n",
      "iteration 1400 / 2000: loss 0.183642\n",
      "iteration 1500 / 2000: loss 0.247399\n",
      "iteration 1600 / 2000: loss 0.187629\n",
      "iteration 1700 / 2000: loss 0.236763\n",
      "iteration 1800 / 2000: loss 0.194661\n",
      "iteration 1900 / 2000: loss 0.198836\n",
      "Hidden Size: 30, Learning Rate: 0.00500005, LR Decay: 0.8175, Regularization: 0.30000000000000004, Validation accuracy: 0.9658\n",
      "iteration 0 / 2000: loss 2.302643\n",
      "iteration 100 / 2000: loss 0.287661\n",
      "iteration 200 / 2000: loss 0.480213\n",
      "iteration 300 / 2000: loss 0.226432\n",
      "iteration 400 / 2000: loss 0.330520\n",
      "iteration 500 / 2000: loss 0.252105\n",
      "iteration 600 / 2000: loss 0.270508\n",
      "iteration 700 / 2000: loss 0.227241\n",
      "iteration 800 / 2000: loss 0.248800\n",
      "iteration 900 / 2000: loss 0.184554\n",
      "iteration 1000 / 2000: loss 0.205094\n",
      "iteration 1100 / 2000: loss 0.211838\n",
      "iteration 1200 / 2000: loss 0.202401\n",
      "iteration 1300 / 2000: loss 0.264350\n",
      "iteration 1400 / 2000: loss 0.308830\n",
      "iteration 1500 / 2000: loss 0.222092\n",
      "iteration 1600 / 2000: loss 0.242632\n",
      "iteration 1700 / 2000: loss 0.225157\n",
      "iteration 1800 / 2000: loss 0.259981\n",
      "iteration 1900 / 2000: loss 0.215702\n",
      "Hidden Size: 30, Learning Rate: 0.00500005, LR Decay: 0.8175, Regularization: 0.4, Validation accuracy: 0.963\n",
      "iteration 0 / 2000: loss 2.302596\n",
      "iteration 100 / 2000: loss 0.308904\n",
      "iteration 200 / 2000: loss 0.351709\n",
      "iteration 300 / 2000: loss 0.195463\n",
      "iteration 400 / 2000: loss 0.234722\n",
      "iteration 500 / 2000: loss 0.104047\n",
      "iteration 600 / 2000: loss 0.171297\n",
      "iteration 700 / 2000: loss 0.158942\n",
      "iteration 800 / 2000: loss 0.145191\n",
      "iteration 900 / 2000: loss 0.158759\n",
      "iteration 1000 / 2000: loss 0.168026\n",
      "iteration 1100 / 2000: loss 0.159800\n",
      "iteration 1200 / 2000: loss 0.115988\n",
      "iteration 1300 / 2000: loss 0.154746\n",
      "iteration 1400 / 2000: loss 0.119241\n",
      "iteration 1500 / 2000: loss 0.178706\n",
      "iteration 1600 / 2000: loss 0.138516\n",
      "iteration 1700 / 2000: loss 0.082811\n",
      "iteration 1800 / 2000: loss 0.121530\n",
      "iteration 1900 / 2000: loss 0.168280\n",
      "Hidden Size: 30, Learning Rate: 0.00500005, LR Decay: 0.645, Regularization: 0.0, Validation accuracy: 0.9618\n",
      "iteration 0 / 2000: loss 2.302584\n",
      "iteration 100 / 2000: loss 0.392610\n",
      "iteration 200 / 2000: loss 0.273223\n",
      "iteration 300 / 2000: loss 0.319840\n",
      "iteration 400 / 2000: loss 0.216125\n",
      "iteration 500 / 2000: loss 0.231616\n",
      "iteration 600 / 2000: loss 0.158901\n",
      "iteration 700 / 2000: loss 0.255164\n",
      "iteration 800 / 2000: loss 0.141660\n",
      "iteration 900 / 2000: loss 0.161517\n",
      "iteration 1000 / 2000: loss 0.158475\n",
      "iteration 1100 / 2000: loss 0.255307\n",
      "iteration 1200 / 2000: loss 0.167901\n",
      "iteration 1300 / 2000: loss 0.122821\n",
      "iteration 1400 / 2000: loss 0.260410\n",
      "iteration 1500 / 2000: loss 0.184502\n",
      "iteration 1600 / 2000: loss 0.126018\n",
      "iteration 1700 / 2000: loss 0.176849\n",
      "iteration 1800 / 2000: loss 0.169273\n",
      "iteration 1900 / 2000: loss 0.168222\n",
      "Hidden Size: 30, Learning Rate: 0.00500005, LR Decay: 0.645, Regularization: 0.1, Validation accuracy: 0.9616\n",
      "iteration 0 / 2000: loss 2.302607\n",
      "iteration 100 / 2000: loss 0.283447\n",
      "iteration 200 / 2000: loss 0.228583\n",
      "iteration 300 / 2000: loss 0.235171\n",
      "iteration 400 / 2000: loss 0.287429\n",
      "iteration 500 / 2000: loss 0.252336\n",
      "iteration 600 / 2000: loss 0.187008\n",
      "iteration 700 / 2000: loss 0.235587\n",
      "iteration 800 / 2000: loss 0.208888\n",
      "iteration 900 / 2000: loss 0.142204\n",
      "iteration 1000 / 2000: loss 0.162761\n",
      "iteration 1100 / 2000: loss 0.289477\n",
      "iteration 1200 / 2000: loss 0.191839\n",
      "iteration 1300 / 2000: loss 0.163002\n",
      "iteration 1400 / 2000: loss 0.218895\n",
      "iteration 1500 / 2000: loss 0.228695\n",
      "iteration 1600 / 2000: loss 0.220698\n",
      "iteration 1700 / 2000: loss 0.157924\n",
      "iteration 1800 / 2000: loss 0.251418\n",
      "iteration 1900 / 2000: loss 0.230752\n",
      "Hidden Size: 30, Learning Rate: 0.00500005, LR Decay: 0.645, Regularization: 0.2, Validation accuracy: 0.9588\n",
      "iteration 0 / 2000: loss 2.302620\n",
      "iteration 100 / 2000: loss 0.349737\n",
      "iteration 200 / 2000: loss 0.334691\n",
      "iteration 300 / 2000: loss 0.275130\n",
      "iteration 400 / 2000: loss 0.215188\n",
      "iteration 500 / 2000: loss 0.219531\n",
      "iteration 600 / 2000: loss 0.240811\n",
      "iteration 700 / 2000: loss 0.190234\n",
      "iteration 800 / 2000: loss 0.273012\n",
      "iteration 900 / 2000: loss 0.194163\n",
      "iteration 1000 / 2000: loss 0.202082\n",
      "iteration 1100 / 2000: loss 0.221632\n",
      "iteration 1200 / 2000: loss 0.275069\n",
      "iteration 1300 / 2000: loss 0.215209\n",
      "iteration 1400 / 2000: loss 0.194421\n",
      "iteration 1500 / 2000: loss 0.213050\n",
      "iteration 1600 / 2000: loss 0.245199\n",
      "iteration 1700 / 2000: loss 0.233505\n",
      "iteration 1800 / 2000: loss 0.257991\n",
      "iteration 1900 / 2000: loss 0.177180\n",
      "Hidden Size: 30, Learning Rate: 0.00500005, LR Decay: 0.645, Regularization: 0.30000000000000004, Validation accuracy: 0.9624\n",
      "iteration 0 / 2000: loss 2.302640\n",
      "iteration 100 / 2000: loss 0.284515\n",
      "iteration 200 / 2000: loss 0.321067\n",
      "iteration 300 / 2000: loss 0.377118\n",
      "iteration 400 / 2000: loss 0.259464\n",
      "iteration 500 / 2000: loss 0.236391\n",
      "iteration 600 / 2000: loss 0.342586\n",
      "iteration 700 / 2000: loss 0.316350\n",
      "iteration 800 / 2000: loss 0.264519\n",
      "iteration 900 / 2000: loss 0.287528\n",
      "iteration 1000 / 2000: loss 0.270918\n",
      "iteration 1100 / 2000: loss 0.310876\n",
      "iteration 1200 / 2000: loss 0.292665\n",
      "iteration 1300 / 2000: loss 0.202243\n",
      "iteration 1400 / 2000: loss 0.222497\n",
      "iteration 1500 / 2000: loss 0.240369\n",
      "iteration 1600 / 2000: loss 0.265892\n",
      "iteration 1700 / 2000: loss 0.165223\n",
      "iteration 1800 / 2000: loss 0.216247\n",
      "iteration 1900 / 2000: loss 0.228348\n",
      "Hidden Size: 30, Learning Rate: 0.00500005, LR Decay: 0.645, Regularization: 0.4, Validation accuracy: 0.9574\n",
      "iteration 0 / 2000: loss 2.302603\n",
      "iteration 100 / 2000: loss 0.325258\n",
      "iteration 200 / 2000: loss 0.234077\n",
      "iteration 300 / 2000: loss 0.239917\n",
      "iteration 400 / 2000: loss 0.216918\n",
      "iteration 500 / 2000: loss 0.146459\n",
      "iteration 600 / 2000: loss 0.155464\n",
      "iteration 700 / 2000: loss 0.168677\n",
      "iteration 800 / 2000: loss 0.259430\n",
      "iteration 900 / 2000: loss 0.262249\n",
      "iteration 1000 / 2000: loss 0.178241\n",
      "iteration 1100 / 2000: loss 0.146569\n",
      "iteration 1200 / 2000: loss 0.184396\n",
      "iteration 1300 / 2000: loss 0.330436\n",
      "iteration 1400 / 2000: loss 0.164153\n",
      "iteration 1500 / 2000: loss 0.247785\n",
      "iteration 1600 / 2000: loss 0.238270\n",
      "iteration 1700 / 2000: loss 0.161140\n",
      "iteration 1800 / 2000: loss 0.210474\n",
      "iteration 1900 / 2000: loss 0.177104\n",
      "Hidden Size: 30, Learning Rate: 0.00500005, LR Decay: 0.47250000000000003, Regularization: 0.0, Validation accuracy: 0.9498\n",
      "iteration 0 / 2000: loss 2.302615\n",
      "iteration 100 / 2000: loss 0.414610\n",
      "iteration 200 / 2000: loss 0.307450\n",
      "iteration 300 / 2000: loss 0.260751\n",
      "iteration 400 / 2000: loss 0.231701\n",
      "iteration 500 / 2000: loss 0.247471\n",
      "iteration 600 / 2000: loss 0.182027\n",
      "iteration 700 / 2000: loss 0.173424\n",
      "iteration 800 / 2000: loss 0.209281\n",
      "iteration 900 / 2000: loss 0.178975\n",
      "iteration 1000 / 2000: loss 0.254643\n",
      "iteration 1100 / 2000: loss 0.142379\n",
      "iteration 1200 / 2000: loss 0.225012\n",
      "iteration 1300 / 2000: loss 0.238469\n",
      "iteration 1400 / 2000: loss 0.180192\n",
      "iteration 1500 / 2000: loss 0.199319\n",
      "iteration 1600 / 2000: loss 0.263073\n",
      "iteration 1700 / 2000: loss 0.203984\n",
      "iteration 1800 / 2000: loss 0.174380\n",
      "iteration 1900 / 2000: loss 0.194498\n",
      "Hidden Size: 30, Learning Rate: 0.00500005, LR Decay: 0.47250000000000003, Regularization: 0.1, Validation accuracy: 0.947\n",
      "iteration 0 / 2000: loss 2.302616\n",
      "iteration 100 / 2000: loss 0.334581\n",
      "iteration 200 / 2000: loss 0.251717\n",
      "iteration 300 / 2000: loss 0.265698\n",
      "iteration 400 / 2000: loss 0.405899\n",
      "iteration 500 / 2000: loss 0.264792\n",
      "iteration 600 / 2000: loss 0.217374\n",
      "iteration 700 / 2000: loss 0.239649\n",
      "iteration 800 / 2000: loss 0.216838\n",
      "iteration 900 / 2000: loss 0.200777\n",
      "iteration 1000 / 2000: loss 0.203821\n",
      "iteration 1100 / 2000: loss 0.201284\n",
      "iteration 1200 / 2000: loss 0.194020\n",
      "iteration 1300 / 2000: loss 0.232149\n",
      "iteration 1400 / 2000: loss 0.260521\n",
      "iteration 1500 / 2000: loss 0.175305\n",
      "iteration 1600 / 2000: loss 0.243740\n",
      "iteration 1700 / 2000: loss 0.222468\n",
      "iteration 1800 / 2000: loss 0.264756\n",
      "iteration 1900 / 2000: loss 0.150849\n",
      "Hidden Size: 30, Learning Rate: 0.00500005, LR Decay: 0.47250000000000003, Regularization: 0.2, Validation accuracy: 0.9472\n",
      "iteration 0 / 2000: loss 2.302611\n",
      "iteration 100 / 2000: loss 0.320357\n",
      "iteration 200 / 2000: loss 0.318350\n",
      "iteration 300 / 2000: loss 0.347831\n",
      "iteration 400 / 2000: loss 0.233860\n",
      "iteration 500 / 2000: loss 0.224816\n",
      "iteration 600 / 2000: loss 0.178995\n",
      "iteration 700 / 2000: loss 0.204179\n",
      "iteration 800 / 2000: loss 0.258827\n",
      "iteration 900 / 2000: loss 0.220417\n",
      "iteration 1000 / 2000: loss 0.188813\n",
      "iteration 1100 / 2000: loss 0.249541\n",
      "iteration 1200 / 2000: loss 0.210920\n",
      "iteration 1300 / 2000: loss 0.185826\n",
      "iteration 1400 / 2000: loss 0.276628\n",
      "iteration 1500 / 2000: loss 0.226958\n",
      "iteration 1600 / 2000: loss 0.290215\n",
      "iteration 1700 / 2000: loss 0.221666\n",
      "iteration 1800 / 2000: loss 0.305704\n",
      "iteration 1900 / 2000: loss 0.250697\n",
      "Hidden Size: 30, Learning Rate: 0.00500005, LR Decay: 0.47250000000000003, Regularization: 0.30000000000000004, Validation accuracy: 0.9514\n",
      "iteration 0 / 2000: loss 2.302634\n",
      "iteration 100 / 2000: loss 0.429086\n",
      "iteration 200 / 2000: loss 0.271287\n",
      "iteration 300 / 2000: loss 0.355933\n",
      "iteration 400 / 2000: loss 0.336857\n",
      "iteration 500 / 2000: loss 0.230955\n",
      "iteration 600 / 2000: loss 0.266580\n",
      "iteration 700 / 2000: loss 0.308656\n",
      "iteration 800 / 2000: loss 0.300456\n",
      "iteration 900 / 2000: loss 0.247347\n",
      "iteration 1000 / 2000: loss 0.291480\n",
      "iteration 1100 / 2000: loss 0.280741\n",
      "iteration 1200 / 2000: loss 0.233807\n",
      "iteration 1300 / 2000: loss 0.276601\n",
      "iteration 1400 / 2000: loss 0.302860\n",
      "iteration 1500 / 2000: loss 0.279101\n",
      "iteration 1600 / 2000: loss 0.244284\n",
      "iteration 1700 / 2000: loss 0.234330\n",
      "iteration 1800 / 2000: loss 0.230270\n",
      "iteration 1900 / 2000: loss 0.199790\n",
      "Hidden Size: 30, Learning Rate: 0.00500005, LR Decay: 0.47250000000000003, Regularization: 0.4, Validation accuracy: 0.9494\n",
      "iteration 0 / 2000: loss 2.302587\n",
      "iteration 100 / 2000: loss 0.508346\n",
      "iteration 200 / 2000: loss 0.423543\n",
      "iteration 300 / 2000: loss 0.356259\n",
      "iteration 400 / 2000: loss 0.251443\n",
      "iteration 500 / 2000: loss 0.249221\n",
      "iteration 600 / 2000: loss 0.321273\n",
      "iteration 700 / 2000: loss 0.209697\n",
      "iteration 800 / 2000: loss 0.296572\n",
      "iteration 900 / 2000: loss 0.286473\n",
      "iteration 1000 / 2000: loss 0.290012\n",
      "iteration 1100 / 2000: loss 0.247358\n",
      "iteration 1200 / 2000: loss 0.319085\n",
      "iteration 1300 / 2000: loss 0.240200\n",
      "iteration 1400 / 2000: loss 0.299441\n",
      "iteration 1500 / 2000: loss 0.262269\n",
      "iteration 1600 / 2000: loss 0.283852\n",
      "iteration 1700 / 2000: loss 0.302500\n",
      "iteration 1800 / 2000: loss 0.202782\n",
      "iteration 1900 / 2000: loss 0.331049\n",
      "Hidden Size: 30, Learning Rate: 0.00500005, LR Decay: 0.3, Regularization: 0.0, Validation accuracy: 0.932\n",
      "iteration 0 / 2000: loss 2.302599\n",
      "iteration 100 / 2000: loss 0.490499\n",
      "iteration 200 / 2000: loss 0.338560\n",
      "iteration 300 / 2000: loss 0.293998\n",
      "iteration 400 / 2000: loss 0.368415\n",
      "iteration 500 / 2000: loss 0.340016\n",
      "iteration 600 / 2000: loss 0.284307\n",
      "iteration 700 / 2000: loss 0.240886\n",
      "iteration 800 / 2000: loss 0.248430\n",
      "iteration 900 / 2000: loss 0.277418\n",
      "iteration 1000 / 2000: loss 0.333186\n",
      "iteration 1100 / 2000: loss 0.200343\n",
      "iteration 1200 / 2000: loss 0.229577\n",
      "iteration 1300 / 2000: loss 0.279178\n",
      "iteration 1400 / 2000: loss 0.257713\n",
      "iteration 1500 / 2000: loss 0.279917\n",
      "iteration 1600 / 2000: loss 0.291388\n",
      "iteration 1700 / 2000: loss 0.220735\n",
      "iteration 1800 / 2000: loss 0.214229\n",
      "iteration 1900 / 2000: loss 0.309019\n",
      "Hidden Size: 30, Learning Rate: 0.00500005, LR Decay: 0.3, Regularization: 0.1, Validation accuracy: 0.9302\n",
      "iteration 0 / 2000: loss 2.302591\n",
      "iteration 100 / 2000: loss 0.460676\n",
      "iteration 200 / 2000: loss 0.359841\n",
      "iteration 300 / 2000: loss 0.272385\n",
      "iteration 400 / 2000: loss 0.361864\n",
      "iteration 500 / 2000: loss 0.190040\n",
      "iteration 600 / 2000: loss 0.298098\n",
      "iteration 700 / 2000: loss 0.221069\n",
      "iteration 800 / 2000: loss 0.280517\n",
      "iteration 900 / 2000: loss 0.294704\n",
      "iteration 1000 / 2000: loss 0.275455\n",
      "iteration 1100 / 2000: loss 0.330525\n",
      "iteration 1200 / 2000: loss 0.244954\n",
      "iteration 1300 / 2000: loss 0.252652\n",
      "iteration 1400 / 2000: loss 0.270416\n",
      "iteration 1500 / 2000: loss 0.246730\n",
      "iteration 1600 / 2000: loss 0.353648\n",
      "iteration 1700 / 2000: loss 0.376184\n",
      "iteration 1800 / 2000: loss 0.331614\n",
      "iteration 1900 / 2000: loss 0.347124\n",
      "Hidden Size: 30, Learning Rate: 0.00500005, LR Decay: 0.3, Regularization: 0.2, Validation accuracy: 0.9252\n",
      "iteration 0 / 2000: loss 2.302619\n",
      "iteration 100 / 2000: loss 0.562741\n",
      "iteration 200 / 2000: loss 0.400801\n",
      "iteration 300 / 2000: loss 0.246498\n",
      "iteration 400 / 2000: loss 0.252819\n",
      "iteration 500 / 2000: loss 0.278298\n",
      "iteration 600 / 2000: loss 0.315929\n",
      "iteration 700 / 2000: loss 0.278140\n",
      "iteration 800 / 2000: loss 0.360656\n",
      "iteration 900 / 2000: loss 0.352223\n",
      "iteration 1000 / 2000: loss 0.270673\n",
      "iteration 1100 / 2000: loss 0.289006\n",
      "iteration 1200 / 2000: loss 0.470729\n",
      "iteration 1300 / 2000: loss 0.333687\n",
      "iteration 1400 / 2000: loss 0.276764\n",
      "iteration 1500 / 2000: loss 0.265707\n",
      "iteration 1600 / 2000: loss 0.264051\n",
      "iteration 1700 / 2000: loss 0.206928\n",
      "iteration 1800 / 2000: loss 0.248732\n",
      "iteration 1900 / 2000: loss 0.234030\n",
      "Hidden Size: 30, Learning Rate: 0.00500005, LR Decay: 0.3, Regularization: 0.30000000000000004, Validation accuracy: 0.9308\n",
      "iteration 0 / 2000: loss 2.302630\n",
      "iteration 100 / 2000: loss 0.568841\n",
      "iteration 200 / 2000: loss 0.356654\n",
      "iteration 300 / 2000: loss 0.291006\n",
      "iteration 400 / 2000: loss 0.312205\n",
      "iteration 500 / 2000: loss 0.362520\n",
      "iteration 600 / 2000: loss 0.401836\n",
      "iteration 700 / 2000: loss 0.384074\n",
      "iteration 800 / 2000: loss 0.296892\n",
      "iteration 900 / 2000: loss 0.290033\n",
      "iteration 1000 / 2000: loss 0.299269\n",
      "iteration 1100 / 2000: loss 0.415028\n",
      "iteration 1200 / 2000: loss 0.269154\n",
      "iteration 1300 / 2000: loss 0.276507\n",
      "iteration 1400 / 2000: loss 0.351733\n",
      "iteration 1500 / 2000: loss 0.269396\n",
      "iteration 1600 / 2000: loss 0.278012\n",
      "iteration 1700 / 2000: loss 0.333833\n",
      "iteration 1800 / 2000: loss 0.367614\n",
      "iteration 1900 / 2000: loss 0.289646\n",
      "Hidden Size: 30, Learning Rate: 0.00500005, LR Decay: 0.3, Regularization: 0.4, Validation accuracy: 0.9286\n",
      "iteration 0 / 2000: loss 2.302601\n",
      "iteration 100 / 2000: loss 0.461880\n",
      "iteration 200 / 2000: loss 0.242646\n",
      "iteration 300 / 2000: loss 0.193394\n",
      "iteration 400 / 2000: loss 0.193710\n",
      "iteration 500 / 2000: loss 0.223753\n",
      "iteration 600 / 2000: loss 0.180925\n",
      "iteration 700 / 2000: loss 0.104160\n",
      "iteration 800 / 2000: loss 0.177334\n",
      "iteration 900 / 2000: loss 0.074392\n",
      "iteration 1000 / 2000: loss 0.079234\n",
      "iteration 1100 / 2000: loss 0.117141\n",
      "iteration 1200 / 2000: loss 0.153339\n",
      "iteration 1300 / 2000: loss 0.124712\n",
      "iteration 1400 / 2000: loss 0.183397\n",
      "iteration 1500 / 2000: loss 0.063703\n",
      "iteration 1600 / 2000: loss 0.087187\n",
      "iteration 1700 / 2000: loss 0.046544\n",
      "iteration 1800 / 2000: loss 0.098475\n",
      "iteration 1900 / 2000: loss 0.104362\n",
      "Hidden Size: 30, Learning Rate: 0.002500074999999999, LR Decay: 0.99, Regularization: 0.0, Validation accuracy: 0.9694\n",
      "iteration 0 / 2000: loss 2.302596\n",
      "iteration 100 / 2000: loss 0.328051\n",
      "iteration 200 / 2000: loss 0.309498\n",
      "iteration 300 / 2000: loss 0.177843\n",
      "iteration 400 / 2000: loss 0.204825\n",
      "iteration 500 / 2000: loss 0.188768\n",
      "iteration 600 / 2000: loss 0.156056\n",
      "iteration 700 / 2000: loss 0.264211\n",
      "iteration 800 / 2000: loss 0.185984\n",
      "iteration 900 / 2000: loss 0.151655\n",
      "iteration 1000 / 2000: loss 0.107528\n",
      "iteration 1100 / 2000: loss 0.145792\n",
      "iteration 1200 / 2000: loss 0.149488\n",
      "iteration 1300 / 2000: loss 0.150948\n",
      "iteration 1400 / 2000: loss 0.134318\n",
      "iteration 1500 / 2000: loss 0.152627\n",
      "iteration 1600 / 2000: loss 0.183722\n",
      "iteration 1700 / 2000: loss 0.118937\n",
      "iteration 1800 / 2000: loss 0.151500\n",
      "iteration 1900 / 2000: loss 0.129195\n",
      "Hidden Size: 30, Learning Rate: 0.002500074999999999, LR Decay: 0.99, Regularization: 0.1, Validation accuracy: 0.9668\n",
      "iteration 0 / 2000: loss 2.302611\n",
      "iteration 100 / 2000: loss 0.287358\n",
      "iteration 200 / 2000: loss 0.311745\n",
      "iteration 300 / 2000: loss 0.337703\n",
      "iteration 400 / 2000: loss 0.271387\n",
      "iteration 500 / 2000: loss 0.268420\n",
      "iteration 600 / 2000: loss 0.230336\n",
      "iteration 700 / 2000: loss 0.192290\n",
      "iteration 800 / 2000: loss 0.221846\n",
      "iteration 900 / 2000: loss 0.199940\n",
      "iteration 1000 / 2000: loss 0.200053\n",
      "iteration 1100 / 2000: loss 0.163645\n",
      "iteration 1200 / 2000: loss 0.174590\n",
      "iteration 1300 / 2000: loss 0.268759\n",
      "iteration 1400 / 2000: loss 0.192872\n",
      "iteration 1500 / 2000: loss 0.195862\n",
      "iteration 1600 / 2000: loss 0.135001\n",
      "iteration 1700 / 2000: loss 0.151837\n",
      "iteration 1800 / 2000: loss 0.212020\n",
      "iteration 1900 / 2000: loss 0.137975\n",
      "Hidden Size: 30, Learning Rate: 0.002500074999999999, LR Decay: 0.99, Regularization: 0.2, Validation accuracy: 0.9658\n",
      "iteration 0 / 2000: loss 2.302630\n",
      "iteration 100 / 2000: loss 0.430390\n",
      "iteration 200 / 2000: loss 0.482699\n",
      "iteration 300 / 2000: loss 0.260176\n",
      "iteration 400 / 2000: loss 0.225387\n",
      "iteration 500 / 2000: loss 0.251561\n",
      "iteration 600 / 2000: loss 0.241361\n",
      "iteration 700 / 2000: loss 0.348563\n",
      "iteration 800 / 2000: loss 0.239675\n",
      "iteration 900 / 2000: loss 0.196925\n",
      "iteration 1000 / 2000: loss 0.192013\n",
      "iteration 1100 / 2000: loss 0.280933\n",
      "iteration 1200 / 2000: loss 0.208748\n",
      "iteration 1300 / 2000: loss 0.199852\n",
      "iteration 1400 / 2000: loss 0.226242\n",
      "iteration 1500 / 2000: loss 0.146987\n",
      "iteration 1600 / 2000: loss 0.215218\n",
      "iteration 1700 / 2000: loss 0.272109\n",
      "iteration 1800 / 2000: loss 0.169361\n",
      "iteration 1900 / 2000: loss 0.236159\n",
      "Hidden Size: 30, Learning Rate: 0.002500074999999999, LR Decay: 0.99, Regularization: 0.30000000000000004, Validation accuracy: 0.9646\n",
      "iteration 0 / 2000: loss 2.302645\n",
      "iteration 100 / 2000: loss 0.467970\n",
      "iteration 200 / 2000: loss 0.350833\n",
      "iteration 300 / 2000: loss 0.266605\n",
      "iteration 400 / 2000: loss 0.234560\n",
      "iteration 500 / 2000: loss 0.323169\n",
      "iteration 600 / 2000: loss 0.338463\n",
      "iteration 700 / 2000: loss 0.223975\n",
      "iteration 800 / 2000: loss 0.270036\n",
      "iteration 900 / 2000: loss 0.274297\n",
      "iteration 1000 / 2000: loss 0.222636\n",
      "iteration 1100 / 2000: loss 0.229525\n",
      "iteration 1200 / 2000: loss 0.256361\n",
      "iteration 1300 / 2000: loss 0.258773\n",
      "iteration 1400 / 2000: loss 0.245651\n",
      "iteration 1500 / 2000: loss 0.245160\n",
      "iteration 1600 / 2000: loss 0.200144\n",
      "iteration 1700 / 2000: loss 0.224167\n",
      "iteration 1800 / 2000: loss 0.239687\n",
      "iteration 1900 / 2000: loss 0.191732\n",
      "Hidden Size: 30, Learning Rate: 0.002500074999999999, LR Decay: 0.99, Regularization: 0.4, Validation accuracy: 0.961\n",
      "iteration 0 / 2000: loss 2.302597\n",
      "iteration 100 / 2000: loss 0.434208\n",
      "iteration 200 / 2000: loss 0.317418\n",
      "iteration 300 / 2000: loss 0.168415\n",
      "iteration 400 / 2000: loss 0.249405\n",
      "iteration 500 / 2000: loss 0.221576\n",
      "iteration 600 / 2000: loss 0.204417\n",
      "iteration 700 / 2000: loss 0.172432\n",
      "iteration 800 / 2000: loss 0.156955\n",
      "iteration 900 / 2000: loss 0.271242\n",
      "iteration 1000 / 2000: loss 0.180334\n",
      "iteration 1100 / 2000: loss 0.095764\n",
      "iteration 1200 / 2000: loss 0.125906\n",
      "iteration 1300 / 2000: loss 0.225911\n",
      "iteration 1400 / 2000: loss 0.125058\n",
      "iteration 1500 / 2000: loss 0.153045\n",
      "iteration 1600 / 2000: loss 0.107280\n",
      "iteration 1700 / 2000: loss 0.110279\n",
      "iteration 1800 / 2000: loss 0.086611\n",
      "iteration 1900 / 2000: loss 0.100771\n",
      "Hidden Size: 30, Learning Rate: 0.002500074999999999, LR Decay: 0.8175, Regularization: 0.0, Validation accuracy: 0.961\n",
      "iteration 0 / 2000: loss 2.302594\n",
      "iteration 100 / 2000: loss 0.354448\n",
      "iteration 200 / 2000: loss 0.338795\n",
      "iteration 300 / 2000: loss 0.193975\n",
      "iteration 400 / 2000: loss 0.209124\n",
      "iteration 500 / 2000: loss 0.190458\n",
      "iteration 600 / 2000: loss 0.248310\n",
      "iteration 700 / 2000: loss 0.264478\n",
      "iteration 800 / 2000: loss 0.195825\n",
      "iteration 900 / 2000: loss 0.173212\n",
      "iteration 1000 / 2000: loss 0.128391\n",
      "iteration 1100 / 2000: loss 0.163714\n",
      "iteration 1200 / 2000: loss 0.169638\n",
      "iteration 1300 / 2000: loss 0.248430\n",
      "iteration 1400 / 2000: loss 0.136578\n",
      "iteration 1500 / 2000: loss 0.186828\n",
      "iteration 1600 / 2000: loss 0.125284\n",
      "iteration 1700 / 2000: loss 0.164194\n",
      "iteration 1800 / 2000: loss 0.126804\n",
      "iteration 1900 / 2000: loss 0.224227\n",
      "Hidden Size: 30, Learning Rate: 0.002500074999999999, LR Decay: 0.8175, Regularization: 0.1, Validation accuracy: 0.959\n",
      "iteration 0 / 2000: loss 2.302614\n",
      "iteration 100 / 2000: loss 0.361390\n",
      "iteration 200 / 2000: loss 0.309494\n",
      "iteration 300 / 2000: loss 0.244591\n",
      "iteration 400 / 2000: loss 0.251023\n",
      "iteration 500 / 2000: loss 0.220754\n",
      "iteration 600 / 2000: loss 0.182962\n",
      "iteration 700 / 2000: loss 0.251850\n",
      "iteration 800 / 2000: loss 0.236778\n",
      "iteration 900 / 2000: loss 0.182014\n",
      "iteration 1000 / 2000: loss 0.275302\n",
      "iteration 1100 / 2000: loss 0.219024\n",
      "iteration 1200 / 2000: loss 0.224592\n",
      "iteration 1300 / 2000: loss 0.153217\n",
      "iteration 1400 / 2000: loss 0.165428\n",
      "iteration 1500 / 2000: loss 0.164756\n",
      "iteration 1600 / 2000: loss 0.177629\n",
      "iteration 1700 / 2000: loss 0.231425\n",
      "iteration 1800 / 2000: loss 0.198639\n",
      "iteration 1900 / 2000: loss 0.183501\n",
      "Hidden Size: 30, Learning Rate: 0.002500074999999999, LR Decay: 0.8175, Regularization: 0.2, Validation accuracy: 0.9594\n",
      "iteration 0 / 2000: loss 2.302624\n",
      "iteration 100 / 2000: loss 0.423147\n",
      "iteration 200 / 2000: loss 0.388273\n",
      "iteration 300 / 2000: loss 0.289889\n",
      "iteration 400 / 2000: loss 0.200520\n",
      "iteration 500 / 2000: loss 0.266565\n",
      "iteration 600 / 2000: loss 0.248522\n",
      "iteration 700 / 2000: loss 0.213985\n",
      "iteration 800 / 2000: loss 0.236701\n",
      "iteration 900 / 2000: loss 0.187108\n",
      "iteration 1000 / 2000: loss 0.206370\n",
      "iteration 1100 / 2000: loss 0.265026\n",
      "iteration 1200 / 2000: loss 0.261435\n",
      "iteration 1300 / 2000: loss 0.187086\n",
      "iteration 1400 / 2000: loss 0.287061\n",
      "iteration 1500 / 2000: loss 0.222589\n",
      "iteration 1600 / 2000: loss 0.187453\n",
      "iteration 1700 / 2000: loss 0.178691\n",
      "iteration 1800 / 2000: loss 0.259790\n",
      "iteration 1900 / 2000: loss 0.142803\n",
      "Hidden Size: 30, Learning Rate: 0.002500074999999999, LR Decay: 0.8175, Regularization: 0.30000000000000004, Validation accuracy: 0.959\n",
      "iteration 0 / 2000: loss 2.302649\n",
      "iteration 100 / 2000: loss 0.536425\n",
      "iteration 200 / 2000: loss 0.438856\n",
      "iteration 300 / 2000: loss 0.240839\n",
      "iteration 400 / 2000: loss 0.254477\n",
      "iteration 500 / 2000: loss 0.250729\n",
      "iteration 600 / 2000: loss 0.335542\n",
      "iteration 700 / 2000: loss 0.325210\n",
      "iteration 800 / 2000: loss 0.299300\n",
      "iteration 900 / 2000: loss 0.264645\n",
      "iteration 1000 / 2000: loss 0.220285\n",
      "iteration 1100 / 2000: loss 0.219098\n",
      "iteration 1200 / 2000: loss 0.298596\n",
      "iteration 1300 / 2000: loss 0.255877\n",
      "iteration 1400 / 2000: loss 0.214636\n",
      "iteration 1500 / 2000: loss 0.301734\n",
      "iteration 1600 / 2000: loss 0.253074\n",
      "iteration 1700 / 2000: loss 0.236435\n",
      "iteration 1800 / 2000: loss 0.221266\n",
      "iteration 1900 / 2000: loss 0.248489\n",
      "Hidden Size: 30, Learning Rate: 0.002500074999999999, LR Decay: 0.8175, Regularization: 0.4, Validation accuracy: 0.9586\n",
      "iteration 0 / 2000: loss 2.302573\n",
      "iteration 100 / 2000: loss 0.676604\n",
      "iteration 200 / 2000: loss 0.273999\n",
      "iteration 300 / 2000: loss 0.272482\n",
      "iteration 400 / 2000: loss 0.241316\n",
      "iteration 500 / 2000: loss 0.180541\n",
      "iteration 600 / 2000: loss 0.236134\n",
      "iteration 700 / 2000: loss 0.256147\n",
      "iteration 800 / 2000: loss 0.174792\n",
      "iteration 900 / 2000: loss 0.246174\n",
      "iteration 1000 / 2000: loss 0.220061\n",
      "iteration 1100 / 2000: loss 0.194501\n",
      "iteration 1200 / 2000: loss 0.189662\n",
      "iteration 1300 / 2000: loss 0.132666\n",
      "iteration 1400 / 2000: loss 0.126922\n",
      "iteration 1500 / 2000: loss 0.256497\n",
      "iteration 1600 / 2000: loss 0.131178\n",
      "iteration 1700 / 2000: loss 0.115608\n",
      "iteration 1800 / 2000: loss 0.164130\n",
      "iteration 1900 / 2000: loss 0.218186\n",
      "Hidden Size: 30, Learning Rate: 0.002500074999999999, LR Decay: 0.645, Regularization: 0.0, Validation accuracy: 0.9486\n",
      "iteration 0 / 2000: loss 2.302603\n",
      "iteration 100 / 2000: loss 0.471487\n",
      "iteration 200 / 2000: loss 0.292665\n",
      "iteration 300 / 2000: loss 0.281350\n",
      "iteration 400 / 2000: loss 0.220459\n",
      "iteration 500 / 2000: loss 0.248875\n",
      "iteration 600 / 2000: loss 0.156342\n",
      "iteration 700 / 2000: loss 0.195142\n",
      "iteration 800 / 2000: loss 0.154285\n",
      "iteration 900 / 2000: loss 0.299656\n",
      "iteration 1000 / 2000: loss 0.178345\n",
      "iteration 1100 / 2000: loss 0.272176\n",
      "iteration 1200 / 2000: loss 0.162735\n",
      "iteration 1300 / 2000: loss 0.223301\n",
      "iteration 1400 / 2000: loss 0.139014\n",
      "iteration 1500 / 2000: loss 0.239315\n",
      "iteration 1600 / 2000: loss 0.185591\n",
      "iteration 1700 / 2000: loss 0.198078\n",
      "iteration 1800 / 2000: loss 0.168168\n",
      "iteration 1900 / 2000: loss 0.164991\n",
      "Hidden Size: 30, Learning Rate: 0.002500074999999999, LR Decay: 0.645, Regularization: 0.1, Validation accuracy: 0.9474\n",
      "iteration 0 / 2000: loss 2.302601\n",
      "iteration 100 / 2000: loss 0.477425\n",
      "iteration 200 / 2000: loss 0.291399\n",
      "iteration 300 / 2000: loss 0.224947\n",
      "iteration 400 / 2000: loss 0.254934\n",
      "iteration 500 / 2000: loss 0.215679\n",
      "iteration 600 / 2000: loss 0.267162\n",
      "iteration 700 / 2000: loss 0.210154\n",
      "iteration 800 / 2000: loss 0.186191\n",
      "iteration 900 / 2000: loss 0.233034\n",
      "iteration 1000 / 2000: loss 0.186227\n",
      "iteration 1100 / 2000: loss 0.216569\n",
      "iteration 1200 / 2000: loss 0.266384\n",
      "iteration 1300 / 2000: loss 0.208417\n",
      "iteration 1400 / 2000: loss 0.214049\n",
      "iteration 1500 / 2000: loss 0.238897\n",
      "iteration 1600 / 2000: loss 0.198242\n",
      "iteration 1700 / 2000: loss 0.266023\n",
      "iteration 1800 / 2000: loss 0.254003\n",
      "iteration 1900 / 2000: loss 0.221483\n",
      "Hidden Size: 30, Learning Rate: 0.002500074999999999, LR Decay: 0.645, Regularization: 0.2, Validation accuracy: 0.9492\n",
      "iteration 0 / 2000: loss 2.302624\n",
      "iteration 100 / 2000: loss 0.455197\n",
      "iteration 200 / 2000: loss 0.306663\n",
      "iteration 300 / 2000: loss 0.287256\n",
      "iteration 400 / 2000: loss 0.345300\n",
      "iteration 500 / 2000: loss 0.277582\n",
      "iteration 600 / 2000: loss 0.334153\n",
      "iteration 700 / 2000: loss 0.435297\n",
      "iteration 800 / 2000: loss 0.266568\n",
      "iteration 900 / 2000: loss 0.227917\n",
      "iteration 1000 / 2000: loss 0.224771\n",
      "iteration 1100 / 2000: loss 0.285577\n",
      "iteration 1200 / 2000: loss 0.232563\n",
      "iteration 1300 / 2000: loss 0.229580\n",
      "iteration 1400 / 2000: loss 0.278389\n",
      "iteration 1500 / 2000: loss 0.258563\n",
      "iteration 1600 / 2000: loss 0.285320\n",
      "iteration 1700 / 2000: loss 0.265323\n",
      "iteration 1800 / 2000: loss 0.254546\n",
      "iteration 1900 / 2000: loss 0.251962\n",
      "Hidden Size: 30, Learning Rate: 0.002500074999999999, LR Decay: 0.645, Regularization: 0.30000000000000004, Validation accuracy: 0.943\n",
      "iteration 0 / 2000: loss 2.302631\n",
      "iteration 100 / 2000: loss 0.492606\n",
      "iteration 200 / 2000: loss 0.412271\n",
      "iteration 300 / 2000: loss 0.327522\n",
      "iteration 400 / 2000: loss 0.203234\n",
      "iteration 500 / 2000: loss 0.267228\n",
      "iteration 600 / 2000: loss 0.219945\n",
      "iteration 700 / 2000: loss 0.246867\n",
      "iteration 800 / 2000: loss 0.301233\n",
      "iteration 900 / 2000: loss 0.170249\n",
      "iteration 1000 / 2000: loss 0.272209\n",
      "iteration 1100 / 2000: loss 0.280253\n",
      "iteration 1200 / 2000: loss 0.242114\n",
      "iteration 1300 / 2000: loss 0.323227\n",
      "iteration 1400 / 2000: loss 0.230376\n",
      "iteration 1500 / 2000: loss 0.283613\n",
      "iteration 1600 / 2000: loss 0.283769\n",
      "iteration 1700 / 2000: loss 0.233170\n",
      "iteration 1800 / 2000: loss 0.262264\n",
      "iteration 1900 / 2000: loss 0.216496\n",
      "Hidden Size: 30, Learning Rate: 0.002500074999999999, LR Decay: 0.645, Regularization: 0.4, Validation accuracy: 0.9466\n",
      "iteration 0 / 2000: loss 2.302597\n",
      "iteration 100 / 2000: loss 0.645571\n",
      "iteration 200 / 2000: loss 0.368142\n",
      "iteration 300 / 2000: loss 0.328694\n",
      "iteration 400 / 2000: loss 0.333228\n",
      "iteration 500 / 2000: loss 0.338442\n",
      "iteration 600 / 2000: loss 0.350828\n",
      "iteration 700 / 2000: loss 0.299908\n",
      "iteration 800 / 2000: loss 0.277583\n",
      "iteration 900 / 2000: loss 0.283076\n",
      "iteration 1000 / 2000: loss 0.226831\n",
      "iteration 1100 / 2000: loss 0.174911\n",
      "iteration 1200 / 2000: loss 0.168094\n",
      "iteration 1300 / 2000: loss 0.306105\n",
      "iteration 1400 / 2000: loss 0.225195\n",
      "iteration 1500 / 2000: loss 0.307174\n",
      "iteration 1600 / 2000: loss 0.198092\n",
      "iteration 1700 / 2000: loss 0.281398\n",
      "iteration 1800 / 2000: loss 0.274729\n",
      "iteration 1900 / 2000: loss 0.250532\n",
      "Hidden Size: 30, Learning Rate: 0.002500074999999999, LR Decay: 0.47250000000000003, Regularization: 0.0, Validation accuracy: 0.9322\n",
      "iteration 0 / 2000: loss 2.302601\n",
      "iteration 100 / 2000: loss 0.870858\n",
      "iteration 200 / 2000: loss 0.381157\n",
      "iteration 300 / 2000: loss 0.291123\n",
      "iteration 400 / 2000: loss 0.286760\n",
      "iteration 500 / 2000: loss 0.270303\n",
      "iteration 600 / 2000: loss 0.270812\n",
      "iteration 700 / 2000: loss 0.220995\n",
      "iteration 800 / 2000: loss 0.226237\n",
      "iteration 900 / 2000: loss 0.274217\n",
      "iteration 1000 / 2000: loss 0.190930\n",
      "iteration 1100 / 2000: loss 0.248302\n",
      "iteration 1200 / 2000: loss 0.280513\n",
      "iteration 1300 / 2000: loss 0.299464\n",
      "iteration 1400 / 2000: loss 0.254894\n",
      "iteration 1500 / 2000: loss 0.256090\n",
      "iteration 1600 / 2000: loss 0.364026\n",
      "iteration 1700 / 2000: loss 0.206609\n",
      "iteration 1800 / 2000: loss 0.196323\n",
      "iteration 1900 / 2000: loss 0.264859\n",
      "Hidden Size: 30, Learning Rate: 0.002500074999999999, LR Decay: 0.47250000000000003, Regularization: 0.1, Validation accuracy: 0.9346\n",
      "iteration 0 / 2000: loss 2.302628\n",
      "iteration 100 / 2000: loss 0.743940\n",
      "iteration 200 / 2000: loss 0.380363\n",
      "iteration 300 / 2000: loss 0.356950\n",
      "iteration 400 / 2000: loss 0.268558\n",
      "iteration 500 / 2000: loss 0.319904\n",
      "iteration 600 / 2000: loss 0.320140\n",
      "iteration 700 / 2000: loss 0.272014\n",
      "iteration 800 / 2000: loss 0.224750\n",
      "iteration 900 / 2000: loss 0.356467\n",
      "iteration 1000 / 2000: loss 0.285669\n",
      "iteration 1100 / 2000: loss 0.381926\n",
      "iteration 1200 / 2000: loss 0.190543\n",
      "iteration 1300 / 2000: loss 0.296546\n",
      "iteration 1400 / 2000: loss 0.278075\n",
      "iteration 1500 / 2000: loss 0.233433\n",
      "iteration 1600 / 2000: loss 0.315309\n",
      "iteration 1700 / 2000: loss 0.274868\n",
      "iteration 1800 / 2000: loss 0.200313\n",
      "iteration 1900 / 2000: loss 0.320320\n",
      "Hidden Size: 30, Learning Rate: 0.002500074999999999, LR Decay: 0.47250000000000003, Regularization: 0.2, Validation accuracy: 0.9292\n",
      "iteration 0 / 2000: loss 2.302630\n",
      "iteration 100 / 2000: loss 0.729589\n",
      "iteration 200 / 2000: loss 0.410243\n",
      "iteration 300 / 2000: loss 0.303035\n",
      "iteration 400 / 2000: loss 0.334523\n",
      "iteration 500 / 2000: loss 0.349403\n",
      "iteration 600 / 2000: loss 0.469956\n",
      "iteration 700 / 2000: loss 0.305748\n",
      "iteration 800 / 2000: loss 0.314029\n",
      "iteration 900 / 2000: loss 0.290991\n",
      "iteration 1000 / 2000: loss 0.362893\n",
      "iteration 1100 / 2000: loss 0.207871\n",
      "iteration 1200 / 2000: loss 0.323914\n",
      "iteration 1300 / 2000: loss 0.275453\n",
      "iteration 1400 / 2000: loss 0.292086\n",
      "iteration 1500 / 2000: loss 0.367002\n",
      "iteration 1600 / 2000: loss 0.299115\n",
      "iteration 1700 / 2000: loss 0.285485\n",
      "iteration 1800 / 2000: loss 0.324607\n",
      "iteration 1900 / 2000: loss 0.369118\n",
      "Hidden Size: 30, Learning Rate: 0.002500074999999999, LR Decay: 0.47250000000000003, Regularization: 0.30000000000000004, Validation accuracy: 0.9312\n",
      "iteration 0 / 2000: loss 2.302625\n",
      "iteration 100 / 2000: loss 0.776855\n",
      "iteration 200 / 2000: loss 0.424672\n",
      "iteration 300 / 2000: loss 0.371691\n",
      "iteration 400 / 2000: loss 0.348161\n",
      "iteration 500 / 2000: loss 0.244857\n",
      "iteration 600 / 2000: loss 0.258731\n",
      "iteration 700 / 2000: loss 0.328025\n",
      "iteration 800 / 2000: loss 0.268032\n",
      "iteration 900 / 2000: loss 0.295058\n",
      "iteration 1000 / 2000: loss 0.316823\n",
      "iteration 1100 / 2000: loss 0.234943\n",
      "iteration 1200 / 2000: loss 0.385029\n",
      "iteration 1300 / 2000: loss 0.262456\n",
      "iteration 1400 / 2000: loss 0.290620\n",
      "iteration 1500 / 2000: loss 0.237886\n",
      "iteration 1600 / 2000: loss 0.401875\n",
      "iteration 1700 / 2000: loss 0.251758\n",
      "iteration 1800 / 2000: loss 0.307779\n",
      "iteration 1900 / 2000: loss 0.329445\n",
      "Hidden Size: 30, Learning Rate: 0.002500074999999999, LR Decay: 0.47250000000000003, Regularization: 0.4, Validation accuracy: 0.9306\n",
      "iteration 0 / 2000: loss 2.302580\n",
      "iteration 100 / 2000: loss 1.385892\n",
      "iteration 200 / 2000: loss 0.469125\n",
      "iteration 300 / 2000: loss 0.347580\n",
      "iteration 400 / 2000: loss 0.359097\n",
      "iteration 500 / 2000: loss 0.419751\n",
      "iteration 600 / 2000: loss 0.372043\n",
      "iteration 700 / 2000: loss 0.290839\n",
      "iteration 800 / 2000: loss 0.231464\n",
      "iteration 900 / 2000: loss 0.389468\n",
      "iteration 1000 / 2000: loss 0.342643\n",
      "iteration 1100 / 2000: loss 0.455805\n",
      "iteration 1200 / 2000: loss 0.339415\n",
      "iteration 1300 / 2000: loss 0.393464\n",
      "iteration 1400 / 2000: loss 0.307078\n",
      "iteration 1500 / 2000: loss 0.297586\n",
      "iteration 1600 / 2000: loss 0.302204\n",
      "iteration 1700 / 2000: loss 0.285706\n",
      "iteration 1800 / 2000: loss 0.500153\n",
      "iteration 1900 / 2000: loss 0.315886\n",
      "Hidden Size: 30, Learning Rate: 0.002500074999999999, LR Decay: 0.3, Regularization: 0.0, Validation accuracy: 0.913\n",
      "iteration 0 / 2000: loss 2.302595\n",
      "iteration 100 / 2000: loss 1.491996\n",
      "iteration 200 / 2000: loss 0.548158\n",
      "iteration 300 / 2000: loss 0.354549\n",
      "iteration 400 / 2000: loss 0.339604\n",
      "iteration 500 / 2000: loss 0.382705\n",
      "iteration 600 / 2000: loss 0.403114\n",
      "iteration 700 / 2000: loss 0.309489\n",
      "iteration 800 / 2000: loss 0.393116\n",
      "iteration 900 / 2000: loss 0.271468\n",
      "iteration 1000 / 2000: loss 0.383286\n",
      "iteration 1100 / 2000: loss 0.304733\n",
      "iteration 1200 / 2000: loss 0.328715\n",
      "iteration 1300 / 2000: loss 0.240193\n",
      "iteration 1400 / 2000: loss 0.349185\n",
      "iteration 1500 / 2000: loss 0.409442\n",
      "iteration 1600 / 2000: loss 0.351030\n",
      "iteration 1700 / 2000: loss 0.464189\n",
      "iteration 1800 / 2000: loss 0.256582\n",
      "iteration 1900 / 2000: loss 0.325685\n",
      "Hidden Size: 30, Learning Rate: 0.002500074999999999, LR Decay: 0.3, Regularization: 0.1, Validation accuracy: 0.9114\n",
      "iteration 0 / 2000: loss 2.302607\n",
      "iteration 100 / 2000: loss 1.436565\n",
      "iteration 200 / 2000: loss 0.522050\n",
      "iteration 300 / 2000: loss 0.411491\n",
      "iteration 400 / 2000: loss 0.339242\n",
      "iteration 500 / 2000: loss 0.438955\n",
      "iteration 600 / 2000: loss 0.322231\n",
      "iteration 700 / 2000: loss 0.413214\n",
      "iteration 800 / 2000: loss 0.351175\n",
      "iteration 900 / 2000: loss 0.372230\n",
      "iteration 1000 / 2000: loss 0.308260\n",
      "iteration 1100 / 2000: loss 0.329126\n",
      "iteration 1200 / 2000: loss 0.330855\n",
      "iteration 1300 / 2000: loss 0.333068\n",
      "iteration 1400 / 2000: loss 0.301867\n",
      "iteration 1500 / 2000: loss 0.365336\n",
      "iteration 1600 / 2000: loss 0.405032\n",
      "iteration 1700 / 2000: loss 0.457565\n",
      "iteration 1800 / 2000: loss 0.326665\n",
      "iteration 1900 / 2000: loss 0.296593\n",
      "Hidden Size: 30, Learning Rate: 0.002500074999999999, LR Decay: 0.3, Regularization: 0.2, Validation accuracy: 0.9112\n",
      "iteration 0 / 2000: loss 2.302612\n",
      "iteration 100 / 2000: loss 1.521380\n",
      "iteration 200 / 2000: loss 0.543116\n",
      "iteration 300 / 2000: loss 0.479926\n",
      "iteration 400 / 2000: loss 0.421031\n",
      "iteration 500 / 2000: loss 0.363189\n",
      "iteration 600 / 2000: loss 0.354106\n",
      "iteration 700 / 2000: loss 0.381423\n",
      "iteration 800 / 2000: loss 0.399834\n",
      "iteration 900 / 2000: loss 0.453973\n",
      "iteration 1000 / 2000: loss 0.430047\n",
      "iteration 1100 / 2000: loss 0.387174\n",
      "iteration 1200 / 2000: loss 0.422391\n",
      "iteration 1300 / 2000: loss 0.340466\n",
      "iteration 1400 / 2000: loss 0.284848\n",
      "iteration 1500 / 2000: loss 0.387913\n",
      "iteration 1600 / 2000: loss 0.332766\n",
      "iteration 1700 / 2000: loss 0.368163\n",
      "iteration 1800 / 2000: loss 0.365258\n",
      "iteration 1900 / 2000: loss 0.367746\n",
      "Hidden Size: 30, Learning Rate: 0.002500074999999999, LR Decay: 0.3, Regularization: 0.30000000000000004, Validation accuracy: 0.911\n",
      "iteration 0 / 2000: loss 2.302624\n",
      "iteration 100 / 2000: loss 1.251585\n",
      "iteration 200 / 2000: loss 0.446968\n",
      "iteration 300 / 2000: loss 0.460819\n",
      "iteration 400 / 2000: loss 0.429506\n",
      "iteration 500 / 2000: loss 0.317290\n",
      "iteration 600 / 2000: loss 0.368211\n",
      "iteration 700 / 2000: loss 0.380619\n",
      "iteration 800 / 2000: loss 0.323825\n",
      "iteration 900 / 2000: loss 0.403436\n",
      "iteration 1000 / 2000: loss 0.373592\n",
      "iteration 1100 / 2000: loss 0.360974\n",
      "iteration 1200 / 2000: loss 0.330956\n",
      "iteration 1300 / 2000: loss 0.349367\n",
      "iteration 1400 / 2000: loss 0.344108\n",
      "iteration 1500 / 2000: loss 0.408894\n",
      "iteration 1600 / 2000: loss 0.524807\n",
      "iteration 1700 / 2000: loss 0.290904\n",
      "iteration 1800 / 2000: loss 0.359516\n",
      "iteration 1900 / 2000: loss 0.413405\n",
      "Hidden Size: 30, Learning Rate: 0.002500074999999999, LR Decay: 0.3, Regularization: 0.4, Validation accuracy: 0.9094\n",
      "iteration 0 / 2000: loss 2.302580\n",
      "iteration 100 / 2000: loss 2.302580\n",
      "iteration 200 / 2000: loss 2.302580\n",
      "iteration 300 / 2000: loss 2.302575\n",
      "iteration 400 / 2000: loss 2.302573\n",
      "iteration 500 / 2000: loss 2.302583\n",
      "iteration 600 / 2000: loss 2.302580\n",
      "iteration 700 / 2000: loss 2.302572\n",
      "iteration 800 / 2000: loss 2.302573\n",
      "iteration 900 / 2000: loss 2.302586\n",
      "iteration 1000 / 2000: loss 2.302590\n",
      "iteration 1100 / 2000: loss 2.302579\n",
      "iteration 1200 / 2000: loss 2.302579\n",
      "iteration 1300 / 2000: loss 2.302574\n",
      "iteration 1400 / 2000: loss 2.302578\n",
      "iteration 1500 / 2000: loss 2.302570\n",
      "iteration 1600 / 2000: loss 2.302578\n",
      "iteration 1700 / 2000: loss 2.302575\n",
      "iteration 1800 / 2000: loss 2.302582\n",
      "iteration 1900 / 2000: loss 2.302586\n",
      "Hidden Size: 30, Learning Rate: 1e-07, LR Decay: 0.99, Regularization: 0.0, Validation accuracy: 0.1368\n",
      "iteration 0 / 2000: loss 2.302611\n",
      "iteration 100 / 2000: loss 2.302615\n",
      "iteration 200 / 2000: loss 2.302607\n",
      "iteration 300 / 2000: loss 2.302609\n",
      "iteration 400 / 2000: loss 2.302609\n",
      "iteration 500 / 2000: loss 2.302618\n",
      "iteration 600 / 2000: loss 2.302608\n",
      "iteration 700 / 2000: loss 2.302608\n",
      "iteration 800 / 2000: loss 2.302606\n",
      "iteration 900 / 2000: loss 2.302609\n",
      "iteration 1000 / 2000: loss 2.302606\n",
      "iteration 1100 / 2000: loss 2.302610\n",
      "iteration 1200 / 2000: loss 2.302609\n",
      "iteration 1300 / 2000: loss 2.302620\n",
      "iteration 1400 / 2000: loss 2.302607\n",
      "iteration 1500 / 2000: loss 2.302614\n",
      "iteration 1600 / 2000: loss 2.302606\n",
      "iteration 1700 / 2000: loss 2.302611\n",
      "iteration 1800 / 2000: loss 2.302608\n",
      "iteration 1900 / 2000: loss 2.302604\n",
      "Hidden Size: 30, Learning Rate: 1e-07, LR Decay: 0.99, Regularization: 0.1, Validation accuracy: 0.0634\n",
      "iteration 0 / 2000: loss 2.302614\n",
      "iteration 100 / 2000: loss 2.302617\n",
      "iteration 200 / 2000: loss 2.302618\n",
      "iteration 300 / 2000: loss 2.302616\n",
      "iteration 400 / 2000: loss 2.302619\n",
      "iteration 500 / 2000: loss 2.302610\n",
      "iteration 600 / 2000: loss 2.302611\n",
      "iteration 700 / 2000: loss 2.302611\n",
      "iteration 800 / 2000: loss 2.302616\n",
      "iteration 900 / 2000: loss 2.302618\n",
      "iteration 1000 / 2000: loss 2.302616\n",
      "iteration 1100 / 2000: loss 2.302620\n",
      "iteration 1200 / 2000: loss 2.302616\n",
      "iteration 1300 / 2000: loss 2.302618\n",
      "iteration 1400 / 2000: loss 2.302609\n",
      "iteration 1500 / 2000: loss 2.302623\n",
      "iteration 1600 / 2000: loss 2.302618\n",
      "iteration 1700 / 2000: loss 2.302622\n",
      "iteration 1800 / 2000: loss 2.302614\n",
      "iteration 1900 / 2000: loss 2.302618\n",
      "Hidden Size: 30, Learning Rate: 1e-07, LR Decay: 0.99, Regularization: 0.2, Validation accuracy: 0.0664\n",
      "iteration 0 / 2000: loss 2.302619\n",
      "iteration 100 / 2000: loss 2.302613\n",
      "iteration 200 / 2000: loss 2.302623\n",
      "iteration 300 / 2000: loss 2.302616\n",
      "iteration 400 / 2000: loss 2.302618\n",
      "iteration 500 / 2000: loss 2.302608\n",
      "iteration 600 / 2000: loss 2.302616\n",
      "iteration 700 / 2000: loss 2.302611\n",
      "iteration 800 / 2000: loss 2.302618\n",
      "iteration 900 / 2000: loss 2.302617\n",
      "iteration 1000 / 2000: loss 2.302613\n",
      "iteration 1100 / 2000: loss 2.302620\n",
      "iteration 1200 / 2000: loss 2.302620\n",
      "iteration 1300 / 2000: loss 2.302615\n",
      "iteration 1400 / 2000: loss 2.302616\n",
      "iteration 1500 / 2000: loss 2.302613\n",
      "iteration 1600 / 2000: loss 2.302613\n",
      "iteration 1700 / 2000: loss 2.302619\n",
      "iteration 1800 / 2000: loss 2.302615\n",
      "iteration 1900 / 2000: loss 2.302611\n",
      "Hidden Size: 30, Learning Rate: 1e-07, LR Decay: 0.99, Regularization: 0.30000000000000004, Validation accuracy: 0.1142\n",
      "iteration 0 / 2000: loss 2.302636\n",
      "iteration 100 / 2000: loss 2.302636\n",
      "iteration 200 / 2000: loss 2.302639\n",
      "iteration 300 / 2000: loss 2.302642\n",
      "iteration 400 / 2000: loss 2.302638\n",
      "iteration 500 / 2000: loss 2.302634\n",
      "iteration 600 / 2000: loss 2.302633\n",
      "iteration 700 / 2000: loss 2.302631\n",
      "iteration 800 / 2000: loss 2.302636\n",
      "iteration 900 / 2000: loss 2.302640\n",
      "iteration 1000 / 2000: loss 2.302630\n",
      "iteration 1100 / 2000: loss 2.302636\n",
      "iteration 1200 / 2000: loss 2.302629\n",
      "iteration 1300 / 2000: loss 2.302636\n",
      "iteration 1400 / 2000: loss 2.302635\n",
      "iteration 1500 / 2000: loss 2.302635\n",
      "iteration 1600 / 2000: loss 2.302638\n",
      "iteration 1700 / 2000: loss 2.302635\n",
      "iteration 1800 / 2000: loss 2.302632\n",
      "iteration 1900 / 2000: loss 2.302637\n",
      "Hidden Size: 30, Learning Rate: 1e-07, LR Decay: 0.99, Regularization: 0.4, Validation accuracy: 0.0758\n",
      "iteration 0 / 2000: loss 2.302580\n",
      "iteration 100 / 2000: loss 2.302565\n",
      "iteration 200 / 2000: loss 2.302574\n",
      "iteration 300 / 2000: loss 2.302585\n",
      "iteration 400 / 2000: loss 2.302568\n",
      "iteration 500 / 2000: loss 2.302575\n",
      "iteration 600 / 2000: loss 2.302564\n",
      "iteration 700 / 2000: loss 2.302578\n",
      "iteration 800 / 2000: loss 2.302572\n",
      "iteration 900 / 2000: loss 2.302568\n",
      "iteration 1000 / 2000: loss 2.302572\n",
      "iteration 1100 / 2000: loss 2.302570\n",
      "iteration 1200 / 2000: loss 2.302571\n",
      "iteration 1300 / 2000: loss 2.302569\n",
      "iteration 1400 / 2000: loss 2.302570\n",
      "iteration 1500 / 2000: loss 2.302575\n",
      "iteration 1600 / 2000: loss 2.302577\n",
      "iteration 1700 / 2000: loss 2.302574\n",
      "iteration 1800 / 2000: loss 2.302571\n",
      "iteration 1900 / 2000: loss 2.302573\n",
      "Hidden Size: 30, Learning Rate: 1e-07, LR Decay: 0.8175, Regularization: 0.0, Validation accuracy: 0.113\n",
      "iteration 0 / 2000: loss 2.302582\n",
      "iteration 100 / 2000: loss 2.302582\n",
      "iteration 200 / 2000: loss 2.302570\n",
      "iteration 300 / 2000: loss 2.302584\n",
      "iteration 400 / 2000: loss 2.302574\n",
      "iteration 500 / 2000: loss 2.302591\n",
      "iteration 600 / 2000: loss 2.302582\n",
      "iteration 700 / 2000: loss 2.302572\n",
      "iteration 800 / 2000: loss 2.302573\n",
      "iteration 900 / 2000: loss 2.302562\n",
      "iteration 1000 / 2000: loss 2.302580\n",
      "iteration 1100 / 2000: loss 2.302579\n",
      "iteration 1200 / 2000: loss 2.302585\n",
      "iteration 1300 / 2000: loss 2.302581\n",
      "iteration 1400 / 2000: loss 2.302580\n",
      "iteration 1500 / 2000: loss 2.302579\n",
      "iteration 1600 / 2000: loss 2.302574\n",
      "iteration 1700 / 2000: loss 2.302571\n",
      "iteration 1800 / 2000: loss 2.302585\n",
      "iteration 1900 / 2000: loss 2.302582\n",
      "Hidden Size: 30, Learning Rate: 1e-07, LR Decay: 0.8175, Regularization: 0.1, Validation accuracy: 0.138\n",
      "iteration 0 / 2000: loss 2.302609\n",
      "iteration 100 / 2000: loss 2.302610\n",
      "iteration 200 / 2000: loss 2.302612\n",
      "iteration 300 / 2000: loss 2.302616\n",
      "iteration 400 / 2000: loss 2.302604\n",
      "iteration 500 / 2000: loss 2.302616\n",
      "iteration 600 / 2000: loss 2.302599\n",
      "iteration 700 / 2000: loss 2.302595\n",
      "iteration 800 / 2000: loss 2.302615\n",
      "iteration 900 / 2000: loss 2.302604\n",
      "iteration 1000 / 2000: loss 2.302610\n",
      "iteration 1100 / 2000: loss 2.302604\n",
      "iteration 1200 / 2000: loss 2.302621\n",
      "iteration 1300 / 2000: loss 2.302610\n",
      "iteration 1400 / 2000: loss 2.302603\n",
      "iteration 1500 / 2000: loss 2.302606\n",
      "iteration 1600 / 2000: loss 2.302610\n",
      "iteration 1700 / 2000: loss 2.302606\n",
      "iteration 1800 / 2000: loss 2.302614\n",
      "iteration 1900 / 2000: loss 2.302608\n",
      "Hidden Size: 30, Learning Rate: 1e-07, LR Decay: 0.8175, Regularization: 0.2, Validation accuracy: 0.1122\n",
      "iteration 0 / 2000: loss 2.302631\n",
      "iteration 100 / 2000: loss 2.302604\n",
      "iteration 200 / 2000: loss 2.302607\n",
      "iteration 300 / 2000: loss 2.302603\n",
      "iteration 400 / 2000: loss 2.302608\n",
      "iteration 500 / 2000: loss 2.302613\n",
      "iteration 600 / 2000: loss 2.302621\n",
      "iteration 700 / 2000: loss 2.302605\n",
      "iteration 800 / 2000: loss 2.302600\n",
      "iteration 900 / 2000: loss 2.302614\n",
      "iteration 1000 / 2000: loss 2.302619\n",
      "iteration 1100 / 2000: loss 2.302616\n",
      "iteration 1200 / 2000: loss 2.302623\n",
      "iteration 1300 / 2000: loss 2.302623\n",
      "iteration 1400 / 2000: loss 2.302610\n",
      "iteration 1500 / 2000: loss 2.302609\n",
      "iteration 1600 / 2000: loss 2.302617\n",
      "iteration 1700 / 2000: loss 2.302616\n",
      "iteration 1800 / 2000: loss 2.302604\n",
      "iteration 1900 / 2000: loss 2.302614\n",
      "Hidden Size: 30, Learning Rate: 1e-07, LR Decay: 0.8175, Regularization: 0.30000000000000004, Validation accuracy: 0.1302\n",
      "iteration 0 / 2000: loss 2.302634\n",
      "iteration 100 / 2000: loss 2.302632\n",
      "iteration 200 / 2000: loss 2.302636\n",
      "iteration 300 / 2000: loss 2.302630\n",
      "iteration 400 / 2000: loss 2.302629\n",
      "iteration 500 / 2000: loss 2.302637\n",
      "iteration 600 / 2000: loss 2.302642\n",
      "iteration 700 / 2000: loss 2.302631\n",
      "iteration 800 / 2000: loss 2.302635\n",
      "iteration 900 / 2000: loss 2.302646\n",
      "iteration 1000 / 2000: loss 2.302638\n",
      "iteration 1100 / 2000: loss 2.302627\n",
      "iteration 1200 / 2000: loss 2.302634\n",
      "iteration 1300 / 2000: loss 2.302635\n",
      "iteration 1400 / 2000: loss 2.302634\n",
      "iteration 1500 / 2000: loss 2.302636\n",
      "iteration 1600 / 2000: loss 2.302631\n",
      "iteration 1700 / 2000: loss 2.302644\n",
      "iteration 1800 / 2000: loss 2.302640\n",
      "iteration 1900 / 2000: loss 2.302629\n",
      "Hidden Size: 30, Learning Rate: 1e-07, LR Decay: 0.8175, Regularization: 0.4, Validation accuracy: 0.0952\n",
      "iteration 0 / 2000: loss 2.302584\n",
      "iteration 100 / 2000: loss 2.302591\n",
      "iteration 200 / 2000: loss 2.302578\n",
      "iteration 300 / 2000: loss 2.302584\n",
      "iteration 400 / 2000: loss 2.302592\n",
      "iteration 500 / 2000: loss 2.302584\n",
      "iteration 600 / 2000: loss 2.302590\n",
      "iteration 700 / 2000: loss 2.302581\n",
      "iteration 800 / 2000: loss 2.302581\n",
      "iteration 900 / 2000: loss 2.302582\n",
      "iteration 1000 / 2000: loss 2.302583\n",
      "iteration 1100 / 2000: loss 2.302584\n",
      "iteration 1200 / 2000: loss 2.302590\n",
      "iteration 1300 / 2000: loss 2.302588\n",
      "iteration 1400 / 2000: loss 2.302577\n",
      "iteration 1500 / 2000: loss 2.302582\n",
      "iteration 1600 / 2000: loss 2.302575\n",
      "iteration 1700 / 2000: loss 2.302577\n",
      "iteration 1800 / 2000: loss 2.302581\n",
      "iteration 1900 / 2000: loss 2.302585\n",
      "Hidden Size: 30, Learning Rate: 1e-07, LR Decay: 0.645, Regularization: 0.0, Validation accuracy: 0.1018\n",
      "iteration 0 / 2000: loss 2.302597\n",
      "iteration 100 / 2000: loss 2.302594\n",
      "iteration 200 / 2000: loss 2.302603\n",
      "iteration 300 / 2000: loss 2.302598\n",
      "iteration 400 / 2000: loss 2.302599\n",
      "iteration 500 / 2000: loss 2.302600\n",
      "iteration 600 / 2000: loss 2.302601\n",
      "iteration 700 / 2000: loss 2.302604\n",
      "iteration 800 / 2000: loss 2.302602\n",
      "iteration 900 / 2000: loss 2.302594\n",
      "iteration 1000 / 2000: loss 2.302598\n",
      "iteration 1100 / 2000: loss 2.302606\n",
      "iteration 1200 / 2000: loss 2.302597\n",
      "iteration 1300 / 2000: loss 2.302585\n",
      "iteration 1400 / 2000: loss 2.302600\n",
      "iteration 1500 / 2000: loss 2.302598\n",
      "iteration 1600 / 2000: loss 2.302591\n",
      "iteration 1700 / 2000: loss 2.302594\n",
      "iteration 1800 / 2000: loss 2.302598\n",
      "iteration 1900 / 2000: loss 2.302598\n",
      "Hidden Size: 30, Learning Rate: 1e-07, LR Decay: 0.645, Regularization: 0.1, Validation accuracy: 0.1006\n",
      "iteration 0 / 2000: loss 2.302609\n",
      "iteration 100 / 2000: loss 2.302607\n",
      "iteration 200 / 2000: loss 2.302603\n",
      "iteration 300 / 2000: loss 2.302611\n",
      "iteration 400 / 2000: loss 2.302606\n",
      "iteration 500 / 2000: loss 2.302603\n",
      "iteration 600 / 2000: loss 2.302610\n",
      "iteration 700 / 2000: loss 2.302605\n",
      "iteration 800 / 2000: loss 2.302606\n",
      "iteration 900 / 2000: loss 2.302618\n",
      "iteration 1000 / 2000: loss 2.302610\n",
      "iteration 1100 / 2000: loss 2.302605\n",
      "iteration 1200 / 2000: loss 2.302613\n",
      "iteration 1300 / 2000: loss 2.302604\n",
      "iteration 1400 / 2000: loss 2.302607\n",
      "iteration 1500 / 2000: loss 2.302610\n",
      "iteration 1600 / 2000: loss 2.302605\n",
      "iteration 1700 / 2000: loss 2.302608\n",
      "iteration 1800 / 2000: loss 2.302613\n",
      "iteration 1900 / 2000: loss 2.302618\n",
      "Hidden Size: 30, Learning Rate: 1e-07, LR Decay: 0.645, Regularization: 0.2, Validation accuracy: 0.1\n",
      "iteration 0 / 2000: loss 2.302611\n",
      "iteration 100 / 2000: loss 2.302607\n",
      "iteration 200 / 2000: loss 2.302611\n",
      "iteration 300 / 2000: loss 2.302609\n",
      "iteration 400 / 2000: loss 2.302602\n",
      "iteration 500 / 2000: loss 2.302607\n",
      "iteration 600 / 2000: loss 2.302609\n",
      "iteration 700 / 2000: loss 2.302610\n",
      "iteration 800 / 2000: loss 2.302601\n",
      "iteration 900 / 2000: loss 2.302613\n",
      "iteration 1000 / 2000: loss 2.302606\n",
      "iteration 1100 / 2000: loss 2.302610\n",
      "iteration 1200 / 2000: loss 2.302609\n",
      "iteration 1300 / 2000: loss 2.302613\n",
      "iteration 1400 / 2000: loss 2.302606\n",
      "iteration 1500 / 2000: loss 2.302601\n",
      "iteration 1600 / 2000: loss 2.302604\n",
      "iteration 1700 / 2000: loss 2.302606\n",
      "iteration 1800 / 2000: loss 2.302611\n",
      "iteration 1900 / 2000: loss 2.302610\n",
      "Hidden Size: 30, Learning Rate: 1e-07, LR Decay: 0.645, Regularization: 0.30000000000000004, Validation accuracy: 0.1272\n",
      "iteration 0 / 2000: loss 2.302651\n",
      "iteration 100 / 2000: loss 2.302641\n",
      "iteration 200 / 2000: loss 2.302644\n",
      "iteration 300 / 2000: loss 2.302640\n",
      "iteration 400 / 2000: loss 2.302635\n",
      "iteration 500 / 2000: loss 2.302644\n",
      "iteration 600 / 2000: loss 2.302636\n",
      "iteration 700 / 2000: loss 2.302643\n",
      "iteration 800 / 2000: loss 2.302651\n",
      "iteration 900 / 2000: loss 2.302641\n",
      "iteration 1000 / 2000: loss 2.302641\n",
      "iteration 1100 / 2000: loss 2.302644\n",
      "iteration 1200 / 2000: loss 2.302642\n",
      "iteration 1300 / 2000: loss 2.302641\n",
      "iteration 1400 / 2000: loss 2.302630\n",
      "iteration 1500 / 2000: loss 2.302639\n",
      "iteration 1600 / 2000: loss 2.302649\n",
      "iteration 1700 / 2000: loss 2.302644\n",
      "iteration 1800 / 2000: loss 2.302643\n",
      "iteration 1900 / 2000: loss 2.302643\n",
      "Hidden Size: 30, Learning Rate: 1e-07, LR Decay: 0.645, Regularization: 0.4, Validation accuracy: 0.103\n",
      "iteration 0 / 2000: loss 2.302577\n",
      "iteration 100 / 2000: loss 2.302579\n",
      "iteration 200 / 2000: loss 2.302584\n",
      "iteration 300 / 2000: loss 2.302584\n",
      "iteration 400 / 2000: loss 2.302592\n",
      "iteration 500 / 2000: loss 2.302577\n",
      "iteration 600 / 2000: loss 2.302585\n",
      "iteration 700 / 2000: loss 2.302569\n",
      "iteration 800 / 2000: loss 2.302585\n",
      "iteration 900 / 2000: loss 2.302578\n",
      "iteration 1000 / 2000: loss 2.302573\n",
      "iteration 1100 / 2000: loss 2.302593\n",
      "iteration 1200 / 2000: loss 2.302586\n",
      "iteration 1300 / 2000: loss 2.302579\n",
      "iteration 1400 / 2000: loss 2.302586\n",
      "iteration 1500 / 2000: loss 2.302577\n",
      "iteration 1600 / 2000: loss 2.302578\n",
      "iteration 1700 / 2000: loss 2.302599\n",
      "iteration 1800 / 2000: loss 2.302584\n",
      "iteration 1900 / 2000: loss 2.302591\n",
      "Hidden Size: 30, Learning Rate: 1e-07, LR Decay: 0.47250000000000003, Regularization: 0.0, Validation accuracy: 0.1052\n",
      "iteration 0 / 2000: loss 2.302615\n",
      "iteration 100 / 2000: loss 2.302601\n",
      "iteration 200 / 2000: loss 2.302598\n",
      "iteration 300 / 2000: loss 2.302604\n",
      "iteration 400 / 2000: loss 2.302613\n",
      "iteration 500 / 2000: loss 2.302596\n",
      "iteration 600 / 2000: loss 2.302612\n",
      "iteration 700 / 2000: loss 2.302607\n",
      "iteration 800 / 2000: loss 2.302605\n",
      "iteration 900 / 2000: loss 2.302606\n",
      "iteration 1000 / 2000: loss 2.302605\n",
      "iteration 1100 / 2000: loss 2.302618\n",
      "iteration 1200 / 2000: loss 2.302596\n",
      "iteration 1300 / 2000: loss 2.302615\n",
      "iteration 1400 / 2000: loss 2.302603\n",
      "iteration 1500 / 2000: loss 2.302615\n",
      "iteration 1600 / 2000: loss 2.302601\n",
      "iteration 1700 / 2000: loss 2.302609\n",
      "iteration 1800 / 2000: loss 2.302610\n",
      "iteration 1900 / 2000: loss 2.302609\n",
      "Hidden Size: 30, Learning Rate: 1e-07, LR Decay: 0.47250000000000003, Regularization: 0.1, Validation accuracy: 0.1276\n",
      "iteration 0 / 2000: loss 2.302612\n",
      "iteration 100 / 2000: loss 2.302613\n",
      "iteration 200 / 2000: loss 2.302614\n",
      "iteration 300 / 2000: loss 2.302615\n",
      "iteration 400 / 2000: loss 2.302606\n",
      "iteration 500 / 2000: loss 2.302620\n",
      "iteration 600 / 2000: loss 2.302627\n",
      "iteration 700 / 2000: loss 2.302613\n",
      "iteration 800 / 2000: loss 2.302618\n",
      "iteration 900 / 2000: loss 2.302604\n",
      "iteration 1000 / 2000: loss 2.302626\n",
      "iteration 1100 / 2000: loss 2.302613\n",
      "iteration 1200 / 2000: loss 2.302626\n",
      "iteration 1300 / 2000: loss 2.302620\n",
      "iteration 1400 / 2000: loss 2.302622\n",
      "iteration 1500 / 2000: loss 2.302606\n",
      "iteration 1600 / 2000: loss 2.302613\n",
      "iteration 1700 / 2000: loss 2.302607\n",
      "iteration 1800 / 2000: loss 2.302613\n",
      "iteration 1900 / 2000: loss 2.302619\n",
      "Hidden Size: 30, Learning Rate: 1e-07, LR Decay: 0.47250000000000003, Regularization: 0.2, Validation accuracy: 0.0908\n",
      "iteration 0 / 2000: loss 2.302625\n",
      "iteration 100 / 2000: loss 2.302628\n",
      "iteration 200 / 2000: loss 2.302625\n",
      "iteration 300 / 2000: loss 2.302618\n",
      "iteration 400 / 2000: loss 2.302617\n",
      "iteration 500 / 2000: loss 2.302626\n",
      "iteration 600 / 2000: loss 2.302614\n",
      "iteration 700 / 2000: loss 2.302621\n",
      "iteration 800 / 2000: loss 2.302619\n",
      "iteration 900 / 2000: loss 2.302617\n",
      "iteration 1000 / 2000: loss 2.302617\n",
      "iteration 1100 / 2000: loss 2.302622\n",
      "iteration 1200 / 2000: loss 2.302621\n",
      "iteration 1300 / 2000: loss 2.302621\n",
      "iteration 1400 / 2000: loss 2.302625\n",
      "iteration 1500 / 2000: loss 2.302619\n",
      "iteration 1600 / 2000: loss 2.302618\n",
      "iteration 1700 / 2000: loss 2.302620\n",
      "iteration 1800 / 2000: loss 2.302623\n",
      "iteration 1900 / 2000: loss 2.302617\n",
      "Hidden Size: 30, Learning Rate: 1e-07, LR Decay: 0.47250000000000003, Regularization: 0.30000000000000004, Validation accuracy: 0.0896\n",
      "iteration 0 / 2000: loss 2.302631\n",
      "iteration 100 / 2000: loss 2.302620\n",
      "iteration 200 / 2000: loss 2.302629\n",
      "iteration 300 / 2000: loss 2.302627\n",
      "iteration 400 / 2000: loss 2.302629\n",
      "iteration 500 / 2000: loss 2.302631\n",
      "iteration 600 / 2000: loss 2.302638\n",
      "iteration 700 / 2000: loss 2.302629\n",
      "iteration 800 / 2000: loss 2.302631\n",
      "iteration 900 / 2000: loss 2.302624\n",
      "iteration 1000 / 2000: loss 2.302626\n",
      "iteration 1100 / 2000: loss 2.302635\n",
      "iteration 1200 / 2000: loss 2.302634\n",
      "iteration 1300 / 2000: loss 2.302623\n",
      "iteration 1400 / 2000: loss 2.302636\n",
      "iteration 1500 / 2000: loss 2.302636\n",
      "iteration 1600 / 2000: loss 2.302631\n",
      "iteration 1700 / 2000: loss 2.302632\n",
      "iteration 1800 / 2000: loss 2.302627\n",
      "iteration 1900 / 2000: loss 2.302631\n",
      "Hidden Size: 30, Learning Rate: 1e-07, LR Decay: 0.47250000000000003, Regularization: 0.4, Validation accuracy: 0.0922\n",
      "iteration 0 / 2000: loss 2.302576\n",
      "iteration 100 / 2000: loss 2.302582\n",
      "iteration 200 / 2000: loss 2.302583\n",
      "iteration 300 / 2000: loss 2.302582\n",
      "iteration 400 / 2000: loss 2.302582\n",
      "iteration 500 / 2000: loss 2.302586\n",
      "iteration 600 / 2000: loss 2.302571\n",
      "iteration 700 / 2000: loss 2.302581\n",
      "iteration 800 / 2000: loss 2.302577\n",
      "iteration 900 / 2000: loss 2.302585\n",
      "iteration 1000 / 2000: loss 2.302573\n",
      "iteration 1100 / 2000: loss 2.302571\n",
      "iteration 1200 / 2000: loss 2.302578\n",
      "iteration 1300 / 2000: loss 2.302578\n",
      "iteration 1400 / 2000: loss 2.302578\n",
      "iteration 1500 / 2000: loss 2.302575\n",
      "iteration 1600 / 2000: loss 2.302564\n",
      "iteration 1700 / 2000: loss 2.302575\n",
      "iteration 1800 / 2000: loss 2.302572\n",
      "iteration 1900 / 2000: loss 2.302579\n",
      "Hidden Size: 30, Learning Rate: 1e-07, LR Decay: 0.3, Regularization: 0.0, Validation accuracy: 0.1022\n",
      "iteration 0 / 2000: loss 2.302578\n",
      "iteration 100 / 2000: loss 2.302583\n",
      "iteration 200 / 2000: loss 2.302582\n",
      "iteration 300 / 2000: loss 2.302590\n",
      "iteration 400 / 2000: loss 2.302584\n",
      "iteration 500 / 2000: loss 2.302581\n",
      "iteration 600 / 2000: loss 2.302584\n",
      "iteration 700 / 2000: loss 2.302584\n",
      "iteration 800 / 2000: loss 2.302588\n",
      "iteration 900 / 2000: loss 2.302587\n",
      "iteration 1000 / 2000: loss 2.302587\n",
      "iteration 1100 / 2000: loss 2.302582\n",
      "iteration 1200 / 2000: loss 2.302579\n",
      "iteration 1300 / 2000: loss 2.302590\n",
      "iteration 1400 / 2000: loss 2.302584\n",
      "iteration 1500 / 2000: loss 2.302578\n",
      "iteration 1600 / 2000: loss 2.302582\n",
      "iteration 1700 / 2000: loss 2.302586\n",
      "iteration 1800 / 2000: loss 2.302587\n",
      "iteration 1900 / 2000: loss 2.302584\n",
      "Hidden Size: 30, Learning Rate: 1e-07, LR Decay: 0.3, Regularization: 0.1, Validation accuracy: 0.1346\n",
      "iteration 0 / 2000: loss 2.302602\n",
      "iteration 100 / 2000: loss 2.302609\n",
      "iteration 200 / 2000: loss 2.302611\n",
      "iteration 300 / 2000: loss 2.302597\n",
      "iteration 400 / 2000: loss 2.302610\n",
      "iteration 500 / 2000: loss 2.302606\n",
      "iteration 600 / 2000: loss 2.302605\n",
      "iteration 700 / 2000: loss 2.302609\n",
      "iteration 800 / 2000: loss 2.302598\n",
      "iteration 900 / 2000: loss 2.302602\n",
      "iteration 1000 / 2000: loss 2.302591\n",
      "iteration 1100 / 2000: loss 2.302596\n",
      "iteration 1200 / 2000: loss 2.302612\n",
      "iteration 1300 / 2000: loss 2.302611\n",
      "iteration 1400 / 2000: loss 2.302608\n",
      "iteration 1500 / 2000: loss 2.302602\n",
      "iteration 1600 / 2000: loss 2.302608\n",
      "iteration 1700 / 2000: loss 2.302606\n",
      "iteration 1800 / 2000: loss 2.302614\n",
      "iteration 1900 / 2000: loss 2.302610\n",
      "Hidden Size: 30, Learning Rate: 1e-07, LR Decay: 0.3, Regularization: 0.2, Validation accuracy: 0.093\n",
      "iteration 0 / 2000: loss 2.302631\n",
      "iteration 100 / 2000: loss 2.302625\n",
      "iteration 200 / 2000: loss 2.302628\n",
      "iteration 300 / 2000: loss 2.302629\n",
      "iteration 400 / 2000: loss 2.302627\n",
      "iteration 500 / 2000: loss 2.302615\n",
      "iteration 600 / 2000: loss 2.302630\n",
      "iteration 700 / 2000: loss 2.302620\n",
      "iteration 800 / 2000: loss 2.302626\n",
      "iteration 900 / 2000: loss 2.302632\n",
      "iteration 1000 / 2000: loss 2.302628\n",
      "iteration 1100 / 2000: loss 2.302626\n",
      "iteration 1200 / 2000: loss 2.302615\n",
      "iteration 1300 / 2000: loss 2.302628\n",
      "iteration 1400 / 2000: loss 2.302626\n",
      "iteration 1500 / 2000: loss 2.302618\n",
      "iteration 1600 / 2000: loss 2.302619\n",
      "iteration 1700 / 2000: loss 2.302620\n",
      "iteration 1800 / 2000: loss 2.302619\n",
      "iteration 1900 / 2000: loss 2.302630\n",
      "Hidden Size: 30, Learning Rate: 1e-07, LR Decay: 0.3, Regularization: 0.30000000000000004, Validation accuracy: 0.0976\n",
      "iteration 0 / 2000: loss 2.302628\n",
      "iteration 100 / 2000: loss 2.302621\n",
      "iteration 200 / 2000: loss 2.302635\n",
      "iteration 300 / 2000: loss 2.302637\n",
      "iteration 400 / 2000: loss 2.302641\n",
      "iteration 500 / 2000: loss 2.302648\n",
      "iteration 600 / 2000: loss 2.302636\n",
      "iteration 700 / 2000: loss 2.302631\n",
      "iteration 800 / 2000: loss 2.302640\n",
      "iteration 900 / 2000: loss 2.302639\n",
      "iteration 1000 / 2000: loss 2.302646\n",
      "iteration 1100 / 2000: loss 2.302634\n",
      "iteration 1200 / 2000: loss 2.302631\n",
      "iteration 1300 / 2000: loss 2.302635\n",
      "iteration 1400 / 2000: loss 2.302642\n",
      "iteration 1500 / 2000: loss 2.302640\n",
      "iteration 1600 / 2000: loss 2.302639\n",
      "iteration 1700 / 2000: loss 2.302639\n",
      "iteration 1800 / 2000: loss 2.302630\n",
      "iteration 1900 / 2000: loss 2.302632\n",
      "Hidden Size: 30, Learning Rate: 1e-07, LR Decay: 0.3, Regularization: 0.4, Validation accuracy: 0.1106\n",
      "iteration 0 / 2000: loss 2.302583\n",
      "iteration 100 / 2000: loss 0.268156\n",
      "iteration 200 / 2000: loss 0.195540\n",
      "iteration 300 / 2000: loss 0.228129\n",
      "iteration 400 / 2000: loss 0.136048\n",
      "iteration 500 / 2000: loss 0.116440\n",
      "iteration 600 / 2000: loss 0.109488\n",
      "iteration 700 / 2000: loss 0.089052\n",
      "iteration 800 / 2000: loss 0.087570\n",
      "iteration 900 / 2000: loss 0.983221\n",
      "iteration 1000 / 2000: loss 0.168203\n",
      "iteration 1100 / 2000: loss 0.050766\n",
      "iteration 1200 / 2000: loss 0.117074\n",
      "iteration 1300 / 2000: loss 0.086004\n",
      "iteration 1400 / 2000: loss 0.076040\n",
      "iteration 1500 / 2000: loss 0.058419\n",
      "iteration 1600 / 2000: loss 0.071890\n",
      "iteration 1700 / 2000: loss 0.175952\n",
      "iteration 1800 / 2000: loss 0.125756\n",
      "iteration 1900 / 2000: loss 0.150866\n",
      "Hidden Size: 40, Learning Rate: 0.01, LR Decay: 0.99, Regularization: 0.0, Validation accuracy: 0.9632\n",
      "iteration 0 / 2000: loss 2.302609\n",
      "iteration 100 / 2000: loss 0.304105\n",
      "iteration 200 / 2000: loss 0.235361\n",
      "iteration 300 / 2000: loss 0.166034\n",
      "iteration 400 / 2000: loss 0.143560\n",
      "iteration 500 / 2000: loss 0.116237\n",
      "iteration 600 / 2000: loss 0.192790\n",
      "iteration 700 / 2000: loss 0.222329\n",
      "iteration 800 / 2000: loss 0.162310\n",
      "iteration 900 / 2000: loss 0.134453\n",
      "iteration 1000 / 2000: loss 0.171043\n",
      "iteration 1100 / 2000: loss 0.217750\n",
      "iteration 1200 / 2000: loss 0.233953\n",
      "iteration 1300 / 2000: loss 0.147970\n",
      "iteration 1400 / 2000: loss 0.145068\n",
      "iteration 1500 / 2000: loss 0.181592\n",
      "iteration 1600 / 2000: loss 0.187983\n",
      "iteration 1700 / 2000: loss 0.130843\n",
      "iteration 1800 / 2000: loss 0.139479\n",
      "iteration 1900 / 2000: loss 0.099884\n",
      "Hidden Size: 40, Learning Rate: 0.01, LR Decay: 0.99, Regularization: 0.1, Validation accuracy: 0.9702\n",
      "iteration 0 / 2000: loss 2.302633\n",
      "iteration 100 / 2000: loss 0.243372\n",
      "iteration 200 / 2000: loss 0.317516\n",
      "iteration 300 / 2000: loss 0.293974\n",
      "iteration 400 / 2000: loss 0.308476\n",
      "iteration 500 / 2000: loss 0.195520\n",
      "iteration 600 / 2000: loss 0.199586\n",
      "iteration 700 / 2000: loss 0.219738\n",
      "iteration 800 / 2000: loss 0.292113\n",
      "iteration 900 / 2000: loss 0.191952\n",
      "iteration 1000 / 2000: loss 0.219789\n",
      "iteration 1100 / 2000: loss 0.185841\n",
      "iteration 1200 / 2000: loss 0.211826\n",
      "iteration 1300 / 2000: loss 0.148566\n",
      "iteration 1400 / 2000: loss 0.207832\n",
      "iteration 1500 / 2000: loss 0.227216\n",
      "iteration 1600 / 2000: loss 0.136407\n",
      "iteration 1700 / 2000: loss 0.199499\n",
      "iteration 1800 / 2000: loss 0.154197\n",
      "iteration 1900 / 2000: loss 0.157487\n",
      "Hidden Size: 40, Learning Rate: 0.01, LR Decay: 0.99, Regularization: 0.2, Validation accuracy: 0.969\n",
      "iteration 0 / 2000: loss 2.302639\n",
      "iteration 100 / 2000: loss 0.393768\n",
      "iteration 200 / 2000: loss 0.218244\n",
      "iteration 300 / 2000: loss 0.240732\n",
      "iteration 400 / 2000: loss 0.229710\n",
      "iteration 500 / 2000: loss 0.217828\n",
      "iteration 600 / 2000: loss 0.212702\n",
      "iteration 700 / 2000: loss 0.227897\n",
      "iteration 800 / 2000: loss 0.228038\n",
      "iteration 900 / 2000: loss 0.242601\n",
      "iteration 1000 / 2000: loss 0.240086\n",
      "iteration 1100 / 2000: loss 0.264273\n",
      "iteration 1200 / 2000: loss 0.202686\n",
      "iteration 1300 / 2000: loss 0.217160\n",
      "iteration 1400 / 2000: loss 0.180703\n",
      "iteration 1500 / 2000: loss 0.215267\n",
      "iteration 1600 / 2000: loss 0.216095\n",
      "iteration 1700 / 2000: loss 0.202418\n",
      "iteration 1800 / 2000: loss 0.285338\n",
      "iteration 1900 / 2000: loss 0.234573\n",
      "Hidden Size: 40, Learning Rate: 0.01, LR Decay: 0.99, Regularization: 0.30000000000000004, Validation accuracy: 0.9646\n",
      "iteration 0 / 2000: loss 2.302651\n",
      "iteration 100 / 2000: loss 0.360637\n",
      "iteration 200 / 2000: loss 0.211475\n",
      "iteration 300 / 2000: loss 0.275840\n",
      "iteration 400 / 2000: loss 0.289296\n",
      "iteration 500 / 2000: loss 0.222857\n",
      "iteration 600 / 2000: loss 0.223873\n",
      "iteration 700 / 2000: loss 0.214711\n",
      "iteration 800 / 2000: loss 0.204775\n",
      "iteration 900 / 2000: loss 0.269644\n",
      "iteration 1000 / 2000: loss 0.260248\n",
      "iteration 1100 / 2000: loss 0.267619\n",
      "iteration 1200 / 2000: loss 0.253098\n",
      "iteration 1300 / 2000: loss 0.331695\n",
      "iteration 1400 / 2000: loss 0.206158\n",
      "iteration 1500 / 2000: loss 0.229790\n",
      "iteration 1600 / 2000: loss 0.273839\n",
      "iteration 1700 / 2000: loss 0.190150\n",
      "iteration 1800 / 2000: loss 0.212422\n",
      "iteration 1900 / 2000: loss 0.215412\n",
      "Hidden Size: 40, Learning Rate: 0.01, LR Decay: 0.99, Regularization: 0.4, Validation accuracy: 0.9654\n",
      "iteration 0 / 2000: loss 2.302614\n",
      "iteration 100 / 2000: loss 0.310673\n",
      "iteration 200 / 2000: loss 0.190048\n",
      "iteration 300 / 2000: loss 0.170826\n",
      "iteration 400 / 2000: loss 0.215031\n",
      "iteration 500 / 2000: loss 0.121582\n",
      "iteration 600 / 2000: loss 0.151415\n",
      "iteration 700 / 2000: loss 0.030471\n",
      "iteration 800 / 2000: loss 0.131931\n",
      "iteration 900 / 2000: loss 0.072221\n",
      "iteration 1000 / 2000: loss 0.047272\n",
      "iteration 1100 / 2000: loss 0.063971\n",
      "iteration 1200 / 2000: loss 0.216114\n",
      "iteration 1300 / 2000: loss 0.086991\n",
      "iteration 1400 / 2000: loss 0.035916\n",
      "iteration 1500 / 2000: loss 0.056116\n",
      "iteration 1600 / 2000: loss 0.046512\n",
      "iteration 1700 / 2000: loss 0.079183\n",
      "iteration 1800 / 2000: loss 0.055394\n",
      "iteration 1900 / 2000: loss 0.018070\n",
      "Hidden Size: 40, Learning Rate: 0.01, LR Decay: 0.8175, Regularization: 0.0, Validation accuracy: 0.9726\n",
      "iteration 0 / 2000: loss 2.302608\n",
      "iteration 100 / 2000: loss 0.220786\n",
      "iteration 200 / 2000: loss 0.184483\n",
      "iteration 300 / 2000: loss 0.167180\n",
      "iteration 400 / 2000: loss 0.175003\n",
      "iteration 500 / 2000: loss 0.150569\n",
      "iteration 600 / 2000: loss 0.091786\n",
      "iteration 700 / 2000: loss 0.118816\n",
      "iteration 800 / 2000: loss 0.115293\n",
      "iteration 900 / 2000: loss 0.134644\n",
      "iteration 1000 / 2000: loss 0.131931\n",
      "iteration 1100 / 2000: loss 0.106975\n",
      "iteration 1200 / 2000: loss 0.145153\n",
      "iteration 1300 / 2000: loss 0.147132\n",
      "iteration 1400 / 2000: loss 0.084633\n",
      "iteration 1500 / 2000: loss 0.149647\n",
      "iteration 1600 / 2000: loss 0.109816\n",
      "iteration 1700 / 2000: loss 0.107165\n",
      "iteration 1800 / 2000: loss 0.116716\n",
      "iteration 1900 / 2000: loss 0.119718\n",
      "Hidden Size: 40, Learning Rate: 0.01, LR Decay: 0.8175, Regularization: 0.1, Validation accuracy: 0.974\n",
      "iteration 0 / 2000: loss 2.302605\n",
      "iteration 100 / 2000: loss 0.318664\n",
      "iteration 200 / 2000: loss 0.288005\n",
      "iteration 300 / 2000: loss 0.183721\n",
      "iteration 400 / 2000: loss 0.180726\n",
      "iteration 500 / 2000: loss 0.288454\n",
      "iteration 600 / 2000: loss 0.249036\n",
      "iteration 700 / 2000: loss 0.174097\n",
      "iteration 800 / 2000: loss 0.163517\n",
      "iteration 900 / 2000: loss 0.233752\n",
      "iteration 1000 / 2000: loss 0.165951\n",
      "iteration 1100 / 2000: loss 0.164331\n",
      "iteration 1200 / 2000: loss 0.192282\n",
      "iteration 1300 / 2000: loss 0.168870\n",
      "iteration 1400 / 2000: loss 0.144587\n",
      "iteration 1500 / 2000: loss 0.160461\n",
      "iteration 1600 / 2000: loss 0.168920\n",
      "iteration 1700 / 2000: loss 0.155897\n",
      "iteration 1800 / 2000: loss 0.155041\n",
      "iteration 1900 / 2000: loss 0.158006\n",
      "Hidden Size: 40, Learning Rate: 0.01, LR Decay: 0.8175, Regularization: 0.2, Validation accuracy: 0.974\n",
      "iteration 0 / 2000: loss 2.302624\n",
      "iteration 100 / 2000: loss 0.287323\n",
      "iteration 200 / 2000: loss 0.225865\n",
      "iteration 300 / 2000: loss 0.295103\n",
      "iteration 400 / 2000: loss 0.194382\n",
      "iteration 500 / 2000: loss 0.190845\n",
      "iteration 600 / 2000: loss 0.175788\n",
      "iteration 700 / 2000: loss 0.202379\n",
      "iteration 800 / 2000: loss 0.229411\n",
      "iteration 900 / 2000: loss 0.196087\n",
      "iteration 1000 / 2000: loss 0.231888\n",
      "iteration 1100 / 2000: loss 0.224274\n",
      "iteration 1200 / 2000: loss 0.172424\n",
      "iteration 1300 / 2000: loss 0.173810\n",
      "iteration 1400 / 2000: loss 0.206992\n",
      "iteration 1500 / 2000: loss 0.189393\n",
      "iteration 1600 / 2000: loss 0.184702\n",
      "iteration 1700 / 2000: loss 0.169185\n",
      "iteration 1800 / 2000: loss 0.192895\n",
      "iteration 1900 / 2000: loss 0.159303\n",
      "Hidden Size: 40, Learning Rate: 0.01, LR Decay: 0.8175, Regularization: 0.30000000000000004, Validation accuracy: 0.972\n",
      "iteration 0 / 2000: loss 2.302648\n",
      "iteration 100 / 2000: loss 0.372993\n",
      "iteration 200 / 2000: loss 0.280231\n",
      "iteration 300 / 2000: loss 0.214994\n",
      "iteration 400 / 2000: loss 0.236797\n",
      "iteration 500 / 2000: loss 0.291752\n",
      "iteration 600 / 2000: loss 0.331498\n",
      "iteration 700 / 2000: loss 0.216622\n",
      "iteration 800 / 2000: loss 0.235313\n",
      "iteration 900 / 2000: loss 0.155068\n",
      "iteration 1000 / 2000: loss 0.288102\n",
      "iteration 1100 / 2000: loss 0.215628\n",
      "iteration 1200 / 2000: loss 0.179980\n",
      "iteration 1300 / 2000: loss 0.252281\n",
      "iteration 1400 / 2000: loss 0.183300\n",
      "iteration 1500 / 2000: loss 0.247940\n",
      "iteration 1600 / 2000: loss 0.238479\n",
      "iteration 1700 / 2000: loss 0.192561\n",
      "iteration 1800 / 2000: loss 0.191645\n",
      "iteration 1900 / 2000: loss 0.196698\n",
      "Hidden Size: 40, Learning Rate: 0.01, LR Decay: 0.8175, Regularization: 0.4, Validation accuracy: 0.968\n",
      "iteration 0 / 2000: loss 2.302597\n",
      "iteration 100 / 2000: loss 0.224429\n",
      "iteration 200 / 2000: loss 0.229320\n",
      "iteration 300 / 2000: loss 0.155603\n",
      "iteration 400 / 2000: loss 0.168803\n",
      "iteration 500 / 2000: loss 0.135475\n",
      "iteration 600 / 2000: loss 0.063314\n",
      "iteration 700 / 2000: loss 0.047503\n",
      "iteration 800 / 2000: loss 0.162547\n",
      "iteration 900 / 2000: loss 0.082689\n",
      "iteration 1000 / 2000: loss 0.099007\n",
      "iteration 1100 / 2000: loss 0.053792\n",
      "iteration 1200 / 2000: loss 0.089613\n",
      "iteration 1300 / 2000: loss 0.053681\n",
      "iteration 1400 / 2000: loss 0.076443\n",
      "iteration 1500 / 2000: loss 0.034773\n",
      "iteration 1600 / 2000: loss 0.075793\n",
      "iteration 1700 / 2000: loss 0.048114\n",
      "iteration 1800 / 2000: loss 0.081513\n",
      "iteration 1900 / 2000: loss 0.072448\n",
      "Hidden Size: 40, Learning Rate: 0.01, LR Decay: 0.645, Regularization: 0.0, Validation accuracy: 0.9736\n",
      "iteration 0 / 2000: loss 2.302592\n",
      "iteration 100 / 2000: loss 0.191211\n",
      "iteration 200 / 2000: loss 0.229034\n",
      "iteration 300 / 2000: loss 0.184794\n",
      "iteration 400 / 2000: loss 0.166656\n",
      "iteration 500 / 2000: loss 0.193709\n",
      "iteration 600 / 2000: loss 0.166849\n",
      "iteration 700 / 2000: loss 0.140503\n",
      "iteration 800 / 2000: loss 0.125595\n",
      "iteration 900 / 2000: loss 0.151441\n",
      "iteration 1000 / 2000: loss 0.098255\n",
      "iteration 1100 / 2000: loss 0.162010\n",
      "iteration 1200 / 2000: loss 0.104826\n",
      "iteration 1300 / 2000: loss 0.096481\n",
      "iteration 1400 / 2000: loss 0.095421\n",
      "iteration 1500 / 2000: loss 0.158394\n",
      "iteration 1600 / 2000: loss 0.129273\n",
      "iteration 1700 / 2000: loss 0.129390\n",
      "iteration 1800 / 2000: loss 0.097630\n",
      "iteration 1900 / 2000: loss 0.106432\n",
      "Hidden Size: 40, Learning Rate: 0.01, LR Decay: 0.645, Regularization: 0.1, Validation accuracy: 0.971\n",
      "iteration 0 / 2000: loss 2.302599\n",
      "iteration 100 / 2000: loss 0.340769\n",
      "iteration 200 / 2000: loss 0.219111\n",
      "iteration 300 / 2000: loss 0.214537\n",
      "iteration 400 / 2000: loss 0.208037\n",
      "iteration 500 / 2000: loss 0.188136\n",
      "iteration 600 / 2000: loss 0.208292\n",
      "iteration 700 / 2000: loss 0.220286\n",
      "iteration 800 / 2000: loss 0.159887\n",
      "iteration 900 / 2000: loss 0.131231\n",
      "iteration 1000 / 2000: loss 0.166518\n",
      "iteration 1100 / 2000: loss 0.176285\n",
      "iteration 1200 / 2000: loss 0.191522\n",
      "iteration 1300 / 2000: loss 0.141756\n",
      "iteration 1400 / 2000: loss 0.225581\n",
      "iteration 1500 / 2000: loss 0.224037\n",
      "iteration 1600 / 2000: loss 0.138396\n",
      "iteration 1700 / 2000: loss 0.178345\n",
      "iteration 1800 / 2000: loss 0.157989\n",
      "iteration 1900 / 2000: loss 0.134991\n",
      "Hidden Size: 40, Learning Rate: 0.01, LR Decay: 0.645, Regularization: 0.2, Validation accuracy: 0.9674\n",
      "iteration 0 / 2000: loss 2.302635\n",
      "iteration 100 / 2000: loss 0.265496\n",
      "iteration 200 / 2000: loss 0.297304\n",
      "iteration 300 / 2000: loss 0.311810\n",
      "iteration 400 / 2000: loss 0.314293\n",
      "iteration 500 / 2000: loss 0.193593\n",
      "iteration 600 / 2000: loss 0.181949\n",
      "iteration 700 / 2000: loss 0.160126\n",
      "iteration 800 / 2000: loss 0.175901\n",
      "iteration 900 / 2000: loss 0.168444\n",
      "iteration 1000 / 2000: loss 0.197287\n",
      "iteration 1100 / 2000: loss 0.178844\n",
      "iteration 1200 / 2000: loss 0.147273\n",
      "iteration 1300 / 2000: loss 0.178843\n",
      "iteration 1400 / 2000: loss 0.213517\n",
      "iteration 1500 / 2000: loss 0.196407\n",
      "iteration 1600 / 2000: loss 0.169812\n",
      "iteration 1700 / 2000: loss 0.183261\n",
      "iteration 1800 / 2000: loss 0.150973\n",
      "iteration 1900 / 2000: loss 0.135007\n",
      "Hidden Size: 40, Learning Rate: 0.01, LR Decay: 0.645, Regularization: 0.30000000000000004, Validation accuracy: 0.9684\n",
      "iteration 0 / 2000: loss 2.302622\n",
      "iteration 100 / 2000: loss 0.361024\n",
      "iteration 200 / 2000: loss 0.298511\n",
      "iteration 300 / 2000: loss 0.258795\n",
      "iteration 400 / 2000: loss 0.234160\n",
      "iteration 500 / 2000: loss 0.302280\n",
      "iteration 600 / 2000: loss 0.214125\n",
      "iteration 700 / 2000: loss 0.222462\n",
      "iteration 800 / 2000: loss 0.271398\n",
      "iteration 900 / 2000: loss 0.259324\n",
      "iteration 1000 / 2000: loss 0.199959\n",
      "iteration 1100 / 2000: loss 0.212031\n",
      "iteration 1200 / 2000: loss 0.205752\n",
      "iteration 1300 / 2000: loss 0.221033\n",
      "iteration 1400 / 2000: loss 0.223374\n",
      "iteration 1500 / 2000: loss 0.234874\n",
      "iteration 1600 / 2000: loss 0.262798\n",
      "iteration 1700 / 2000: loss 0.191718\n",
      "iteration 1800 / 2000: loss 0.208557\n",
      "iteration 1900 / 2000: loss 0.223707\n",
      "Hidden Size: 40, Learning Rate: 0.01, LR Decay: 0.645, Regularization: 0.4, Validation accuracy: 0.9686\n",
      "iteration 0 / 2000: loss 2.302587\n",
      "iteration 100 / 2000: loss 0.198644\n",
      "iteration 200 / 2000: loss 0.180690\n",
      "iteration 300 / 2000: loss 0.202465\n",
      "iteration 400 / 2000: loss 0.148708\n",
      "iteration 500 / 2000: loss 0.213884\n",
      "iteration 600 / 2000: loss 0.074446\n",
      "iteration 700 / 2000: loss 0.083543\n",
      "iteration 800 / 2000: loss 0.078517\n",
      "iteration 900 / 2000: loss 0.182077\n",
      "iteration 1000 / 2000: loss 0.091154\n",
      "iteration 1100 / 2000: loss 0.137302\n",
      "iteration 1200 / 2000: loss 0.081071\n",
      "iteration 1300 / 2000: loss 0.094735\n",
      "iteration 1400 / 2000: loss 0.077179\n",
      "iteration 1500 / 2000: loss 0.095821\n",
      "iteration 1600 / 2000: loss 0.074787\n",
      "iteration 1700 / 2000: loss 0.112814\n",
      "iteration 1800 / 2000: loss 0.116917\n",
      "iteration 1900 / 2000: loss 0.106416\n",
      "Hidden Size: 40, Learning Rate: 0.01, LR Decay: 0.47250000000000003, Regularization: 0.0, Validation accuracy: 0.966\n",
      "iteration 0 / 2000: loss 2.302592\n",
      "iteration 100 / 2000: loss 0.341753\n",
      "iteration 200 / 2000: loss 0.169249\n",
      "iteration 300 / 2000: loss 0.291664\n",
      "iteration 400 / 2000: loss 0.337407\n",
      "iteration 500 / 2000: loss 0.148761\n",
      "iteration 600 / 2000: loss 0.114171\n",
      "iteration 700 / 2000: loss 0.155178\n",
      "iteration 800 / 2000: loss 0.122782\n",
      "iteration 900 / 2000: loss 0.138800\n",
      "iteration 1000 / 2000: loss 0.157770\n",
      "iteration 1100 / 2000: loss 0.112931\n",
      "iteration 1200 / 2000: loss 0.090952\n",
      "iteration 1300 / 2000: loss 0.148604\n",
      "iteration 1400 / 2000: loss 0.113068\n",
      "iteration 1500 / 2000: loss 0.115105\n",
      "iteration 1600 / 2000: loss 0.213540\n",
      "iteration 1700 / 2000: loss 0.148935\n",
      "iteration 1800 / 2000: loss 0.145369\n",
      "iteration 1900 / 2000: loss 0.288320\n",
      "Hidden Size: 40, Learning Rate: 0.01, LR Decay: 0.47250000000000003, Regularization: 0.1, Validation accuracy: 0.9634\n",
      "iteration 0 / 2000: loss 2.302619\n",
      "iteration 100 / 2000: loss 0.256165\n",
      "iteration 200 / 2000: loss 0.267016\n",
      "iteration 300 / 2000: loss 0.260877\n",
      "iteration 400 / 2000: loss 0.174151\n",
      "iteration 500 / 2000: loss 0.192275\n",
      "iteration 600 / 2000: loss 0.179359\n",
      "iteration 700 / 2000: loss 0.143428\n",
      "iteration 800 / 2000: loss 0.186524\n",
      "iteration 900 / 2000: loss 0.181045\n",
      "iteration 1000 / 2000: loss 0.182214\n",
      "iteration 1100 / 2000: loss 0.150429\n",
      "iteration 1200 / 2000: loss 0.216570\n",
      "iteration 1300 / 2000: loss 0.205039\n",
      "iteration 1400 / 2000: loss 0.271082\n",
      "iteration 1500 / 2000: loss 0.205810\n",
      "iteration 1600 / 2000: loss 0.209451\n",
      "iteration 1700 / 2000: loss 0.180909\n",
      "iteration 1800 / 2000: loss 0.163824\n",
      "iteration 1900 / 2000: loss 0.160105\n",
      "Hidden Size: 40, Learning Rate: 0.01, LR Decay: 0.47250000000000003, Regularization: 0.2, Validation accuracy: 0.9632\n",
      "iteration 0 / 2000: loss 2.302626\n",
      "iteration 100 / 2000: loss 0.311805\n",
      "iteration 200 / 2000: loss 0.278328\n",
      "iteration 300 / 2000: loss 0.258443\n",
      "iteration 400 / 2000: loss 0.219865\n",
      "iteration 500 / 2000: loss 0.309316\n",
      "iteration 600 / 2000: loss 0.217293\n",
      "iteration 700 / 2000: loss 0.184051\n",
      "iteration 800 / 2000: loss 0.201041\n",
      "iteration 900 / 2000: loss 0.175750\n",
      "iteration 1000 / 2000: loss 0.220693\n",
      "iteration 1100 / 2000: loss 0.177918\n",
      "iteration 1200 / 2000: loss 0.138374\n",
      "iteration 1300 / 2000: loss 0.234638\n",
      "iteration 1400 / 2000: loss 0.149020\n",
      "iteration 1500 / 2000: loss 0.323086\n",
      "iteration 1600 / 2000: loss 0.161730\n",
      "iteration 1700 / 2000: loss 0.180307\n",
      "iteration 1800 / 2000: loss 0.208623\n",
      "iteration 1900 / 2000: loss 0.178511\n",
      "Hidden Size: 40, Learning Rate: 0.01, LR Decay: 0.47250000000000003, Regularization: 0.30000000000000004, Validation accuracy: 0.9638\n",
      "iteration 0 / 2000: loss 2.302663\n",
      "iteration 100 / 2000: loss 0.418559\n",
      "iteration 200 / 2000: loss 0.340084\n",
      "iteration 300 / 2000: loss 0.213109\n",
      "iteration 400 / 2000: loss 0.227480\n",
      "iteration 500 / 2000: loss 0.235867\n",
      "iteration 600 / 2000: loss 0.243607\n",
      "iteration 700 / 2000: loss 0.186965\n",
      "iteration 800 / 2000: loss 0.195732\n",
      "iteration 900 / 2000: loss 0.284467\n",
      "iteration 1000 / 2000: loss 0.211470\n",
      "iteration 1100 / 2000: loss 0.228436\n",
      "iteration 1200 / 2000: loss 0.223376\n",
      "iteration 1300 / 2000: loss 0.184762\n",
      "iteration 1400 / 2000: loss 0.224952\n",
      "iteration 1500 / 2000: loss 0.230869\n",
      "iteration 1600 / 2000: loss 0.199521\n",
      "iteration 1700 / 2000: loss 0.222224\n",
      "iteration 1800 / 2000: loss 0.316310\n",
      "iteration 1900 / 2000: loss 0.214072\n",
      "Hidden Size: 40, Learning Rate: 0.01, LR Decay: 0.47250000000000003, Regularization: 0.4, Validation accuracy: 0.9608\n",
      "iteration 0 / 2000: loss 2.302579\n",
      "iteration 100 / 2000: loss 0.240769\n",
      "iteration 200 / 2000: loss 0.245990\n",
      "iteration 300 / 2000: loss 0.223175\n",
      "iteration 400 / 2000: loss 0.240137\n",
      "iteration 500 / 2000: loss 0.165290\n",
      "iteration 600 / 2000: loss 0.213387\n",
      "iteration 700 / 2000: loss 0.229435\n",
      "iteration 800 / 2000: loss 0.136002\n",
      "iteration 900 / 2000: loss 0.244717\n",
      "iteration 1000 / 2000: loss 0.190741\n",
      "iteration 1100 / 2000: loss 0.211529\n",
      "iteration 1200 / 2000: loss 0.158557\n",
      "iteration 1300 / 2000: loss 0.076091\n",
      "iteration 1400 / 2000: loss 0.173165\n",
      "iteration 1500 / 2000: loss 0.176419\n",
      "iteration 1600 / 2000: loss 0.244210\n",
      "iteration 1700 / 2000: loss 0.137368\n",
      "iteration 1800 / 2000: loss 0.148295\n",
      "iteration 1900 / 2000: loss 0.093737\n",
      "Hidden Size: 40, Learning Rate: 0.01, LR Decay: 0.3, Regularization: 0.0, Validation accuracy: 0.9522\n",
      "iteration 0 / 2000: loss 2.302605\n",
      "iteration 100 / 2000: loss 0.212581\n",
      "iteration 200 / 2000: loss 0.278300\n",
      "iteration 300 / 2000: loss 0.152210\n",
      "iteration 400 / 2000: loss 0.257926\n",
      "iteration 500 / 2000: loss 0.269958\n",
      "iteration 600 / 2000: loss 0.167892\n",
      "iteration 700 / 2000: loss 0.341501\n",
      "iteration 800 / 2000: loss 0.174960\n",
      "iteration 900 / 2000: loss 0.265956\n",
      "iteration 1000 / 2000: loss 0.139016\n",
      "iteration 1100 / 2000: loss 0.215561\n",
      "iteration 1200 / 2000: loss 0.290458\n",
      "iteration 1300 / 2000: loss 0.206935\n",
      "iteration 1400 / 2000: loss 0.205704\n",
      "iteration 1500 / 2000: loss 0.248823\n",
      "iteration 1600 / 2000: loss 0.248738\n",
      "iteration 1700 / 2000: loss 0.251644\n",
      "iteration 1800 / 2000: loss 0.170295\n",
      "iteration 1900 / 2000: loss 0.169397\n",
      "Hidden Size: 40, Learning Rate: 0.01, LR Decay: 0.3, Regularization: 0.1, Validation accuracy: 0.9502\n",
      "iteration 0 / 2000: loss 2.302592\n",
      "iteration 100 / 2000: loss 0.304357\n",
      "iteration 200 / 2000: loss 0.288629\n",
      "iteration 300 / 2000: loss 0.324950\n",
      "iteration 400 / 2000: loss 0.210868\n",
      "iteration 500 / 2000: loss 0.148258\n",
      "iteration 600 / 2000: loss 0.282519\n",
      "iteration 700 / 2000: loss 0.198110\n",
      "iteration 800 / 2000: loss 0.259364\n",
      "iteration 900 / 2000: loss 0.277861\n",
      "iteration 1000 / 2000: loss 0.268142\n",
      "iteration 1100 / 2000: loss 0.173881\n",
      "iteration 1200 / 2000: loss 0.322361\n",
      "iteration 1300 / 2000: loss 0.264840\n",
      "iteration 1400 / 2000: loss 0.166954\n",
      "iteration 1500 / 2000: loss 0.235706\n",
      "iteration 1600 / 2000: loss 0.238855\n",
      "iteration 1700 / 2000: loss 0.249547\n",
      "iteration 1800 / 2000: loss 0.191780\n",
      "iteration 1900 / 2000: loss 0.252211\n",
      "Hidden Size: 40, Learning Rate: 0.01, LR Decay: 0.3, Regularization: 0.2, Validation accuracy: 0.9456\n",
      "iteration 0 / 2000: loss 2.302626\n",
      "iteration 100 / 2000: loss 0.382123\n",
      "iteration 200 / 2000: loss 0.317171\n",
      "iteration 300 / 2000: loss 0.194050\n",
      "iteration 400 / 2000: loss 0.211729\n",
      "iteration 500 / 2000: loss 0.209647\n",
      "iteration 600 / 2000: loss 0.185977\n",
      "iteration 700 / 2000: loss 0.278966\n",
      "iteration 800 / 2000: loss 0.260872\n",
      "iteration 900 / 2000: loss 0.219686\n",
      "iteration 1000 / 2000: loss 0.240476\n",
      "iteration 1100 / 2000: loss 0.310753\n",
      "iteration 1200 / 2000: loss 0.204488\n",
      "iteration 1300 / 2000: loss 0.316213\n",
      "iteration 1400 / 2000: loss 0.301756\n",
      "iteration 1500 / 2000: loss 0.249508\n",
      "iteration 1600 / 2000: loss 0.281459\n",
      "iteration 1700 / 2000: loss 0.176121\n",
      "iteration 1800 / 2000: loss 0.228867\n",
      "iteration 1900 / 2000: loss 0.219925\n",
      "Hidden Size: 40, Learning Rate: 0.01, LR Decay: 0.3, Regularization: 0.30000000000000004, Validation accuracy: 0.9462\n",
      "iteration 0 / 2000: loss 2.302643\n",
      "iteration 100 / 2000: loss 0.239413\n",
      "iteration 200 / 2000: loss 0.274112\n",
      "iteration 300 / 2000: loss 0.330577\n",
      "iteration 400 / 2000: loss 0.304311\n",
      "iteration 500 / 2000: loss 0.205812\n",
      "iteration 600 / 2000: loss 0.253602\n",
      "iteration 700 / 2000: loss 0.348778\n",
      "iteration 800 / 2000: loss 0.243338\n",
      "iteration 900 / 2000: loss 0.226249\n",
      "iteration 1000 / 2000: loss 0.245368\n",
      "iteration 1100 / 2000: loss 0.281838\n",
      "iteration 1200 / 2000: loss 0.265422\n",
      "iteration 1300 / 2000: loss 0.221428\n",
      "iteration 1400 / 2000: loss 0.264585\n",
      "iteration 1500 / 2000: loss 0.338809\n",
      "iteration 1600 / 2000: loss 0.328315\n",
      "iteration 1700 / 2000: loss 0.253811\n",
      "iteration 1800 / 2000: loss 0.245623\n",
      "iteration 1900 / 2000: loss 0.373299\n",
      "Hidden Size: 40, Learning Rate: 0.01, LR Decay: 0.3, Regularization: 0.4, Validation accuracy: 0.9476\n",
      "iteration 0 / 2000: loss 2.302597\n",
      "iteration 100 / 2000: loss 0.349209\n",
      "iteration 200 / 2000: loss 0.155299\n",
      "iteration 300 / 2000: loss 0.155474\n",
      "iteration 400 / 2000: loss 0.194693\n",
      "iteration 500 / 2000: loss 0.062261\n",
      "iteration 600 / 2000: loss 0.088827\n",
      "iteration 700 / 2000: loss 0.103983\n",
      "iteration 800 / 2000: loss 0.033847\n",
      "iteration 900 / 2000: loss 0.103698\n",
      "iteration 1000 / 2000: loss 0.049465\n",
      "iteration 1100 / 2000: loss 0.067410\n",
      "iteration 1200 / 2000: loss 0.046574\n",
      "iteration 1300 / 2000: loss 0.040207\n",
      "iteration 1400 / 2000: loss 0.084222\n",
      "iteration 1500 / 2000: loss 0.064680\n",
      "iteration 1600 / 2000: loss 0.018380\n",
      "iteration 1700 / 2000: loss 0.072809\n",
      "iteration 1800 / 2000: loss 0.059075\n",
      "iteration 1900 / 2000: loss 0.060161\n",
      "Hidden Size: 40, Learning Rate: 0.0075000250000000004, LR Decay: 0.99, Regularization: 0.0, Validation accuracy: 0.97\n",
      "iteration 0 / 2000: loss 2.302612\n",
      "iteration 100 / 2000: loss 0.352426\n",
      "iteration 200 / 2000: loss 0.202116\n",
      "iteration 300 / 2000: loss 0.182392\n",
      "iteration 400 / 2000: loss 0.249354\n",
      "iteration 500 / 2000: loss 0.145368\n",
      "iteration 600 / 2000: loss 0.120457\n",
      "iteration 700 / 2000: loss 0.138254\n",
      "iteration 800 / 2000: loss 0.123164\n",
      "iteration 900 / 2000: loss 0.169911\n",
      "iteration 1000 / 2000: loss 0.121912\n",
      "iteration 1100 / 2000: loss 0.093288\n",
      "iteration 1200 / 2000: loss 0.143439\n",
      "iteration 1300 / 2000: loss 0.152924\n",
      "iteration 1400 / 2000: loss 0.110465\n",
      "iteration 1500 / 2000: loss 0.077831\n",
      "iteration 1600 / 2000: loss 0.104304\n",
      "iteration 1700 / 2000: loss 0.130787\n",
      "iteration 1800 / 2000: loss 0.116893\n",
      "iteration 1900 / 2000: loss 0.113770\n",
      "Hidden Size: 40, Learning Rate: 0.0075000250000000004, LR Decay: 0.99, Regularization: 0.1, Validation accuracy: 0.972\n",
      "iteration 0 / 2000: loss 2.302630\n",
      "iteration 100 / 2000: loss 0.345264\n",
      "iteration 200 / 2000: loss 0.300846\n",
      "iteration 300 / 2000: loss 0.166523\n",
      "iteration 400 / 2000: loss 0.186728\n",
      "iteration 500 / 2000: loss 0.235075\n",
      "iteration 600 / 2000: loss 0.197961\n",
      "iteration 700 / 2000: loss 0.167258\n",
      "iteration 800 / 2000: loss 0.154180\n",
      "iteration 900 / 2000: loss 0.170535\n",
      "iteration 1000 / 2000: loss 0.194938\n",
      "iteration 1100 / 2000: loss 0.142787\n",
      "iteration 1200 / 2000: loss 0.197540\n",
      "iteration 1300 / 2000: loss 0.184717\n",
      "iteration 1400 / 2000: loss 0.180919\n",
      "iteration 1500 / 2000: loss 0.210605\n",
      "iteration 1600 / 2000: loss 0.166564\n",
      "iteration 1700 / 2000: loss 0.170257\n",
      "iteration 1800 / 2000: loss 0.160355\n",
      "iteration 1900 / 2000: loss 0.142543\n",
      "Hidden Size: 40, Learning Rate: 0.0075000250000000004, LR Decay: 0.99, Regularization: 0.2, Validation accuracy: 0.9672\n",
      "iteration 0 / 2000: loss 2.302630\n",
      "iteration 100 / 2000: loss 0.265392\n",
      "iteration 200 / 2000: loss 0.334146\n",
      "iteration 300 / 2000: loss 0.243555\n",
      "iteration 400 / 2000: loss 0.304699\n",
      "iteration 500 / 2000: loss 0.194356\n",
      "iteration 600 / 2000: loss 0.215634\n",
      "iteration 700 / 2000: loss 0.262241\n",
      "iteration 800 / 2000: loss 0.224542\n",
      "iteration 900 / 2000: loss 0.205090\n",
      "iteration 1000 / 2000: loss 0.207931\n",
      "iteration 1100 / 2000: loss 0.167757\n",
      "iteration 1200 / 2000: loss 0.197749\n",
      "iteration 1300 / 2000: loss 0.263918\n",
      "iteration 1400 / 2000: loss 0.205086\n",
      "iteration 1500 / 2000: loss 0.181540\n",
      "iteration 1600 / 2000: loss 0.191271\n",
      "iteration 1700 / 2000: loss 0.197509\n",
      "iteration 1800 / 2000: loss 0.228444\n",
      "iteration 1900 / 2000: loss 0.225949\n",
      "Hidden Size: 40, Learning Rate: 0.0075000250000000004, LR Decay: 0.99, Regularization: 0.30000000000000004, Validation accuracy: 0.9668\n",
      "iteration 0 / 2000: loss 2.302631\n",
      "iteration 100 / 2000: loss 0.373304\n",
      "iteration 200 / 2000: loss 0.296490\n",
      "iteration 300 / 2000: loss 0.325615\n",
      "iteration 400 / 2000: loss 0.285453\n",
      "iteration 500 / 2000: loss 0.267505\n",
      "iteration 600 / 2000: loss 0.347095\n",
      "iteration 700 / 2000: loss 0.329575\n",
      "iteration 800 / 2000: loss 0.236147\n",
      "iteration 900 / 2000: loss 0.195522\n",
      "iteration 1000 / 2000: loss 0.198144\n",
      "iteration 1100 / 2000: loss 0.248188\n",
      "iteration 1200 / 2000: loss 0.237623\n",
      "iteration 1300 / 2000: loss 0.231445\n",
      "iteration 1400 / 2000: loss 0.279357\n",
      "iteration 1500 / 2000: loss 0.242861\n",
      "iteration 1600 / 2000: loss 0.237552\n",
      "iteration 1700 / 2000: loss 0.230044\n",
      "iteration 1800 / 2000: loss 0.174121\n",
      "iteration 1900 / 2000: loss 0.189936\n",
      "Hidden Size: 40, Learning Rate: 0.0075000250000000004, LR Decay: 0.99, Regularization: 0.4, Validation accuracy: 0.9642\n",
      "iteration 0 / 2000: loss 2.302578\n",
      "iteration 100 / 2000: loss 0.324895\n",
      "iteration 200 / 2000: loss 0.158344\n",
      "iteration 300 / 2000: loss 0.174063\n",
      "iteration 400 / 2000: loss 0.245803\n",
      "iteration 500 / 2000: loss 0.172711\n",
      "iteration 600 / 2000: loss 0.107824\n",
      "iteration 700 / 2000: loss 0.108656\n",
      "iteration 800 / 2000: loss 0.064984\n",
      "iteration 900 / 2000: loss 0.057490\n",
      "iteration 1000 / 2000: loss 0.084248\n",
      "iteration 1100 / 2000: loss 0.105269\n",
      "iteration 1200 / 2000: loss 0.077867\n",
      "iteration 1300 / 2000: loss 0.080487\n",
      "iteration 1400 / 2000: loss 0.065254\n",
      "iteration 1500 / 2000: loss 0.026522\n",
      "iteration 1600 / 2000: loss 0.065751\n",
      "iteration 1700 / 2000: loss 0.019717\n",
      "iteration 1800 / 2000: loss 0.045448\n",
      "iteration 1900 / 2000: loss 0.045558\n",
      "Hidden Size: 40, Learning Rate: 0.0075000250000000004, LR Decay: 0.8175, Regularization: 0.0, Validation accuracy: 0.9718\n",
      "iteration 0 / 2000: loss 2.302593\n",
      "iteration 100 / 2000: loss 0.276211\n",
      "iteration 200 / 2000: loss 0.177331\n",
      "iteration 300 / 2000: loss 0.147542\n",
      "iteration 400 / 2000: loss 0.162151\n",
      "iteration 500 / 2000: loss 0.158384\n",
      "iteration 600 / 2000: loss 0.114223\n",
      "iteration 700 / 2000: loss 0.117204\n",
      "iteration 800 / 2000: loss 0.133203\n",
      "iteration 900 / 2000: loss 0.158639\n",
      "iteration 1000 / 2000: loss 0.156124\n",
      "iteration 1100 / 2000: loss 0.103968\n",
      "iteration 1200 / 2000: loss 0.108672\n",
      "iteration 1300 / 2000: loss 0.112227\n",
      "iteration 1400 / 2000: loss 0.164579\n",
      "iteration 1500 / 2000: loss 0.127939\n",
      "iteration 1600 / 2000: loss 0.128821\n",
      "iteration 1700 / 2000: loss 0.108467\n",
      "iteration 1800 / 2000: loss 0.104500\n",
      "iteration 1900 / 2000: loss 0.096347\n",
      "Hidden Size: 40, Learning Rate: 0.0075000250000000004, LR Decay: 0.8175, Regularization: 0.1, Validation accuracy: 0.971\n",
      "iteration 0 / 2000: loss 2.302615\n",
      "iteration 100 / 2000: loss 0.349510\n",
      "iteration 200 / 2000: loss 0.175821\n",
      "iteration 300 / 2000: loss 0.163421\n",
      "iteration 400 / 2000: loss 0.125635\n",
      "iteration 500 / 2000: loss 0.306788\n",
      "iteration 600 / 2000: loss 0.146159\n",
      "iteration 700 / 2000: loss 0.176553\n",
      "iteration 800 / 2000: loss 0.150563\n",
      "iteration 900 / 2000: loss 0.146425\n",
      "iteration 1000 / 2000: loss 0.160822\n",
      "iteration 1100 / 2000: loss 0.139539\n",
      "iteration 1200 / 2000: loss 0.155413\n",
      "iteration 1300 / 2000: loss 0.129139\n",
      "iteration 1400 / 2000: loss 0.144900\n",
      "iteration 1500 / 2000: loss 0.153518\n",
      "iteration 1600 / 2000: loss 0.155282\n",
      "iteration 1700 / 2000: loss 0.158217\n",
      "iteration 1800 / 2000: loss 0.159580\n",
      "iteration 1900 / 2000: loss 0.143663\n",
      "Hidden Size: 40, Learning Rate: 0.0075000250000000004, LR Decay: 0.8175, Regularization: 0.2, Validation accuracy: 0.9702\n",
      "iteration 0 / 2000: loss 2.302638\n",
      "iteration 100 / 2000: loss 0.315529\n",
      "iteration 200 / 2000: loss 0.359403\n",
      "iteration 300 / 2000: loss 0.230090\n",
      "iteration 400 / 2000: loss 0.253824\n",
      "iteration 500 / 2000: loss 0.207716\n",
      "iteration 600 / 2000: loss 0.223965\n",
      "iteration 700 / 2000: loss 0.266384\n",
      "iteration 800 / 2000: loss 0.240624\n",
      "iteration 900 / 2000: loss 0.193520\n",
      "iteration 1000 / 2000: loss 0.214471\n",
      "iteration 1100 / 2000: loss 0.187895\n",
      "iteration 1200 / 2000: loss 0.180889\n",
      "iteration 1300 / 2000: loss 0.218010\n",
      "iteration 1400 / 2000: loss 0.163480\n",
      "iteration 1500 / 2000: loss 0.205853\n",
      "iteration 1600 / 2000: loss 0.190422\n",
      "iteration 1700 / 2000: loss 0.211648\n",
      "iteration 1800 / 2000: loss 0.208750\n",
      "iteration 1900 / 2000: loss 0.220611\n",
      "Hidden Size: 40, Learning Rate: 0.0075000250000000004, LR Decay: 0.8175, Regularization: 0.30000000000000004, Validation accuracy: 0.9708\n",
      "iteration 0 / 2000: loss 2.302631\n",
      "iteration 100 / 2000: loss 0.324462\n",
      "iteration 200 / 2000: loss 0.323359\n",
      "iteration 300 / 2000: loss 0.215937\n",
      "iteration 400 / 2000: loss 0.222532\n",
      "iteration 500 / 2000: loss 0.302708\n",
      "iteration 600 / 2000: loss 0.222259\n",
      "iteration 700 / 2000: loss 0.204127\n",
      "iteration 800 / 2000: loss 0.274482\n",
      "iteration 900 / 2000: loss 0.305473\n",
      "iteration 1000 / 2000: loss 0.257338\n",
      "iteration 1100 / 2000: loss 0.274367\n",
      "iteration 1200 / 2000: loss 0.182085\n",
      "iteration 1300 / 2000: loss 0.226437\n",
      "iteration 1400 / 2000: loss 0.210144\n",
      "iteration 1500 / 2000: loss 0.224890\n",
      "iteration 1600 / 2000: loss 0.251884\n",
      "iteration 1700 / 2000: loss 0.244156\n",
      "iteration 1800 / 2000: loss 0.233802\n",
      "iteration 1900 / 2000: loss 0.236018\n",
      "Hidden Size: 40, Learning Rate: 0.0075000250000000004, LR Decay: 0.8175, Regularization: 0.4, Validation accuracy: 0.9704\n",
      "iteration 0 / 2000: loss 2.302599\n",
      "iteration 100 / 2000: loss 0.297308\n",
      "iteration 200 / 2000: loss 0.178816\n",
      "iteration 300 / 2000: loss 0.154395\n",
      "iteration 400 / 2000: loss 0.247990\n",
      "iteration 500 / 2000: loss 0.104853\n",
      "iteration 600 / 2000: loss 0.137156\n",
      "iteration 700 / 2000: loss 0.126674\n",
      "iteration 800 / 2000: loss 0.048779\n",
      "iteration 900 / 2000: loss 0.160769\n",
      "iteration 1000 / 2000: loss 0.111945\n",
      "iteration 1100 / 2000: loss 0.065195\n",
      "iteration 1200 / 2000: loss 0.050396\n",
      "iteration 1300 / 2000: loss 0.141195\n",
      "iteration 1400 / 2000: loss 0.057336\n",
      "iteration 1500 / 2000: loss 0.043180\n",
      "iteration 1600 / 2000: loss 0.098787\n",
      "iteration 1700 / 2000: loss 0.202831\n",
      "iteration 1800 / 2000: loss 0.123625\n",
      "iteration 1900 / 2000: loss 0.051248\n",
      "Hidden Size: 40, Learning Rate: 0.0075000250000000004, LR Decay: 0.645, Regularization: 0.0, Validation accuracy: 0.9694\n",
      "iteration 0 / 2000: loss 2.302590\n",
      "iteration 100 / 2000: loss 0.234752\n",
      "iteration 200 / 2000: loss 0.263420\n",
      "iteration 300 / 2000: loss 0.160182\n",
      "iteration 400 / 2000: loss 0.292735\n",
      "iteration 500 / 2000: loss 0.192930\n",
      "iteration 600 / 2000: loss 0.123259\n",
      "iteration 700 / 2000: loss 0.160055\n",
      "iteration 800 / 2000: loss 0.124906\n",
      "iteration 900 / 2000: loss 0.114571\n",
      "iteration 1000 / 2000: loss 0.127909\n",
      "iteration 1100 / 2000: loss 0.140703\n",
      "iteration 1200 / 2000: loss 0.126318\n",
      "iteration 1300 / 2000: loss 0.097982\n",
      "iteration 1400 / 2000: loss 0.091618\n",
      "iteration 1500 / 2000: loss 0.089407\n",
      "iteration 1600 / 2000: loss 0.144377\n",
      "iteration 1700 / 2000: loss 0.113315\n",
      "iteration 1800 / 2000: loss 0.135624\n",
      "iteration 1900 / 2000: loss 0.103849\n",
      "Hidden Size: 40, Learning Rate: 0.0075000250000000004, LR Decay: 0.645, Regularization: 0.1, Validation accuracy: 0.97\n",
      "iteration 0 / 2000: loss 2.302613\n",
      "iteration 100 / 2000: loss 0.199500\n",
      "iteration 200 / 2000: loss 0.267750\n",
      "iteration 300 / 2000: loss 0.235694\n",
      "iteration 400 / 2000: loss 0.166001\n",
      "iteration 500 / 2000: loss 0.223040\n",
      "iteration 600 / 2000: loss 0.159341\n",
      "iteration 700 / 2000: loss 0.190305\n",
      "iteration 800 / 2000: loss 0.225255\n",
      "iteration 900 / 2000: loss 0.191515\n",
      "iteration 1000 / 2000: loss 0.231826\n",
      "iteration 1100 / 2000: loss 0.177988\n",
      "iteration 1200 / 2000: loss 0.167688\n",
      "iteration 1300 / 2000: loss 0.148636\n",
      "iteration 1400 / 2000: loss 0.157965\n",
      "iteration 1500 / 2000: loss 0.209068\n",
      "iteration 1600 / 2000: loss 0.144362\n",
      "iteration 1700 / 2000: loss 0.138493\n",
      "iteration 1800 / 2000: loss 0.188369\n",
      "iteration 1900 / 2000: loss 0.217554\n",
      "Hidden Size: 40, Learning Rate: 0.0075000250000000004, LR Decay: 0.645, Regularization: 0.2, Validation accuracy: 0.9672\n",
      "iteration 0 / 2000: loss 2.302618\n",
      "iteration 100 / 2000: loss 0.394102\n",
      "iteration 200 / 2000: loss 0.229640\n",
      "iteration 300 / 2000: loss 0.266329\n",
      "iteration 400 / 2000: loss 0.203965\n",
      "iteration 500 / 2000: loss 0.191730\n",
      "iteration 600 / 2000: loss 0.230959\n",
      "iteration 700 / 2000: loss 0.180379\n",
      "iteration 800 / 2000: loss 0.220096\n",
      "iteration 900 / 2000: loss 0.211376\n",
      "iteration 1000 / 2000: loss 0.196944\n",
      "iteration 1100 / 2000: loss 0.168450\n",
      "iteration 1200 / 2000: loss 0.178583\n",
      "iteration 1300 / 2000: loss 0.165929\n",
      "iteration 1400 / 2000: loss 0.201121\n",
      "iteration 1500 / 2000: loss 0.160068\n",
      "iteration 1600 / 2000: loss 0.163534\n",
      "iteration 1700 / 2000: loss 0.180659\n",
      "iteration 1800 / 2000: loss 0.139590\n",
      "iteration 1900 / 2000: loss 0.169564\n",
      "Hidden Size: 40, Learning Rate: 0.0075000250000000004, LR Decay: 0.645, Regularization: 0.30000000000000004, Validation accuracy: 0.9674\n",
      "iteration 0 / 2000: loss 2.302651\n",
      "iteration 100 / 2000: loss 0.360573\n",
      "iteration 200 / 2000: loss 0.393200\n",
      "iteration 300 / 2000: loss 0.331504\n",
      "iteration 400 / 2000: loss 0.278024\n",
      "iteration 500 / 2000: loss 0.219914\n",
      "iteration 600 / 2000: loss 0.255267\n",
      "iteration 700 / 2000: loss 0.254523\n",
      "iteration 800 / 2000: loss 0.226919\n",
      "iteration 900 / 2000: loss 0.200182\n",
      "iteration 1000 / 2000: loss 0.191334\n",
      "iteration 1100 / 2000: loss 0.215774\n",
      "iteration 1200 / 2000: loss 0.282497\n",
      "iteration 1300 / 2000: loss 0.234150\n",
      "iteration 1400 / 2000: loss 0.246137\n",
      "iteration 1500 / 2000: loss 0.169528\n",
      "iteration 1600 / 2000: loss 0.250705\n",
      "iteration 1700 / 2000: loss 0.256666\n",
      "iteration 1800 / 2000: loss 0.176819\n",
      "iteration 1900 / 2000: loss 0.207342\n",
      "Hidden Size: 40, Learning Rate: 0.0075000250000000004, LR Decay: 0.645, Regularization: 0.4, Validation accuracy: 0.9658\n",
      "iteration 0 / 2000: loss 2.302594\n",
      "iteration 100 / 2000: loss 0.426393\n",
      "iteration 200 / 2000: loss 0.266910\n",
      "iteration 300 / 2000: loss 0.179547\n",
      "iteration 400 / 2000: loss 0.151366\n",
      "iteration 500 / 2000: loss 0.208011\n",
      "iteration 600 / 2000: loss 0.132346\n",
      "iteration 700 / 2000: loss 0.107908\n",
      "iteration 800 / 2000: loss 0.166035\n",
      "iteration 900 / 2000: loss 0.107288\n",
      "iteration 1000 / 2000: loss 0.171766\n",
      "iteration 1100 / 2000: loss 0.138029\n",
      "iteration 1200 / 2000: loss 0.127019\n",
      "iteration 1300 / 2000: loss 0.137628\n",
      "iteration 1400 / 2000: loss 0.097215\n",
      "iteration 1500 / 2000: loss 0.125951\n",
      "iteration 1600 / 2000: loss 0.118703\n",
      "iteration 1700 / 2000: loss 0.098905\n",
      "iteration 1800 / 2000: loss 0.167866\n",
      "iteration 1900 / 2000: loss 0.188427\n",
      "Hidden Size: 40, Learning Rate: 0.0075000250000000004, LR Decay: 0.47250000000000003, Regularization: 0.0, Validation accuracy: 0.96\n",
      "iteration 0 / 2000: loss 2.302614\n",
      "iteration 100 / 2000: loss 0.229233\n",
      "iteration 200 / 2000: loss 0.290138\n",
      "iteration 300 / 2000: loss 0.215865\n",
      "iteration 400 / 2000: loss 0.241213\n",
      "iteration 500 / 2000: loss 0.201918\n",
      "iteration 600 / 2000: loss 0.158564\n",
      "iteration 700 / 2000: loss 0.202385\n",
      "iteration 800 / 2000: loss 0.128304\n",
      "iteration 900 / 2000: loss 0.140867\n",
      "iteration 1000 / 2000: loss 0.131964\n",
      "iteration 1100 / 2000: loss 0.209072\n",
      "iteration 1200 / 2000: loss 0.189888\n",
      "iteration 1300 / 2000: loss 0.272045\n",
      "iteration 1400 / 2000: loss 0.160498\n",
      "iteration 1500 / 2000: loss 0.111762\n",
      "iteration 1600 / 2000: loss 0.157410\n",
      "iteration 1700 / 2000: loss 0.141318\n",
      "iteration 1800 / 2000: loss 0.157851\n",
      "iteration 1900 / 2000: loss 0.195829\n",
      "Hidden Size: 40, Learning Rate: 0.0075000250000000004, LR Decay: 0.47250000000000003, Regularization: 0.1, Validation accuracy: 0.9602\n",
      "iteration 0 / 2000: loss 2.302635\n",
      "iteration 100 / 2000: loss 0.375894\n",
      "iteration 200 / 2000: loss 0.257263\n",
      "iteration 300 / 2000: loss 0.210460\n",
      "iteration 400 / 2000: loss 0.257516\n",
      "iteration 500 / 2000: loss 0.244952\n",
      "iteration 600 / 2000: loss 0.214980\n",
      "iteration 700 / 2000: loss 0.237028\n",
      "iteration 800 / 2000: loss 0.190573\n",
      "iteration 900 / 2000: loss 0.262918\n",
      "iteration 1000 / 2000: loss 0.214548\n",
      "iteration 1100 / 2000: loss 0.171930\n",
      "iteration 1200 / 2000: loss 0.203542\n",
      "iteration 1300 / 2000: loss 0.263399\n",
      "iteration 1400 / 2000: loss 0.245356\n",
      "iteration 1500 / 2000: loss 0.196325\n",
      "iteration 1600 / 2000: loss 0.254687\n",
      "iteration 1700 / 2000: loss 0.220641\n",
      "iteration 1800 / 2000: loss 0.208406\n",
      "iteration 1900 / 2000: loss 0.201305\n",
      "Hidden Size: 40, Learning Rate: 0.0075000250000000004, LR Decay: 0.47250000000000003, Regularization: 0.2, Validation accuracy: 0.9582\n",
      "iteration 0 / 2000: loss 2.302630\n",
      "iteration 100 / 2000: loss 0.297092\n",
      "iteration 200 / 2000: loss 0.286899\n",
      "iteration 300 / 2000: loss 0.199337\n",
      "iteration 400 / 2000: loss 0.298823\n",
      "iteration 500 / 2000: loss 0.181791\n",
      "iteration 600 / 2000: loss 0.249885\n",
      "iteration 700 / 2000: loss 0.195618\n",
      "iteration 800 / 2000: loss 0.198769\n",
      "iteration 900 / 2000: loss 0.311427\n",
      "iteration 1000 / 2000: loss 0.195846\n",
      "iteration 1100 / 2000: loss 0.202079\n",
      "iteration 1200 / 2000: loss 0.235664\n",
      "iteration 1300 / 2000: loss 0.204908\n",
      "iteration 1400 / 2000: loss 0.239168\n",
      "iteration 1500 / 2000: loss 0.253067\n",
      "iteration 1600 / 2000: loss 0.221253\n",
      "iteration 1700 / 2000: loss 0.175952\n",
      "iteration 1800 / 2000: loss 0.262372\n",
      "iteration 1900 / 2000: loss 0.252230\n",
      "Hidden Size: 40, Learning Rate: 0.0075000250000000004, LR Decay: 0.47250000000000003, Regularization: 0.30000000000000004, Validation accuracy: 0.9588\n",
      "iteration 0 / 2000: loss 2.302651\n",
      "iteration 100 / 2000: loss 0.409392\n",
      "iteration 200 / 2000: loss 0.296583\n",
      "iteration 300 / 2000: loss 0.368642\n",
      "iteration 400 / 2000: loss 0.276021\n",
      "iteration 500 / 2000: loss 0.315508\n",
      "iteration 600 / 2000: loss 0.226521\n",
      "iteration 700 / 2000: loss 0.237478\n",
      "iteration 800 / 2000: loss 0.226412\n",
      "iteration 900 / 2000: loss 0.243127\n",
      "iteration 1000 / 2000: loss 0.203993\n",
      "iteration 1100 / 2000: loss 0.264618\n",
      "iteration 1200 / 2000: loss 0.232164\n",
      "iteration 1300 / 2000: loss 0.159205\n",
      "iteration 1400 / 2000: loss 0.229247\n",
      "iteration 1500 / 2000: loss 0.245245\n",
      "iteration 1600 / 2000: loss 0.223051\n",
      "iteration 1700 / 2000: loss 0.248175\n",
      "iteration 1800 / 2000: loss 0.247576\n",
      "iteration 1900 / 2000: loss 0.217358\n",
      "Hidden Size: 40, Learning Rate: 0.0075000250000000004, LR Decay: 0.47250000000000003, Regularization: 0.4, Validation accuracy: 0.958\n",
      "iteration 0 / 2000: loss 2.302591\n",
      "iteration 100 / 2000: loss 0.418738\n",
      "iteration 200 / 2000: loss 0.312511\n",
      "iteration 300 / 2000: loss 0.209587\n",
      "iteration 400 / 2000: loss 0.183330\n",
      "iteration 500 / 2000: loss 0.249453\n",
      "iteration 600 / 2000: loss 0.235885\n",
      "iteration 700 / 2000: loss 0.223452\n",
      "iteration 800 / 2000: loss 0.212191\n",
      "iteration 900 / 2000: loss 0.217818\n",
      "iteration 1000 / 2000: loss 0.137069\n",
      "iteration 1100 / 2000: loss 0.238641\n",
      "iteration 1200 / 2000: loss 0.160430\n",
      "iteration 1300 / 2000: loss 0.198934\n",
      "iteration 1400 / 2000: loss 0.151440\n",
      "iteration 1500 / 2000: loss 0.178890\n",
      "iteration 1600 / 2000: loss 0.179060\n",
      "iteration 1700 / 2000: loss 0.262456\n",
      "iteration 1800 / 2000: loss 0.163320\n",
      "iteration 1900 / 2000: loss 0.238718\n",
      "Hidden Size: 40, Learning Rate: 0.0075000250000000004, LR Decay: 0.3, Regularization: 0.0, Validation accuracy: 0.945\n",
      "iteration 0 / 2000: loss 2.302598\n",
      "iteration 100 / 2000: loss 0.398943\n",
      "iteration 200 / 2000: loss 0.259880\n",
      "iteration 300 / 2000: loss 0.263913\n",
      "iteration 400 / 2000: loss 0.224407\n",
      "iteration 500 / 2000: loss 0.233833\n",
      "iteration 600 / 2000: loss 0.239864\n",
      "iteration 700 / 2000: loss 0.175072\n",
      "iteration 800 / 2000: loss 0.274077\n",
      "iteration 900 / 2000: loss 0.258554\n",
      "iteration 1000 / 2000: loss 0.338778\n",
      "iteration 1100 / 2000: loss 0.209430\n",
      "iteration 1200 / 2000: loss 0.240323\n",
      "iteration 1300 / 2000: loss 0.235521\n",
      "iteration 1400 / 2000: loss 0.257773\n",
      "iteration 1500 / 2000: loss 0.246631\n",
      "iteration 1600 / 2000: loss 0.294421\n",
      "iteration 1700 / 2000: loss 0.148912\n",
      "iteration 1800 / 2000: loss 0.199358\n",
      "iteration 1900 / 2000: loss 0.273489\n",
      "Hidden Size: 40, Learning Rate: 0.0075000250000000004, LR Decay: 0.3, Regularization: 0.1, Validation accuracy: 0.9452\n",
      "iteration 0 / 2000: loss 2.302617\n",
      "iteration 100 / 2000: loss 0.473920\n",
      "iteration 200 / 2000: loss 0.328781\n",
      "iteration 300 / 2000: loss 0.283051\n",
      "iteration 400 / 2000: loss 0.238721\n",
      "iteration 500 / 2000: loss 0.281607\n",
      "iteration 600 / 2000: loss 0.182612\n",
      "iteration 700 / 2000: loss 0.274512\n",
      "iteration 800 / 2000: loss 0.266896\n",
      "iteration 900 / 2000: loss 0.210576\n",
      "iteration 1000 / 2000: loss 0.253779\n",
      "iteration 1100 / 2000: loss 0.254347\n",
      "iteration 1200 / 2000: loss 0.279469\n",
      "iteration 1300 / 2000: loss 0.282247\n",
      "iteration 1400 / 2000: loss 0.226766\n",
      "iteration 1500 / 2000: loss 0.188066\n",
      "iteration 1600 / 2000: loss 0.230338\n",
      "iteration 1700 / 2000: loss 0.222168\n",
      "iteration 1800 / 2000: loss 0.198770\n",
      "iteration 1900 / 2000: loss 0.173033\n",
      "Hidden Size: 40, Learning Rate: 0.0075000250000000004, LR Decay: 0.3, Regularization: 0.2, Validation accuracy: 0.9446\n",
      "iteration 0 / 2000: loss 2.302621\n",
      "iteration 100 / 2000: loss 0.332561\n",
      "iteration 200 / 2000: loss 0.347739\n",
      "iteration 300 / 2000: loss 0.190609\n",
      "iteration 400 / 2000: loss 0.369449\n",
      "iteration 500 / 2000: loss 0.266315\n",
      "iteration 600 / 2000: loss 0.252947\n",
      "iteration 700 / 2000: loss 0.319868\n",
      "iteration 800 / 2000: loss 0.232395\n",
      "iteration 900 / 2000: loss 0.270905\n",
      "iteration 1000 / 2000: loss 0.208851\n",
      "iteration 1100 / 2000: loss 0.180095\n",
      "iteration 1200 / 2000: loss 0.296573\n",
      "iteration 1300 / 2000: loss 0.232508\n",
      "iteration 1400 / 2000: loss 0.218688\n",
      "iteration 1500 / 2000: loss 0.259046\n",
      "iteration 1600 / 2000: loss 0.219703\n",
      "iteration 1700 / 2000: loss 0.289297\n",
      "iteration 1800 / 2000: loss 0.311179\n",
      "iteration 1900 / 2000: loss 0.310822\n",
      "Hidden Size: 40, Learning Rate: 0.0075000250000000004, LR Decay: 0.3, Regularization: 0.30000000000000004, Validation accuracy: 0.947\n",
      "iteration 0 / 2000: loss 2.302637\n",
      "iteration 100 / 2000: loss 0.388651\n",
      "iteration 200 / 2000: loss 0.353724\n",
      "iteration 300 / 2000: loss 0.263777\n",
      "iteration 400 / 2000: loss 0.382311\n",
      "iteration 500 / 2000: loss 0.394172\n",
      "iteration 600 / 2000: loss 0.290727\n",
      "iteration 700 / 2000: loss 0.299053\n",
      "iteration 800 / 2000: loss 0.334727\n",
      "iteration 900 / 2000: loss 0.250685\n",
      "iteration 1000 / 2000: loss 0.325869\n",
      "iteration 1100 / 2000: loss 0.284973\n",
      "iteration 1200 / 2000: loss 0.312050\n",
      "iteration 1300 / 2000: loss 0.277940\n",
      "iteration 1400 / 2000: loss 0.342054\n",
      "iteration 1500 / 2000: loss 0.284118\n",
      "iteration 1600 / 2000: loss 0.327602\n",
      "iteration 1700 / 2000: loss 0.249125\n",
      "iteration 1800 / 2000: loss 0.242868\n",
      "iteration 1900 / 2000: loss 0.270374\n",
      "Hidden Size: 40, Learning Rate: 0.0075000250000000004, LR Decay: 0.3, Regularization: 0.4, Validation accuracy: 0.9438\n",
      "iteration 0 / 2000: loss 2.302601\n",
      "iteration 100 / 2000: loss 0.316758\n",
      "iteration 200 / 2000: loss 0.173871\n",
      "iteration 300 / 2000: loss 0.132639\n",
      "iteration 400 / 2000: loss 0.193360\n",
      "iteration 500 / 2000: loss 0.099527\n",
      "iteration 600 / 2000: loss 0.179828\n",
      "iteration 700 / 2000: loss 0.182994\n",
      "iteration 800 / 2000: loss 0.136734\n",
      "iteration 900 / 2000: loss 0.063013\n",
      "iteration 1000 / 2000: loss 0.143363\n",
      "iteration 1100 / 2000: loss 0.079289\n",
      "iteration 1200 / 2000: loss 0.152606\n",
      "iteration 1300 / 2000: loss 0.047461\n",
      "iteration 1400 / 2000: loss 0.056896\n",
      "iteration 1500 / 2000: loss 0.051982\n",
      "iteration 1600 / 2000: loss 0.103766\n",
      "iteration 1700 / 2000: loss 0.085865\n",
      "iteration 1800 / 2000: loss 0.036696\n",
      "iteration 1900 / 2000: loss 0.062099\n",
      "Hidden Size: 40, Learning Rate: 0.00500005, LR Decay: 0.99, Regularization: 0.0, Validation accuracy: 0.971\n",
      "iteration 0 / 2000: loss 2.302595\n",
      "iteration 100 / 2000: loss 0.289560\n",
      "iteration 200 / 2000: loss 0.223552\n",
      "iteration 300 / 2000: loss 0.134472\n",
      "iteration 400 / 2000: loss 0.139785\n",
      "iteration 500 / 2000: loss 0.158403\n",
      "iteration 600 / 2000: loss 0.109529\n",
      "iteration 700 / 2000: loss 0.193418\n",
      "iteration 800 / 2000: loss 0.190729\n",
      "iteration 900 / 2000: loss 0.127979\n",
      "iteration 1000 / 2000: loss 0.147538\n",
      "iteration 1100 / 2000: loss 0.149042\n",
      "iteration 1200 / 2000: loss 0.108863\n",
      "iteration 1300 / 2000: loss 0.110508\n",
      "iteration 1400 / 2000: loss 0.201724\n",
      "iteration 1500 / 2000: loss 0.176518\n",
      "iteration 1600 / 2000: loss 0.130182\n",
      "iteration 1700 / 2000: loss 0.118147\n",
      "iteration 1800 / 2000: loss 0.093311\n",
      "iteration 1900 / 2000: loss 0.100242\n",
      "Hidden Size: 40, Learning Rate: 0.00500005, LR Decay: 0.99, Regularization: 0.1, Validation accuracy: 0.9722\n",
      "iteration 0 / 2000: loss 2.302588\n",
      "iteration 100 / 2000: loss 0.349275\n",
      "iteration 200 / 2000: loss 0.202236\n",
      "iteration 300 / 2000: loss 0.223502\n",
      "iteration 400 / 2000: loss 0.230928\n",
      "iteration 500 / 2000: loss 0.277159\n",
      "iteration 600 / 2000: loss 0.151604\n",
      "iteration 700 / 2000: loss 0.170719\n",
      "iteration 800 / 2000: loss 0.164845\n",
      "iteration 900 / 2000: loss 0.189081\n",
      "iteration 1000 / 2000: loss 0.156716\n",
      "iteration 1100 / 2000: loss 0.143138\n",
      "iteration 1200 / 2000: loss 0.166532\n",
      "iteration 1300 / 2000: loss 0.130594\n",
      "iteration 1400 / 2000: loss 0.137728\n",
      "iteration 1500 / 2000: loss 0.173645\n",
      "iteration 1600 / 2000: loss 0.124564\n",
      "iteration 1700 / 2000: loss 0.161256\n",
      "iteration 1800 / 2000: loss 0.125307\n",
      "iteration 1900 / 2000: loss 0.137735\n",
      "Hidden Size: 40, Learning Rate: 0.00500005, LR Decay: 0.99, Regularization: 0.2, Validation accuracy: 0.9722\n",
      "iteration 0 / 2000: loss 2.302633\n",
      "iteration 100 / 2000: loss 0.399718\n",
      "iteration 200 / 2000: loss 0.281506\n",
      "iteration 300 / 2000: loss 0.262754\n",
      "iteration 400 / 2000: loss 0.269520\n",
      "iteration 500 / 2000: loss 0.292167\n",
      "iteration 600 / 2000: loss 0.266565\n",
      "iteration 700 / 2000: loss 0.267931\n",
      "iteration 800 / 2000: loss 0.199767\n",
      "iteration 900 / 2000: loss 0.193883\n",
      "iteration 1000 / 2000: loss 0.188768\n",
      "iteration 1100 / 2000: loss 0.212124\n",
      "iteration 1200 / 2000: loss 0.171796\n",
      "iteration 1300 / 2000: loss 0.168108\n",
      "iteration 1400 / 2000: loss 0.153857\n",
      "iteration 1500 / 2000: loss 0.174759\n",
      "iteration 1600 / 2000: loss 0.250629\n",
      "iteration 1700 / 2000: loss 0.200294\n",
      "iteration 1800 / 2000: loss 0.190008\n",
      "iteration 1900 / 2000: loss 0.179184\n",
      "Hidden Size: 40, Learning Rate: 0.00500005, LR Decay: 0.99, Regularization: 0.30000000000000004, Validation accuracy: 0.97\n",
      "iteration 0 / 2000: loss 2.302653\n",
      "iteration 100 / 2000: loss 0.403756\n",
      "iteration 200 / 2000: loss 0.299200\n",
      "iteration 300 / 2000: loss 0.259938\n",
      "iteration 400 / 2000: loss 0.247972\n",
      "iteration 500 / 2000: loss 0.216365\n",
      "iteration 600 / 2000: loss 0.247078\n",
      "iteration 700 / 2000: loss 0.195134\n",
      "iteration 800 / 2000: loss 0.222923\n",
      "iteration 900 / 2000: loss 0.246339\n",
      "iteration 1000 / 2000: loss 0.240759\n",
      "iteration 1100 / 2000: loss 0.209822\n",
      "iteration 1200 / 2000: loss 0.259839\n",
      "iteration 1300 / 2000: loss 0.181923\n",
      "iteration 1400 / 2000: loss 0.244196\n",
      "iteration 1500 / 2000: loss 0.180337\n",
      "iteration 1600 / 2000: loss 0.283231\n",
      "iteration 1700 / 2000: loss 0.207080\n",
      "iteration 1800 / 2000: loss 0.229093\n",
      "iteration 1900 / 2000: loss 0.224596\n",
      "Hidden Size: 40, Learning Rate: 0.00500005, LR Decay: 0.99, Regularization: 0.4, Validation accuracy: 0.9664\n",
      "iteration 0 / 2000: loss 2.302586\n",
      "iteration 100 / 2000: loss 0.347883\n",
      "iteration 200 / 2000: loss 0.196285\n",
      "iteration 300 / 2000: loss 0.187195\n",
      "iteration 400 / 2000: loss 0.196168\n",
      "iteration 500 / 2000: loss 0.109751\n",
      "iteration 600 / 2000: loss 0.209742\n",
      "iteration 700 / 2000: loss 0.083620\n",
      "iteration 800 / 2000: loss 0.095798\n",
      "iteration 900 / 2000: loss 0.060266\n",
      "iteration 1000 / 2000: loss 0.066576\n",
      "iteration 1100 / 2000: loss 0.088215\n",
      "iteration 1200 / 2000: loss 0.035113\n",
      "iteration 1300 / 2000: loss 0.061640\n",
      "iteration 1400 / 2000: loss 0.092277\n",
      "iteration 1500 / 2000: loss 0.053896\n",
      "iteration 1600 / 2000: loss 0.085025\n",
      "iteration 1700 / 2000: loss 0.091722\n",
      "iteration 1800 / 2000: loss 0.015678\n",
      "iteration 1900 / 2000: loss 0.075488\n",
      "Hidden Size: 40, Learning Rate: 0.00500005, LR Decay: 0.8175, Regularization: 0.0, Validation accuracy: 0.969\n",
      "iteration 0 / 2000: loss 2.302606\n",
      "iteration 100 / 2000: loss 0.314886\n",
      "iteration 200 / 2000: loss 0.236630\n",
      "iteration 300 / 2000: loss 0.304269\n",
      "iteration 400 / 2000: loss 0.169659\n",
      "iteration 500 / 2000: loss 0.209884\n",
      "iteration 600 / 2000: loss 0.139977\n",
      "iteration 700 / 2000: loss 0.163819\n",
      "iteration 800 / 2000: loss 0.149681\n",
      "iteration 900 / 2000: loss 0.130172\n",
      "iteration 1000 / 2000: loss 0.135208\n",
      "iteration 1100 / 2000: loss 0.188891\n",
      "iteration 1200 / 2000: loss 0.094947\n",
      "iteration 1300 / 2000: loss 0.093777\n",
      "iteration 1400 / 2000: loss 0.089893\n",
      "iteration 1500 / 2000: loss 0.098453\n",
      "iteration 1600 / 2000: loss 0.107991\n",
      "iteration 1700 / 2000: loss 0.112390\n",
      "iteration 1800 / 2000: loss 0.126602\n",
      "iteration 1900 / 2000: loss 0.168533\n",
      "Hidden Size: 40, Learning Rate: 0.00500005, LR Decay: 0.8175, Regularization: 0.1, Validation accuracy: 0.9702\n",
      "iteration 0 / 2000: loss 2.302612\n",
      "iteration 100 / 2000: loss 0.242483\n",
      "iteration 200 / 2000: loss 0.262570\n",
      "iteration 300 / 2000: loss 0.167813\n",
      "iteration 400 / 2000: loss 0.191569\n",
      "iteration 500 / 2000: loss 0.184542\n",
      "iteration 600 / 2000: loss 0.205532\n",
      "iteration 700 / 2000: loss 0.163124\n",
      "iteration 800 / 2000: loss 0.194990\n",
      "iteration 900 / 2000: loss 0.197369\n",
      "iteration 1000 / 2000: loss 0.191666\n",
      "iteration 1100 / 2000: loss 0.149494\n",
      "iteration 1200 / 2000: loss 0.180781\n",
      "iteration 1300 / 2000: loss 0.192666\n",
      "iteration 1400 / 2000: loss 0.139899\n",
      "iteration 1500 / 2000: loss 0.158040\n",
      "iteration 1600 / 2000: loss 0.127158\n",
      "iteration 1700 / 2000: loss 0.197244\n",
      "iteration 1800 / 2000: loss 0.115958\n",
      "iteration 1900 / 2000: loss 0.156354\n",
      "Hidden Size: 40, Learning Rate: 0.00500005, LR Decay: 0.8175, Regularization: 0.2, Validation accuracy: 0.9702\n",
      "iteration 0 / 2000: loss 2.302636\n",
      "iteration 100 / 2000: loss 0.378967\n",
      "iteration 200 / 2000: loss 0.273308\n",
      "iteration 300 / 2000: loss 0.227352\n",
      "iteration 400 / 2000: loss 0.283377\n",
      "iteration 500 / 2000: loss 0.230279\n",
      "iteration 600 / 2000: loss 0.259750\n",
      "iteration 700 / 2000: loss 0.253919\n",
      "iteration 800 / 2000: loss 0.187225\n",
      "iteration 900 / 2000: loss 0.226622\n",
      "iteration 1000 / 2000: loss 0.223343\n",
      "iteration 1100 / 2000: loss 0.151853\n",
      "iteration 1200 / 2000: loss 0.142894\n",
      "iteration 1300 / 2000: loss 0.185562\n",
      "iteration 1400 / 2000: loss 0.228801\n",
      "iteration 1500 / 2000: loss 0.166649\n",
      "iteration 1600 / 2000: loss 0.181293\n",
      "iteration 1700 / 2000: loss 0.205580\n",
      "iteration 1800 / 2000: loss 0.220432\n",
      "iteration 1900 / 2000: loss 0.189007\n",
      "Hidden Size: 40, Learning Rate: 0.00500005, LR Decay: 0.8175, Regularization: 0.30000000000000004, Validation accuracy: 0.968\n",
      "iteration 0 / 2000: loss 2.302639\n",
      "iteration 100 / 2000: loss 0.316253\n",
      "iteration 200 / 2000: loss 0.280424\n",
      "iteration 300 / 2000: loss 0.262656\n",
      "iteration 400 / 2000: loss 0.224182\n",
      "iteration 500 / 2000: loss 0.217239\n",
      "iteration 600 / 2000: loss 0.243004\n",
      "iteration 700 / 2000: loss 0.200747\n",
      "iteration 800 / 2000: loss 0.294773\n",
      "iteration 900 / 2000: loss 0.242576\n",
      "iteration 1000 / 2000: loss 0.190547\n",
      "iteration 1100 / 2000: loss 0.259484\n",
      "iteration 1200 / 2000: loss 0.206449\n",
      "iteration 1300 / 2000: loss 0.208583\n",
      "iteration 1400 / 2000: loss 0.210862\n",
      "iteration 1500 / 2000: loss 0.229081\n",
      "iteration 1600 / 2000: loss 0.184654\n",
      "iteration 1700 / 2000: loss 0.204219\n",
      "iteration 1800 / 2000: loss 0.195491\n",
      "iteration 1900 / 2000: loss 0.237950\n",
      "Hidden Size: 40, Learning Rate: 0.00500005, LR Decay: 0.8175, Regularization: 0.4, Validation accuracy: 0.9644\n",
      "iteration 0 / 2000: loss 2.302583\n",
      "iteration 100 / 2000: loss 0.345959\n",
      "iteration 200 / 2000: loss 0.203642\n",
      "iteration 300 / 2000: loss 0.198250\n",
      "iteration 400 / 2000: loss 0.256554\n",
      "iteration 500 / 2000: loss 0.150218\n",
      "iteration 600 / 2000: loss 0.163478\n",
      "iteration 700 / 2000: loss 0.106141\n",
      "iteration 800 / 2000: loss 0.146027\n",
      "iteration 900 / 2000: loss 0.243620\n",
      "iteration 1000 / 2000: loss 0.156703\n",
      "iteration 1100 / 2000: loss 0.120757\n",
      "iteration 1200 / 2000: loss 0.078797\n",
      "iteration 1300 / 2000: loss 0.052438\n",
      "iteration 1400 / 2000: loss 0.142757\n",
      "iteration 1500 / 2000: loss 0.188170\n",
      "iteration 1600 / 2000: loss 0.162055\n",
      "iteration 1700 / 2000: loss 0.122895\n",
      "iteration 1800 / 2000: loss 0.086640\n",
      "iteration 1900 / 2000: loss 0.061006\n",
      "Hidden Size: 40, Learning Rate: 0.00500005, LR Decay: 0.645, Regularization: 0.0, Validation accuracy: 0.9648\n",
      "iteration 0 / 2000: loss 2.302599\n",
      "iteration 100 / 2000: loss 0.336687\n",
      "iteration 200 / 2000: loss 0.415019\n",
      "iteration 300 / 2000: loss 0.247492\n",
      "iteration 400 / 2000: loss 0.182525\n",
      "iteration 500 / 2000: loss 0.214777\n",
      "iteration 600 / 2000: loss 0.153201\n",
      "iteration 700 / 2000: loss 0.139170\n",
      "iteration 800 / 2000: loss 0.211785\n",
      "iteration 900 / 2000: loss 0.168001\n",
      "iteration 1000 / 2000: loss 0.138963\n",
      "iteration 1100 / 2000: loss 0.189101\n",
      "iteration 1200 / 2000: loss 0.202845\n",
      "iteration 1300 / 2000: loss 0.175861\n",
      "iteration 1400 / 2000: loss 0.118583\n",
      "iteration 1500 / 2000: loss 0.121702\n",
      "iteration 1600 / 2000: loss 0.217810\n",
      "iteration 1700 / 2000: loss 0.147937\n",
      "iteration 1800 / 2000: loss 0.111377\n",
      "iteration 1900 / 2000: loss 0.153397\n",
      "Hidden Size: 40, Learning Rate: 0.00500005, LR Decay: 0.645, Regularization: 0.1, Validation accuracy: 0.9646\n",
      "iteration 0 / 2000: loss 2.302619\n",
      "iteration 100 / 2000: loss 0.293399\n",
      "iteration 200 / 2000: loss 0.387733\n",
      "iteration 300 / 2000: loss 0.324103\n",
      "iteration 400 / 2000: loss 0.211585\n",
      "iteration 500 / 2000: loss 0.184412\n",
      "iteration 600 / 2000: loss 0.185882\n",
      "iteration 700 / 2000: loss 0.180867\n",
      "iteration 800 / 2000: loss 0.234924\n",
      "iteration 900 / 2000: loss 0.205082\n",
      "iteration 1000 / 2000: loss 0.218186\n",
      "iteration 1100 / 2000: loss 0.163628\n",
      "iteration 1200 / 2000: loss 0.189438\n",
      "iteration 1300 / 2000: loss 0.184461\n",
      "iteration 1400 / 2000: loss 0.240781\n",
      "iteration 1500 / 2000: loss 0.171409\n",
      "iteration 1600 / 2000: loss 0.165152\n",
      "iteration 1700 / 2000: loss 0.134482\n",
      "iteration 1800 / 2000: loss 0.163475\n",
      "iteration 1900 / 2000: loss 0.193857\n",
      "Hidden Size: 40, Learning Rate: 0.00500005, LR Decay: 0.645, Regularization: 0.2, Validation accuracy: 0.9628\n",
      "iteration 0 / 2000: loss 2.302649\n",
      "iteration 100 / 2000: loss 0.302343\n",
      "iteration 200 / 2000: loss 0.417209\n",
      "iteration 300 / 2000: loss 0.267283\n",
      "iteration 400 / 2000: loss 0.288790\n",
      "iteration 500 / 2000: loss 0.230706\n",
      "iteration 600 / 2000: loss 0.236541\n",
      "iteration 700 / 2000: loss 0.243844\n",
      "iteration 800 / 2000: loss 0.230549\n",
      "iteration 900 / 2000: loss 0.244084\n",
      "iteration 1000 / 2000: loss 0.168476\n",
      "iteration 1100 / 2000: loss 0.221055\n",
      "iteration 1200 / 2000: loss 0.209581\n",
      "iteration 1300 / 2000: loss 0.157612\n",
      "iteration 1400 / 2000: loss 0.208357\n",
      "iteration 1500 / 2000: loss 0.266850\n",
      "iteration 1600 / 2000: loss 0.253447\n",
      "iteration 1700 / 2000: loss 0.229086\n",
      "iteration 1800 / 2000: loss 0.199767\n",
      "iteration 1900 / 2000: loss 0.215760\n",
      "Hidden Size: 40, Learning Rate: 0.00500005, LR Decay: 0.645, Regularization: 0.30000000000000004, Validation accuracy: 0.961\n",
      "iteration 0 / 2000: loss 2.302658\n",
      "iteration 100 / 2000: loss 0.449652\n",
      "iteration 200 / 2000: loss 0.289221\n",
      "iteration 300 / 2000: loss 0.280544\n",
      "iteration 400 / 2000: loss 0.253638\n",
      "iteration 500 / 2000: loss 0.252510\n",
      "iteration 600 / 2000: loss 0.260057\n",
      "iteration 700 / 2000: loss 0.259963\n",
      "iteration 800 / 2000: loss 0.281141\n",
      "iteration 900 / 2000: loss 0.230082\n",
      "iteration 1000 / 2000: loss 0.264562\n",
      "iteration 1100 / 2000: loss 0.204453\n",
      "iteration 1200 / 2000: loss 0.235267\n",
      "iteration 1300 / 2000: loss 0.184657\n",
      "iteration 1400 / 2000: loss 0.253681\n",
      "iteration 1500 / 2000: loss 0.239986\n",
      "iteration 1600 / 2000: loss 0.240029\n",
      "iteration 1700 / 2000: loss 0.205558\n",
      "iteration 1800 / 2000: loss 0.222240\n",
      "iteration 1900 / 2000: loss 0.229346\n",
      "Hidden Size: 40, Learning Rate: 0.00500005, LR Decay: 0.645, Regularization: 0.4, Validation accuracy: 0.961\n",
      "iteration 0 / 2000: loss 2.302592\n",
      "iteration 100 / 2000: loss 0.410490\n",
      "iteration 200 / 2000: loss 0.239052\n",
      "iteration 300 / 2000: loss 0.258170\n",
      "iteration 400 / 2000: loss 0.155320\n",
      "iteration 500 / 2000: loss 0.246231\n",
      "iteration 600 / 2000: loss 0.169325\n",
      "iteration 700 / 2000: loss 0.128165\n",
      "iteration 800 / 2000: loss 0.197799\n",
      "iteration 900 / 2000: loss 0.133511\n",
      "iteration 1000 / 2000: loss 0.179460\n",
      "iteration 1100 / 2000: loss 0.180306\n",
      "iteration 1200 / 2000: loss 0.131561\n",
      "iteration 1300 / 2000: loss 0.115036\n",
      "iteration 1400 / 2000: loss 0.095995\n",
      "iteration 1500 / 2000: loss 0.180774\n",
      "iteration 1600 / 2000: loss 0.140860\n",
      "iteration 1700 / 2000: loss 0.146562\n",
      "iteration 1800 / 2000: loss 0.153774\n",
      "iteration 1900 / 2000: loss 0.155902\n",
      "Hidden Size: 40, Learning Rate: 0.00500005, LR Decay: 0.47250000000000003, Regularization: 0.0, Validation accuracy: 0.9528\n",
      "iteration 0 / 2000: loss 2.302604\n",
      "iteration 100 / 2000: loss 0.282186\n",
      "iteration 200 / 2000: loss 0.260700\n",
      "iteration 300 / 2000: loss 0.238085\n",
      "iteration 400 / 2000: loss 0.291340\n",
      "iteration 500 / 2000: loss 0.197725\n",
      "iteration 600 / 2000: loss 0.160535\n",
      "iteration 700 / 2000: loss 0.227029\n",
      "iteration 800 / 2000: loss 0.293410\n",
      "iteration 900 / 2000: loss 0.219269\n",
      "iteration 1000 / 2000: loss 0.201922\n",
      "iteration 1100 / 2000: loss 0.163729\n",
      "iteration 1200 / 2000: loss 0.174308\n",
      "iteration 1300 / 2000: loss 0.179498\n",
      "iteration 1400 / 2000: loss 0.213742\n",
      "iteration 1500 / 2000: loss 0.269737\n",
      "iteration 1600 / 2000: loss 0.250697\n",
      "iteration 1700 / 2000: loss 0.202041\n",
      "iteration 1800 / 2000: loss 0.184573\n",
      "iteration 1900 / 2000: loss 0.158859\n",
      "Hidden Size: 40, Learning Rate: 0.00500005, LR Decay: 0.47250000000000003, Regularization: 0.1, Validation accuracy: 0.953\n",
      "iteration 0 / 2000: loss 2.302650\n",
      "iteration 100 / 2000: loss 0.309018\n",
      "iteration 200 / 2000: loss 0.269211\n",
      "iteration 300 / 2000: loss 0.259778\n",
      "iteration 400 / 2000: loss 0.233948\n",
      "iteration 500 / 2000: loss 0.211668\n",
      "iteration 600 / 2000: loss 0.262460\n",
      "iteration 700 / 2000: loss 0.316049\n",
      "iteration 800 / 2000: loss 0.186483\n",
      "iteration 900 / 2000: loss 0.265674\n",
      "iteration 1000 / 2000: loss 0.214012\n",
      "iteration 1100 / 2000: loss 0.241994\n",
      "iteration 1200 / 2000: loss 0.317678\n",
      "iteration 1300 / 2000: loss 0.307960\n",
      "iteration 1400 / 2000: loss 0.260523\n",
      "iteration 1500 / 2000: loss 0.360401\n",
      "iteration 1600 / 2000: loss 0.252629\n",
      "iteration 1700 / 2000: loss 0.193096\n",
      "iteration 1800 / 2000: loss 0.238452\n",
      "iteration 1900 / 2000: loss 0.192491\n",
      "Hidden Size: 40, Learning Rate: 0.00500005, LR Decay: 0.47250000000000003, Regularization: 0.2, Validation accuracy: 0.9478\n",
      "iteration 0 / 2000: loss 2.302639\n",
      "iteration 100 / 2000: loss 0.345642\n",
      "iteration 200 / 2000: loss 0.360400\n",
      "iteration 300 / 2000: loss 0.223727\n",
      "iteration 400 / 2000: loss 0.249644\n",
      "iteration 500 / 2000: loss 0.225260\n",
      "iteration 600 / 2000: loss 0.216075\n",
      "iteration 700 / 2000: loss 0.222020\n",
      "iteration 800 / 2000: loss 0.187974\n",
      "iteration 900 / 2000: loss 0.230659\n",
      "iteration 1000 / 2000: loss 0.303991\n",
      "iteration 1100 / 2000: loss 0.206536\n",
      "iteration 1200 / 2000: loss 0.255310\n",
      "iteration 1300 / 2000: loss 0.232077\n",
      "iteration 1400 / 2000: loss 0.265724\n",
      "iteration 1500 / 2000: loss 0.196212\n",
      "iteration 1600 / 2000: loss 0.307699\n",
      "iteration 1700 / 2000: loss 0.186521\n",
      "iteration 1800 / 2000: loss 0.217881\n",
      "iteration 1900 / 2000: loss 0.228793\n",
      "Hidden Size: 40, Learning Rate: 0.00500005, LR Decay: 0.47250000000000003, Regularization: 0.30000000000000004, Validation accuracy: 0.9508\n",
      "iteration 0 / 2000: loss 2.302658\n",
      "iteration 100 / 2000: loss 0.479302\n",
      "iteration 200 / 2000: loss 0.313732\n",
      "iteration 300 / 2000: loss 0.326176\n",
      "iteration 400 / 2000: loss 0.310147\n",
      "iteration 500 / 2000: loss 0.212591\n",
      "iteration 600 / 2000: loss 0.265973\n",
      "iteration 700 / 2000: loss 0.298060\n",
      "iteration 800 / 2000: loss 0.324638\n",
      "iteration 900 / 2000: loss 0.303280\n",
      "iteration 1000 / 2000: loss 0.207749\n",
      "iteration 1100 / 2000: loss 0.311959\n",
      "iteration 1200 / 2000: loss 0.346342\n",
      "iteration 1300 / 2000: loss 0.270184\n",
      "iteration 1400 / 2000: loss 0.291781\n",
      "iteration 1500 / 2000: loss 0.261162\n",
      "iteration 1600 / 2000: loss 0.207383\n",
      "iteration 1700 / 2000: loss 0.258480\n",
      "iteration 1800 / 2000: loss 0.244027\n",
      "iteration 1900 / 2000: loss 0.306877\n",
      "Hidden Size: 40, Learning Rate: 0.00500005, LR Decay: 0.47250000000000003, Regularization: 0.4, Validation accuracy: 0.948\n",
      "iteration 0 / 2000: loss 2.302582\n",
      "iteration 100 / 2000: loss 0.555858\n",
      "iteration 200 / 2000: loss 0.325214\n",
      "iteration 300 / 2000: loss 0.322516\n",
      "iteration 400 / 2000: loss 0.306559\n",
      "iteration 500 / 2000: loss 0.216942\n",
      "iteration 600 / 2000: loss 0.314871\n",
      "iteration 700 / 2000: loss 0.255053\n",
      "iteration 800 / 2000: loss 0.317804\n",
      "iteration 900 / 2000: loss 0.205821\n",
      "iteration 1000 / 2000: loss 0.308587\n",
      "iteration 1100 / 2000: loss 0.214059\n",
      "iteration 1200 / 2000: loss 0.168135\n",
      "iteration 1300 / 2000: loss 0.200070\n",
      "iteration 1400 / 2000: loss 0.166265\n",
      "iteration 1500 / 2000: loss 0.275991\n",
      "iteration 1600 / 2000: loss 0.234204\n",
      "iteration 1700 / 2000: loss 0.204050\n",
      "iteration 1800 / 2000: loss 0.246941\n",
      "iteration 1900 / 2000: loss 0.275458\n",
      "Hidden Size: 40, Learning Rate: 0.00500005, LR Decay: 0.3, Regularization: 0.0, Validation accuracy: 0.9324\n",
      "iteration 0 / 2000: loss 2.302605\n",
      "iteration 100 / 2000: loss 0.648129\n",
      "iteration 200 / 2000: loss 0.257156\n",
      "iteration 300 / 2000: loss 0.355743\n",
      "iteration 400 / 2000: loss 0.333162\n",
      "iteration 500 / 2000: loss 0.363745\n",
      "iteration 600 / 2000: loss 0.288373\n",
      "iteration 700 / 2000: loss 0.196740\n",
      "iteration 800 / 2000: loss 0.286351\n",
      "iteration 900 / 2000: loss 0.234306\n",
      "iteration 1000 / 2000: loss 0.289082\n",
      "iteration 1100 / 2000: loss 0.249475\n",
      "iteration 1200 / 2000: loss 0.198803\n",
      "iteration 1300 / 2000: loss 0.234611\n",
      "iteration 1400 / 2000: loss 0.232483\n",
      "iteration 1500 / 2000: loss 0.276750\n",
      "iteration 1600 / 2000: loss 0.206415\n",
      "iteration 1700 / 2000: loss 0.326738\n",
      "iteration 1800 / 2000: loss 0.201694\n",
      "iteration 1900 / 2000: loss 0.169903\n",
      "Hidden Size: 40, Learning Rate: 0.00500005, LR Decay: 0.3, Regularization: 0.1, Validation accuracy: 0.9358\n",
      "iteration 0 / 2000: loss 2.302600\n",
      "iteration 100 / 2000: loss 0.682630\n",
      "iteration 200 / 2000: loss 0.292263\n",
      "iteration 300 / 2000: loss 0.221269\n",
      "iteration 400 / 2000: loss 0.237397\n",
      "iteration 500 / 2000: loss 0.247709\n",
      "iteration 600 / 2000: loss 0.344316\n",
      "iteration 700 / 2000: loss 0.355595\n",
      "iteration 800 / 2000: loss 0.260484\n",
      "iteration 900 / 2000: loss 0.279845\n",
      "iteration 1000 / 2000: loss 0.250431\n",
      "iteration 1100 / 2000: loss 0.261751\n",
      "iteration 1200 / 2000: loss 0.209612\n",
      "iteration 1300 / 2000: loss 0.236668\n",
      "iteration 1400 / 2000: loss 0.291607\n",
      "iteration 1500 / 2000: loss 0.215841\n",
      "iteration 1600 / 2000: loss 0.211326\n",
      "iteration 1700 / 2000: loss 0.306272\n",
      "iteration 1800 / 2000: loss 0.311739\n",
      "iteration 1900 / 2000: loss 0.213222\n",
      "Hidden Size: 40, Learning Rate: 0.00500005, LR Decay: 0.3, Regularization: 0.2, Validation accuracy: 0.9306\n",
      "iteration 0 / 2000: loss 2.302645\n",
      "iteration 100 / 2000: loss 0.621079\n",
      "iteration 200 / 2000: loss 0.416570\n",
      "iteration 300 / 2000: loss 0.391489\n",
      "iteration 400 / 2000: loss 0.344044\n",
      "iteration 500 / 2000: loss 0.321488\n",
      "iteration 600 / 2000: loss 0.246449\n",
      "iteration 700 / 2000: loss 0.292379\n",
      "iteration 800 / 2000: loss 0.386181\n",
      "iteration 900 / 2000: loss 0.302054\n",
      "iteration 1000 / 2000: loss 0.230468\n",
      "iteration 1100 / 2000: loss 0.287405\n",
      "iteration 1200 / 2000: loss 0.246971\n",
      "iteration 1300 / 2000: loss 0.298031\n",
      "iteration 1400 / 2000: loss 0.271305\n",
      "iteration 1500 / 2000: loss 0.215641\n",
      "iteration 1600 / 2000: loss 0.398170\n",
      "iteration 1700 / 2000: loss 0.254066\n",
      "iteration 1800 / 2000: loss 0.294457\n",
      "iteration 1900 / 2000: loss 0.296965\n",
      "Hidden Size: 40, Learning Rate: 0.00500005, LR Decay: 0.3, Regularization: 0.30000000000000004, Validation accuracy: 0.9326\n",
      "iteration 0 / 2000: loss 2.302643\n",
      "iteration 100 / 2000: loss 0.398841\n",
      "iteration 200 / 2000: loss 0.312856\n",
      "iteration 300 / 2000: loss 0.334730\n",
      "iteration 400 / 2000: loss 0.282315\n",
      "iteration 500 / 2000: loss 0.330541\n",
      "iteration 600 / 2000: loss 0.297267\n",
      "iteration 700 / 2000: loss 0.307723\n",
      "iteration 800 / 2000: loss 0.309143\n",
      "iteration 900 / 2000: loss 0.206397\n",
      "iteration 1000 / 2000: loss 0.296339\n",
      "iteration 1100 / 2000: loss 0.266103\n",
      "iteration 1200 / 2000: loss 0.273351\n",
      "iteration 1300 / 2000: loss 0.286662\n",
      "iteration 1400 / 2000: loss 0.186761\n",
      "iteration 1500 / 2000: loss 0.403941\n",
      "iteration 1600 / 2000: loss 0.273507\n",
      "iteration 1700 / 2000: loss 0.276620\n",
      "iteration 1800 / 2000: loss 0.287161\n",
      "iteration 1900 / 2000: loss 0.285814\n",
      "Hidden Size: 40, Learning Rate: 0.00500005, LR Decay: 0.3, Regularization: 0.4, Validation accuracy: 0.9336\n",
      "iteration 0 / 2000: loss 2.302586\n",
      "iteration 100 / 2000: loss 0.367782\n",
      "iteration 200 / 2000: loss 0.197055\n",
      "iteration 300 / 2000: loss 0.168198\n",
      "iteration 400 / 2000: loss 0.209576\n",
      "iteration 500 / 2000: loss 0.187201\n",
      "iteration 600 / 2000: loss 0.174493\n",
      "iteration 700 / 2000: loss 0.135300\n",
      "iteration 800 / 2000: loss 0.173691\n",
      "iteration 900 / 2000: loss 0.138536\n",
      "iteration 1000 / 2000: loss 0.082139\n",
      "iteration 1100 / 2000: loss 0.144247\n",
      "iteration 1200 / 2000: loss 0.160966\n",
      "iteration 1300 / 2000: loss 0.113154\n",
      "iteration 1400 / 2000: loss 0.164067\n",
      "iteration 1500 / 2000: loss 0.097723\n",
      "iteration 1600 / 2000: loss 0.135197\n",
      "iteration 1700 / 2000: loss 0.117902\n",
      "iteration 1800 / 2000: loss 0.055235\n",
      "iteration 1900 / 2000: loss 0.152803\n",
      "Hidden Size: 40, Learning Rate: 0.002500074999999999, LR Decay: 0.99, Regularization: 0.0, Validation accuracy: 0.9706\n",
      "iteration 0 / 2000: loss 2.302594\n",
      "iteration 100 / 2000: loss 0.347231\n",
      "iteration 200 / 2000: loss 0.320587\n",
      "iteration 300 / 2000: loss 0.206478\n",
      "iteration 400 / 2000: loss 0.258811\n",
      "iteration 500 / 2000: loss 0.177794\n",
      "iteration 600 / 2000: loss 0.201729\n",
      "iteration 700 / 2000: loss 0.177789\n",
      "iteration 800 / 2000: loss 0.153897\n",
      "iteration 900 / 2000: loss 0.153755\n",
      "iteration 1000 / 2000: loss 0.115084\n",
      "iteration 1100 / 2000: loss 0.132050\n",
      "iteration 1200 / 2000: loss 0.147929\n",
      "iteration 1300 / 2000: loss 0.108371\n",
      "iteration 1400 / 2000: loss 0.140141\n",
      "iteration 1500 / 2000: loss 0.131022\n",
      "iteration 1600 / 2000: loss 0.113898\n",
      "iteration 1700 / 2000: loss 0.098280\n",
      "iteration 1800 / 2000: loss 0.123719\n",
      "iteration 1900 / 2000: loss 0.098212\n",
      "Hidden Size: 40, Learning Rate: 0.002500074999999999, LR Decay: 0.99, Regularization: 0.1, Validation accuracy: 0.9696\n",
      "iteration 0 / 2000: loss 2.302616\n",
      "iteration 100 / 2000: loss 0.300774\n",
      "iteration 200 / 2000: loss 0.293093\n",
      "iteration 300 / 2000: loss 0.308487\n",
      "iteration 400 / 2000: loss 0.226995\n",
      "iteration 500 / 2000: loss 0.248271\n",
      "iteration 600 / 2000: loss 0.162309\n",
      "iteration 700 / 2000: loss 0.143712\n",
      "iteration 800 / 2000: loss 0.273678\n",
      "iteration 900 / 2000: loss 0.150467\n",
      "iteration 1000 / 2000: loss 0.116483\n",
      "iteration 1100 / 2000: loss 0.258994\n",
      "iteration 1200 / 2000: loss 0.170235\n",
      "iteration 1300 / 2000: loss 0.209764\n",
      "iteration 1400 / 2000: loss 0.182606\n",
      "iteration 1500 / 2000: loss 0.145747\n",
      "iteration 1600 / 2000: loss 0.181388\n",
      "iteration 1700 / 2000: loss 0.145544\n",
      "iteration 1800 / 2000: loss 0.162232\n",
      "iteration 1900 / 2000: loss 0.121627\n",
      "Hidden Size: 40, Learning Rate: 0.002500074999999999, LR Decay: 0.99, Regularization: 0.2, Validation accuracy: 0.9708\n",
      "iteration 0 / 2000: loss 2.302648\n",
      "iteration 100 / 2000: loss 0.424343\n",
      "iteration 200 / 2000: loss 0.284576\n",
      "iteration 300 / 2000: loss 0.354570\n",
      "iteration 400 / 2000: loss 0.256723\n",
      "iteration 500 / 2000: loss 0.237601\n",
      "iteration 600 / 2000: loss 0.270048\n",
      "iteration 700 / 2000: loss 0.292572\n",
      "iteration 800 / 2000: loss 0.242167\n",
      "iteration 900 / 2000: loss 0.206427\n",
      "iteration 1000 / 2000: loss 0.168105\n",
      "iteration 1100 / 2000: loss 0.178675\n",
      "iteration 1200 / 2000: loss 0.223775\n",
      "iteration 1300 / 2000: loss 0.220255\n",
      "iteration 1400 / 2000: loss 0.202958\n",
      "iteration 1500 / 2000: loss 0.273525\n",
      "iteration 1600 / 2000: loss 0.191056\n",
      "iteration 1700 / 2000: loss 0.213362\n",
      "iteration 1800 / 2000: loss 0.208503\n",
      "iteration 1900 / 2000: loss 0.197513\n",
      "Hidden Size: 40, Learning Rate: 0.002500074999999999, LR Decay: 0.99, Regularization: 0.30000000000000004, Validation accuracy: 0.9678\n",
      "iteration 0 / 2000: loss 2.302649\n",
      "iteration 100 / 2000: loss 0.483326\n",
      "iteration 200 / 2000: loss 0.374320\n",
      "iteration 300 / 2000: loss 0.332871\n",
      "iteration 400 / 2000: loss 0.239275\n",
      "iteration 500 / 2000: loss 0.283962\n",
      "iteration 600 / 2000: loss 0.252890\n",
      "iteration 700 / 2000: loss 0.239176\n",
      "iteration 800 / 2000: loss 0.333853\n",
      "iteration 900 / 2000: loss 0.163329\n",
      "iteration 1000 / 2000: loss 0.256842\n",
      "iteration 1100 / 2000: loss 0.299928\n",
      "iteration 1200 / 2000: loss 0.220716\n",
      "iteration 1300 / 2000: loss 0.243463\n",
      "iteration 1400 / 2000: loss 0.186264\n",
      "iteration 1500 / 2000: loss 0.229616\n",
      "iteration 1600 / 2000: loss 0.209622\n",
      "iteration 1700 / 2000: loss 0.230529\n",
      "iteration 1800 / 2000: loss 0.212104\n",
      "iteration 1900 / 2000: loss 0.225718\n",
      "Hidden Size: 40, Learning Rate: 0.002500074999999999, LR Decay: 0.99, Regularization: 0.4, Validation accuracy: 0.9654\n",
      "iteration 0 / 2000: loss 2.302592\n",
      "iteration 100 / 2000: loss 0.370842\n",
      "iteration 200 / 2000: loss 0.224201\n",
      "iteration 300 / 2000: loss 0.173659\n",
      "iteration 400 / 2000: loss 0.294463\n",
      "iteration 500 / 2000: loss 0.166240\n",
      "iteration 600 / 2000: loss 0.148031\n",
      "iteration 700 / 2000: loss 0.145607\n",
      "iteration 800 / 2000: loss 0.170151\n",
      "iteration 900 / 2000: loss 0.127731\n",
      "iteration 1000 / 2000: loss 0.083709\n",
      "iteration 1100 / 2000: loss 0.099506\n",
      "iteration 1200 / 2000: loss 0.144089\n",
      "iteration 1300 / 2000: loss 0.141596\n",
      "iteration 1400 / 2000: loss 0.083530\n",
      "iteration 1500 / 2000: loss 0.155630\n",
      "iteration 1600 / 2000: loss 0.129828\n",
      "iteration 1700 / 2000: loss 0.178162\n",
      "iteration 1800 / 2000: loss 0.111557\n",
      "iteration 1900 / 2000: loss 0.117791\n",
      "Hidden Size: 40, Learning Rate: 0.002500074999999999, LR Decay: 0.8175, Regularization: 0.0, Validation accuracy: 0.9632\n",
      "iteration 0 / 2000: loss 2.302598\n",
      "iteration 100 / 2000: loss 0.340301\n",
      "iteration 200 / 2000: loss 0.397794\n",
      "iteration 300 / 2000: loss 0.296382\n",
      "iteration 400 / 2000: loss 0.256305\n",
      "iteration 500 / 2000: loss 0.170829\n",
      "iteration 600 / 2000: loss 0.144752\n",
      "iteration 700 / 2000: loss 0.145060\n",
      "iteration 800 / 2000: loss 0.239897\n",
      "iteration 900 / 2000: loss 0.133476\n",
      "iteration 1000 / 2000: loss 0.142305\n",
      "iteration 1100 / 2000: loss 0.132438\n",
      "iteration 1200 / 2000: loss 0.197798\n",
      "iteration 1300 / 2000: loss 0.147825\n",
      "iteration 1400 / 2000: loss 0.212828\n",
      "iteration 1500 / 2000: loss 0.193095\n",
      "iteration 1600 / 2000: loss 0.187481\n",
      "iteration 1700 / 2000: loss 0.158645\n",
      "iteration 1800 / 2000: loss 0.150326\n",
      "iteration 1900 / 2000: loss 0.124006\n",
      "Hidden Size: 40, Learning Rate: 0.002500074999999999, LR Decay: 0.8175, Regularization: 0.1, Validation accuracy: 0.9622\n",
      "iteration 0 / 2000: loss 2.302591\n",
      "iteration 100 / 2000: loss 0.384402\n",
      "iteration 200 / 2000: loss 0.345438\n",
      "iteration 300 / 2000: loss 0.295432\n",
      "iteration 400 / 2000: loss 0.209127\n",
      "iteration 500 / 2000: loss 0.275236\n",
      "iteration 600 / 2000: loss 0.315409\n",
      "iteration 700 / 2000: loss 0.213343\n",
      "iteration 800 / 2000: loss 0.255336\n",
      "iteration 900 / 2000: loss 0.159869\n",
      "iteration 1000 / 2000: loss 0.204006\n",
      "iteration 1100 / 2000: loss 0.144680\n",
      "iteration 1200 / 2000: loss 0.182333\n",
      "iteration 1300 / 2000: loss 0.137858\n",
      "iteration 1400 / 2000: loss 0.227954\n",
      "iteration 1500 / 2000: loss 0.183229\n",
      "iteration 1600 / 2000: loss 0.164096\n",
      "iteration 1700 / 2000: loss 0.172624\n",
      "iteration 1800 / 2000: loss 0.166852\n",
      "iteration 1900 / 2000: loss 0.156953\n",
      "Hidden Size: 40, Learning Rate: 0.002500074999999999, LR Decay: 0.8175, Regularization: 0.2, Validation accuracy: 0.9636\n",
      "iteration 0 / 2000: loss 2.302626\n",
      "iteration 100 / 2000: loss 0.409031\n",
      "iteration 200 / 2000: loss 0.283334\n",
      "iteration 300 / 2000: loss 0.235414\n",
      "iteration 400 / 2000: loss 0.318948\n",
      "iteration 500 / 2000: loss 0.283959\n",
      "iteration 600 / 2000: loss 0.217007\n",
      "iteration 700 / 2000: loss 0.252313\n",
      "iteration 800 / 2000: loss 0.202201\n",
      "iteration 900 / 2000: loss 0.194466\n",
      "iteration 1000 / 2000: loss 0.197911\n",
      "iteration 1100 / 2000: loss 0.190969\n",
      "iteration 1200 / 2000: loss 0.219995\n",
      "iteration 1300 / 2000: loss 0.192926\n",
      "iteration 1400 / 2000: loss 0.139181\n",
      "iteration 1500 / 2000: loss 0.195881\n",
      "iteration 1600 / 2000: loss 0.222757\n",
      "iteration 1700 / 2000: loss 0.177627\n",
      "iteration 1800 / 2000: loss 0.258944\n",
      "iteration 1900 / 2000: loss 0.239815\n",
      "Hidden Size: 40, Learning Rate: 0.002500074999999999, LR Decay: 0.8175, Regularization: 0.30000000000000004, Validation accuracy: 0.9612\n",
      "iteration 0 / 2000: loss 2.302645\n",
      "iteration 100 / 2000: loss 0.484282\n",
      "iteration 200 / 2000: loss 0.388969\n",
      "iteration 300 / 2000: loss 0.337991\n",
      "iteration 400 / 2000: loss 0.286812\n",
      "iteration 500 / 2000: loss 0.257690\n",
      "iteration 600 / 2000: loss 0.216503\n",
      "iteration 700 / 2000: loss 0.241328\n",
      "iteration 800 / 2000: loss 0.191645\n",
      "iteration 900 / 2000: loss 0.289286\n",
      "iteration 1000 / 2000: loss 0.273556\n",
      "iteration 1100 / 2000: loss 0.241479\n",
      "iteration 1200 / 2000: loss 0.234220\n",
      "iteration 1300 / 2000: loss 0.256811\n",
      "iteration 1400 / 2000: loss 0.235565\n",
      "iteration 1500 / 2000: loss 0.209674\n",
      "iteration 1600 / 2000: loss 0.243926\n",
      "iteration 1700 / 2000: loss 0.215768\n",
      "iteration 1800 / 2000: loss 0.181550\n",
      "iteration 1900 / 2000: loss 0.200914\n",
      "Hidden Size: 40, Learning Rate: 0.002500074999999999, LR Decay: 0.8175, Regularization: 0.4, Validation accuracy: 0.9602\n",
      "iteration 0 / 2000: loss 2.302582\n",
      "iteration 100 / 2000: loss 0.597658\n",
      "iteration 200 / 2000: loss 0.300757\n",
      "iteration 300 / 2000: loss 0.342794\n",
      "iteration 400 / 2000: loss 0.239140\n",
      "iteration 500 / 2000: loss 0.228756\n",
      "iteration 600 / 2000: loss 0.224492\n",
      "iteration 700 / 2000: loss 0.244118\n",
      "iteration 800 / 2000: loss 0.179963\n",
      "iteration 900 / 2000: loss 0.170776\n",
      "iteration 1000 / 2000: loss 0.130751\n",
      "iteration 1100 / 2000: loss 0.179099\n",
      "iteration 1200 / 2000: loss 0.223279\n",
      "iteration 1300 / 2000: loss 0.083386\n",
      "iteration 1400 / 2000: loss 0.175632\n",
      "iteration 1500 / 2000: loss 0.129304\n",
      "iteration 1600 / 2000: loss 0.215631\n",
      "iteration 1700 / 2000: loss 0.166820\n",
      "iteration 1800 / 2000: loss 0.259000\n",
      "iteration 1900 / 2000: loss 0.122985\n",
      "Hidden Size: 40, Learning Rate: 0.002500074999999999, LR Decay: 0.645, Regularization: 0.0, Validation accuracy: 0.9514\n",
      "iteration 0 / 2000: loss 2.302595\n",
      "iteration 100 / 2000: loss 0.466333\n",
      "iteration 200 / 2000: loss 0.299792\n",
      "iteration 300 / 2000: loss 0.345654\n",
      "iteration 400 / 2000: loss 0.214814\n",
      "iteration 500 / 2000: loss 0.227228\n",
      "iteration 600 / 2000: loss 0.257209\n",
      "iteration 700 / 2000: loss 0.229092\n",
      "iteration 800 / 2000: loss 0.173757\n",
      "iteration 900 / 2000: loss 0.247654\n",
      "iteration 1000 / 2000: loss 0.116861\n",
      "iteration 1100 / 2000: loss 0.171760\n",
      "iteration 1200 / 2000: loss 0.124503\n",
      "iteration 1300 / 2000: loss 0.194437\n",
      "iteration 1400 / 2000: loss 0.171541\n",
      "iteration 1500 / 2000: loss 0.125891\n",
      "iteration 1600 / 2000: loss 0.179256\n",
      "iteration 1700 / 2000: loss 0.198504\n",
      "iteration 1800 / 2000: loss 0.159796\n",
      "iteration 1900 / 2000: loss 0.135912\n",
      "Hidden Size: 40, Learning Rate: 0.002500074999999999, LR Decay: 0.645, Regularization: 0.1, Validation accuracy: 0.9518\n",
      "iteration 0 / 2000: loss 2.302615\n",
      "iteration 100 / 2000: loss 0.422823\n",
      "iteration 200 / 2000: loss 0.271978\n",
      "iteration 300 / 2000: loss 0.305004\n",
      "iteration 400 / 2000: loss 0.315601\n",
      "iteration 500 / 2000: loss 0.274089\n",
      "iteration 600 / 2000: loss 0.216867\n",
      "iteration 700 / 2000: loss 0.323749\n",
      "iteration 800 / 2000: loss 0.245699\n",
      "iteration 900 / 2000: loss 0.239980\n",
      "iteration 1000 / 2000: loss 0.360022\n",
      "iteration 1100 / 2000: loss 0.280553\n",
      "iteration 1200 / 2000: loss 0.207615\n",
      "iteration 1300 / 2000: loss 0.257809\n",
      "iteration 1400 / 2000: loss 0.218028\n",
      "iteration 1500 / 2000: loss 0.195641\n",
      "iteration 1600 / 2000: loss 0.215594\n",
      "iteration 1700 / 2000: loss 0.294761\n",
      "iteration 1800 / 2000: loss 0.242321\n",
      "iteration 1900 / 2000: loss 0.183572\n",
      "Hidden Size: 40, Learning Rate: 0.002500074999999999, LR Decay: 0.645, Regularization: 0.2, Validation accuracy: 0.9512\n",
      "iteration 0 / 2000: loss 2.302619\n",
      "iteration 100 / 2000: loss 0.501492\n",
      "iteration 200 / 2000: loss 0.364560\n",
      "iteration 300 / 2000: loss 0.420704\n",
      "iteration 400 / 2000: loss 0.296575\n",
      "iteration 500 / 2000: loss 0.224562\n",
      "iteration 600 / 2000: loss 0.320117\n",
      "iteration 700 / 2000: loss 0.250346\n",
      "iteration 800 / 2000: loss 0.244656\n",
      "iteration 900 / 2000: loss 0.299225\n",
      "iteration 1000 / 2000: loss 0.204861\n",
      "iteration 1100 / 2000: loss 0.289229\n",
      "iteration 1200 / 2000: loss 0.277139\n",
      "iteration 1300 / 2000: loss 0.151751\n",
      "iteration 1400 / 2000: loss 0.216123\n",
      "iteration 1500 / 2000: loss 0.288648\n",
      "iteration 1600 / 2000: loss 0.246832\n",
      "iteration 1700 / 2000: loss 0.235434\n",
      "iteration 1800 / 2000: loss 0.349995\n",
      "iteration 1900 / 2000: loss 0.230806\n",
      "Hidden Size: 40, Learning Rate: 0.002500074999999999, LR Decay: 0.645, Regularization: 0.30000000000000004, Validation accuracy: 0.9532\n",
      "iteration 0 / 2000: loss 2.302653\n",
      "iteration 100 / 2000: loss 0.503399\n",
      "iteration 200 / 2000: loss 0.426251\n",
      "iteration 300 / 2000: loss 0.343506\n",
      "iteration 400 / 2000: loss 0.353992\n",
      "iteration 500 / 2000: loss 0.380046\n",
      "iteration 600 / 2000: loss 0.324829\n",
      "iteration 700 / 2000: loss 0.282386\n",
      "iteration 800 / 2000: loss 0.291291\n",
      "iteration 900 / 2000: loss 0.302425\n",
      "iteration 1000 / 2000: loss 0.362750\n",
      "iteration 1100 / 2000: loss 0.238951\n",
      "iteration 1200 / 2000: loss 0.384401\n",
      "iteration 1300 / 2000: loss 0.195135\n",
      "iteration 1400 / 2000: loss 0.274205\n",
      "iteration 1500 / 2000: loss 0.213798\n",
      "iteration 1600 / 2000: loss 0.214534\n",
      "iteration 1700 / 2000: loss 0.283907\n",
      "iteration 1800 / 2000: loss 0.213951\n",
      "iteration 1900 / 2000: loss 0.254226\n",
      "Hidden Size: 40, Learning Rate: 0.002500074999999999, LR Decay: 0.645, Regularization: 0.4, Validation accuracy: 0.9526\n",
      "iteration 0 / 2000: loss 2.302599\n",
      "iteration 100 / 2000: loss 0.661131\n",
      "iteration 200 / 2000: loss 0.466106\n",
      "iteration 300 / 2000: loss 0.440525\n",
      "iteration 400 / 2000: loss 0.230418\n",
      "iteration 500 / 2000: loss 0.351330\n",
      "iteration 600 / 2000: loss 0.257252\n",
      "iteration 700 / 2000: loss 0.262032\n",
      "iteration 800 / 2000: loss 0.297886\n",
      "iteration 900 / 2000: loss 0.254293\n",
      "iteration 1000 / 2000: loss 0.289423\n",
      "iteration 1100 / 2000: loss 0.235004\n",
      "iteration 1200 / 2000: loss 0.285336\n",
      "iteration 1300 / 2000: loss 0.240022\n",
      "iteration 1400 / 2000: loss 0.268291\n",
      "iteration 1500 / 2000: loss 0.194799\n",
      "iteration 1600 / 2000: loss 0.209160\n",
      "iteration 1700 / 2000: loss 0.286770\n",
      "iteration 1800 / 2000: loss 0.201511\n",
      "iteration 1900 / 2000: loss 0.247813\n",
      "Hidden Size: 40, Learning Rate: 0.002500074999999999, LR Decay: 0.47250000000000003, Regularization: 0.0, Validation accuracy: 0.9334\n",
      "iteration 0 / 2000: loss 2.302593\n",
      "iteration 100 / 2000: loss 0.689004\n",
      "iteration 200 / 2000: loss 0.378702\n",
      "iteration 300 / 2000: loss 0.306786\n",
      "iteration 400 / 2000: loss 0.313732\n",
      "iteration 500 / 2000: loss 0.247373\n",
      "iteration 600 / 2000: loss 0.293949\n",
      "iteration 700 / 2000: loss 0.208780\n",
      "iteration 800 / 2000: loss 0.326208\n",
      "iteration 900 / 2000: loss 0.262124\n",
      "iteration 1000 / 2000: loss 0.311395\n",
      "iteration 1100 / 2000: loss 0.363592\n",
      "iteration 1200 / 2000: loss 0.238896\n",
      "iteration 1300 / 2000: loss 0.245575\n",
      "iteration 1400 / 2000: loss 0.231598\n",
      "iteration 1500 / 2000: loss 0.239211\n",
      "iteration 1600 / 2000: loss 0.248479\n",
      "iteration 1700 / 2000: loss 0.251689\n",
      "iteration 1800 / 2000: loss 0.330578\n",
      "iteration 1900 / 2000: loss 0.170213\n",
      "Hidden Size: 40, Learning Rate: 0.002500074999999999, LR Decay: 0.47250000000000003, Regularization: 0.1, Validation accuracy: 0.9362\n",
      "iteration 0 / 2000: loss 2.302608\n",
      "iteration 100 / 2000: loss 0.655231\n",
      "iteration 200 / 2000: loss 0.401858\n",
      "iteration 300 / 2000: loss 0.318292\n",
      "iteration 400 / 2000: loss 0.210896\n",
      "iteration 500 / 2000: loss 0.312824\n",
      "iteration 600 / 2000: loss 0.236241\n",
      "iteration 700 / 2000: loss 0.323421\n",
      "iteration 800 / 2000: loss 0.258406\n",
      "iteration 900 / 2000: loss 0.259239\n",
      "iteration 1000 / 2000: loss 0.281866\n",
      "iteration 1100 / 2000: loss 0.335957\n",
      "iteration 1200 / 2000: loss 0.197219\n",
      "iteration 1300 / 2000: loss 0.234861\n",
      "iteration 1400 / 2000: loss 0.262597\n",
      "iteration 1500 / 2000: loss 0.246654\n",
      "iteration 1600 / 2000: loss 0.292283\n",
      "iteration 1700 / 2000: loss 0.316443\n",
      "iteration 1800 / 2000: loss 0.257639\n",
      "iteration 1900 / 2000: loss 0.210016\n",
      "Hidden Size: 40, Learning Rate: 0.002500074999999999, LR Decay: 0.47250000000000003, Regularization: 0.2, Validation accuracy: 0.9314\n",
      "iteration 0 / 2000: loss 2.302642\n",
      "iteration 100 / 2000: loss 0.835585\n",
      "iteration 200 / 2000: loss 0.407977\n",
      "iteration 300 / 2000: loss 0.365663\n",
      "iteration 400 / 2000: loss 0.280324\n",
      "iteration 500 / 2000: loss 0.365717\n",
      "iteration 600 / 2000: loss 0.321463\n",
      "iteration 700 / 2000: loss 0.295293\n",
      "iteration 800 / 2000: loss 0.197752\n",
      "iteration 900 / 2000: loss 0.302527\n",
      "iteration 1000 / 2000: loss 0.341379\n",
      "iteration 1100 / 2000: loss 0.377406\n",
      "iteration 1200 / 2000: loss 0.297842\n",
      "iteration 1300 / 2000: loss 0.376180\n",
      "iteration 1400 / 2000: loss 0.340644\n",
      "iteration 1500 / 2000: loss 0.299429\n",
      "iteration 1600 / 2000: loss 0.268566\n",
      "iteration 1700 / 2000: loss 0.265565\n",
      "iteration 1800 / 2000: loss 0.223665\n",
      "iteration 1900 / 2000: loss 0.307116\n",
      "Hidden Size: 40, Learning Rate: 0.002500074999999999, LR Decay: 0.47250000000000003, Regularization: 0.30000000000000004, Validation accuracy: 0.9326\n",
      "iteration 0 / 2000: loss 2.302658\n",
      "iteration 100 / 2000: loss 0.771957\n",
      "iteration 200 / 2000: loss 0.358356\n",
      "iteration 300 / 2000: loss 0.359361\n",
      "iteration 400 / 2000: loss 0.331457\n",
      "iteration 500 / 2000: loss 0.279911\n",
      "iteration 600 / 2000: loss 0.423140\n",
      "iteration 700 / 2000: loss 0.273339\n",
      "iteration 800 / 2000: loss 0.282139\n",
      "iteration 900 / 2000: loss 0.383430\n",
      "iteration 1000 / 2000: loss 0.297404\n",
      "iteration 1100 / 2000: loss 0.329413\n",
      "iteration 1200 / 2000: loss 0.264012\n",
      "iteration 1300 / 2000: loss 0.351380\n",
      "iteration 1400 / 2000: loss 0.341497\n",
      "iteration 1500 / 2000: loss 0.334154\n",
      "iteration 1600 / 2000: loss 0.327475\n",
      "iteration 1700 / 2000: loss 0.246272\n",
      "iteration 1800 / 2000: loss 0.338522\n",
      "iteration 1900 / 2000: loss 0.285706\n",
      "Hidden Size: 40, Learning Rate: 0.002500074999999999, LR Decay: 0.47250000000000003, Regularization: 0.4, Validation accuracy: 0.9326\n",
      "iteration 0 / 2000: loss 2.302580\n",
      "iteration 100 / 2000: loss 1.228123\n",
      "iteration 200 / 2000: loss 0.467419\n",
      "iteration 300 / 2000: loss 0.364687\n",
      "iteration 400 / 2000: loss 0.237760\n",
      "iteration 500 / 2000: loss 0.365174\n",
      "iteration 600 / 2000: loss 0.333353\n",
      "iteration 700 / 2000: loss 0.270369\n",
      "iteration 800 / 2000: loss 0.319883\n",
      "iteration 900 / 2000: loss 0.295141\n",
      "iteration 1000 / 2000: loss 0.311126\n",
      "iteration 1100 / 2000: loss 0.408857\n",
      "iteration 1200 / 2000: loss 0.356647\n",
      "iteration 1300 / 2000: loss 0.275084\n",
      "iteration 1400 / 2000: loss 0.261067\n",
      "iteration 1500 / 2000: loss 0.273324\n",
      "iteration 1600 / 2000: loss 0.271259\n",
      "iteration 1700 / 2000: loss 0.434274\n",
      "iteration 1800 / 2000: loss 0.332983\n",
      "iteration 1900 / 2000: loss 0.245125\n",
      "Hidden Size: 40, Learning Rate: 0.002500074999999999, LR Decay: 0.3, Regularization: 0.0, Validation accuracy: 0.9124\n",
      "iteration 0 / 2000: loss 2.302611\n",
      "iteration 100 / 2000: loss 1.473350\n",
      "iteration 200 / 2000: loss 0.572976\n",
      "iteration 300 / 2000: loss 0.329113\n",
      "iteration 400 / 2000: loss 0.443247\n",
      "iteration 500 / 2000: loss 0.317192\n",
      "iteration 600 / 2000: loss 0.317624\n",
      "iteration 700 / 2000: loss 0.268910\n",
      "iteration 800 / 2000: loss 0.362386\n",
      "iteration 900 / 2000: loss 0.347953\n",
      "iteration 1000 / 2000: loss 0.304437\n",
      "iteration 1100 / 2000: loss 0.324616\n",
      "iteration 1200 / 2000: loss 0.486262\n",
      "iteration 1300 / 2000: loss 0.378176\n",
      "iteration 1400 / 2000: loss 0.303443\n",
      "iteration 1500 / 2000: loss 0.308133\n",
      "iteration 1600 / 2000: loss 0.324292\n",
      "iteration 1700 / 2000: loss 0.365380\n",
      "iteration 1800 / 2000: loss 0.342475\n",
      "iteration 1900 / 2000: loss 0.334007\n",
      "Hidden Size: 40, Learning Rate: 0.002500074999999999, LR Decay: 0.3, Regularization: 0.1, Validation accuracy: 0.9136\n",
      "iteration 0 / 2000: loss 2.302643\n",
      "iteration 100 / 2000: loss 1.478741\n",
      "iteration 200 / 2000: loss 0.548894\n",
      "iteration 300 / 2000: loss 0.456377\n",
      "iteration 400 / 2000: loss 0.304272\n",
      "iteration 500 / 2000: loss 0.329205\n",
      "iteration 600 / 2000: loss 0.363870\n",
      "iteration 700 / 2000: loss 0.281270\n",
      "iteration 800 / 2000: loss 0.380520\n",
      "iteration 900 / 2000: loss 0.301025\n",
      "iteration 1000 / 2000: loss 0.346999\n",
      "iteration 1100 / 2000: loss 0.401760\n",
      "iteration 1200 / 2000: loss 0.407142\n",
      "iteration 1300 / 2000: loss 0.281224\n",
      "iteration 1400 / 2000: loss 0.263524\n",
      "iteration 1500 / 2000: loss 0.361148\n",
      "iteration 1600 / 2000: loss 0.366621\n",
      "iteration 1700 / 2000: loss 0.302781\n",
      "iteration 1800 / 2000: loss 0.378220\n",
      "iteration 1900 / 2000: loss 0.291419\n",
      "Hidden Size: 40, Learning Rate: 0.002500074999999999, LR Decay: 0.3, Regularization: 0.2, Validation accuracy: 0.9152\n",
      "iteration 0 / 2000: loss 2.302641\n",
      "iteration 100 / 2000: loss 1.265940\n",
      "iteration 200 / 2000: loss 0.453697\n",
      "iteration 300 / 2000: loss 0.328788\n",
      "iteration 400 / 2000: loss 0.365608\n",
      "iteration 500 / 2000: loss 0.462427\n",
      "iteration 600 / 2000: loss 0.450776\n",
      "iteration 700 / 2000: loss 0.421422\n",
      "iteration 800 / 2000: loss 0.423809\n",
      "iteration 900 / 2000: loss 0.333475\n",
      "iteration 1000 / 2000: loss 0.305713\n",
      "iteration 1100 / 2000: loss 0.387357\n",
      "iteration 1200 / 2000: loss 0.299541\n",
      "iteration 1300 / 2000: loss 0.307013\n",
      "iteration 1400 / 2000: loss 0.323168\n",
      "iteration 1500 / 2000: loss 0.389401\n",
      "iteration 1600 / 2000: loss 0.334634\n",
      "iteration 1700 / 2000: loss 0.336893\n",
      "iteration 1800 / 2000: loss 0.478258\n",
      "iteration 1900 / 2000: loss 0.407494\n",
      "Hidden Size: 40, Learning Rate: 0.002500074999999999, LR Decay: 0.3, Regularization: 0.30000000000000004, Validation accuracy: 0.9142\n",
      "iteration 0 / 2000: loss 2.302652\n",
      "iteration 100 / 2000: loss 1.381373\n",
      "iteration 200 / 2000: loss 0.492276\n",
      "iteration 300 / 2000: loss 0.373984\n",
      "iteration 400 / 2000: loss 0.406977\n",
      "iteration 500 / 2000: loss 0.343605\n",
      "iteration 600 / 2000: loss 0.329644\n",
      "iteration 700 / 2000: loss 0.348633\n",
      "iteration 800 / 2000: loss 0.356581\n",
      "iteration 900 / 2000: loss 0.314688\n",
      "iteration 1000 / 2000: loss 0.355068\n",
      "iteration 1100 / 2000: loss 0.436710\n",
      "iteration 1200 / 2000: loss 0.410277\n",
      "iteration 1300 / 2000: loss 0.359112\n",
      "iteration 1400 / 2000: loss 0.431847\n",
      "iteration 1500 / 2000: loss 0.269738\n",
      "iteration 1600 / 2000: loss 0.360092\n",
      "iteration 1700 / 2000: loss 0.302807\n",
      "iteration 1800 / 2000: loss 0.353264\n",
      "iteration 1900 / 2000: loss 0.378966\n",
      "Hidden Size: 40, Learning Rate: 0.002500074999999999, LR Decay: 0.3, Regularization: 0.4, Validation accuracy: 0.9128\n",
      "iteration 0 / 2000: loss 2.302586\n",
      "iteration 100 / 2000: loss 2.302568\n",
      "iteration 200 / 2000: loss 2.302579\n",
      "iteration 300 / 2000: loss 2.302572\n",
      "iteration 400 / 2000: loss 2.302588\n",
      "iteration 500 / 2000: loss 2.302582\n",
      "iteration 600 / 2000: loss 2.302568\n",
      "iteration 700 / 2000: loss 2.302584\n",
      "iteration 800 / 2000: loss 2.302587\n",
      "iteration 900 / 2000: loss 2.302581\n",
      "iteration 1000 / 2000: loss 2.302577\n",
      "iteration 1100 / 2000: loss 2.302578\n",
      "iteration 1200 / 2000: loss 2.302574\n",
      "iteration 1300 / 2000: loss 2.302572\n",
      "iteration 1400 / 2000: loss 2.302577\n",
      "iteration 1500 / 2000: loss 2.302576\n",
      "iteration 1600 / 2000: loss 2.302578\n",
      "iteration 1700 / 2000: loss 2.302582\n",
      "iteration 1800 / 2000: loss 2.302568\n",
      "iteration 1900 / 2000: loss 2.302575\n",
      "Hidden Size: 40, Learning Rate: 1e-07, LR Decay: 0.99, Regularization: 0.0, Validation accuracy: 0.1002\n",
      "iteration 0 / 2000: loss 2.302599\n",
      "iteration 100 / 2000: loss 2.302598\n",
      "iteration 200 / 2000: loss 2.302598\n",
      "iteration 300 / 2000: loss 2.302590\n",
      "iteration 400 / 2000: loss 2.302598\n",
      "iteration 500 / 2000: loss 2.302602\n",
      "iteration 600 / 2000: loss 2.302593\n",
      "iteration 700 / 2000: loss 2.302601\n",
      "iteration 800 / 2000: loss 2.302583\n",
      "iteration 900 / 2000: loss 2.302603\n",
      "iteration 1000 / 2000: loss 2.302600\n",
      "iteration 1100 / 2000: loss 2.302597\n",
      "iteration 1200 / 2000: loss 2.302596\n",
      "iteration 1300 / 2000: loss 2.302596\n",
      "iteration 1400 / 2000: loss 2.302592\n",
      "iteration 1500 / 2000: loss 2.302590\n",
      "iteration 1600 / 2000: loss 2.302592\n",
      "iteration 1700 / 2000: loss 2.302590\n",
      "iteration 1800 / 2000: loss 2.302593\n",
      "iteration 1900 / 2000: loss 2.302590\n",
      "Hidden Size: 40, Learning Rate: 1e-07, LR Decay: 0.99, Regularization: 0.1, Validation accuracy: 0.1434\n",
      "iteration 0 / 2000: loss 2.302619\n",
      "iteration 100 / 2000: loss 2.302616\n",
      "iteration 200 / 2000: loss 2.302611\n",
      "iteration 300 / 2000: loss 2.302620\n",
      "iteration 400 / 2000: loss 2.302612\n",
      "iteration 500 / 2000: loss 2.302618\n",
      "iteration 600 / 2000: loss 2.302619\n",
      "iteration 700 / 2000: loss 2.302609\n",
      "iteration 800 / 2000: loss 2.302610\n",
      "iteration 900 / 2000: loss 2.302624\n",
      "iteration 1000 / 2000: loss 2.302615\n",
      "iteration 1100 / 2000: loss 2.302614\n",
      "iteration 1200 / 2000: loss 2.302608\n",
      "iteration 1300 / 2000: loss 2.302614\n",
      "iteration 1400 / 2000: loss 2.302610\n",
      "iteration 1500 / 2000: loss 2.302616\n",
      "iteration 1600 / 2000: loss 2.302614\n",
      "iteration 1700 / 2000: loss 2.302608\n",
      "iteration 1800 / 2000: loss 2.302611\n",
      "iteration 1900 / 2000: loss 2.302617\n",
      "Hidden Size: 40, Learning Rate: 1e-07, LR Decay: 0.99, Regularization: 0.2, Validation accuracy: 0.112\n",
      "iteration 0 / 2000: loss 2.302618\n",
      "iteration 100 / 2000: loss 2.302616\n",
      "iteration 200 / 2000: loss 2.302620\n",
      "iteration 300 / 2000: loss 2.302624\n",
      "iteration 400 / 2000: loss 2.302623\n",
      "iteration 500 / 2000: loss 2.302626\n",
      "iteration 600 / 2000: loss 2.302622\n",
      "iteration 700 / 2000: loss 2.302624\n",
      "iteration 800 / 2000: loss 2.302616\n",
      "iteration 900 / 2000: loss 2.302610\n",
      "iteration 1000 / 2000: loss 2.302615\n",
      "iteration 1100 / 2000: loss 2.302619\n",
      "iteration 1200 / 2000: loss 2.302626\n",
      "iteration 1300 / 2000: loss 2.302627\n",
      "iteration 1400 / 2000: loss 2.302623\n",
      "iteration 1500 / 2000: loss 2.302615\n",
      "iteration 1600 / 2000: loss 2.302621\n",
      "iteration 1700 / 2000: loss 2.302614\n",
      "iteration 1800 / 2000: loss 2.302618\n",
      "iteration 1900 / 2000: loss 2.302623\n",
      "Hidden Size: 40, Learning Rate: 1e-07, LR Decay: 0.99, Regularization: 0.30000000000000004, Validation accuracy: 0.1194\n",
      "iteration 0 / 2000: loss 2.302649\n",
      "iteration 100 / 2000: loss 2.302639\n",
      "iteration 200 / 2000: loss 2.302649\n",
      "iteration 300 / 2000: loss 2.302642\n",
      "iteration 400 / 2000: loss 2.302639\n",
      "iteration 500 / 2000: loss 2.302652\n",
      "iteration 600 / 2000: loss 2.302650\n",
      "iteration 700 / 2000: loss 2.302646\n",
      "iteration 800 / 2000: loss 2.302643\n",
      "iteration 900 / 2000: loss 2.302656\n",
      "iteration 1000 / 2000: loss 2.302639\n",
      "iteration 1100 / 2000: loss 2.302640\n",
      "iteration 1200 / 2000: loss 2.302637\n",
      "iteration 1300 / 2000: loss 2.302642\n",
      "iteration 1400 / 2000: loss 2.302650\n",
      "iteration 1500 / 2000: loss 2.302642\n",
      "iteration 1600 / 2000: loss 2.302641\n",
      "iteration 1700 / 2000: loss 2.302639\n",
      "iteration 1800 / 2000: loss 2.302633\n",
      "iteration 1900 / 2000: loss 2.302641\n",
      "Hidden Size: 40, Learning Rate: 1e-07, LR Decay: 0.99, Regularization: 0.4, Validation accuracy: 0.121\n",
      "iteration 0 / 2000: loss 2.302561\n",
      "iteration 100 / 2000: loss 2.302589\n",
      "iteration 200 / 2000: loss 2.302568\n",
      "iteration 300 / 2000: loss 2.302572\n",
      "iteration 400 / 2000: loss 2.302579\n",
      "iteration 500 / 2000: loss 2.302572\n",
      "iteration 600 / 2000: loss 2.302572\n",
      "iteration 700 / 2000: loss 2.302583\n",
      "iteration 800 / 2000: loss 2.302578\n",
      "iteration 900 / 2000: loss 2.302565\n",
      "iteration 1000 / 2000: loss 2.302576\n",
      "iteration 1100 / 2000: loss 2.302578\n",
      "iteration 1200 / 2000: loss 2.302562\n",
      "iteration 1300 / 2000: loss 2.302566\n",
      "iteration 1400 / 2000: loss 2.302575\n",
      "iteration 1500 / 2000: loss 2.302570\n",
      "iteration 1600 / 2000: loss 2.302570\n",
      "iteration 1700 / 2000: loss 2.302580\n",
      "iteration 1800 / 2000: loss 2.302571\n",
      "iteration 1900 / 2000: loss 2.302571\n",
      "Hidden Size: 40, Learning Rate: 1e-07, LR Decay: 0.8175, Regularization: 0.0, Validation accuracy: 0.1198\n",
      "iteration 0 / 2000: loss 2.302601\n",
      "iteration 100 / 2000: loss 2.302601\n",
      "iteration 200 / 2000: loss 2.302602\n",
      "iteration 300 / 2000: loss 2.302597\n",
      "iteration 400 / 2000: loss 2.302609\n",
      "iteration 500 / 2000: loss 2.302602\n",
      "iteration 600 / 2000: loss 2.302602\n",
      "iteration 700 / 2000: loss 2.302604\n",
      "iteration 800 / 2000: loss 2.302602\n",
      "iteration 900 / 2000: loss 2.302601\n",
      "iteration 1000 / 2000: loss 2.302599\n",
      "iteration 1100 / 2000: loss 2.302606\n",
      "iteration 1200 / 2000: loss 2.302606\n",
      "iteration 1300 / 2000: loss 2.302605\n",
      "iteration 1400 / 2000: loss 2.302601\n",
      "iteration 1500 / 2000: loss 2.302603\n",
      "iteration 1600 / 2000: loss 2.302604\n",
      "iteration 1700 / 2000: loss 2.302597\n",
      "iteration 1800 / 2000: loss 2.302598\n",
      "iteration 1900 / 2000: loss 2.302604\n",
      "Hidden Size: 40, Learning Rate: 1e-07, LR Decay: 0.8175, Regularization: 0.1, Validation accuracy: 0.0914\n",
      "iteration 0 / 2000: loss 2.302619\n",
      "iteration 100 / 2000: loss 2.302618\n",
      "iteration 200 / 2000: loss 2.302610\n",
      "iteration 300 / 2000: loss 2.302607\n",
      "iteration 400 / 2000: loss 2.302608\n",
      "iteration 500 / 2000: loss 2.302613\n",
      "iteration 600 / 2000: loss 2.302616\n",
      "iteration 700 / 2000: loss 2.302615\n",
      "iteration 800 / 2000: loss 2.302612\n",
      "iteration 900 / 2000: loss 2.302606\n",
      "iteration 1000 / 2000: loss 2.302605\n",
      "iteration 1100 / 2000: loss 2.302611\n",
      "iteration 1200 / 2000: loss 2.302611\n",
      "iteration 1300 / 2000: loss 2.302622\n",
      "iteration 1400 / 2000: loss 2.302616\n",
      "iteration 1500 / 2000: loss 2.302604\n",
      "iteration 1600 / 2000: loss 2.302616\n",
      "iteration 1700 / 2000: loss 2.302605\n",
      "iteration 1800 / 2000: loss 2.302613\n",
      "iteration 1900 / 2000: loss 2.302607\n",
      "Hidden Size: 40, Learning Rate: 1e-07, LR Decay: 0.8175, Regularization: 0.2, Validation accuracy: 0.1282\n",
      "iteration 0 / 2000: loss 2.302642\n",
      "iteration 100 / 2000: loss 2.302633\n",
      "iteration 200 / 2000: loss 2.302638\n",
      "iteration 300 / 2000: loss 2.302636\n",
      "iteration 400 / 2000: loss 2.302632\n",
      "iteration 500 / 2000: loss 2.302636\n",
      "iteration 600 / 2000: loss 2.302637\n",
      "iteration 700 / 2000: loss 2.302644\n",
      "iteration 800 / 2000: loss 2.302633\n",
      "iteration 900 / 2000: loss 2.302643\n",
      "iteration 1000 / 2000: loss 2.302642\n",
      "iteration 1100 / 2000: loss 2.302635\n",
      "iteration 1200 / 2000: loss 2.302638\n",
      "iteration 1300 / 2000: loss 2.302632\n",
      "iteration 1400 / 2000: loss 2.302639\n",
      "iteration 1500 / 2000: loss 2.302639\n",
      "iteration 1600 / 2000: loss 2.302636\n",
      "iteration 1700 / 2000: loss 2.302636\n",
      "iteration 1800 / 2000: loss 2.302640\n",
      "iteration 1900 / 2000: loss 2.302630\n",
      "Hidden Size: 40, Learning Rate: 1e-07, LR Decay: 0.8175, Regularization: 0.30000000000000004, Validation accuracy: 0.1206\n",
      "iteration 0 / 2000: loss 2.302658\n",
      "iteration 100 / 2000: loss 2.302654\n",
      "iteration 200 / 2000: loss 2.302651\n",
      "iteration 300 / 2000: loss 2.302657\n",
      "iteration 400 / 2000: loss 2.302659\n",
      "iteration 500 / 2000: loss 2.302660\n",
      "iteration 600 / 2000: loss 2.302660\n",
      "iteration 700 / 2000: loss 2.302657\n",
      "iteration 800 / 2000: loss 2.302660\n",
      "iteration 900 / 2000: loss 2.302652\n",
      "iteration 1000 / 2000: loss 2.302664\n",
      "iteration 1100 / 2000: loss 2.302653\n",
      "iteration 1200 / 2000: loss 2.302663\n",
      "iteration 1300 / 2000: loss 2.302663\n",
      "iteration 1400 / 2000: loss 2.302657\n",
      "iteration 1500 / 2000: loss 2.302663\n",
      "iteration 1600 / 2000: loss 2.302658\n",
      "iteration 1700 / 2000: loss 2.302658\n",
      "iteration 1800 / 2000: loss 2.302664\n",
      "iteration 1900 / 2000: loss 2.302660\n",
      "Hidden Size: 40, Learning Rate: 1e-07, LR Decay: 0.8175, Regularization: 0.4, Validation accuracy: 0.0736\n",
      "iteration 0 / 2000: loss 2.302584\n",
      "iteration 100 / 2000: loss 2.302587\n",
      "iteration 200 / 2000: loss 2.302583\n",
      "iteration 300 / 2000: loss 2.302590\n",
      "iteration 400 / 2000: loss 2.302590\n",
      "iteration 500 / 2000: loss 2.302580\n",
      "iteration 600 / 2000: loss 2.302586\n",
      "iteration 700 / 2000: loss 2.302584\n",
      "iteration 800 / 2000: loss 2.302576\n",
      "iteration 900 / 2000: loss 2.302578\n",
      "iteration 1000 / 2000: loss 2.302586\n",
      "iteration 1100 / 2000: loss 2.302582\n",
      "iteration 1200 / 2000: loss 2.302588\n",
      "iteration 1300 / 2000: loss 2.302582\n",
      "iteration 1400 / 2000: loss 2.302577\n",
      "iteration 1500 / 2000: loss 2.302581\n",
      "iteration 1600 / 2000: loss 2.302576\n",
      "iteration 1700 / 2000: loss 2.302572\n",
      "iteration 1800 / 2000: loss 2.302584\n",
      "iteration 1900 / 2000: loss 2.302594\n",
      "Hidden Size: 40, Learning Rate: 1e-07, LR Decay: 0.645, Regularization: 0.0, Validation accuracy: 0.0904\n",
      "iteration 0 / 2000: loss 2.302586\n",
      "iteration 100 / 2000: loss 2.302606\n",
      "iteration 200 / 2000: loss 2.302591\n",
      "iteration 300 / 2000: loss 2.302601\n",
      "iteration 400 / 2000: loss 2.302594\n",
      "iteration 500 / 2000: loss 2.302593\n",
      "iteration 600 / 2000: loss 2.302601\n",
      "iteration 700 / 2000: loss 2.302599\n",
      "iteration 800 / 2000: loss 2.302604\n",
      "iteration 900 / 2000: loss 2.302607\n",
      "iteration 1000 / 2000: loss 2.302599\n",
      "iteration 1100 / 2000: loss 2.302600\n",
      "iteration 1200 / 2000: loss 2.302609\n",
      "iteration 1300 / 2000: loss 2.302604\n",
      "iteration 1400 / 2000: loss 2.302607\n",
      "iteration 1500 / 2000: loss 2.302597\n",
      "iteration 1600 / 2000: loss 2.302603\n",
      "iteration 1700 / 2000: loss 2.302600\n",
      "iteration 1800 / 2000: loss 2.302593\n",
      "iteration 1900 / 2000: loss 2.302587\n",
      "Hidden Size: 40, Learning Rate: 1e-07, LR Decay: 0.645, Regularization: 0.1, Validation accuracy: 0.1056\n",
      "iteration 0 / 2000: loss 2.302620\n",
      "iteration 100 / 2000: loss 2.302608\n",
      "iteration 200 / 2000: loss 2.302609\n",
      "iteration 300 / 2000: loss 2.302613\n",
      "iteration 400 / 2000: loss 2.302608\n",
      "iteration 500 / 2000: loss 2.302622\n",
      "iteration 600 / 2000: loss 2.302612\n",
      "iteration 700 / 2000: loss 2.302615\n",
      "iteration 800 / 2000: loss 2.302617\n",
      "iteration 900 / 2000: loss 2.302607\n",
      "iteration 1000 / 2000: loss 2.302611\n",
      "iteration 1100 / 2000: loss 2.302615\n",
      "iteration 1200 / 2000: loss 2.302607\n",
      "iteration 1300 / 2000: loss 2.302608\n",
      "iteration 1400 / 2000: loss 2.302613\n",
      "iteration 1500 / 2000: loss 2.302618\n",
      "iteration 1600 / 2000: loss 2.302601\n",
      "iteration 1700 / 2000: loss 2.302618\n",
      "iteration 1800 / 2000: loss 2.302610\n",
      "iteration 1900 / 2000: loss 2.302614\n",
      "Hidden Size: 40, Learning Rate: 1e-07, LR Decay: 0.645, Regularization: 0.2, Validation accuracy: 0.1008\n",
      "iteration 0 / 2000: loss 2.302627\n",
      "iteration 100 / 2000: loss 2.302638\n",
      "iteration 200 / 2000: loss 2.302623\n",
      "iteration 300 / 2000: loss 2.302635\n",
      "iteration 400 / 2000: loss 2.302633\n",
      "iteration 500 / 2000: loss 2.302633\n",
      "iteration 600 / 2000: loss 2.302638\n",
      "iteration 700 / 2000: loss 2.302620\n",
      "iteration 800 / 2000: loss 2.302627\n",
      "iteration 900 / 2000: loss 2.302627\n",
      "iteration 1000 / 2000: loss 2.302633\n",
      "iteration 1100 / 2000: loss 2.302632\n",
      "iteration 1200 / 2000: loss 2.302628\n",
      "iteration 1300 / 2000: loss 2.302638\n",
      "iteration 1400 / 2000: loss 2.302642\n",
      "iteration 1500 / 2000: loss 2.302627\n",
      "iteration 1600 / 2000: loss 2.302635\n",
      "iteration 1700 / 2000: loss 2.302637\n",
      "iteration 1800 / 2000: loss 2.302630\n",
      "iteration 1900 / 2000: loss 2.302635\n",
      "Hidden Size: 40, Learning Rate: 1e-07, LR Decay: 0.645, Regularization: 0.30000000000000004, Validation accuracy: 0.0882\n",
      "iteration 0 / 2000: loss 2.302658\n",
      "iteration 100 / 2000: loss 2.302656\n",
      "iteration 200 / 2000: loss 2.302652\n",
      "iteration 300 / 2000: loss 2.302645\n",
      "iteration 400 / 2000: loss 2.302649\n",
      "iteration 500 / 2000: loss 2.302648\n",
      "iteration 600 / 2000: loss 2.302652\n",
      "iteration 700 / 2000: loss 2.302649\n",
      "iteration 800 / 2000: loss 2.302661\n",
      "iteration 900 / 2000: loss 2.302654\n",
      "iteration 1000 / 2000: loss 2.302643\n",
      "iteration 1100 / 2000: loss 2.302659\n",
      "iteration 1200 / 2000: loss 2.302645\n",
      "iteration 1300 / 2000: loss 2.302653\n",
      "iteration 1400 / 2000: loss 2.302645\n",
      "iteration 1500 / 2000: loss 2.302644\n",
      "iteration 1600 / 2000: loss 2.302656\n",
      "iteration 1700 / 2000: loss 2.302645\n",
      "iteration 1800 / 2000: loss 2.302655\n",
      "iteration 1900 / 2000: loss 2.302654\n",
      "Hidden Size: 40, Learning Rate: 1e-07, LR Decay: 0.645, Regularization: 0.4, Validation accuracy: 0.107\n",
      "iteration 0 / 2000: loss 2.302574\n",
      "iteration 100 / 2000: loss 2.302566\n",
      "iteration 200 / 2000: loss 2.302571\n",
      "iteration 300 / 2000: loss 2.302566\n",
      "iteration 400 / 2000: loss 2.302559\n",
      "iteration 500 / 2000: loss 2.302563\n",
      "iteration 600 / 2000: loss 2.302564\n",
      "iteration 700 / 2000: loss 2.302564\n",
      "iteration 800 / 2000: loss 2.302561\n",
      "iteration 900 / 2000: loss 2.302569\n",
      "iteration 1000 / 2000: loss 2.302566\n",
      "iteration 1100 / 2000: loss 2.302570\n",
      "iteration 1200 / 2000: loss 2.302575\n",
      "iteration 1300 / 2000: loss 2.302565\n",
      "iteration 1400 / 2000: loss 2.302576\n",
      "iteration 1500 / 2000: loss 2.302564\n",
      "iteration 1600 / 2000: loss 2.302564\n",
      "iteration 1700 / 2000: loss 2.302571\n",
      "iteration 1800 / 2000: loss 2.302570\n",
      "iteration 1900 / 2000: loss 2.302573\n",
      "Hidden Size: 40, Learning Rate: 1e-07, LR Decay: 0.47250000000000003, Regularization: 0.0, Validation accuracy: 0.1348\n",
      "iteration 0 / 2000: loss 2.302608\n",
      "iteration 100 / 2000: loss 2.302610\n",
      "iteration 200 / 2000: loss 2.302606\n",
      "iteration 300 / 2000: loss 2.302597\n",
      "iteration 400 / 2000: loss 2.302606\n",
      "iteration 500 / 2000: loss 2.302604\n",
      "iteration 600 / 2000: loss 2.302601\n",
      "iteration 700 / 2000: loss 2.302613\n",
      "iteration 800 / 2000: loss 2.302603\n",
      "iteration 900 / 2000: loss 2.302607\n",
      "iteration 1000 / 2000: loss 2.302597\n",
      "iteration 1100 / 2000: loss 2.302611\n",
      "iteration 1200 / 2000: loss 2.302611\n",
      "iteration 1300 / 2000: loss 2.302605\n",
      "iteration 1400 / 2000: loss 2.302605\n",
      "iteration 1500 / 2000: loss 2.302610\n",
      "iteration 1600 / 2000: loss 2.302608\n",
      "iteration 1700 / 2000: loss 2.302605\n",
      "iteration 1800 / 2000: loss 2.302607\n",
      "iteration 1900 / 2000: loss 2.302612\n",
      "Hidden Size: 40, Learning Rate: 1e-07, LR Decay: 0.47250000000000003, Regularization: 0.1, Validation accuracy: 0.0774\n",
      "iteration 0 / 2000: loss 2.302623\n",
      "iteration 100 / 2000: loss 2.302610\n",
      "iteration 200 / 2000: loss 2.302619\n",
      "iteration 300 / 2000: loss 2.302616\n",
      "iteration 400 / 2000: loss 2.302616\n",
      "iteration 500 / 2000: loss 2.302614\n",
      "iteration 600 / 2000: loss 2.302611\n",
      "iteration 700 / 2000: loss 2.302621\n",
      "iteration 800 / 2000: loss 2.302612\n",
      "iteration 900 / 2000: loss 2.302613\n",
      "iteration 1000 / 2000: loss 2.302618\n",
      "iteration 1100 / 2000: loss 2.302614\n",
      "iteration 1200 / 2000: loss 2.302619\n",
      "iteration 1300 / 2000: loss 2.302612\n",
      "iteration 1400 / 2000: loss 2.302613\n",
      "iteration 1500 / 2000: loss 2.302612\n",
      "iteration 1600 / 2000: loss 2.302613\n",
      "iteration 1700 / 2000: loss 2.302617\n",
      "iteration 1800 / 2000: loss 2.302612\n",
      "iteration 1900 / 2000: loss 2.302612\n",
      "Hidden Size: 40, Learning Rate: 1e-07, LR Decay: 0.47250000000000003, Regularization: 0.2, Validation accuracy: 0.1022\n",
      "iteration 0 / 2000: loss 2.302609\n",
      "iteration 100 / 2000: loss 2.302612\n",
      "iteration 200 / 2000: loss 2.302614\n",
      "iteration 300 / 2000: loss 2.302617\n",
      "iteration 400 / 2000: loss 2.302602\n",
      "iteration 500 / 2000: loss 2.302610\n",
      "iteration 600 / 2000: loss 2.302619\n",
      "iteration 700 / 2000: loss 2.302618\n",
      "iteration 800 / 2000: loss 2.302603\n",
      "iteration 900 / 2000: loss 2.302609\n",
      "iteration 1000 / 2000: loss 2.302614\n",
      "iteration 1100 / 2000: loss 2.302609\n",
      "iteration 1200 / 2000: loss 2.302604\n",
      "iteration 1300 / 2000: loss 2.302605\n",
      "iteration 1400 / 2000: loss 2.302602\n",
      "iteration 1500 / 2000: loss 2.302614\n",
      "iteration 1600 / 2000: loss 2.302609\n",
      "iteration 1700 / 2000: loss 2.302617\n",
      "iteration 1800 / 2000: loss 2.302611\n",
      "iteration 1900 / 2000: loss 2.302612\n",
      "Hidden Size: 40, Learning Rate: 1e-07, LR Decay: 0.47250000000000003, Regularization: 0.30000000000000004, Validation accuracy: 0.1588\n",
      "iteration 0 / 2000: loss 2.302645\n",
      "iteration 100 / 2000: loss 2.302647\n",
      "iteration 200 / 2000: loss 2.302648\n",
      "iteration 300 / 2000: loss 2.302637\n",
      "iteration 400 / 2000: loss 2.302639\n",
      "iteration 500 / 2000: loss 2.302646\n",
      "iteration 600 / 2000: loss 2.302636\n",
      "iteration 700 / 2000: loss 2.302638\n",
      "iteration 800 / 2000: loss 2.302647\n",
      "iteration 900 / 2000: loss 2.302643\n",
      "iteration 1000 / 2000: loss 2.302636\n",
      "iteration 1100 / 2000: loss 2.302649\n",
      "iteration 1200 / 2000: loss 2.302650\n",
      "iteration 1300 / 2000: loss 2.302648\n",
      "iteration 1400 / 2000: loss 2.302644\n",
      "iteration 1500 / 2000: loss 2.302649\n",
      "iteration 1600 / 2000: loss 2.302647\n",
      "iteration 1700 / 2000: loss 2.302645\n",
      "iteration 1800 / 2000: loss 2.302653\n",
      "iteration 1900 / 2000: loss 2.302648\n",
      "Hidden Size: 40, Learning Rate: 1e-07, LR Decay: 0.47250000000000003, Regularization: 0.4, Validation accuracy: 0.0978\n",
      "iteration 0 / 2000: loss 2.302584\n",
      "iteration 100 / 2000: loss 2.302592\n",
      "iteration 200 / 2000: loss 2.302593\n",
      "iteration 300 / 2000: loss 2.302592\n",
      "iteration 400 / 2000: loss 2.302590\n",
      "iteration 500 / 2000: loss 2.302584\n",
      "iteration 600 / 2000: loss 2.302593\n",
      "iteration 700 / 2000: loss 2.302596\n",
      "iteration 800 / 2000: loss 2.302575\n",
      "iteration 900 / 2000: loss 2.302600\n",
      "iteration 1000 / 2000: loss 2.302578\n",
      "iteration 1100 / 2000: loss 2.302586\n",
      "iteration 1200 / 2000: loss 2.302580\n",
      "iteration 1300 / 2000: loss 2.302586\n",
      "iteration 1400 / 2000: loss 2.302584\n",
      "iteration 1500 / 2000: loss 2.302581\n",
      "iteration 1600 / 2000: loss 2.302594\n",
      "iteration 1700 / 2000: loss 2.302600\n",
      "iteration 1800 / 2000: loss 2.302582\n",
      "iteration 1900 / 2000: loss 2.302586\n",
      "Hidden Size: 40, Learning Rate: 1e-07, LR Decay: 0.3, Regularization: 0.0, Validation accuracy: 0.1056\n",
      "iteration 0 / 2000: loss 2.302616\n",
      "iteration 100 / 2000: loss 2.302614\n",
      "iteration 200 / 2000: loss 2.302625\n",
      "iteration 300 / 2000: loss 2.302619\n",
      "iteration 400 / 2000: loss 2.302607\n",
      "iteration 500 / 2000: loss 2.302610\n",
      "iteration 600 / 2000: loss 2.302607\n",
      "iteration 700 / 2000: loss 2.302605\n",
      "iteration 800 / 2000: loss 2.302612\n",
      "iteration 900 / 2000: loss 2.302613\n",
      "iteration 1000 / 2000: loss 2.302611\n",
      "iteration 1100 / 2000: loss 2.302620\n",
      "iteration 1200 / 2000: loss 2.302613\n",
      "iteration 1300 / 2000: loss 2.302612\n",
      "iteration 1400 / 2000: loss 2.302612\n",
      "iteration 1500 / 2000: loss 2.302616\n",
      "iteration 1600 / 2000: loss 2.302609\n",
      "iteration 1700 / 2000: loss 2.302621\n",
      "iteration 1800 / 2000: loss 2.302606\n",
      "iteration 1900 / 2000: loss 2.302608\n",
      "Hidden Size: 40, Learning Rate: 1e-07, LR Decay: 0.3, Regularization: 0.1, Validation accuracy: 0.0438\n",
      "iteration 0 / 2000: loss 2.302628\n",
      "iteration 100 / 2000: loss 2.302628\n",
      "iteration 200 / 2000: loss 2.302620\n",
      "iteration 300 / 2000: loss 2.302624\n",
      "iteration 400 / 2000: loss 2.302630\n",
      "iteration 500 / 2000: loss 2.302624\n",
      "iteration 600 / 2000: loss 2.302618\n",
      "iteration 700 / 2000: loss 2.302622\n",
      "iteration 800 / 2000: loss 2.302622\n",
      "iteration 900 / 2000: loss 2.302633\n",
      "iteration 1000 / 2000: loss 2.302625\n",
      "iteration 1100 / 2000: loss 2.302622\n",
      "iteration 1200 / 2000: loss 2.302612\n",
      "iteration 1300 / 2000: loss 2.302623\n",
      "iteration 1400 / 2000: loss 2.302620\n",
      "iteration 1500 / 2000: loss 2.302615\n",
      "iteration 1600 / 2000: loss 2.302621\n",
      "iteration 1700 / 2000: loss 2.302626\n",
      "iteration 1800 / 2000: loss 2.302623\n",
      "iteration 1900 / 2000: loss 2.302624\n",
      "Hidden Size: 40, Learning Rate: 1e-07, LR Decay: 0.3, Regularization: 0.2, Validation accuracy: 0.0624\n",
      "iteration 0 / 2000: loss 2.302653\n",
      "iteration 100 / 2000: loss 2.302642\n",
      "iteration 200 / 2000: loss 2.302644\n",
      "iteration 300 / 2000: loss 2.302648\n",
      "iteration 400 / 2000: loss 2.302649\n",
      "iteration 500 / 2000: loss 2.302648\n",
      "iteration 600 / 2000: loss 2.302645\n",
      "iteration 700 / 2000: loss 2.302640\n",
      "iteration 800 / 2000: loss 2.302637\n",
      "iteration 900 / 2000: loss 2.302651\n",
      "iteration 1000 / 2000: loss 2.302655\n",
      "iteration 1100 / 2000: loss 2.302649\n",
      "iteration 1200 / 2000: loss 2.302647\n",
      "iteration 1300 / 2000: loss 2.302642\n",
      "iteration 1400 / 2000: loss 2.302644\n",
      "iteration 1500 / 2000: loss 2.302643\n",
      "iteration 1600 / 2000: loss 2.302642\n",
      "iteration 1700 / 2000: loss 2.302641\n",
      "iteration 1800 / 2000: loss 2.302642\n",
      "iteration 1900 / 2000: loss 2.302641\n",
      "Hidden Size: 40, Learning Rate: 1e-07, LR Decay: 0.3, Regularization: 0.30000000000000004, Validation accuracy: 0.0666\n",
      "iteration 0 / 2000: loss 2.302639\n",
      "iteration 100 / 2000: loss 2.302637\n",
      "iteration 200 / 2000: loss 2.302647\n",
      "iteration 300 / 2000: loss 2.302642\n",
      "iteration 400 / 2000: loss 2.302627\n",
      "iteration 500 / 2000: loss 2.302637\n",
      "iteration 600 / 2000: loss 2.302635\n",
      "iteration 700 / 2000: loss 2.302635\n",
      "iteration 800 / 2000: loss 2.302640\n",
      "iteration 900 / 2000: loss 2.302643\n",
      "iteration 1000 / 2000: loss 2.302639\n",
      "iteration 1100 / 2000: loss 2.302634\n",
      "iteration 1200 / 2000: loss 2.302646\n",
      "iteration 1300 / 2000: loss 2.302629\n",
      "iteration 1400 / 2000: loss 2.302644\n",
      "iteration 1500 / 2000: loss 2.302647\n",
      "iteration 1600 / 2000: loss 2.302639\n",
      "iteration 1700 / 2000: loss 2.302647\n",
      "iteration 1800 / 2000: loss 2.302632\n",
      "iteration 1900 / 2000: loss 2.302645\n",
      "Hidden Size: 40, Learning Rate: 1e-07, LR Decay: 0.3, Regularization: 0.4, Validation accuracy: 0.115\n",
      "iteration 0 / 2000: loss 2.302572\n",
      "iteration 100 / 2000: loss 0.334663\n",
      "iteration 200 / 2000: loss 0.184939\n",
      "iteration 300 / 2000: loss 0.095412\n",
      "iteration 400 / 2000: loss 0.110377\n",
      "iteration 500 / 2000: loss 0.067066\n",
      "iteration 600 / 2000: loss 0.166420\n",
      "iteration 700 / 2000: loss 0.173550\n",
      "iteration 800 / 2000: loss 0.090252\n",
      "iteration 900 / 2000: loss 0.081415\n",
      "iteration 1000 / 2000: loss 0.054436\n",
      "iteration 1100 / 2000: loss 0.142344\n",
      "iteration 1200 / 2000: loss 0.070834\n",
      "iteration 1300 / 2000: loss 0.116138\n",
      "iteration 1400 / 2000: loss 0.096322\n",
      "iteration 1500 / 2000: loss 0.045475\n",
      "iteration 1600 / 2000: loss 0.050593\n",
      "iteration 1700 / 2000: loss 0.157921\n",
      "iteration 1800 / 2000: loss 0.071706\n",
      "iteration 1900 / 2000: loss 0.060228\n",
      "Hidden Size: 50, Learning Rate: 0.01, LR Decay: 0.99, Regularization: 0.0, Validation accuracy: 0.9702\n",
      "iteration 0 / 2000: loss 2.302607\n",
      "iteration 100 / 2000: loss 0.187349\n",
      "iteration 200 / 2000: loss 0.206226\n",
      "iteration 300 / 2000: loss 0.151368\n",
      "iteration 400 / 2000: loss 0.192515\n",
      "iteration 500 / 2000: loss 0.244967\n",
      "iteration 600 / 2000: loss 0.148515\n",
      "iteration 700 / 2000: loss 0.151960\n",
      "iteration 800 / 2000: loss 0.122023\n",
      "iteration 900 / 2000: loss 0.091162\n",
      "iteration 1000 / 2000: loss 0.148527\n",
      "iteration 1100 / 2000: loss 0.129911\n",
      "iteration 1200 / 2000: loss 0.144830\n",
      "iteration 1300 / 2000: loss 0.142957\n",
      "iteration 1400 / 2000: loss 0.131839\n",
      "iteration 1500 / 2000: loss 0.144175\n",
      "iteration 1600 / 2000: loss 0.143730\n",
      "iteration 1700 / 2000: loss 0.143274\n",
      "iteration 1800 / 2000: loss 0.104211\n",
      "iteration 1900 / 2000: loss 0.115103\n",
      "Hidden Size: 50, Learning Rate: 0.01, LR Decay: 0.99, Regularization: 0.1, Validation accuracy: 0.968\n",
      "iteration 0 / 2000: loss 2.302633\n",
      "iteration 100 / 2000: loss 0.299548\n",
      "iteration 200 / 2000: loss 0.213443\n",
      "iteration 300 / 2000: loss 0.201445\n",
      "iteration 400 / 2000: loss 0.130727\n",
      "iteration 500 / 2000: loss 0.164752\n",
      "iteration 600 / 2000: loss 0.169207\n",
      "iteration 700 / 2000: loss 0.187183\n",
      "iteration 800 / 2000: loss 0.146000\n",
      "iteration 900 / 2000: loss 0.174067\n",
      "iteration 1000 / 2000: loss 0.192931\n",
      "iteration 1100 / 2000: loss 0.247832\n",
      "iteration 1200 / 2000: loss 0.209207\n",
      "iteration 1300 / 2000: loss 0.134147\n",
      "iteration 1400 / 2000: loss 0.246248\n",
      "iteration 1500 / 2000: loss 0.217142\n",
      "iteration 1600 / 2000: loss 0.143095\n",
      "iteration 1700 / 2000: loss 0.260072\n",
      "iteration 1800 / 2000: loss 0.195828\n",
      "iteration 1900 / 2000: loss 0.191543\n",
      "Hidden Size: 50, Learning Rate: 0.01, LR Decay: 0.99, Regularization: 0.2, Validation accuracy: 0.9684\n",
      "iteration 0 / 2000: loss 2.302641\n",
      "iteration 100 / 2000: loss 0.283765\n",
      "iteration 200 / 2000: loss 0.313955\n",
      "iteration 300 / 2000: loss 0.230658\n",
      "iteration 400 / 2000: loss 0.268244\n",
      "iteration 500 / 2000: loss 0.252416\n",
      "iteration 600 / 2000: loss 0.202775\n",
      "iteration 700 / 2000: loss 0.219173\n",
      "iteration 800 / 2000: loss 0.254897\n",
      "iteration 900 / 2000: loss 0.231793\n",
      "iteration 1000 / 2000: loss 0.164138\n",
      "iteration 1100 / 2000: loss 0.186467\n",
      "iteration 1200 / 2000: loss 0.217472\n",
      "iteration 1300 / 2000: loss 0.212892\n",
      "iteration 1400 / 2000: loss 0.202421\n",
      "iteration 1500 / 2000: loss 0.229858\n",
      "iteration 1600 / 2000: loss 0.224601\n",
      "iteration 1700 / 2000: loss 0.213041\n",
      "iteration 1800 / 2000: loss 0.212850\n",
      "iteration 1900 / 2000: loss 0.261186\n",
      "Hidden Size: 50, Learning Rate: 0.01, LR Decay: 0.99, Regularization: 0.30000000000000004, Validation accuracy: 0.9688\n",
      "iteration 0 / 2000: loss 2.302668\n",
      "iteration 100 / 2000: loss 0.415530\n",
      "iteration 200 / 2000: loss 0.279537\n",
      "iteration 300 / 2000: loss 0.274415\n",
      "iteration 400 / 2000: loss 0.299253\n",
      "iteration 500 / 2000: loss 0.244682\n",
      "iteration 600 / 2000: loss 0.335281\n",
      "iteration 700 / 2000: loss 0.224407\n",
      "iteration 800 / 2000: loss 0.219686\n",
      "iteration 900 / 2000: loss 0.280373\n",
      "iteration 1000 / 2000: loss 0.220671\n",
      "iteration 1100 / 2000: loss 0.183826\n",
      "iteration 1200 / 2000: loss 0.274848\n",
      "iteration 1300 / 2000: loss 0.260577\n",
      "iteration 1400 / 2000: loss 0.247061\n",
      "iteration 1500 / 2000: loss 0.228763\n",
      "iteration 1600 / 2000: loss 0.266588\n",
      "iteration 1700 / 2000: loss 0.245735\n",
      "iteration 1800 / 2000: loss 0.236236\n",
      "iteration 1900 / 2000: loss 0.235767\n",
      "Hidden Size: 50, Learning Rate: 0.01, LR Decay: 0.99, Regularization: 0.4, Validation accuracy: 0.9624\n",
      "iteration 0 / 2000: loss 2.302566\n",
      "iteration 100 / 2000: loss 0.335150\n",
      "iteration 200 / 2000: loss 0.259544\n",
      "iteration 300 / 2000: loss 0.064097\n",
      "iteration 400 / 2000: loss 0.079802\n",
      "iteration 500 / 2000: loss 0.155379\n",
      "iteration 600 / 2000: loss 0.074307\n",
      "iteration 700 / 2000: loss 0.036783\n",
      "iteration 800 / 2000: loss 0.116918\n",
      "iteration 900 / 2000: loss 0.104413\n",
      "iteration 1000 / 2000: loss 0.091373\n",
      "iteration 1100 / 2000: loss 0.064534\n",
      "iteration 1200 / 2000: loss 0.038112\n",
      "iteration 1300 / 2000: loss 0.049806\n",
      "iteration 1400 / 2000: loss 0.029255\n",
      "iteration 1500 / 2000: loss 0.042970\n",
      "iteration 1600 / 2000: loss 0.038448\n",
      "iteration 1700 / 2000: loss 0.047808\n",
      "iteration 1800 / 2000: loss 0.042573\n",
      "iteration 1900 / 2000: loss 0.043895\n",
      "Hidden Size: 50, Learning Rate: 0.01, LR Decay: 0.8175, Regularization: 0.0, Validation accuracy: 0.9758\n",
      "iteration 0 / 2000: loss 2.302599\n",
      "iteration 100 / 2000: loss 0.313436\n",
      "iteration 200 / 2000: loss 0.189138\n",
      "iteration 300 / 2000: loss 0.199615\n",
      "iteration 400 / 2000: loss 0.164749\n",
      "iteration 500 / 2000: loss 0.145291\n",
      "iteration 600 / 2000: loss 0.267977\n",
      "iteration 700 / 2000: loss 0.131398\n",
      "iteration 800 / 2000: loss 0.130414\n",
      "iteration 900 / 2000: loss 0.157057\n",
      "iteration 1000 / 2000: loss 0.149131\n",
      "iteration 1100 / 2000: loss 0.128659\n",
      "iteration 1200 / 2000: loss 0.106973\n",
      "iteration 1300 / 2000: loss 0.093810\n",
      "iteration 1400 / 2000: loss 0.166607\n",
      "iteration 1500 / 2000: loss 0.085568\n",
      "iteration 1600 / 2000: loss 0.085033\n",
      "iteration 1700 / 2000: loss 0.083185\n",
      "iteration 1800 / 2000: loss 0.100050\n",
      "iteration 1900 / 2000: loss 0.125665\n",
      "Hidden Size: 50, Learning Rate: 0.01, LR Decay: 0.8175, Regularization: 0.1, Validation accuracy: 0.9758\n",
      "iteration 0 / 2000: loss 2.302629\n",
      "iteration 100 / 2000: loss 0.332652\n",
      "iteration 200 / 2000: loss 0.220790\n",
      "iteration 300 / 2000: loss 0.148705\n",
      "iteration 400 / 2000: loss 0.191955\n",
      "iteration 500 / 2000: loss 0.239030\n",
      "iteration 600 / 2000: loss 0.186565\n",
      "iteration 700 / 2000: loss 0.206566\n",
      "iteration 800 / 2000: loss 0.154527\n",
      "iteration 900 / 2000: loss 0.140907\n",
      "iteration 1000 / 2000: loss 0.150296\n",
      "iteration 1100 / 2000: loss 0.172257\n",
      "iteration 1200 / 2000: loss 0.143189\n",
      "iteration 1300 / 2000: loss 0.192070\n",
      "iteration 1400 / 2000: loss 0.150616\n",
      "iteration 1500 / 2000: loss 0.170774\n",
      "iteration 1600 / 2000: loss 0.153895\n",
      "iteration 1700 / 2000: loss 0.149186\n",
      "iteration 1800 / 2000: loss 0.161701\n",
      "iteration 1900 / 2000: loss 0.140038\n",
      "Hidden Size: 50, Learning Rate: 0.01, LR Decay: 0.8175, Regularization: 0.2, Validation accuracy: 0.9752\n",
      "iteration 0 / 2000: loss 2.302649\n",
      "iteration 100 / 2000: loss 0.376953\n",
      "iteration 200 / 2000: loss 0.195761\n",
      "iteration 300 / 2000: loss 0.225380\n",
      "iteration 400 / 2000: loss 0.224685\n",
      "iteration 500 / 2000: loss 0.276849\n",
      "iteration 600 / 2000: loss 0.163640\n",
      "iteration 700 / 2000: loss 0.220083\n",
      "iteration 800 / 2000: loss 0.224491\n",
      "iteration 900 / 2000: loss 0.200832\n",
      "iteration 1000 / 2000: loss 0.212938\n",
      "iteration 1100 / 2000: loss 0.200298\n",
      "iteration 1200 / 2000: loss 0.176824\n",
      "iteration 1300 / 2000: loss 0.191965\n",
      "iteration 1400 / 2000: loss 0.168099\n",
      "iteration 1500 / 2000: loss 0.194928\n",
      "iteration 1600 / 2000: loss 0.196345\n",
      "iteration 1700 / 2000: loss 0.185111\n",
      "iteration 1800 / 2000: loss 0.178856\n",
      "iteration 1900 / 2000: loss 0.137878\n",
      "Hidden Size: 50, Learning Rate: 0.01, LR Decay: 0.8175, Regularization: 0.30000000000000004, Validation accuracy: 0.9722\n",
      "iteration 0 / 2000: loss 2.302658\n",
      "iteration 100 / 2000: loss 0.362388\n",
      "iteration 200 / 2000: loss 0.277093\n",
      "iteration 300 / 2000: loss 0.321791\n",
      "iteration 400 / 2000: loss 0.209518\n",
      "iteration 500 / 2000: loss 0.363153\n",
      "iteration 600 / 2000: loss 0.264503\n",
      "iteration 700 / 2000: loss 0.223090\n",
      "iteration 800 / 2000: loss 0.226490\n",
      "iteration 900 / 2000: loss 0.237644\n",
      "iteration 1000 / 2000: loss 0.190776\n",
      "iteration 1100 / 2000: loss 0.183029\n",
      "iteration 1200 / 2000: loss 0.244028\n",
      "iteration 1300 / 2000: loss 0.193213\n",
      "iteration 1400 / 2000: loss 0.199639\n",
      "iteration 1500 / 2000: loss 0.185650\n",
      "iteration 1600 / 2000: loss 0.257009\n",
      "iteration 1700 / 2000: loss 0.296761\n",
      "iteration 1800 / 2000: loss 0.235542\n",
      "iteration 1900 / 2000: loss 0.294907\n",
      "Hidden Size: 50, Learning Rate: 0.01, LR Decay: 0.8175, Regularization: 0.4, Validation accuracy: 0.9724\n",
      "iteration 0 / 2000: loss 2.302573\n",
      "iteration 100 / 2000: loss 0.339630\n",
      "iteration 200 / 2000: loss 0.217767\n",
      "iteration 300 / 2000: loss 0.131925\n",
      "iteration 400 / 2000: loss 0.205592\n",
      "iteration 500 / 2000: loss 0.104394\n",
      "iteration 600 / 2000: loss 0.079025\n",
      "iteration 700 / 2000: loss 0.041919\n",
      "iteration 800 / 2000: loss 0.152534\n",
      "iteration 900 / 2000: loss 0.099984\n",
      "iteration 1000 / 2000: loss 0.041308\n",
      "iteration 1100 / 2000: loss 0.095959\n",
      "iteration 1200 / 2000: loss 0.097791\n",
      "iteration 1300 / 2000: loss 0.077977\n",
      "iteration 1400 / 2000: loss 0.051004\n",
      "iteration 1500 / 2000: loss 0.087026\n",
      "iteration 1600 / 2000: loss 0.044441\n",
      "iteration 1700 / 2000: loss 0.114800\n",
      "iteration 1800 / 2000: loss 0.068381\n",
      "iteration 1900 / 2000: loss 0.089955\n",
      "Hidden Size: 50, Learning Rate: 0.01, LR Decay: 0.645, Regularization: 0.0, Validation accuracy: 0.9744\n",
      "iteration 0 / 2000: loss 2.302612\n",
      "iteration 100 / 2000: loss 0.237932\n",
      "iteration 200 / 2000: loss 0.202245\n",
      "iteration 300 / 2000: loss 0.152843\n",
      "iteration 400 / 2000: loss 0.138202\n",
      "iteration 500 / 2000: loss 0.126424\n",
      "iteration 600 / 2000: loss 0.143391\n",
      "iteration 700 / 2000: loss 0.136107\n",
      "iteration 800 / 2000: loss 0.155345\n",
      "iteration 900 / 2000: loss 0.105286\n",
      "iteration 1000 / 2000: loss 0.123892\n",
      "iteration 1100 / 2000: loss 0.116467\n",
      "iteration 1200 / 2000: loss 0.093414\n",
      "iteration 1300 / 2000: loss 0.155398\n",
      "iteration 1400 / 2000: loss 0.127321\n",
      "iteration 1500 / 2000: loss 0.106756\n",
      "iteration 1600 / 2000: loss 0.104544\n",
      "iteration 1700 / 2000: loss 0.151313\n",
      "iteration 1800 / 2000: loss 0.148056\n",
      "iteration 1900 / 2000: loss 0.119108\n",
      "Hidden Size: 50, Learning Rate: 0.01, LR Decay: 0.645, Regularization: 0.1, Validation accuracy: 0.9726\n",
      "iteration 0 / 2000: loss 2.302609\n",
      "iteration 100 / 2000: loss 0.309805\n",
      "iteration 200 / 2000: loss 0.309272\n",
      "iteration 300 / 2000: loss 0.283888\n",
      "iteration 400 / 2000: loss 0.176417\n",
      "iteration 500 / 2000: loss 0.177018\n",
      "iteration 600 / 2000: loss 0.172062\n",
      "iteration 700 / 2000: loss 0.190303\n",
      "iteration 800 / 2000: loss 0.167416\n",
      "iteration 900 / 2000: loss 0.214074\n",
      "iteration 1000 / 2000: loss 0.137344\n",
      "iteration 1100 / 2000: loss 0.132026\n",
      "iteration 1200 / 2000: loss 0.143170\n",
      "iteration 1300 / 2000: loss 0.145393\n",
      "iteration 1400 / 2000: loss 0.180134\n",
      "iteration 1500 / 2000: loss 0.143457\n",
      "iteration 1600 / 2000: loss 0.199430\n",
      "iteration 1700 / 2000: loss 0.156588\n",
      "iteration 1800 / 2000: loss 0.152924\n",
      "iteration 1900 / 2000: loss 0.144290\n",
      "Hidden Size: 50, Learning Rate: 0.01, LR Decay: 0.645, Regularization: 0.2, Validation accuracy: 0.9722\n",
      "iteration 0 / 2000: loss 2.302639\n",
      "iteration 100 / 2000: loss 0.407597\n",
      "iteration 200 / 2000: loss 0.338040\n",
      "iteration 300 / 2000: loss 0.171901\n",
      "iteration 400 / 2000: loss 0.162742\n",
      "iteration 500 / 2000: loss 0.240028\n",
      "iteration 600 / 2000: loss 0.236282\n",
      "iteration 700 / 2000: loss 0.224130\n",
      "iteration 800 / 2000: loss 0.251164\n",
      "iteration 900 / 2000: loss 0.155266\n",
      "iteration 1000 / 2000: loss 0.159646\n",
      "iteration 1100 / 2000: loss 0.193891\n",
      "iteration 1200 / 2000: loss 0.189237\n",
      "iteration 1300 / 2000: loss 0.191634\n",
      "iteration 1400 / 2000: loss 0.203588\n",
      "iteration 1500 / 2000: loss 0.153650\n",
      "iteration 1600 / 2000: loss 0.181567\n",
      "iteration 1700 / 2000: loss 0.191476\n",
      "iteration 1800 / 2000: loss 0.166984\n",
      "iteration 1900 / 2000: loss 0.176791\n",
      "Hidden Size: 50, Learning Rate: 0.01, LR Decay: 0.645, Regularization: 0.30000000000000004, Validation accuracy: 0.9714\n",
      "iteration 0 / 2000: loss 2.302646\n",
      "iteration 100 / 2000: loss 0.334766\n",
      "iteration 200 / 2000: loss 0.215222\n",
      "iteration 300 / 2000: loss 0.264471\n",
      "iteration 400 / 2000: loss 0.279928\n",
      "iteration 500 / 2000: loss 0.304413\n",
      "iteration 600 / 2000: loss 0.254232\n",
      "iteration 700 / 2000: loss 0.201059\n",
      "iteration 800 / 2000: loss 0.210764\n",
      "iteration 900 / 2000: loss 0.279212\n",
      "iteration 1000 / 2000: loss 0.241180\n",
      "iteration 1100 / 2000: loss 0.190934\n",
      "iteration 1200 / 2000: loss 0.181073\n",
      "iteration 1300 / 2000: loss 0.189224\n",
      "iteration 1400 / 2000: loss 0.199110\n",
      "iteration 1500 / 2000: loss 0.215482\n",
      "iteration 1600 / 2000: loss 0.206544\n",
      "iteration 1700 / 2000: loss 0.183067\n",
      "iteration 1800 / 2000: loss 0.194189\n",
      "iteration 1900 / 2000: loss 0.178456\n",
      "Hidden Size: 50, Learning Rate: 0.01, LR Decay: 0.645, Regularization: 0.4, Validation accuracy: 0.9706\n",
      "iteration 0 / 2000: loss 2.302580\n",
      "iteration 100 / 2000: loss 0.320344\n",
      "iteration 200 / 2000: loss 0.227968\n",
      "iteration 300 / 2000: loss 0.117401\n",
      "iteration 400 / 2000: loss 0.162835\n",
      "iteration 500 / 2000: loss 0.150756\n",
      "iteration 600 / 2000: loss 0.155008\n",
      "iteration 700 / 2000: loss 0.081563\n",
      "iteration 800 / 2000: loss 0.123026\n",
      "iteration 900 / 2000: loss 0.116351\n",
      "iteration 1000 / 2000: loss 0.101836\n",
      "iteration 1100 / 2000: loss 0.134979\n",
      "iteration 1200 / 2000: loss 0.082640\n",
      "iteration 1300 / 2000: loss 0.137672\n",
      "iteration 1400 / 2000: loss 0.079456\n",
      "iteration 1500 / 2000: loss 0.105813\n",
      "iteration 1600 / 2000: loss 0.107099\n",
      "iteration 1700 / 2000: loss 0.116875\n",
      "iteration 1800 / 2000: loss 0.106364\n",
      "iteration 1900 / 2000: loss 0.165022\n",
      "Hidden Size: 50, Learning Rate: 0.01, LR Decay: 0.47250000000000003, Regularization: 0.0, Validation accuracy: 0.967\n",
      "iteration 0 / 2000: loss 2.302577\n",
      "iteration 100 / 2000: loss 0.294657\n",
      "iteration 200 / 2000: loss 0.185575\n",
      "iteration 300 / 2000: loss 0.165580\n",
      "iteration 400 / 2000: loss 0.220253\n",
      "iteration 500 / 2000: loss 0.153098\n",
      "iteration 600 / 2000: loss 0.169826\n",
      "iteration 700 / 2000: loss 0.139386\n",
      "iteration 800 / 2000: loss 0.128354\n",
      "iteration 900 / 2000: loss 0.109238\n",
      "iteration 1000 / 2000: loss 0.170682\n",
      "iteration 1100 / 2000: loss 0.121526\n",
      "iteration 1200 / 2000: loss 0.158245\n",
      "iteration 1300 / 2000: loss 0.199950\n",
      "iteration 1400 / 2000: loss 0.136305\n",
      "iteration 1500 / 2000: loss 0.100385\n",
      "iteration 1600 / 2000: loss 0.151722\n",
      "iteration 1700 / 2000: loss 0.130512\n",
      "iteration 1800 / 2000: loss 0.140660\n",
      "iteration 1900 / 2000: loss 0.132830\n",
      "Hidden Size: 50, Learning Rate: 0.01, LR Decay: 0.47250000000000003, Regularization: 0.1, Validation accuracy: 0.9648\n",
      "iteration 0 / 2000: loss 2.302629\n",
      "iteration 100 / 2000: loss 0.411024\n",
      "iteration 200 / 2000: loss 0.278895\n",
      "iteration 300 / 2000: loss 0.338695\n",
      "iteration 400 / 2000: loss 0.226853\n",
      "iteration 500 / 2000: loss 0.170131\n",
      "iteration 600 / 2000: loss 0.243596\n",
      "iteration 700 / 2000: loss 0.236193\n",
      "iteration 800 / 2000: loss 0.159491\n",
      "iteration 900 / 2000: loss 0.200652\n",
      "iteration 1000 / 2000: loss 0.130805\n",
      "iteration 1100 / 2000: loss 0.184610\n",
      "iteration 1200 / 2000: loss 0.173999\n",
      "iteration 1300 / 2000: loss 0.255841\n",
      "iteration 1400 / 2000: loss 0.165343\n",
      "iteration 1500 / 2000: loss 0.172512\n",
      "iteration 1600 / 2000: loss 0.157850\n",
      "iteration 1700 / 2000: loss 0.133757\n",
      "iteration 1800 / 2000: loss 0.155230\n",
      "iteration 1900 / 2000: loss 0.135245\n",
      "Hidden Size: 50, Learning Rate: 0.01, LR Decay: 0.47250000000000003, Regularization: 0.2, Validation accuracy: 0.9632\n",
      "iteration 0 / 2000: loss 2.302652\n",
      "iteration 100 / 2000: loss 0.295884\n",
      "iteration 200 / 2000: loss 0.246220\n",
      "iteration 300 / 2000: loss 0.226589\n",
      "iteration 400 / 2000: loss 0.256433\n",
      "iteration 500 / 2000: loss 0.190434\n",
      "iteration 600 / 2000: loss 0.177537\n",
      "iteration 700 / 2000: loss 0.210434\n",
      "iteration 800 / 2000: loss 0.230381\n",
      "iteration 900 / 2000: loss 0.219437\n",
      "iteration 1000 / 2000: loss 0.191211\n",
      "iteration 1100 / 2000: loss 0.189141\n",
      "iteration 1200 / 2000: loss 0.217300\n",
      "iteration 1300 / 2000: loss 0.177519\n",
      "iteration 1400 / 2000: loss 0.235790\n",
      "iteration 1500 / 2000: loss 0.173100\n",
      "iteration 1600 / 2000: loss 0.169521\n",
      "iteration 1700 / 2000: loss 0.208065\n",
      "iteration 1800 / 2000: loss 0.238329\n",
      "iteration 1900 / 2000: loss 0.215134\n",
      "Hidden Size: 50, Learning Rate: 0.01, LR Decay: 0.47250000000000003, Regularization: 0.30000000000000004, Validation accuracy: 0.9616\n",
      "iteration 0 / 2000: loss 2.302667\n",
      "iteration 100 / 2000: loss 0.364606\n",
      "iteration 200 / 2000: loss 0.329696\n",
      "iteration 300 / 2000: loss 0.273954\n",
      "iteration 400 / 2000: loss 0.285166\n",
      "iteration 500 / 2000: loss 0.274784\n",
      "iteration 600 / 2000: loss 0.221938\n",
      "iteration 700 / 2000: loss 0.307456\n",
      "iteration 800 / 2000: loss 0.193127\n",
      "iteration 900 / 2000: loss 0.348512\n",
      "iteration 1000 / 2000: loss 0.246358\n",
      "iteration 1100 / 2000: loss 0.214403\n",
      "iteration 1200 / 2000: loss 0.232339\n",
      "iteration 1300 / 2000: loss 0.233208\n",
      "iteration 1400 / 2000: loss 0.197855\n",
      "iteration 1500 / 2000: loss 0.225501\n",
      "iteration 1600 / 2000: loss 0.201683\n",
      "iteration 1700 / 2000: loss 0.186755\n",
      "iteration 1800 / 2000: loss 0.272608\n",
      "iteration 1900 / 2000: loss 0.241032\n",
      "Hidden Size: 50, Learning Rate: 0.01, LR Decay: 0.47250000000000003, Regularization: 0.4, Validation accuracy: 0.9628\n",
      "iteration 0 / 2000: loss 2.302582\n",
      "iteration 100 / 2000: loss 0.312529\n",
      "iteration 200 / 2000: loss 0.247222\n",
      "iteration 300 / 2000: loss 0.161922\n",
      "iteration 400 / 2000: loss 0.120999\n",
      "iteration 500 / 2000: loss 0.203153\n",
      "iteration 600 / 2000: loss 0.206376\n",
      "iteration 700 / 2000: loss 0.235772\n",
      "iteration 800 / 2000: loss 0.211136\n",
      "iteration 900 / 2000: loss 0.152219\n",
      "iteration 1000 / 2000: loss 0.122717\n",
      "iteration 1100 / 2000: loss 0.194309\n",
      "iteration 1200 / 2000: loss 0.092208\n",
      "iteration 1300 / 2000: loss 0.151711\n",
      "iteration 1400 / 2000: loss 0.126799\n",
      "iteration 1500 / 2000: loss 0.205695\n",
      "iteration 1600 / 2000: loss 0.151575\n",
      "iteration 1700 / 2000: loss 0.111208\n",
      "iteration 1800 / 2000: loss 0.177908\n",
      "iteration 1900 / 2000: loss 0.128376\n",
      "Hidden Size: 50, Learning Rate: 0.01, LR Decay: 0.3, Regularization: 0.0, Validation accuracy: 0.9548\n",
      "iteration 0 / 2000: loss 2.302598\n",
      "iteration 100 / 2000: loss 0.305595\n",
      "iteration 200 / 2000: loss 0.237551\n",
      "iteration 300 / 2000: loss 0.242665\n",
      "iteration 400 / 2000: loss 0.242115\n",
      "iteration 500 / 2000: loss 0.154992\n",
      "iteration 600 / 2000: loss 0.164542\n",
      "iteration 700 / 2000: loss 0.188983\n",
      "iteration 800 / 2000: loss 0.189012\n",
      "iteration 900 / 2000: loss 0.213453\n",
      "iteration 1000 / 2000: loss 0.255476\n",
      "iteration 1100 / 2000: loss 0.181616\n",
      "iteration 1200 / 2000: loss 0.156963\n",
      "iteration 1300 / 2000: loss 0.253725\n",
      "iteration 1400 / 2000: loss 0.241643\n",
      "iteration 1500 / 2000: loss 0.166953\n",
      "iteration 1600 / 2000: loss 0.156679\n",
      "iteration 1700 / 2000: loss 0.174775\n",
      "iteration 1800 / 2000: loss 0.146148\n",
      "iteration 1900 / 2000: loss 0.133628\n",
      "Hidden Size: 50, Learning Rate: 0.01, LR Decay: 0.3, Regularization: 0.1, Validation accuracy: 0.9528\n",
      "iteration 0 / 2000: loss 2.302604\n",
      "iteration 100 / 2000: loss 0.382415\n",
      "iteration 200 / 2000: loss 0.223606\n",
      "iteration 300 / 2000: loss 0.230721\n",
      "iteration 400 / 2000: loss 0.250561\n",
      "iteration 500 / 2000: loss 0.166409\n",
      "iteration 600 / 2000: loss 0.221017\n",
      "iteration 700 / 2000: loss 0.237641\n",
      "iteration 800 / 2000: loss 0.259264\n",
      "iteration 900 / 2000: loss 0.164443\n",
      "iteration 1000 / 2000: loss 0.175631\n",
      "iteration 1100 / 2000: loss 0.190684\n",
      "iteration 1200 / 2000: loss 0.193754\n",
      "iteration 1300 / 2000: loss 0.221682\n",
      "iteration 1400 / 2000: loss 0.201973\n",
      "iteration 1500 / 2000: loss 0.242124\n",
      "iteration 1600 / 2000: loss 0.287181\n",
      "iteration 1700 / 2000: loss 0.190777\n",
      "iteration 1800 / 2000: loss 0.142794\n",
      "iteration 1900 / 2000: loss 0.287328\n",
      "Hidden Size: 50, Learning Rate: 0.01, LR Decay: 0.3, Regularization: 0.2, Validation accuracy: 0.9524\n",
      "iteration 0 / 2000: loss 2.302641\n",
      "iteration 100 / 2000: loss 0.347836\n",
      "iteration 200 / 2000: loss 0.393340\n",
      "iteration 300 / 2000: loss 0.331052\n",
      "iteration 400 / 2000: loss 0.335611\n",
      "iteration 500 / 2000: loss 0.293066\n",
      "iteration 600 / 2000: loss 0.218761\n",
      "iteration 700 / 2000: loss 0.287133\n",
      "iteration 800 / 2000: loss 0.254168\n",
      "iteration 900 / 2000: loss 0.172411\n",
      "iteration 1000 / 2000: loss 0.261445\n",
      "iteration 1100 / 2000: loss 0.233977\n",
      "iteration 1200 / 2000: loss 0.268643\n",
      "iteration 1300 / 2000: loss 0.187453\n",
      "iteration 1400 / 2000: loss 0.246030\n",
      "iteration 1500 / 2000: loss 0.239743\n",
      "iteration 1600 / 2000: loss 0.214936\n",
      "iteration 1700 / 2000: loss 0.221141\n",
      "iteration 1800 / 2000: loss 0.253492\n",
      "iteration 1900 / 2000: loss 0.259791\n",
      "Hidden Size: 50, Learning Rate: 0.01, LR Decay: 0.3, Regularization: 0.30000000000000004, Validation accuracy: 0.951\n",
      "iteration 0 / 2000: loss 2.302659\n",
      "iteration 100 / 2000: loss 0.391733\n",
      "iteration 200 / 2000: loss 0.319968\n",
      "iteration 300 / 2000: loss 0.336263\n",
      "iteration 400 / 2000: loss 0.283069\n",
      "iteration 500 / 2000: loss 0.281859\n",
      "iteration 600 / 2000: loss 0.333251\n",
      "iteration 700 / 2000: loss 0.249109\n",
      "iteration 800 / 2000: loss 0.216992\n",
      "iteration 900 / 2000: loss 0.225114\n",
      "iteration 1000 / 2000: loss 0.211008\n",
      "iteration 1100 / 2000: loss 0.279727\n",
      "iteration 1200 / 2000: loss 0.270992\n",
      "iteration 1300 / 2000: loss 0.261867\n",
      "iteration 1400 / 2000: loss 0.246085\n",
      "iteration 1500 / 2000: loss 0.230970\n",
      "iteration 1600 / 2000: loss 0.231662\n",
      "iteration 1700 / 2000: loss 0.251017\n",
      "iteration 1800 / 2000: loss 0.326203\n",
      "iteration 1900 / 2000: loss 0.267898\n",
      "Hidden Size: 50, Learning Rate: 0.01, LR Decay: 0.3, Regularization: 0.4, Validation accuracy: 0.949\n",
      "iteration 0 / 2000: loss 2.302582\n",
      "iteration 100 / 2000: loss 0.254758\n",
      "iteration 200 / 2000: loss 0.242839\n",
      "iteration 300 / 2000: loss 0.179586\n",
      "iteration 400 / 2000: loss 0.158365\n",
      "iteration 500 / 2000: loss 0.240645\n",
      "iteration 600 / 2000: loss 0.073461\n",
      "iteration 700 / 2000: loss 0.067801\n",
      "iteration 800 / 2000: loss 0.056970\n",
      "iteration 900 / 2000: loss 0.076526\n",
      "iteration 1000 / 2000: loss 0.065566\n",
      "iteration 1100 / 2000: loss 0.067988\n",
      "iteration 1200 / 2000: loss 0.127564\n",
      "iteration 1300 / 2000: loss 0.018316\n",
      "iteration 1400 / 2000: loss 0.074050\n",
      "iteration 1500 / 2000: loss 0.033616\n",
      "iteration 1600 / 2000: loss 0.015883\n",
      "iteration 1700 / 2000: loss 0.039916\n",
      "iteration 1800 / 2000: loss 0.048594\n",
      "iteration 1900 / 2000: loss 0.045118\n",
      "Hidden Size: 50, Learning Rate: 0.0075000250000000004, LR Decay: 0.99, Regularization: 0.0, Validation accuracy: 0.9712\n",
      "iteration 0 / 2000: loss 2.302603\n",
      "iteration 100 / 2000: loss 0.232375\n",
      "iteration 200 / 2000: loss 0.146015\n",
      "iteration 300 / 2000: loss 0.167600\n",
      "iteration 400 / 2000: loss 0.171322\n",
      "iteration 500 / 2000: loss 0.170169\n",
      "iteration 600 / 2000: loss 0.218182\n",
      "iteration 700 / 2000: loss 0.118948\n",
      "iteration 800 / 2000: loss 0.115819\n",
      "iteration 900 / 2000: loss 0.108710\n",
      "iteration 1000 / 2000: loss 0.130522\n",
      "iteration 1100 / 2000: loss 0.178433\n",
      "iteration 1200 / 2000: loss 0.098834\n",
      "iteration 1300 / 2000: loss 0.104666\n",
      "iteration 1400 / 2000: loss 0.139054\n",
      "iteration 1500 / 2000: loss 0.114715\n",
      "iteration 1600 / 2000: loss 0.141627\n",
      "iteration 1700 / 2000: loss 0.112217\n",
      "iteration 1800 / 2000: loss 0.147692\n",
      "iteration 1900 / 2000: loss 0.117421\n",
      "Hidden Size: 50, Learning Rate: 0.0075000250000000004, LR Decay: 0.99, Regularization: 0.1, Validation accuracy: 0.9704\n",
      "iteration 0 / 2000: loss 2.302619\n",
      "iteration 100 / 2000: loss 0.292223\n",
      "iteration 200 / 2000: loss 0.192337\n",
      "iteration 300 / 2000: loss 0.314593\n",
      "iteration 400 / 2000: loss 0.196698\n",
      "iteration 500 / 2000: loss 0.217513\n",
      "iteration 600 / 2000: loss 0.159168\n",
      "iteration 700 / 2000: loss 0.233856\n",
      "iteration 800 / 2000: loss 0.164920\n",
      "iteration 900 / 2000: loss 0.129745\n",
      "iteration 1000 / 2000: loss 0.158551\n",
      "iteration 1100 / 2000: loss 0.182575\n",
      "iteration 1200 / 2000: loss 0.171902\n",
      "iteration 1300 / 2000: loss 0.136936\n",
      "iteration 1400 / 2000: loss 0.230628\n",
      "iteration 1500 / 2000: loss 0.157333\n",
      "iteration 1600 / 2000: loss 0.161513\n",
      "iteration 1700 / 2000: loss 0.135593\n",
      "iteration 1800 / 2000: loss 0.152732\n",
      "iteration 1900 / 2000: loss 0.150061\n",
      "Hidden Size: 50, Learning Rate: 0.0075000250000000004, LR Decay: 0.99, Regularization: 0.2, Validation accuracy: 0.971\n",
      "iteration 0 / 2000: loss 2.302652\n",
      "iteration 100 / 2000: loss 0.364432\n",
      "iteration 200 / 2000: loss 0.276709\n",
      "iteration 300 / 2000: loss 0.251371\n",
      "iteration 400 / 2000: loss 0.248004\n",
      "iteration 500 / 2000: loss 0.204192\n",
      "iteration 600 / 2000: loss 0.231604\n",
      "iteration 700 / 2000: loss 0.201669\n",
      "iteration 800 / 2000: loss 0.182141\n",
      "iteration 900 / 2000: loss 0.185914\n",
      "iteration 1000 / 2000: loss 0.164485\n",
      "iteration 1100 / 2000: loss 0.221057\n",
      "iteration 1200 / 2000: loss 0.159033\n",
      "iteration 1300 / 2000: loss 0.256283\n",
      "iteration 1400 / 2000: loss 0.176675\n",
      "iteration 1500 / 2000: loss 0.153932\n",
      "iteration 1600 / 2000: loss 0.190196\n",
      "iteration 1700 / 2000: loss 0.215018\n",
      "iteration 1800 / 2000: loss 0.174953\n",
      "iteration 1900 / 2000: loss 0.173906\n",
      "Hidden Size: 50, Learning Rate: 0.0075000250000000004, LR Decay: 0.99, Regularization: 0.30000000000000004, Validation accuracy: 0.9676\n",
      "iteration 0 / 2000: loss 2.302669\n",
      "iteration 100 / 2000: loss 0.347172\n",
      "iteration 200 / 2000: loss 0.226445\n",
      "iteration 300 / 2000: loss 0.279722\n",
      "iteration 400 / 2000: loss 0.260484\n",
      "iteration 500 / 2000: loss 0.309397\n",
      "iteration 600 / 2000: loss 0.212450\n",
      "iteration 700 / 2000: loss 0.212286\n",
      "iteration 800 / 2000: loss 0.239131\n",
      "iteration 900 / 2000: loss 0.234047\n",
      "iteration 1000 / 2000: loss 0.238789\n",
      "iteration 1100 / 2000: loss 0.226179\n",
      "iteration 1200 / 2000: loss 0.220091\n",
      "iteration 1300 / 2000: loss 0.196537\n",
      "iteration 1400 / 2000: loss 0.206150\n",
      "iteration 1500 / 2000: loss 0.240326\n",
      "iteration 1600 / 2000: loss 0.294148\n",
      "iteration 1700 / 2000: loss 0.194651\n",
      "iteration 1800 / 2000: loss 0.230349\n",
      "iteration 1900 / 2000: loss 0.264340\n",
      "Hidden Size: 50, Learning Rate: 0.0075000250000000004, LR Decay: 0.99, Regularization: 0.4, Validation accuracy: 0.9662\n",
      "iteration 0 / 2000: loss 2.302587\n",
      "iteration 100 / 2000: loss 0.300038\n",
      "iteration 200 / 2000: loss 0.201465\n",
      "iteration 300 / 2000: loss 0.173503\n",
      "iteration 400 / 2000: loss 0.080459\n",
      "iteration 500 / 2000: loss 0.109637\n",
      "iteration 600 / 2000: loss 0.065579\n",
      "iteration 700 / 2000: loss 0.059487\n",
      "iteration 800 / 2000: loss 0.125413\n",
      "iteration 900 / 2000: loss 0.085300\n",
      "iteration 1000 / 2000: loss 0.056030\n",
      "iteration 1100 / 2000: loss 0.037362\n",
      "iteration 1200 / 2000: loss 0.074690\n",
      "iteration 1300 / 2000: loss 0.027218\n",
      "iteration 1400 / 2000: loss 0.027746\n",
      "iteration 1500 / 2000: loss 0.075155\n",
      "iteration 1600 / 2000: loss 0.068536\n",
      "iteration 1700 / 2000: loss 0.036551\n",
      "iteration 1800 / 2000: loss 0.121615\n",
      "iteration 1900 / 2000: loss 0.036992\n",
      "Hidden Size: 50, Learning Rate: 0.0075000250000000004, LR Decay: 0.8175, Regularization: 0.0, Validation accuracy: 0.9738\n",
      "iteration 0 / 2000: loss 2.302596\n",
      "iteration 100 / 2000: loss 0.288749\n",
      "iteration 200 / 2000: loss 0.191217\n",
      "iteration 300 / 2000: loss 0.135539\n",
      "iteration 400 / 2000: loss 0.173524\n",
      "iteration 500 / 2000: loss 0.157239\n",
      "iteration 600 / 2000: loss 0.089146\n",
      "iteration 700 / 2000: loss 0.151318\n",
      "iteration 800 / 2000: loss 0.158886\n",
      "iteration 900 / 2000: loss 0.104393\n",
      "iteration 1000 / 2000: loss 0.134331\n",
      "iteration 1100 / 2000: loss 0.134575\n",
      "iteration 1200 / 2000: loss 0.130927\n",
      "iteration 1300 / 2000: loss 0.155940\n",
      "iteration 1400 / 2000: loss 0.106961\n",
      "iteration 1500 / 2000: loss 0.122218\n",
      "iteration 1600 / 2000: loss 0.135266\n",
      "iteration 1700 / 2000: loss 0.151589\n",
      "iteration 1800 / 2000: loss 0.099668\n",
      "iteration 1900 / 2000: loss 0.115188\n",
      "Hidden Size: 50, Learning Rate: 0.0075000250000000004, LR Decay: 0.8175, Regularization: 0.1, Validation accuracy: 0.9756\n",
      "iteration 0 / 2000: loss 2.302628\n",
      "iteration 100 / 2000: loss 0.291096\n",
      "iteration 200 / 2000: loss 0.337557\n",
      "iteration 300 / 2000: loss 0.151640\n",
      "iteration 400 / 2000: loss 0.198666\n",
      "iteration 500 / 2000: loss 0.180468\n",
      "iteration 600 / 2000: loss 0.191282\n",
      "iteration 700 / 2000: loss 0.151872\n",
      "iteration 800 / 2000: loss 0.196787\n",
      "iteration 900 / 2000: loss 0.174995\n",
      "iteration 1000 / 2000: loss 0.178400\n",
      "iteration 1100 / 2000: loss 0.128948\n",
      "iteration 1200 / 2000: loss 0.218696\n",
      "iteration 1300 / 2000: loss 0.148942\n",
      "iteration 1400 / 2000: loss 0.172251\n",
      "iteration 1500 / 2000: loss 0.110211\n",
      "iteration 1600 / 2000: loss 0.134506\n",
      "iteration 1700 / 2000: loss 0.143318\n",
      "iteration 1800 / 2000: loss 0.139264\n",
      "iteration 1900 / 2000: loss 0.114236\n",
      "Hidden Size: 50, Learning Rate: 0.0075000250000000004, LR Decay: 0.8175, Regularization: 0.2, Validation accuracy: 0.9738\n",
      "iteration 0 / 2000: loss 2.302644\n",
      "iteration 100 / 2000: loss 0.368259\n",
      "iteration 200 / 2000: loss 0.309337\n",
      "iteration 300 / 2000: loss 0.262370\n",
      "iteration 400 / 2000: loss 0.234598\n",
      "iteration 500 / 2000: loss 0.365506\n",
      "iteration 600 / 2000: loss 0.199763\n",
      "iteration 700 / 2000: loss 0.171880\n",
      "iteration 800 / 2000: loss 0.224163\n",
      "iteration 900 / 2000: loss 0.238809\n",
      "iteration 1000 / 2000: loss 0.226149\n",
      "iteration 1100 / 2000: loss 0.204525\n",
      "iteration 1200 / 2000: loss 0.205377\n",
      "iteration 1300 / 2000: loss 0.174223\n",
      "iteration 1400 / 2000: loss 0.194848\n",
      "iteration 1500 / 2000: loss 0.243837\n",
      "iteration 1600 / 2000: loss 0.215233\n",
      "iteration 1700 / 2000: loss 0.198377\n",
      "iteration 1800 / 2000: loss 0.187849\n",
      "iteration 1900 / 2000: loss 0.154076\n",
      "Hidden Size: 50, Learning Rate: 0.0075000250000000004, LR Decay: 0.8175, Regularization: 0.30000000000000004, Validation accuracy: 0.9718\n",
      "iteration 0 / 2000: loss 2.302657\n",
      "iteration 100 / 2000: loss 0.339706\n",
      "iteration 200 / 2000: loss 0.279412\n",
      "iteration 300 / 2000: loss 0.201049\n",
      "iteration 400 / 2000: loss 0.280753\n",
      "iteration 500 / 2000: loss 0.256274\n",
      "iteration 600 / 2000: loss 0.215730\n",
      "iteration 700 / 2000: loss 0.249831\n",
      "iteration 800 / 2000: loss 0.225817\n",
      "iteration 900 / 2000: loss 0.233258\n",
      "iteration 1000 / 2000: loss 0.202195\n",
      "iteration 1100 / 2000: loss 0.231151\n",
      "iteration 1200 / 2000: loss 0.203421\n",
      "iteration 1300 / 2000: loss 0.190413\n",
      "iteration 1400 / 2000: loss 0.224116\n",
      "iteration 1500 / 2000: loss 0.261851\n",
      "iteration 1600 / 2000: loss 0.289703\n",
      "iteration 1700 / 2000: loss 0.222872\n",
      "iteration 1800 / 2000: loss 0.207919\n",
      "iteration 1900 / 2000: loss 0.209702\n",
      "Hidden Size: 50, Learning Rate: 0.0075000250000000004, LR Decay: 0.8175, Regularization: 0.4, Validation accuracy: 0.9702\n",
      "iteration 0 / 2000: loss 2.302591\n",
      "iteration 100 / 2000: loss 0.214694\n",
      "iteration 200 / 2000: loss 0.142255\n",
      "iteration 300 / 2000: loss 0.167740\n",
      "iteration 400 / 2000: loss 0.129048\n",
      "iteration 500 / 2000: loss 0.070429\n",
      "iteration 600 / 2000: loss 0.103801\n",
      "iteration 700 / 2000: loss 0.124482\n",
      "iteration 800 / 2000: loss 0.082888\n",
      "iteration 900 / 2000: loss 0.078988\n",
      "iteration 1000 / 2000: loss 0.089436\n",
      "iteration 1100 / 2000: loss 0.105704\n",
      "iteration 1200 / 2000: loss 0.076475\n",
      "iteration 1300 / 2000: loss 0.045098\n",
      "iteration 1400 / 2000: loss 0.062668\n",
      "iteration 1500 / 2000: loss 0.070419\n",
      "iteration 1600 / 2000: loss 0.101977\n",
      "iteration 1700 / 2000: loss 0.034041\n",
      "iteration 1800 / 2000: loss 0.110972\n",
      "iteration 1900 / 2000: loss 0.056943\n",
      "Hidden Size: 50, Learning Rate: 0.0075000250000000004, LR Decay: 0.645, Regularization: 0.0, Validation accuracy: 0.9718\n",
      "iteration 0 / 2000: loss 2.302610\n",
      "iteration 100 / 2000: loss 0.315694\n",
      "iteration 200 / 2000: loss 0.226678\n",
      "iteration 300 / 2000: loss 0.285541\n",
      "iteration 400 / 2000: loss 0.185797\n",
      "iteration 500 / 2000: loss 0.149206\n",
      "iteration 600 / 2000: loss 0.162103\n",
      "iteration 700 / 2000: loss 0.148869\n",
      "iteration 800 / 2000: loss 0.205615\n",
      "iteration 900 / 2000: loss 0.154502\n",
      "iteration 1000 / 2000: loss 0.177260\n",
      "iteration 1100 / 2000: loss 0.130779\n",
      "iteration 1200 / 2000: loss 0.097932\n",
      "iteration 1300 / 2000: loss 0.129599\n",
      "iteration 1400 / 2000: loss 0.144003\n",
      "iteration 1500 / 2000: loss 0.144743\n",
      "iteration 1600 / 2000: loss 0.137955\n",
      "iteration 1700 / 2000: loss 0.125972\n",
      "iteration 1800 / 2000: loss 0.131238\n",
      "iteration 1900 / 2000: loss 0.127992\n",
      "Hidden Size: 50, Learning Rate: 0.0075000250000000004, LR Decay: 0.645, Regularization: 0.1, Validation accuracy: 0.97\n",
      "iteration 0 / 2000: loss 2.302627\n",
      "iteration 100 / 2000: loss 0.357420\n",
      "iteration 200 / 2000: loss 0.218818\n",
      "iteration 300 / 2000: loss 0.268919\n",
      "iteration 400 / 2000: loss 0.188937\n",
      "iteration 500 / 2000: loss 0.157841\n",
      "iteration 600 / 2000: loss 0.207951\n",
      "iteration 700 / 2000: loss 0.155752\n",
      "iteration 800 / 2000: loss 0.152665\n",
      "iteration 900 / 2000: loss 0.143147\n",
      "iteration 1000 / 2000: loss 0.201823\n",
      "iteration 1100 / 2000: loss 0.185234\n",
      "iteration 1200 / 2000: loss 0.182963\n",
      "iteration 1300 / 2000: loss 0.171841\n",
      "iteration 1400 / 2000: loss 0.164729\n",
      "iteration 1500 / 2000: loss 0.122079\n",
      "iteration 1600 / 2000: loss 0.142182\n",
      "iteration 1700 / 2000: loss 0.134754\n",
      "iteration 1800 / 2000: loss 0.199852\n",
      "iteration 1900 / 2000: loss 0.180078\n",
      "Hidden Size: 50, Learning Rate: 0.0075000250000000004, LR Decay: 0.645, Regularization: 0.2, Validation accuracy: 0.9696\n",
      "iteration 0 / 2000: loss 2.302672\n",
      "iteration 100 / 2000: loss 0.382819\n",
      "iteration 200 / 2000: loss 0.231930\n",
      "iteration 300 / 2000: loss 0.322820\n",
      "iteration 400 / 2000: loss 0.255688\n",
      "iteration 500 / 2000: loss 0.298731\n",
      "iteration 600 / 2000: loss 0.193647\n",
      "iteration 700 / 2000: loss 0.191252\n",
      "iteration 800 / 2000: loss 0.186111\n",
      "iteration 900 / 2000: loss 0.163235\n",
      "iteration 1000 / 2000: loss 0.213343\n",
      "iteration 1100 / 2000: loss 0.225937\n",
      "iteration 1200 / 2000: loss 0.183282\n",
      "iteration 1300 / 2000: loss 0.227178\n",
      "iteration 1400 / 2000: loss 0.187541\n",
      "iteration 1500 / 2000: loss 0.235524\n",
      "iteration 1600 / 2000: loss 0.185060\n",
      "iteration 1700 / 2000: loss 0.228809\n",
      "iteration 1800 / 2000: loss 0.177888\n",
      "iteration 1900 / 2000: loss 0.182622\n",
      "Hidden Size: 50, Learning Rate: 0.0075000250000000004, LR Decay: 0.645, Regularization: 0.30000000000000004, Validation accuracy: 0.9694\n",
      "iteration 0 / 2000: loss 2.302676\n",
      "iteration 100 / 2000: loss 0.430396\n",
      "iteration 200 / 2000: loss 0.318719\n",
      "iteration 300 / 2000: loss 0.207557\n",
      "iteration 400 / 2000: loss 0.251760\n",
      "iteration 500 / 2000: loss 0.252763\n",
      "iteration 600 / 2000: loss 0.242673\n",
      "iteration 700 / 2000: loss 0.221046\n",
      "iteration 800 / 2000: loss 0.219202\n",
      "iteration 900 / 2000: loss 0.231925\n",
      "iteration 1000 / 2000: loss 0.190813\n",
      "iteration 1100 / 2000: loss 0.173644\n",
      "iteration 1200 / 2000: loss 0.198964\n",
      "iteration 1300 / 2000: loss 0.229204\n",
      "iteration 1400 / 2000: loss 0.201325\n",
      "iteration 1500 / 2000: loss 0.235926\n",
      "iteration 1600 / 2000: loss 0.267810\n",
      "iteration 1700 / 2000: loss 0.195710\n",
      "iteration 1800 / 2000: loss 0.229109\n",
      "iteration 1900 / 2000: loss 0.266932\n",
      "Hidden Size: 50, Learning Rate: 0.0075000250000000004, LR Decay: 0.645, Regularization: 0.4, Validation accuracy: 0.9674\n",
      "iteration 0 / 2000: loss 2.302575\n",
      "iteration 100 / 2000: loss 0.240556\n",
      "iteration 200 / 2000: loss 0.161240\n",
      "iteration 300 / 2000: loss 0.163979\n",
      "iteration 400 / 2000: loss 0.121983\n",
      "iteration 500 / 2000: loss 0.122053\n",
      "iteration 600 / 2000: loss 0.123957\n",
      "iteration 700 / 2000: loss 0.141630\n",
      "iteration 800 / 2000: loss 0.178966\n",
      "iteration 900 / 2000: loss 0.057033\n",
      "iteration 1000 / 2000: loss 0.169876\n",
      "iteration 1100 / 2000: loss 0.178966\n",
      "iteration 1200 / 2000: loss 0.128160\n",
      "iteration 1300 / 2000: loss 0.143605\n",
      "iteration 1400 / 2000: loss 0.064948\n",
      "iteration 1500 / 2000: loss 0.071618\n",
      "iteration 1600 / 2000: loss 0.166324\n",
      "iteration 1700 / 2000: loss 0.142820\n",
      "iteration 1800 / 2000: loss 0.208642\n",
      "iteration 1900 / 2000: loss 0.087281\n",
      "Hidden Size: 50, Learning Rate: 0.0075000250000000004, LR Decay: 0.47250000000000003, Regularization: 0.0, Validation accuracy: 0.9638\n",
      "iteration 0 / 2000: loss 2.302610\n",
      "iteration 100 / 2000: loss 0.339165\n",
      "iteration 200 / 2000: loss 0.187388\n",
      "iteration 300 / 2000: loss 0.230745\n",
      "iteration 400 / 2000: loss 0.185050\n",
      "iteration 500 / 2000: loss 0.147484\n",
      "iteration 600 / 2000: loss 0.226422\n",
      "iteration 700 / 2000: loss 0.167988\n",
      "iteration 800 / 2000: loss 0.155688\n",
      "iteration 900 / 2000: loss 0.220597\n",
      "iteration 1000 / 2000: loss 0.153916\n",
      "iteration 1100 / 2000: loss 0.200424\n",
      "iteration 1200 / 2000: loss 0.207856\n",
      "iteration 1300 / 2000: loss 0.170992\n",
      "iteration 1400 / 2000: loss 0.104146\n",
      "iteration 1500 / 2000: loss 0.199933\n",
      "iteration 1600 / 2000: loss 0.180863\n",
      "iteration 1700 / 2000: loss 0.131535\n",
      "iteration 1800 / 2000: loss 0.167516\n",
      "iteration 1900 / 2000: loss 0.250225\n",
      "Hidden Size: 50, Learning Rate: 0.0075000250000000004, LR Decay: 0.47250000000000003, Regularization: 0.1, Validation accuracy: 0.9574\n",
      "iteration 0 / 2000: loss 2.302605\n",
      "iteration 100 / 2000: loss 0.347157\n",
      "iteration 200 / 2000: loss 0.325568\n",
      "iteration 300 / 2000: loss 0.209528\n",
      "iteration 400 / 2000: loss 0.203312\n",
      "iteration 500 / 2000: loss 0.205957\n",
      "iteration 600 / 2000: loss 0.263742\n",
      "iteration 700 / 2000: loss 0.169689\n",
      "iteration 800 / 2000: loss 0.173608\n",
      "iteration 900 / 2000: loss 0.174659\n",
      "iteration 1000 / 2000: loss 0.199133\n",
      "iteration 1100 / 2000: loss 0.169887\n",
      "iteration 1200 / 2000: loss 0.125884\n",
      "iteration 1300 / 2000: loss 0.210538\n",
      "iteration 1400 / 2000: loss 0.144057\n",
      "iteration 1500 / 2000: loss 0.175443\n",
      "iteration 1600 / 2000: loss 0.194748\n",
      "iteration 1700 / 2000: loss 0.172289\n",
      "iteration 1800 / 2000: loss 0.144319\n",
      "iteration 1900 / 2000: loss 0.237690\n",
      "Hidden Size: 50, Learning Rate: 0.0075000250000000004, LR Decay: 0.47250000000000003, Regularization: 0.2, Validation accuracy: 0.9598\n",
      "iteration 0 / 2000: loss 2.302657\n",
      "iteration 100 / 2000: loss 0.349699\n",
      "iteration 200 / 2000: loss 0.250683\n",
      "iteration 300 / 2000: loss 0.225404\n",
      "iteration 400 / 2000: loss 0.208024\n",
      "iteration 500 / 2000: loss 0.224022\n",
      "iteration 600 / 2000: loss 0.218539\n",
      "iteration 700 / 2000: loss 0.259650\n",
      "iteration 800 / 2000: loss 0.251497\n",
      "iteration 900 / 2000: loss 0.218845\n",
      "iteration 1000 / 2000: loss 0.223060\n",
      "iteration 1100 / 2000: loss 0.177891\n",
      "iteration 1200 / 2000: loss 0.219962\n",
      "iteration 1300 / 2000: loss 0.195991\n",
      "iteration 1400 / 2000: loss 0.230380\n",
      "iteration 1500 / 2000: loss 0.217760\n",
      "iteration 1600 / 2000: loss 0.253429\n",
      "iteration 1700 / 2000: loss 0.256246\n",
      "iteration 1800 / 2000: loss 0.237424\n",
      "iteration 1900 / 2000: loss 0.206148\n",
      "Hidden Size: 50, Learning Rate: 0.0075000250000000004, LR Decay: 0.47250000000000003, Regularization: 0.30000000000000004, Validation accuracy: 0.9598\n",
      "iteration 0 / 2000: loss 2.302659\n",
      "iteration 100 / 2000: loss 0.286423\n",
      "iteration 200 / 2000: loss 0.294208\n",
      "iteration 300 / 2000: loss 0.269330\n",
      "iteration 400 / 2000: loss 0.319895\n",
      "iteration 500 / 2000: loss 0.206888\n",
      "iteration 600 / 2000: loss 0.267429\n",
      "iteration 700 / 2000: loss 0.221933\n",
      "iteration 800 / 2000: loss 0.267886\n",
      "iteration 900 / 2000: loss 0.200389\n",
      "iteration 1000 / 2000: loss 0.231804\n",
      "iteration 1100 / 2000: loss 0.288988\n",
      "iteration 1200 / 2000: loss 0.175774\n",
      "iteration 1300 / 2000: loss 0.201538\n",
      "iteration 1400 / 2000: loss 0.205706\n",
      "iteration 1500 / 2000: loss 0.240118\n",
      "iteration 1600 / 2000: loss 0.280179\n",
      "iteration 1700 / 2000: loss 0.265198\n",
      "iteration 1800 / 2000: loss 0.221306\n",
      "iteration 1900 / 2000: loss 0.244444\n",
      "Hidden Size: 50, Learning Rate: 0.0075000250000000004, LR Decay: 0.47250000000000003, Regularization: 0.4, Validation accuracy: 0.96\n",
      "iteration 0 / 2000: loss 2.302599\n",
      "iteration 100 / 2000: loss 0.270868\n",
      "iteration 200 / 2000: loss 0.217636\n",
      "iteration 300 / 2000: loss 0.224187\n",
      "iteration 400 / 2000: loss 0.226419\n",
      "iteration 500 / 2000: loss 0.153161\n",
      "iteration 600 / 2000: loss 0.289752\n",
      "iteration 700 / 2000: loss 0.317218\n",
      "iteration 800 / 2000: loss 0.274439\n",
      "iteration 900 / 2000: loss 0.208598\n",
      "iteration 1000 / 2000: loss 0.168362\n",
      "iteration 1100 / 2000: loss 0.164229\n",
      "iteration 1200 / 2000: loss 0.192369\n",
      "iteration 1300 / 2000: loss 0.257084\n",
      "iteration 1400 / 2000: loss 0.163911\n",
      "iteration 1500 / 2000: loss 0.211047\n",
      "iteration 1600 / 2000: loss 0.189761\n",
      "iteration 1700 / 2000: loss 0.253712\n",
      "iteration 1800 / 2000: loss 0.141175\n",
      "iteration 1900 / 2000: loss 0.181828\n",
      "Hidden Size: 50, Learning Rate: 0.0075000250000000004, LR Decay: 0.3, Regularization: 0.0, Validation accuracy: 0.9468\n",
      "iteration 0 / 2000: loss 2.302620\n",
      "iteration 100 / 2000: loss 0.292311\n",
      "iteration 200 / 2000: loss 0.236529\n",
      "iteration 300 / 2000: loss 0.159568\n",
      "iteration 400 / 2000: loss 0.244233\n",
      "iteration 500 / 2000: loss 0.185736\n",
      "iteration 600 / 2000: loss 0.221777\n",
      "iteration 700 / 2000: loss 0.217497\n",
      "iteration 800 / 2000: loss 0.212859\n",
      "iteration 900 / 2000: loss 0.272443\n",
      "iteration 1000 / 2000: loss 0.191763\n",
      "iteration 1100 / 2000: loss 0.259141\n",
      "iteration 1200 / 2000: loss 0.115897\n",
      "iteration 1300 / 2000: loss 0.192833\n",
      "iteration 1400 / 2000: loss 0.230716\n",
      "iteration 1500 / 2000: loss 0.205325\n",
      "iteration 1600 / 2000: loss 0.160427\n",
      "iteration 1700 / 2000: loss 0.247667\n",
      "iteration 1800 / 2000: loss 0.259327\n",
      "iteration 1900 / 2000: loss 0.298843\n",
      "Hidden Size: 50, Learning Rate: 0.0075000250000000004, LR Decay: 0.3, Regularization: 0.1, Validation accuracy: 0.948\n",
      "iteration 0 / 2000: loss 2.302631\n",
      "iteration 100 / 2000: loss 0.353102\n",
      "iteration 200 / 2000: loss 0.384300\n",
      "iteration 300 / 2000: loss 0.236152\n",
      "iteration 400 / 2000: loss 0.274839\n",
      "iteration 500 / 2000: loss 0.287516\n",
      "iteration 600 / 2000: loss 0.315258\n",
      "iteration 700 / 2000: loss 0.346301\n",
      "iteration 800 / 2000: loss 0.246887\n",
      "iteration 900 / 2000: loss 0.211260\n",
      "iteration 1000 / 2000: loss 0.198364\n",
      "iteration 1100 / 2000: loss 0.301685\n",
      "iteration 1200 / 2000: loss 0.283575\n",
      "iteration 1300 / 2000: loss 0.241682\n",
      "iteration 1400 / 2000: loss 0.173332\n",
      "iteration 1500 / 2000: loss 0.270570\n",
      "iteration 1600 / 2000: loss 0.227803\n",
      "iteration 1700 / 2000: loss 0.180321\n",
      "iteration 1800 / 2000: loss 0.232924\n",
      "iteration 1900 / 2000: loss 0.306082\n",
      "Hidden Size: 50, Learning Rate: 0.0075000250000000004, LR Decay: 0.3, Regularization: 0.2, Validation accuracy: 0.9408\n",
      "iteration 0 / 2000: loss 2.302645\n",
      "iteration 100 / 2000: loss 0.394128\n",
      "iteration 200 / 2000: loss 0.300686\n",
      "iteration 300 / 2000: loss 0.319358\n",
      "iteration 400 / 2000: loss 0.320400\n",
      "iteration 500 / 2000: loss 0.288007\n",
      "iteration 600 / 2000: loss 0.265389\n",
      "iteration 700 / 2000: loss 0.214472\n",
      "iteration 800 / 2000: loss 0.238457\n",
      "iteration 900 / 2000: loss 0.260465\n",
      "iteration 1000 / 2000: loss 0.288848\n",
      "iteration 1100 / 2000: loss 0.229069\n",
      "iteration 1200 / 2000: loss 0.209801\n",
      "iteration 1300 / 2000: loss 0.225697\n",
      "iteration 1400 / 2000: loss 0.274975\n",
      "iteration 1500 / 2000: loss 0.253506\n",
      "iteration 1600 / 2000: loss 0.297612\n",
      "iteration 1700 / 2000: loss 0.233839\n",
      "iteration 1800 / 2000: loss 0.298605\n",
      "iteration 1900 / 2000: loss 0.334796\n",
      "Hidden Size: 50, Learning Rate: 0.0075000250000000004, LR Decay: 0.3, Regularization: 0.30000000000000004, Validation accuracy: 0.9446\n",
      "iteration 0 / 2000: loss 2.302662\n",
      "iteration 100 / 2000: loss 0.474869\n",
      "iteration 200 / 2000: loss 0.358131\n",
      "iteration 300 / 2000: loss 0.309993\n",
      "iteration 400 / 2000: loss 0.207402\n",
      "iteration 500 / 2000: loss 0.352514\n",
      "iteration 600 / 2000: loss 0.210656\n",
      "iteration 700 / 2000: loss 0.268448\n",
      "iteration 800 / 2000: loss 0.171045\n",
      "iteration 900 / 2000: loss 0.292467\n",
      "iteration 1000 / 2000: loss 0.274514\n",
      "iteration 1100 / 2000: loss 0.328975\n",
      "iteration 1200 / 2000: loss 0.340260\n",
      "iteration 1300 / 2000: loss 0.225269\n",
      "iteration 1400 / 2000: loss 0.281369\n",
      "iteration 1500 / 2000: loss 0.282130\n",
      "iteration 1600 / 2000: loss 0.282044\n",
      "iteration 1700 / 2000: loss 0.244230\n",
      "iteration 1800 / 2000: loss 0.252178\n",
      "iteration 1900 / 2000: loss 0.275297\n",
      "Hidden Size: 50, Learning Rate: 0.0075000250000000004, LR Decay: 0.3, Regularization: 0.4, Validation accuracy: 0.9406\n",
      "iteration 0 / 2000: loss 2.302592\n",
      "iteration 100 / 2000: loss 0.260784\n",
      "iteration 200 / 2000: loss 0.186593\n",
      "iteration 300 / 2000: loss 0.082615\n",
      "iteration 400 / 2000: loss 0.126382\n",
      "iteration 500 / 2000: loss 0.183490\n",
      "iteration 600 / 2000: loss 0.064734\n",
      "iteration 700 / 2000: loss 0.099327\n",
      "iteration 800 / 2000: loss 0.094487\n",
      "iteration 900 / 2000: loss 0.128595\n",
      "iteration 1000 / 2000: loss 0.064129\n",
      "iteration 1100 / 2000: loss 0.068990\n",
      "iteration 1200 / 2000: loss 0.031954\n",
      "iteration 1300 / 2000: loss 0.089734\n",
      "iteration 1400 / 2000: loss 0.115198\n",
      "iteration 1500 / 2000: loss 0.069930\n",
      "iteration 1600 / 2000: loss 0.038512\n",
      "iteration 1700 / 2000: loss 0.077166\n",
      "iteration 1800 / 2000: loss 0.128569\n",
      "iteration 1900 / 2000: loss 0.023734\n",
      "Hidden Size: 50, Learning Rate: 0.00500005, LR Decay: 0.99, Regularization: 0.0, Validation accuracy: 0.974\n",
      "iteration 0 / 2000: loss 2.302598\n",
      "iteration 100 / 2000: loss 0.247435\n",
      "iteration 200 / 2000: loss 0.256766\n",
      "iteration 300 / 2000: loss 0.218850\n",
      "iteration 400 / 2000: loss 0.164078\n",
      "iteration 500 / 2000: loss 0.137417\n",
      "iteration 600 / 2000: loss 0.148298\n",
      "iteration 700 / 2000: loss 0.109573\n",
      "iteration 800 / 2000: loss 0.121307\n",
      "iteration 900 / 2000: loss 0.110750\n",
      "iteration 1000 / 2000: loss 0.108005\n",
      "iteration 1100 / 2000: loss 0.085400\n",
      "iteration 1200 / 2000: loss 0.144040\n",
      "iteration 1300 / 2000: loss 0.124101\n",
      "iteration 1400 / 2000: loss 0.129423\n",
      "iteration 1500 / 2000: loss 0.103586\n",
      "iteration 1600 / 2000: loss 0.156554\n",
      "iteration 1700 / 2000: loss 0.113331\n",
      "iteration 1800 / 2000: loss 0.106120\n",
      "iteration 1900 / 2000: loss 0.133163\n",
      "Hidden Size: 50, Learning Rate: 0.00500005, LR Decay: 0.99, Regularization: 0.1, Validation accuracy: 0.9734\n",
      "iteration 0 / 2000: loss 2.302624\n",
      "iteration 100 / 2000: loss 0.382744\n",
      "iteration 200 / 2000: loss 0.213697\n",
      "iteration 300 / 2000: loss 0.201463\n",
      "iteration 400 / 2000: loss 0.331309\n",
      "iteration 500 / 2000: loss 0.211977\n",
      "iteration 600 / 2000: loss 0.178951\n",
      "iteration 700 / 2000: loss 0.159409\n",
      "iteration 800 / 2000: loss 0.205603\n",
      "iteration 900 / 2000: loss 0.229603\n",
      "iteration 1000 / 2000: loss 0.182686\n",
      "iteration 1100 / 2000: loss 0.159122\n",
      "iteration 1200 / 2000: loss 0.160961\n",
      "iteration 1300 / 2000: loss 0.184382\n",
      "iteration 1400 / 2000: loss 0.146900\n",
      "iteration 1500 / 2000: loss 0.184401\n",
      "iteration 1600 / 2000: loss 0.142554\n",
      "iteration 1700 / 2000: loss 0.156088\n",
      "iteration 1800 / 2000: loss 0.191138\n",
      "iteration 1900 / 2000: loss 0.154574\n",
      "Hidden Size: 50, Learning Rate: 0.00500005, LR Decay: 0.99, Regularization: 0.2, Validation accuracy: 0.971\n",
      "iteration 0 / 2000: loss 2.302651\n",
      "iteration 100 / 2000: loss 0.415315\n",
      "iteration 200 / 2000: loss 0.223184\n",
      "iteration 300 / 2000: loss 0.223657\n",
      "iteration 400 / 2000: loss 0.239826\n",
      "iteration 500 / 2000: loss 0.210915\n",
      "iteration 600 / 2000: loss 0.185231\n",
      "iteration 700 / 2000: loss 0.129779\n",
      "iteration 800 / 2000: loss 0.183115\n",
      "iteration 900 / 2000: loss 0.236459\n",
      "iteration 1000 / 2000: loss 0.164536\n",
      "iteration 1100 / 2000: loss 0.166034\n",
      "iteration 1200 / 2000: loss 0.191703\n",
      "iteration 1300 / 2000: loss 0.213675\n",
      "iteration 1400 / 2000: loss 0.183401\n",
      "iteration 1500 / 2000: loss 0.184938\n",
      "iteration 1600 / 2000: loss 0.163036\n",
      "iteration 1700 / 2000: loss 0.176554\n",
      "iteration 1800 / 2000: loss 0.162997\n",
      "iteration 1900 / 2000: loss 0.150027\n",
      "Hidden Size: 50, Learning Rate: 0.00500005, LR Decay: 0.99, Regularization: 0.30000000000000004, Validation accuracy: 0.9698\n",
      "iteration 0 / 2000: loss 2.302654\n",
      "iteration 100 / 2000: loss 0.353052\n",
      "iteration 200 / 2000: loss 0.216057\n",
      "iteration 300 / 2000: loss 0.271901\n",
      "iteration 400 / 2000: loss 0.230521\n",
      "iteration 500 / 2000: loss 0.222945\n",
      "iteration 600 / 2000: loss 0.218998\n",
      "iteration 700 / 2000: loss 0.254643\n",
      "iteration 800 / 2000: loss 0.248210\n",
      "iteration 900 / 2000: loss 0.219743\n",
      "iteration 1000 / 2000: loss 0.241832\n",
      "iteration 1100 / 2000: loss 0.238167\n",
      "iteration 1200 / 2000: loss 0.191497\n",
      "iteration 1300 / 2000: loss 0.245656\n",
      "iteration 1400 / 2000: loss 0.187758\n",
      "iteration 1500 / 2000: loss 0.228272\n",
      "iteration 1600 / 2000: loss 0.201489\n",
      "iteration 1700 / 2000: loss 0.246354\n",
      "iteration 1800 / 2000: loss 0.256417\n",
      "iteration 1900 / 2000: loss 0.241105\n",
      "Hidden Size: 50, Learning Rate: 0.00500005, LR Decay: 0.99, Regularization: 0.4, Validation accuracy: 0.9644\n",
      "iteration 0 / 2000: loss 2.302572\n",
      "iteration 100 / 2000: loss 0.275793\n",
      "iteration 200 / 2000: loss 0.244516\n",
      "iteration 300 / 2000: loss 0.300069\n",
      "iteration 400 / 2000: loss 0.116105\n",
      "iteration 500 / 2000: loss 0.172159\n",
      "iteration 600 / 2000: loss 0.149004\n",
      "iteration 700 / 2000: loss 0.178594\n",
      "iteration 800 / 2000: loss 0.131352\n",
      "iteration 900 / 2000: loss 0.062647\n",
      "iteration 1000 / 2000: loss 0.114818\n",
      "iteration 1100 / 2000: loss 0.089933\n",
      "iteration 1200 / 2000: loss 0.069903\n",
      "iteration 1300 / 2000: loss 0.050184\n",
      "iteration 1400 / 2000: loss 0.097583\n",
      "iteration 1500 / 2000: loss 0.124392\n",
      "iteration 1600 / 2000: loss 0.098159\n",
      "iteration 1700 / 2000: loss 0.042381\n",
      "iteration 1800 / 2000: loss 0.059496\n",
      "iteration 1900 / 2000: loss 0.068916\n",
      "Hidden Size: 50, Learning Rate: 0.00500005, LR Decay: 0.8175, Regularization: 0.0, Validation accuracy: 0.97\n",
      "iteration 0 / 2000: loss 2.302609\n",
      "iteration 100 / 2000: loss 0.357418\n",
      "iteration 200 / 2000: loss 0.295034\n",
      "iteration 300 / 2000: loss 0.233389\n",
      "iteration 400 / 2000: loss 0.158492\n",
      "iteration 500 / 2000: loss 0.183594\n",
      "iteration 600 / 2000: loss 0.153636\n",
      "iteration 700 / 2000: loss 0.166370\n",
      "iteration 800 / 2000: loss 0.092133\n",
      "iteration 900 / 2000: loss 0.097190\n",
      "iteration 1000 / 2000: loss 0.107122\n",
      "iteration 1100 / 2000: loss 0.095272\n",
      "iteration 1200 / 2000: loss 0.184197\n",
      "iteration 1300 / 2000: loss 0.135281\n",
      "iteration 1400 / 2000: loss 0.117500\n",
      "iteration 1500 / 2000: loss 0.083925\n",
      "iteration 1600 / 2000: loss 0.150132\n",
      "iteration 1700 / 2000: loss 0.147866\n",
      "iteration 1800 / 2000: loss 0.075170\n",
      "iteration 1900 / 2000: loss 0.095990\n",
      "Hidden Size: 50, Learning Rate: 0.00500005, LR Decay: 0.8175, Regularization: 0.1, Validation accuracy: 0.9698\n",
      "iteration 0 / 2000: loss 2.302626\n",
      "iteration 100 / 2000: loss 0.324721\n",
      "iteration 200 / 2000: loss 0.271287\n",
      "iteration 300 / 2000: loss 0.196146\n",
      "iteration 400 / 2000: loss 0.222869\n",
      "iteration 500 / 2000: loss 0.249739\n",
      "iteration 600 / 2000: loss 0.134489\n",
      "iteration 700 / 2000: loss 0.168148\n",
      "iteration 800 / 2000: loss 0.215586\n",
      "iteration 900 / 2000: loss 0.178800\n",
      "iteration 1000 / 2000: loss 0.257036\n",
      "iteration 1100 / 2000: loss 0.118539\n",
      "iteration 1200 / 2000: loss 0.157716\n",
      "iteration 1300 / 2000: loss 0.131718\n",
      "iteration 1400 / 2000: loss 0.128302\n",
      "iteration 1500 / 2000: loss 0.115084\n",
      "iteration 1600 / 2000: loss 0.130005\n",
      "iteration 1700 / 2000: loss 0.168973\n",
      "iteration 1800 / 2000: loss 0.151353\n",
      "iteration 1900 / 2000: loss 0.178230\n",
      "Hidden Size: 50, Learning Rate: 0.00500005, LR Decay: 0.8175, Regularization: 0.2, Validation accuracy: 0.973\n",
      "iteration 0 / 2000: loss 2.302629\n",
      "iteration 100 / 2000: loss 0.324429\n",
      "iteration 200 / 2000: loss 0.296451\n",
      "iteration 300 / 2000: loss 0.209584\n",
      "iteration 400 / 2000: loss 0.208613\n",
      "iteration 500 / 2000: loss 0.243620\n",
      "iteration 600 / 2000: loss 0.261051\n",
      "iteration 700 / 2000: loss 0.220459\n",
      "iteration 800 / 2000: loss 0.328470\n",
      "iteration 900 / 2000: loss 0.217133\n",
      "iteration 1000 / 2000: loss 0.179111\n",
      "iteration 1100 / 2000: loss 0.174162\n",
      "iteration 1200 / 2000: loss 0.242844\n",
      "iteration 1300 / 2000: loss 0.186319\n",
      "iteration 1400 / 2000: loss 0.160957\n",
      "iteration 1500 / 2000: loss 0.160159\n",
      "iteration 1600 / 2000: loss 0.177409\n",
      "iteration 1700 / 2000: loss 0.159552\n",
      "iteration 1800 / 2000: loss 0.141574\n",
      "iteration 1900 / 2000: loss 0.174100\n",
      "Hidden Size: 50, Learning Rate: 0.00500005, LR Decay: 0.8175, Regularization: 0.30000000000000004, Validation accuracy: 0.9724\n",
      "iteration 0 / 2000: loss 2.302645\n",
      "iteration 100 / 2000: loss 0.376466\n",
      "iteration 200 / 2000: loss 0.274195\n",
      "iteration 300 / 2000: loss 0.217646\n",
      "iteration 400 / 2000: loss 0.269128\n",
      "iteration 500 / 2000: loss 0.229527\n",
      "iteration 600 / 2000: loss 0.260433\n",
      "iteration 700 / 2000: loss 0.282978\n",
      "iteration 800 / 2000: loss 0.273884\n",
      "iteration 900 / 2000: loss 0.176201\n",
      "iteration 1000 / 2000: loss 0.188085\n",
      "iteration 1100 / 2000: loss 0.207049\n",
      "iteration 1200 / 2000: loss 0.189612\n",
      "iteration 1300 / 2000: loss 0.244682\n",
      "iteration 1400 / 2000: loss 0.223394\n",
      "iteration 1500 / 2000: loss 0.243341\n",
      "iteration 1600 / 2000: loss 0.197146\n",
      "iteration 1700 / 2000: loss 0.220436\n",
      "iteration 1800 / 2000: loss 0.208546\n",
      "iteration 1900 / 2000: loss 0.230568\n",
      "Hidden Size: 50, Learning Rate: 0.00500005, LR Decay: 0.8175, Regularization: 0.4, Validation accuracy: 0.969\n",
      "iteration 0 / 2000: loss 2.302603\n",
      "iteration 100 / 2000: loss 0.254109\n",
      "iteration 200 / 2000: loss 0.233130\n",
      "iteration 300 / 2000: loss 0.104125\n",
      "iteration 400 / 2000: loss 0.121512\n",
      "iteration 500 / 2000: loss 0.155976\n",
      "iteration 600 / 2000: loss 0.153981\n",
      "iteration 700 / 2000: loss 0.116609\n",
      "iteration 800 / 2000: loss 0.138062\n",
      "iteration 900 / 2000: loss 0.085903\n",
      "iteration 1000 / 2000: loss 0.128420\n",
      "iteration 1100 / 2000: loss 0.145349\n",
      "iteration 1200 / 2000: loss 0.108059\n",
      "iteration 1300 / 2000: loss 0.129713\n",
      "iteration 1400 / 2000: loss 0.098064\n",
      "iteration 1500 / 2000: loss 0.090324\n",
      "iteration 1600 / 2000: loss 0.145816\n",
      "iteration 1700 / 2000: loss 0.098807\n",
      "iteration 1800 / 2000: loss 0.096816\n",
      "iteration 1900 / 2000: loss 0.126103\n",
      "Hidden Size: 50, Learning Rate: 0.00500005, LR Decay: 0.645, Regularization: 0.0, Validation accuracy: 0.9672\n",
      "iteration 0 / 2000: loss 2.302611\n",
      "iteration 100 / 2000: loss 0.292825\n",
      "iteration 200 / 2000: loss 0.229370\n",
      "iteration 300 / 2000: loss 0.320459\n",
      "iteration 400 / 2000: loss 0.232808\n",
      "iteration 500 / 2000: loss 0.253163\n",
      "iteration 600 / 2000: loss 0.182318\n",
      "iteration 700 / 2000: loss 0.177031\n",
      "iteration 800 / 2000: loss 0.157262\n",
      "iteration 900 / 2000: loss 0.177309\n",
      "iteration 1000 / 2000: loss 0.186391\n",
      "iteration 1100 / 2000: loss 0.115436\n",
      "iteration 1200 / 2000: loss 0.099805\n",
      "iteration 1300 / 2000: loss 0.214359\n",
      "iteration 1400 / 2000: loss 0.116436\n",
      "iteration 1500 / 2000: loss 0.115762\n",
      "iteration 1600 / 2000: loss 0.157315\n",
      "iteration 1700 / 2000: loss 0.120340\n",
      "iteration 1800 / 2000: loss 0.120954\n",
      "iteration 1900 / 2000: loss 0.230323\n",
      "Hidden Size: 50, Learning Rate: 0.00500005, LR Decay: 0.645, Regularization: 0.1, Validation accuracy: 0.9658\n",
      "iteration 0 / 2000: loss 2.302627\n",
      "iteration 100 / 2000: loss 0.283235\n",
      "iteration 200 / 2000: loss 0.401631\n",
      "iteration 300 / 2000: loss 0.173266\n",
      "iteration 400 / 2000: loss 0.302918\n",
      "iteration 500 / 2000: loss 0.190028\n",
      "iteration 600 / 2000: loss 0.152848\n",
      "iteration 700 / 2000: loss 0.227077\n",
      "iteration 800 / 2000: loss 0.216321\n",
      "iteration 900 / 2000: loss 0.183490\n",
      "iteration 1000 / 2000: loss 0.158143\n",
      "iteration 1100 / 2000: loss 0.187073\n",
      "iteration 1200 / 2000: loss 0.224538\n",
      "iteration 1300 / 2000: loss 0.226570\n",
      "iteration 1400 / 2000: loss 0.234618\n",
      "iteration 1500 / 2000: loss 0.193894\n",
      "iteration 1600 / 2000: loss 0.160476\n",
      "iteration 1700 / 2000: loss 0.179847\n",
      "iteration 1800 / 2000: loss 0.127276\n",
      "iteration 1900 / 2000: loss 0.143941\n",
      "Hidden Size: 50, Learning Rate: 0.00500005, LR Decay: 0.645, Regularization: 0.2, Validation accuracy: 0.963\n",
      "iteration 0 / 2000: loss 2.302623\n",
      "iteration 100 / 2000: loss 0.362205\n",
      "iteration 200 / 2000: loss 0.286083\n",
      "iteration 300 / 2000: loss 0.238302\n",
      "iteration 400 / 2000: loss 0.302788\n",
      "iteration 500 / 2000: loss 0.211311\n",
      "iteration 600 / 2000: loss 0.225158\n",
      "iteration 700 / 2000: loss 0.238232\n",
      "iteration 800 / 2000: loss 0.240217\n",
      "iteration 900 / 2000: loss 0.171850\n",
      "iteration 1000 / 2000: loss 0.206109\n",
      "iteration 1100 / 2000: loss 0.268268\n",
      "iteration 1200 / 2000: loss 0.194037\n",
      "iteration 1300 / 2000: loss 0.250336\n",
      "iteration 1400 / 2000: loss 0.204779\n",
      "iteration 1500 / 2000: loss 0.201334\n",
      "iteration 1600 / 2000: loss 0.141653\n",
      "iteration 1700 / 2000: loss 0.190885\n",
      "iteration 1800 / 2000: loss 0.230904\n",
      "iteration 1900 / 2000: loss 0.272878\n",
      "Hidden Size: 50, Learning Rate: 0.00500005, LR Decay: 0.645, Regularization: 0.30000000000000004, Validation accuracy: 0.9622\n",
      "iteration 0 / 2000: loss 2.302657\n",
      "iteration 100 / 2000: loss 0.390939\n",
      "iteration 200 / 2000: loss 0.302315\n",
      "iteration 300 / 2000: loss 0.157025\n",
      "iteration 400 / 2000: loss 0.342341\n",
      "iteration 500 / 2000: loss 0.234989\n",
      "iteration 600 / 2000: loss 0.275507\n",
      "iteration 700 / 2000: loss 0.278311\n",
      "iteration 800 / 2000: loss 0.200835\n",
      "iteration 900 / 2000: loss 0.204133\n",
      "iteration 1000 / 2000: loss 0.214060\n",
      "iteration 1100 / 2000: loss 0.220430\n",
      "iteration 1200 / 2000: loss 0.239496\n",
      "iteration 1300 / 2000: loss 0.215650\n",
      "iteration 1400 / 2000: loss 0.299927\n",
      "iteration 1500 / 2000: loss 0.168324\n",
      "iteration 1600 / 2000: loss 0.190042\n",
      "iteration 1700 / 2000: loss 0.212992\n",
      "iteration 1800 / 2000: loss 0.191498\n",
      "iteration 1900 / 2000: loss 0.224601\n",
      "Hidden Size: 50, Learning Rate: 0.00500005, LR Decay: 0.645, Regularization: 0.4, Validation accuracy: 0.9642\n",
      "iteration 0 / 2000: loss 2.302591\n",
      "iteration 100 / 2000: loss 0.382546\n",
      "iteration 200 / 2000: loss 0.281930\n",
      "iteration 300 / 2000: loss 0.174905\n",
      "iteration 400 / 2000: loss 0.131560\n",
      "iteration 500 / 2000: loss 0.170456\n",
      "iteration 600 / 2000: loss 0.144863\n",
      "iteration 700 / 2000: loss 0.121037\n",
      "iteration 800 / 2000: loss 0.292498\n",
      "iteration 900 / 2000: loss 0.171873\n",
      "iteration 1000 / 2000: loss 0.152853\n",
      "iteration 1100 / 2000: loss 0.164924\n",
      "iteration 1200 / 2000: loss 0.209405\n",
      "iteration 1300 / 2000: loss 0.180027\n",
      "iteration 1400 / 2000: loss 0.122697\n",
      "iteration 1500 / 2000: loss 0.127548\n",
      "iteration 1600 / 2000: loss 0.172763\n",
      "iteration 1700 / 2000: loss 0.165645\n",
      "iteration 1800 / 2000: loss 0.241098\n",
      "iteration 1900 / 2000: loss 0.089600\n",
      "Hidden Size: 50, Learning Rate: 0.00500005, LR Decay: 0.47250000000000003, Regularization: 0.0, Validation accuracy: 0.9554\n",
      "iteration 0 / 2000: loss 2.302605\n",
      "iteration 100 / 2000: loss 0.368639\n",
      "iteration 200 / 2000: loss 0.286782\n",
      "iteration 300 / 2000: loss 0.280201\n",
      "iteration 400 / 2000: loss 0.203785\n",
      "iteration 500 / 2000: loss 0.251491\n",
      "iteration 600 / 2000: loss 0.222975\n",
      "iteration 700 / 2000: loss 0.199491\n",
      "iteration 800 / 2000: loss 0.147935\n",
      "iteration 900 / 2000: loss 0.278647\n",
      "iteration 1000 / 2000: loss 0.192927\n",
      "iteration 1100 / 2000: loss 0.152542\n",
      "iteration 1200 / 2000: loss 0.267717\n",
      "iteration 1300 / 2000: loss 0.147472\n",
      "iteration 1400 / 2000: loss 0.245846\n",
      "iteration 1500 / 2000: loss 0.136828\n",
      "iteration 1600 / 2000: loss 0.224780\n",
      "iteration 1700 / 2000: loss 0.175958\n",
      "iteration 1800 / 2000: loss 0.196485\n",
      "iteration 1900 / 2000: loss 0.204633\n",
      "Hidden Size: 50, Learning Rate: 0.00500005, LR Decay: 0.47250000000000003, Regularization: 0.1, Validation accuracy: 0.9522\n",
      "iteration 0 / 2000: loss 2.302634\n",
      "iteration 100 / 2000: loss 0.333234\n",
      "iteration 200 / 2000: loss 0.213289\n",
      "iteration 300 / 2000: loss 0.261164\n",
      "iteration 400 / 2000: loss 0.276032\n",
      "iteration 500 / 2000: loss 0.256837\n",
      "iteration 600 / 2000: loss 0.289119\n",
      "iteration 700 / 2000: loss 0.236150\n",
      "iteration 800 / 2000: loss 0.191536\n",
      "iteration 900 / 2000: loss 0.277973\n",
      "iteration 1000 / 2000: loss 0.210339\n",
      "iteration 1100 / 2000: loss 0.176787\n",
      "iteration 1200 / 2000: loss 0.204366\n",
      "iteration 1300 / 2000: loss 0.276932\n",
      "iteration 1400 / 2000: loss 0.224213\n",
      "iteration 1500 / 2000: loss 0.175597\n",
      "iteration 1600 / 2000: loss 0.271190\n",
      "iteration 1700 / 2000: loss 0.246064\n",
      "iteration 1800 / 2000: loss 0.151916\n",
      "iteration 1900 / 2000: loss 0.204850\n",
      "Hidden Size: 50, Learning Rate: 0.00500005, LR Decay: 0.47250000000000003, Regularization: 0.2, Validation accuracy: 0.9528\n",
      "iteration 0 / 2000: loss 2.302658\n",
      "iteration 100 / 2000: loss 0.323437\n",
      "iteration 200 / 2000: loss 0.368145\n",
      "iteration 300 / 2000: loss 0.314356\n",
      "iteration 400 / 2000: loss 0.301661\n",
      "iteration 500 / 2000: loss 0.298965\n",
      "iteration 600 / 2000: loss 0.257930\n",
      "iteration 700 / 2000: loss 0.155058\n",
      "iteration 800 / 2000: loss 0.241550\n",
      "iteration 900 / 2000: loss 0.199271\n",
      "iteration 1000 / 2000: loss 0.276108\n",
      "iteration 1100 / 2000: loss 0.256034\n",
      "iteration 1200 / 2000: loss 0.328494\n",
      "iteration 1300 / 2000: loss 0.192665\n",
      "iteration 1400 / 2000: loss 0.270000\n",
      "iteration 1500 / 2000: loss 0.219610\n",
      "iteration 1600 / 2000: loss 0.218515\n",
      "iteration 1700 / 2000: loss 0.274130\n",
      "iteration 1800 / 2000: loss 0.220902\n",
      "iteration 1900 / 2000: loss 0.165435\n",
      "Hidden Size: 50, Learning Rate: 0.00500005, LR Decay: 0.47250000000000003, Regularization: 0.30000000000000004, Validation accuracy: 0.9498\n",
      "iteration 0 / 2000: loss 2.302665\n",
      "iteration 100 / 2000: loss 0.434013\n",
      "iteration 200 / 2000: loss 0.358619\n",
      "iteration 300 / 2000: loss 0.296047\n",
      "iteration 400 / 2000: loss 0.328280\n",
      "iteration 500 / 2000: loss 0.366166\n",
      "iteration 600 / 2000: loss 0.228172\n",
      "iteration 700 / 2000: loss 0.305091\n",
      "iteration 800 / 2000: loss 0.361414\n",
      "iteration 900 / 2000: loss 0.239535\n",
      "iteration 1000 / 2000: loss 0.268738\n",
      "iteration 1100 / 2000: loss 0.201552\n",
      "iteration 1200 / 2000: loss 0.296139\n",
      "iteration 1300 / 2000: loss 0.258246\n",
      "iteration 1400 / 2000: loss 0.259086\n",
      "iteration 1500 / 2000: loss 0.288131\n",
      "iteration 1600 / 2000: loss 0.223467\n",
      "iteration 1700 / 2000: loss 0.339102\n",
      "iteration 1800 / 2000: loss 0.349435\n",
      "iteration 1900 / 2000: loss 0.305618\n",
      "Hidden Size: 50, Learning Rate: 0.00500005, LR Decay: 0.47250000000000003, Regularization: 0.4, Validation accuracy: 0.9506\n",
      "iteration 0 / 2000: loss 2.302588\n",
      "iteration 100 / 2000: loss 0.387656\n",
      "iteration 200 / 2000: loss 0.399383\n",
      "iteration 300 / 2000: loss 0.245897\n",
      "iteration 400 / 2000: loss 0.331975\n",
      "iteration 500 / 2000: loss 0.364464\n",
      "iteration 600 / 2000: loss 0.336810\n",
      "iteration 700 / 2000: loss 0.280884\n",
      "iteration 800 / 2000: loss 0.254330\n",
      "iteration 900 / 2000: loss 0.156004\n",
      "iteration 1000 / 2000: loss 0.213680\n",
      "iteration 1100 / 2000: loss 0.165301\n",
      "iteration 1200 / 2000: loss 0.236438\n",
      "iteration 1300 / 2000: loss 0.260018\n",
      "iteration 1400 / 2000: loss 0.280028\n",
      "iteration 1500 / 2000: loss 0.223148\n",
      "iteration 1600 / 2000: loss 0.260369\n",
      "iteration 1700 / 2000: loss 0.217451\n",
      "iteration 1800 / 2000: loss 0.218455\n",
      "iteration 1900 / 2000: loss 0.216398\n",
      "Hidden Size: 50, Learning Rate: 0.00500005, LR Decay: 0.3, Regularization: 0.0, Validation accuracy: 0.9346\n",
      "iteration 0 / 2000: loss 2.302611\n",
      "iteration 100 / 2000: loss 0.499972\n",
      "iteration 200 / 2000: loss 0.346555\n",
      "iteration 300 / 2000: loss 0.397359\n",
      "iteration 400 / 2000: loss 0.283520\n",
      "iteration 500 / 2000: loss 0.293294\n",
      "iteration 600 / 2000: loss 0.277384\n",
      "iteration 700 / 2000: loss 0.236655\n",
      "iteration 800 / 2000: loss 0.168680\n",
      "iteration 900 / 2000: loss 0.272454\n",
      "iteration 1000 / 2000: loss 0.201260\n",
      "iteration 1100 / 2000: loss 0.226591\n",
      "iteration 1200 / 2000: loss 0.231289\n",
      "iteration 1300 / 2000: loss 0.265865\n",
      "iteration 1400 / 2000: loss 0.247914\n",
      "iteration 1500 / 2000: loss 0.328044\n",
      "iteration 1600 / 2000: loss 0.335812\n",
      "iteration 1700 / 2000: loss 0.176512\n",
      "iteration 1800 / 2000: loss 0.248909\n",
      "iteration 1900 / 2000: loss 0.243327\n",
      "Hidden Size: 50, Learning Rate: 0.00500005, LR Decay: 0.3, Regularization: 0.1, Validation accuracy: 0.9318\n",
      "iteration 0 / 2000: loss 2.302629\n",
      "iteration 100 / 2000: loss 0.502100\n",
      "iteration 200 / 2000: loss 0.404443\n",
      "iteration 300 / 2000: loss 0.293320\n",
      "iteration 400 / 2000: loss 0.295163\n",
      "iteration 500 / 2000: loss 0.393528\n",
      "iteration 600 / 2000: loss 0.271268\n",
      "iteration 700 / 2000: loss 0.316084\n",
      "iteration 800 / 2000: loss 0.243292\n",
      "iteration 900 / 2000: loss 0.318428\n",
      "iteration 1000 / 2000: loss 0.263697\n",
      "iteration 1100 / 2000: loss 0.279112\n",
      "iteration 1200 / 2000: loss 0.230111\n",
      "iteration 1300 / 2000: loss 0.257463\n",
      "iteration 1400 / 2000: loss 0.199899\n",
      "iteration 1500 / 2000: loss 0.234210\n",
      "iteration 1600 / 2000: loss 0.239771\n",
      "iteration 1700 / 2000: loss 0.341807\n",
      "iteration 1800 / 2000: loss 0.297213\n",
      "iteration 1900 / 2000: loss 0.254407\n",
      "Hidden Size: 50, Learning Rate: 0.00500005, LR Decay: 0.3, Regularization: 0.2, Validation accuracy: 0.9334\n",
      "iteration 0 / 2000: loss 2.302677\n",
      "iteration 100 / 2000: loss 0.390687\n",
      "iteration 200 / 2000: loss 0.327698\n",
      "iteration 300 / 2000: loss 0.316871\n",
      "iteration 400 / 2000: loss 0.319959\n",
      "iteration 500 / 2000: loss 0.184340\n",
      "iteration 600 / 2000: loss 0.248209\n",
      "iteration 700 / 2000: loss 0.232606\n",
      "iteration 800 / 2000: loss 0.325006\n",
      "iteration 900 / 2000: loss 0.320296\n",
      "iteration 1000 / 2000: loss 0.262843\n",
      "iteration 1100 / 2000: loss 0.181957\n",
      "iteration 1200 / 2000: loss 0.193133\n",
      "iteration 1300 / 2000: loss 0.226293\n",
      "iteration 1400 / 2000: loss 0.346147\n",
      "iteration 1500 / 2000: loss 0.190908\n",
      "iteration 1600 / 2000: loss 0.232473\n",
      "iteration 1700 / 2000: loss 0.266457\n",
      "iteration 1800 / 2000: loss 0.327294\n",
      "iteration 1900 / 2000: loss 0.242837\n",
      "Hidden Size: 50, Learning Rate: 0.00500005, LR Decay: 0.3, Regularization: 0.30000000000000004, Validation accuracy: 0.9332\n",
      "iteration 0 / 2000: loss 2.302677\n",
      "iteration 100 / 2000: loss 0.585284\n",
      "iteration 200 / 2000: loss 0.475223\n",
      "iteration 300 / 2000: loss 0.342001\n",
      "iteration 400 / 2000: loss 0.265512\n",
      "iteration 500 / 2000: loss 0.342869\n",
      "iteration 600 / 2000: loss 0.383921\n",
      "iteration 700 / 2000: loss 0.323957\n",
      "iteration 800 / 2000: loss 0.414055\n",
      "iteration 900 / 2000: loss 0.304735\n",
      "iteration 1000 / 2000: loss 0.321128\n",
      "iteration 1100 / 2000: loss 0.352673\n",
      "iteration 1200 / 2000: loss 0.249143\n",
      "iteration 1300 / 2000: loss 0.275049\n",
      "iteration 1400 / 2000: loss 0.332802\n",
      "iteration 1500 / 2000: loss 0.256676\n",
      "iteration 1600 / 2000: loss 0.371375\n",
      "iteration 1700 / 2000: loss 0.331748\n",
      "iteration 1800 / 2000: loss 0.294674\n",
      "iteration 1900 / 2000: loss 0.291710\n",
      "Hidden Size: 50, Learning Rate: 0.00500005, LR Decay: 0.3, Regularization: 0.4, Validation accuracy: 0.93\n",
      "iteration 0 / 2000: loss 2.302600\n",
      "iteration 100 / 2000: loss 0.353567\n",
      "iteration 200 / 2000: loss 0.188188\n",
      "iteration 300 / 2000: loss 0.197354\n",
      "iteration 400 / 2000: loss 0.200170\n",
      "iteration 500 / 2000: loss 0.128150\n",
      "iteration 600 / 2000: loss 0.188325\n",
      "iteration 700 / 2000: loss 0.220159\n",
      "iteration 800 / 2000: loss 0.160190\n",
      "iteration 900 / 2000: loss 0.105458\n",
      "iteration 1000 / 2000: loss 0.168978\n",
      "iteration 1100 / 2000: loss 0.097151\n",
      "iteration 1200 / 2000: loss 0.097263\n",
      "iteration 1300 / 2000: loss 0.081112\n",
      "iteration 1400 / 2000: loss 0.116263\n",
      "iteration 1500 / 2000: loss 0.145259\n",
      "iteration 1600 / 2000: loss 0.060695\n",
      "iteration 1700 / 2000: loss 0.102058\n",
      "iteration 1800 / 2000: loss 0.067639\n",
      "iteration 1900 / 2000: loss 0.127455\n",
      "Hidden Size: 50, Learning Rate: 0.002500074999999999, LR Decay: 0.99, Regularization: 0.0, Validation accuracy: 0.9726\n",
      "iteration 0 / 2000: loss 2.302613\n",
      "iteration 100 / 2000: loss 0.319391\n",
      "iteration 200 / 2000: loss 0.250598\n",
      "iteration 300 / 2000: loss 0.191466\n",
      "iteration 400 / 2000: loss 0.200447\n",
      "iteration 500 / 2000: loss 0.191604\n",
      "iteration 600 / 2000: loss 0.131421\n",
      "iteration 700 / 2000: loss 0.137500\n",
      "iteration 800 / 2000: loss 0.193420\n",
      "iteration 900 / 2000: loss 0.182676\n",
      "iteration 1000 / 2000: loss 0.143118\n",
      "iteration 1100 / 2000: loss 0.130909\n",
      "iteration 1200 / 2000: loss 0.167627\n",
      "iteration 1300 / 2000: loss 0.163454\n",
      "iteration 1400 / 2000: loss 0.129918\n",
      "iteration 1500 / 2000: loss 0.143407\n",
      "iteration 1600 / 2000: loss 0.134949\n",
      "iteration 1700 / 2000: loss 0.151469\n",
      "iteration 1800 / 2000: loss 0.166245\n",
      "iteration 1900 / 2000: loss 0.144026\n",
      "Hidden Size: 50, Learning Rate: 0.002500074999999999, LR Decay: 0.99, Regularization: 0.1, Validation accuracy: 0.9702\n",
      "iteration 0 / 2000: loss 2.302607\n",
      "iteration 100 / 2000: loss 0.322426\n",
      "iteration 200 / 2000: loss 0.373112\n",
      "iteration 300 / 2000: loss 0.295669\n",
      "iteration 400 / 2000: loss 0.224154\n",
      "iteration 500 / 2000: loss 0.183346\n",
      "iteration 600 / 2000: loss 0.227400\n",
      "iteration 700 / 2000: loss 0.172753\n",
      "iteration 800 / 2000: loss 0.230059\n",
      "iteration 900 / 2000: loss 0.135108\n",
      "iteration 1000 / 2000: loss 0.145365\n",
      "iteration 1100 / 2000: loss 0.180886\n",
      "iteration 1200 / 2000: loss 0.140043\n",
      "iteration 1300 / 2000: loss 0.154506\n",
      "iteration 1400 / 2000: loss 0.179966\n",
      "iteration 1500 / 2000: loss 0.108444\n",
      "iteration 1600 / 2000: loss 0.146724\n",
      "iteration 1700 / 2000: loss 0.147681\n",
      "iteration 1800 / 2000: loss 0.145241\n",
      "iteration 1900 / 2000: loss 0.154569\n",
      "Hidden Size: 50, Learning Rate: 0.002500074999999999, LR Decay: 0.99, Regularization: 0.2, Validation accuracy: 0.9718\n",
      "iteration 0 / 2000: loss 2.302655\n",
      "iteration 100 / 2000: loss 0.369956\n",
      "iteration 200 / 2000: loss 0.317522\n",
      "iteration 300 / 2000: loss 0.252121\n",
      "iteration 400 / 2000: loss 0.189089\n",
      "iteration 500 / 2000: loss 0.218326\n",
      "iteration 600 / 2000: loss 0.243898\n",
      "iteration 700 / 2000: loss 0.213576\n",
      "iteration 800 / 2000: loss 0.192508\n",
      "iteration 900 / 2000: loss 0.185634\n",
      "iteration 1000 / 2000: loss 0.187222\n",
      "iteration 1100 / 2000: loss 0.161559\n",
      "iteration 1200 / 2000: loss 0.173542\n",
      "iteration 1300 / 2000: loss 0.173467\n",
      "iteration 1400 / 2000: loss 0.217095\n",
      "iteration 1500 / 2000: loss 0.206139\n",
      "iteration 1600 / 2000: loss 0.192140\n",
      "iteration 1700 / 2000: loss 0.176897\n",
      "iteration 1800 / 2000: loss 0.186267\n",
      "iteration 1900 / 2000: loss 0.205392\n",
      "Hidden Size: 50, Learning Rate: 0.002500074999999999, LR Decay: 0.99, Regularization: 0.30000000000000004, Validation accuracy: 0.967\n",
      "iteration 0 / 2000: loss 2.302647\n",
      "iteration 100 / 2000: loss 0.408756\n",
      "iteration 200 / 2000: loss 0.328137\n",
      "iteration 300 / 2000: loss 0.302719\n",
      "iteration 400 / 2000: loss 0.261878\n",
      "iteration 500 / 2000: loss 0.200739\n",
      "iteration 600 / 2000: loss 0.201322\n",
      "iteration 700 / 2000: loss 0.269789\n",
      "iteration 800 / 2000: loss 0.250269\n",
      "iteration 900 / 2000: loss 0.287134\n",
      "iteration 1000 / 2000: loss 0.256509\n",
      "iteration 1100 / 2000: loss 0.307834\n",
      "iteration 1200 / 2000: loss 0.232185\n",
      "iteration 1300 / 2000: loss 0.236184\n",
      "iteration 1400 / 2000: loss 0.260158\n",
      "iteration 1500 / 2000: loss 0.293034\n",
      "iteration 1600 / 2000: loss 0.192000\n",
      "iteration 1700 / 2000: loss 0.174293\n",
      "iteration 1800 / 2000: loss 0.231671\n",
      "iteration 1900 / 2000: loss 0.238210\n",
      "Hidden Size: 50, Learning Rate: 0.002500074999999999, LR Decay: 0.99, Regularization: 0.4, Validation accuracy: 0.9654\n",
      "iteration 0 / 2000: loss 2.302593\n",
      "iteration 100 / 2000: loss 0.372734\n",
      "iteration 200 / 2000: loss 0.340632\n",
      "iteration 300 / 2000: loss 0.260724\n",
      "iteration 400 / 2000: loss 0.227803\n",
      "iteration 500 / 2000: loss 0.144083\n",
      "iteration 600 / 2000: loss 0.195704\n",
      "iteration 700 / 2000: loss 0.167672\n",
      "iteration 800 / 2000: loss 0.164748\n",
      "iteration 900 / 2000: loss 0.174308\n",
      "iteration 1000 / 2000: loss 0.177912\n",
      "iteration 1100 / 2000: loss 0.159613\n",
      "iteration 1200 / 2000: loss 0.109281\n",
      "iteration 1300 / 2000: loss 0.162699\n",
      "iteration 1400 / 2000: loss 0.100131\n",
      "iteration 1500 / 2000: loss 0.142525\n",
      "iteration 1600 / 2000: loss 0.076989\n",
      "iteration 1700 / 2000: loss 0.060590\n",
      "iteration 1800 / 2000: loss 0.100361\n",
      "iteration 1900 / 2000: loss 0.099556\n",
      "Hidden Size: 50, Learning Rate: 0.002500074999999999, LR Decay: 0.8175, Regularization: 0.0, Validation accuracy: 0.9664\n",
      "iteration 0 / 2000: loss 2.302590\n",
      "iteration 100 / 2000: loss 0.471161\n",
      "iteration 200 / 2000: loss 0.264299\n",
      "iteration 300 / 2000: loss 0.291340\n",
      "iteration 400 / 2000: loss 0.230555\n",
      "iteration 500 / 2000: loss 0.193718\n",
      "iteration 600 / 2000: loss 0.183293\n",
      "iteration 700 / 2000: loss 0.290331\n",
      "iteration 800 / 2000: loss 0.159861\n",
      "iteration 900 / 2000: loss 0.152410\n",
      "iteration 1000 / 2000: loss 0.129470\n",
      "iteration 1100 / 2000: loss 0.157707\n",
      "iteration 1200 / 2000: loss 0.171169\n",
      "iteration 1300 / 2000: loss 0.150748\n",
      "iteration 1400 / 2000: loss 0.136059\n",
      "iteration 1500 / 2000: loss 0.134027\n",
      "iteration 1600 / 2000: loss 0.101019\n",
      "iteration 1700 / 2000: loss 0.143622\n",
      "iteration 1800 / 2000: loss 0.181445\n",
      "iteration 1900 / 2000: loss 0.171260\n",
      "Hidden Size: 50, Learning Rate: 0.002500074999999999, LR Decay: 0.8175, Regularization: 0.1, Validation accuracy: 0.9664\n",
      "iteration 0 / 2000: loss 2.302618\n",
      "iteration 100 / 2000: loss 0.417226\n",
      "iteration 200 / 2000: loss 0.359133\n",
      "iteration 300 / 2000: loss 0.320378\n",
      "iteration 400 / 2000: loss 0.350403\n",
      "iteration 500 / 2000: loss 0.236137\n",
      "iteration 600 / 2000: loss 0.324276\n",
      "iteration 700 / 2000: loss 0.202767\n",
      "iteration 800 / 2000: loss 0.217946\n",
      "iteration 900 / 2000: loss 0.190327\n",
      "iteration 1000 / 2000: loss 0.210830\n",
      "iteration 1100 / 2000: loss 0.197755\n",
      "iteration 1200 / 2000: loss 0.306090\n",
      "iteration 1300 / 2000: loss 0.145711\n",
      "iteration 1400 / 2000: loss 0.157970\n",
      "iteration 1500 / 2000: loss 0.200453\n",
      "iteration 1600 / 2000: loss 0.151218\n",
      "iteration 1700 / 2000: loss 0.162635\n",
      "iteration 1800 / 2000: loss 0.203720\n",
      "iteration 1900 / 2000: loss 0.148441\n",
      "Hidden Size: 50, Learning Rate: 0.002500074999999999, LR Decay: 0.8175, Regularization: 0.2, Validation accuracy: 0.9636\n",
      "iteration 0 / 2000: loss 2.302635\n",
      "iteration 100 / 2000: loss 0.456685\n",
      "iteration 200 / 2000: loss 0.331463\n",
      "iteration 300 / 2000: loss 0.252660\n",
      "iteration 400 / 2000: loss 0.215219\n",
      "iteration 500 / 2000: loss 0.248896\n",
      "iteration 600 / 2000: loss 0.243918\n",
      "iteration 700 / 2000: loss 0.210057\n",
      "iteration 800 / 2000: loss 0.227872\n",
      "iteration 900 / 2000: loss 0.297530\n",
      "iteration 1000 / 2000: loss 0.172451\n",
      "iteration 1100 / 2000: loss 0.215258\n",
      "iteration 1200 / 2000: loss 0.207080\n",
      "iteration 1300 / 2000: loss 0.195056\n",
      "iteration 1400 / 2000: loss 0.174768\n",
      "iteration 1500 / 2000: loss 0.179022\n",
      "iteration 1600 / 2000: loss 0.202240\n",
      "iteration 1700 / 2000: loss 0.266777\n",
      "iteration 1800 / 2000: loss 0.191674\n",
      "iteration 1900 / 2000: loss 0.194130\n",
      "Hidden Size: 50, Learning Rate: 0.002500074999999999, LR Decay: 0.8175, Regularization: 0.30000000000000004, Validation accuracy: 0.963\n",
      "iteration 0 / 2000: loss 2.302690\n",
      "iteration 100 / 2000: loss 0.389427\n",
      "iteration 200 / 2000: loss 0.326987\n",
      "iteration 300 / 2000: loss 0.331932\n",
      "iteration 400 / 2000: loss 0.303344\n",
      "iteration 500 / 2000: loss 0.238998\n",
      "iteration 600 / 2000: loss 0.220540\n",
      "iteration 700 / 2000: loss 0.292686\n",
      "iteration 800 / 2000: loss 0.306783\n",
      "iteration 900 / 2000: loss 0.220964\n",
      "iteration 1000 / 2000: loss 0.236739\n",
      "iteration 1100 / 2000: loss 0.234922\n",
      "iteration 1200 / 2000: loss 0.256575\n",
      "iteration 1300 / 2000: loss 0.244812\n",
      "iteration 1400 / 2000: loss 0.247715\n",
      "iteration 1500 / 2000: loss 0.214842\n",
      "iteration 1600 / 2000: loss 0.229068\n",
      "iteration 1700 / 2000: loss 0.240116\n",
      "iteration 1800 / 2000: loss 0.214914\n",
      "iteration 1900 / 2000: loss 0.180129\n",
      "Hidden Size: 50, Learning Rate: 0.002500074999999999, LR Decay: 0.8175, Regularization: 0.4, Validation accuracy: 0.9632\n",
      "iteration 0 / 2000: loss 2.302579\n",
      "iteration 100 / 2000: loss 0.438246\n",
      "iteration 200 / 2000: loss 0.317895\n",
      "iteration 300 / 2000: loss 0.360475\n",
      "iteration 400 / 2000: loss 0.200607\n",
      "iteration 500 / 2000: loss 0.202583\n",
      "iteration 600 / 2000: loss 0.181824\n",
      "iteration 700 / 2000: loss 0.206916\n",
      "iteration 800 / 2000: loss 0.181858\n",
      "iteration 900 / 2000: loss 0.153197\n",
      "iteration 1000 / 2000: loss 0.137454\n",
      "iteration 1100 / 2000: loss 0.177278\n",
      "iteration 1200 / 2000: loss 0.216429\n",
      "iteration 1300 / 2000: loss 0.191138\n",
      "iteration 1400 / 2000: loss 0.169458\n",
      "iteration 1500 / 2000: loss 0.217723\n",
      "iteration 1600 / 2000: loss 0.203115\n",
      "iteration 1700 / 2000: loss 0.149254\n",
      "iteration 1800 / 2000: loss 0.165829\n",
      "iteration 1900 / 2000: loss 0.232872\n",
      "Hidden Size: 50, Learning Rate: 0.002500074999999999, LR Decay: 0.645, Regularization: 0.0, Validation accuracy: 0.9544\n",
      "iteration 0 / 2000: loss 2.302595\n",
      "iteration 100 / 2000: loss 0.418271\n",
      "iteration 200 / 2000: loss 0.402856\n",
      "iteration 300 / 2000: loss 0.401494\n",
      "iteration 400 / 2000: loss 0.334144\n",
      "iteration 500 / 2000: loss 0.310042\n",
      "iteration 600 / 2000: loss 0.180875\n",
      "iteration 700 / 2000: loss 0.216002\n",
      "iteration 800 / 2000: loss 0.168004\n",
      "iteration 900 / 2000: loss 0.255949\n",
      "iteration 1000 / 2000: loss 0.114382\n",
      "iteration 1100 / 2000: loss 0.182620\n",
      "iteration 1200 / 2000: loss 0.195471\n",
      "iteration 1300 / 2000: loss 0.246096\n",
      "iteration 1400 / 2000: loss 0.176174\n",
      "iteration 1500 / 2000: loss 0.175514\n",
      "iteration 1600 / 2000: loss 0.221960\n",
      "iteration 1700 / 2000: loss 0.168918\n",
      "iteration 1800 / 2000: loss 0.229537\n",
      "iteration 1900 / 2000: loss 0.195247\n",
      "Hidden Size: 50, Learning Rate: 0.002500074999999999, LR Decay: 0.645, Regularization: 0.1, Validation accuracy: 0.9514\n",
      "iteration 0 / 2000: loss 2.302603\n",
      "iteration 100 / 2000: loss 0.413630\n",
      "iteration 200 / 2000: loss 0.407763\n",
      "iteration 300 / 2000: loss 0.305770\n",
      "iteration 400 / 2000: loss 0.388748\n",
      "iteration 500 / 2000: loss 0.284393\n",
      "iteration 600 / 2000: loss 0.303634\n",
      "iteration 700 / 2000: loss 0.252762\n",
      "iteration 800 / 2000: loss 0.157731\n",
      "iteration 900 / 2000: loss 0.222989\n",
      "iteration 1000 / 2000: loss 0.196204\n",
      "iteration 1100 / 2000: loss 0.193480\n",
      "iteration 1200 / 2000: loss 0.225457\n",
      "iteration 1300 / 2000: loss 0.205687\n",
      "iteration 1400 / 2000: loss 0.196719\n",
      "iteration 1500 / 2000: loss 0.204297\n",
      "iteration 1600 / 2000: loss 0.188965\n",
      "iteration 1700 / 2000: loss 0.216284\n",
      "iteration 1800 / 2000: loss 0.226218\n",
      "iteration 1900 / 2000: loss 0.259855\n",
      "Hidden Size: 50, Learning Rate: 0.002500074999999999, LR Decay: 0.645, Regularization: 0.2, Validation accuracy: 0.9538\n",
      "iteration 0 / 2000: loss 2.302644\n",
      "iteration 100 / 2000: loss 0.525630\n",
      "iteration 200 / 2000: loss 0.320453\n",
      "iteration 300 / 2000: loss 0.315367\n",
      "iteration 400 / 2000: loss 0.307466\n",
      "iteration 500 / 2000: loss 0.270857\n",
      "iteration 600 / 2000: loss 0.335094\n",
      "iteration 700 / 2000: loss 0.245473\n",
      "iteration 800 / 2000: loss 0.271510\n",
      "iteration 900 / 2000: loss 0.214179\n",
      "iteration 1000 / 2000: loss 0.273801\n",
      "iteration 1100 / 2000: loss 0.219950\n",
      "iteration 1200 / 2000: loss 0.236494\n",
      "iteration 1300 / 2000: loss 0.252211\n",
      "iteration 1400 / 2000: loss 0.281646\n",
      "iteration 1500 / 2000: loss 0.227478\n",
      "iteration 1600 / 2000: loss 0.264967\n",
      "iteration 1700 / 2000: loss 0.184363\n",
      "iteration 1800 / 2000: loss 0.204647\n",
      "iteration 1900 / 2000: loss 0.217996\n",
      "Hidden Size: 50, Learning Rate: 0.002500074999999999, LR Decay: 0.645, Regularization: 0.30000000000000004, Validation accuracy: 0.9522\n",
      "iteration 0 / 2000: loss 2.302650\n",
      "iteration 100 / 2000: loss 0.476182\n",
      "iteration 200 / 2000: loss 0.338406\n",
      "iteration 300 / 2000: loss 0.307965\n",
      "iteration 400 / 2000: loss 0.259228\n",
      "iteration 500 / 2000: loss 0.228532\n",
      "iteration 600 / 2000: loss 0.356604\n",
      "iteration 700 / 2000: loss 0.303953\n",
      "iteration 800 / 2000: loss 0.283560\n",
      "iteration 900 / 2000: loss 0.235702\n",
      "iteration 1000 / 2000: loss 0.257156\n",
      "iteration 1100 / 2000: loss 0.254475\n",
      "iteration 1200 / 2000: loss 0.204873\n",
      "iteration 1300 / 2000: loss 0.245529\n",
      "iteration 1400 / 2000: loss 0.261877\n",
      "iteration 1500 / 2000: loss 0.252160\n",
      "iteration 1600 / 2000: loss 0.219151\n",
      "iteration 1700 / 2000: loss 0.300506\n",
      "iteration 1800 / 2000: loss 0.262433\n",
      "iteration 1900 / 2000: loss 0.251681\n",
      "Hidden Size: 50, Learning Rate: 0.002500074999999999, LR Decay: 0.645, Regularization: 0.4, Validation accuracy: 0.9514\n",
      "iteration 0 / 2000: loss 2.302580\n",
      "iteration 100 / 2000: loss 0.546989\n",
      "iteration 200 / 2000: loss 0.315786\n",
      "iteration 300 / 2000: loss 0.261721\n",
      "iteration 400 / 2000: loss 0.296846\n",
      "iteration 500 / 2000: loss 0.272760\n",
      "iteration 600 / 2000: loss 0.353306\n",
      "iteration 700 / 2000: loss 0.211849\n",
      "iteration 800 / 2000: loss 0.293334\n",
      "iteration 900 / 2000: loss 0.176716\n",
      "iteration 1000 / 2000: loss 0.256881\n",
      "iteration 1100 / 2000: loss 0.228402\n",
      "iteration 1200 / 2000: loss 0.214871\n",
      "iteration 1300 / 2000: loss 0.238120\n",
      "iteration 1400 / 2000: loss 0.325181\n",
      "iteration 1500 / 2000: loss 0.273840\n",
      "iteration 1600 / 2000: loss 0.211492\n",
      "iteration 1700 / 2000: loss 0.218187\n",
      "iteration 1800 / 2000: loss 0.365347\n",
      "iteration 1900 / 2000: loss 0.217281\n",
      "Hidden Size: 50, Learning Rate: 0.002500074999999999, LR Decay: 0.47250000000000003, Regularization: 0.0, Validation accuracy: 0.9366\n",
      "iteration 0 / 2000: loss 2.302618\n",
      "iteration 100 / 2000: loss 0.624914\n",
      "iteration 200 / 2000: loss 0.304394\n",
      "iteration 300 / 2000: loss 0.364555\n",
      "iteration 400 / 2000: loss 0.202564\n",
      "iteration 500 / 2000: loss 0.258785\n",
      "iteration 600 / 2000: loss 0.269210\n",
      "iteration 700 / 2000: loss 0.229286\n",
      "iteration 800 / 2000: loss 0.273691\n",
      "iteration 900 / 2000: loss 0.239648\n",
      "iteration 1000 / 2000: loss 0.186087\n",
      "iteration 1100 / 2000: loss 0.217711\n",
      "iteration 1200 / 2000: loss 0.268497\n",
      "iteration 1300 / 2000: loss 0.182148\n",
      "iteration 1400 / 2000: loss 0.202621\n",
      "iteration 1500 / 2000: loss 0.171930\n",
      "iteration 1600 / 2000: loss 0.214986\n",
      "iteration 1700 / 2000: loss 0.273356\n",
      "iteration 1800 / 2000: loss 0.251234\n",
      "iteration 1900 / 2000: loss 0.241099\n",
      "Hidden Size: 50, Learning Rate: 0.002500074999999999, LR Decay: 0.47250000000000003, Regularization: 0.1, Validation accuracy: 0.9364\n",
      "iteration 0 / 2000: loss 2.302611\n",
      "iteration 100 / 2000: loss 0.595004\n",
      "iteration 200 / 2000: loss 0.422347\n",
      "iteration 300 / 2000: loss 0.363903\n",
      "iteration 400 / 2000: loss 0.355443\n",
      "iteration 500 / 2000: loss 0.307218\n",
      "iteration 600 / 2000: loss 0.229276\n",
      "iteration 700 / 2000: loss 0.309227\n",
      "iteration 800 / 2000: loss 0.329482\n",
      "iteration 900 / 2000: loss 0.188527\n",
      "iteration 1000 / 2000: loss 0.201746\n",
      "iteration 1100 / 2000: loss 0.342151\n",
      "iteration 1200 / 2000: loss 0.409659\n",
      "iteration 1300 / 2000: loss 0.186353\n",
      "iteration 1400 / 2000: loss 0.307215\n",
      "iteration 1500 / 2000: loss 0.293281\n",
      "iteration 1600 / 2000: loss 0.271274\n",
      "iteration 1700 / 2000: loss 0.300270\n",
      "iteration 1800 / 2000: loss 0.300435\n",
      "iteration 1900 / 2000: loss 0.174924\n",
      "Hidden Size: 50, Learning Rate: 0.002500074999999999, LR Decay: 0.47250000000000003, Regularization: 0.2, Validation accuracy: 0.9336\n",
      "iteration 0 / 2000: loss 2.302648\n",
      "iteration 100 / 2000: loss 0.711465\n",
      "iteration 200 / 2000: loss 0.438615\n",
      "iteration 300 / 2000: loss 0.351019\n",
      "iteration 400 / 2000: loss 0.363026\n",
      "iteration 500 / 2000: loss 0.220928\n",
      "iteration 600 / 2000: loss 0.281905\n",
      "iteration 700 / 2000: loss 0.302131\n",
      "iteration 800 / 2000: loss 0.284272\n",
      "iteration 900 / 2000: loss 0.296318\n",
      "iteration 1000 / 2000: loss 0.223135\n",
      "iteration 1100 / 2000: loss 0.273240\n",
      "iteration 1200 / 2000: loss 0.286015\n",
      "iteration 1300 / 2000: loss 0.381077\n",
      "iteration 1400 / 2000: loss 0.289946\n",
      "iteration 1500 / 2000: loss 0.239929\n",
      "iteration 1600 / 2000: loss 0.323655\n",
      "iteration 1700 / 2000: loss 0.237290\n",
      "iteration 1800 / 2000: loss 0.321325\n",
      "iteration 1900 / 2000: loss 0.267466\n",
      "Hidden Size: 50, Learning Rate: 0.002500074999999999, LR Decay: 0.47250000000000003, Regularization: 0.30000000000000004, Validation accuracy: 0.9316\n",
      "iteration 0 / 2000: loss 2.302649\n",
      "iteration 100 / 2000: loss 0.680878\n",
      "iteration 200 / 2000: loss 0.453479\n",
      "iteration 300 / 2000: loss 0.366892\n",
      "iteration 400 / 2000: loss 0.271322\n",
      "iteration 500 / 2000: loss 0.336156\n",
      "iteration 600 / 2000: loss 0.304025\n",
      "iteration 700 / 2000: loss 0.406548\n",
      "iteration 800 / 2000: loss 0.323477\n",
      "iteration 900 / 2000: loss 0.371576\n",
      "iteration 1000 / 2000: loss 0.360206\n",
      "iteration 1100 / 2000: loss 0.322899\n",
      "iteration 1200 / 2000: loss 0.331816\n",
      "iteration 1300 / 2000: loss 0.174527\n",
      "iteration 1400 / 2000: loss 0.266244\n",
      "iteration 1500 / 2000: loss 0.296214\n",
      "iteration 1600 / 2000: loss 0.264665\n",
      "iteration 1700 / 2000: loss 0.287311\n",
      "iteration 1800 / 2000: loss 0.273602\n",
      "iteration 1900 / 2000: loss 0.209818\n",
      "Hidden Size: 50, Learning Rate: 0.002500074999999999, LR Decay: 0.47250000000000003, Regularization: 0.4, Validation accuracy: 0.932\n",
      "iteration 0 / 2000: loss 2.302570\n",
      "iteration 100 / 2000: loss 1.269680\n",
      "iteration 200 / 2000: loss 0.389330\n",
      "iteration 300 / 2000: loss 0.318537\n",
      "iteration 400 / 2000: loss 0.281552\n",
      "iteration 500 / 2000: loss 0.291217\n",
      "iteration 600 / 2000: loss 0.449248\n",
      "iteration 700 / 2000: loss 0.370027\n",
      "iteration 800 / 2000: loss 0.292289\n",
      "iteration 900 / 2000: loss 0.268805\n",
      "iteration 1000 / 2000: loss 0.341079\n",
      "iteration 1100 / 2000: loss 0.306263\n",
      "iteration 1200 / 2000: loss 0.334983\n",
      "iteration 1300 / 2000: loss 0.401007\n",
      "iteration 1400 / 2000: loss 0.305001\n",
      "iteration 1500 / 2000: loss 0.291577\n",
      "iteration 1600 / 2000: loss 0.383177\n",
      "iteration 1700 / 2000: loss 0.403213\n",
      "iteration 1800 / 2000: loss 0.418414\n",
      "iteration 1900 / 2000: loss 0.355597\n",
      "Hidden Size: 50, Learning Rate: 0.002500074999999999, LR Decay: 0.3, Regularization: 0.0, Validation accuracy: 0.9148\n",
      "iteration 0 / 2000: loss 2.302606\n",
      "iteration 100 / 2000: loss 1.292778\n",
      "iteration 200 / 2000: loss 0.468229\n",
      "iteration 300 / 2000: loss 0.357073\n",
      "iteration 400 / 2000: loss 0.339806\n",
      "iteration 500 / 2000: loss 0.364782\n",
      "iteration 600 / 2000: loss 0.359507\n",
      "iteration 700 / 2000: loss 0.270632\n",
      "iteration 800 / 2000: loss 0.291195\n",
      "iteration 900 / 2000: loss 0.375452\n",
      "iteration 1000 / 2000: loss 0.301863\n",
      "iteration 1100 / 2000: loss 0.309594\n",
      "iteration 1200 / 2000: loss 0.233525\n",
      "iteration 1300 / 2000: loss 0.324097\n",
      "iteration 1400 / 2000: loss 0.297028\n",
      "iteration 1500 / 2000: loss 0.289064\n",
      "iteration 1600 / 2000: loss 0.257538\n",
      "iteration 1700 / 2000: loss 0.230288\n",
      "iteration 1800 / 2000: loss 0.343771\n",
      "iteration 1900 / 2000: loss 0.350523\n",
      "Hidden Size: 50, Learning Rate: 0.002500074999999999, LR Decay: 0.3, Regularization: 0.1, Validation accuracy: 0.9156\n",
      "iteration 0 / 2000: loss 2.302609\n",
      "iteration 100 / 2000: loss 1.412529\n",
      "iteration 200 / 2000: loss 0.478655\n",
      "iteration 300 / 2000: loss 0.316885\n",
      "iteration 400 / 2000: loss 0.513498\n",
      "iteration 500 / 2000: loss 0.363751\n",
      "iteration 600 / 2000: loss 0.494850\n",
      "iteration 700 / 2000: loss 0.419352\n",
      "iteration 800 / 2000: loss 0.323035\n",
      "iteration 900 / 2000: loss 0.348337\n",
      "iteration 1000 / 2000: loss 0.387422\n",
      "iteration 1100 / 2000: loss 0.295010\n",
      "iteration 1200 / 2000: loss 0.202590\n",
      "iteration 1300 / 2000: loss 0.351008\n",
      "iteration 1400 / 2000: loss 0.337780\n",
      "iteration 1500 / 2000: loss 0.274525\n",
      "iteration 1600 / 2000: loss 0.400492\n",
      "iteration 1700 / 2000: loss 0.390989\n",
      "iteration 1800 / 2000: loss 0.376849\n",
      "iteration 1900 / 2000: loss 0.460440\n",
      "Hidden Size: 50, Learning Rate: 0.002500074999999999, LR Decay: 0.3, Regularization: 0.2, Validation accuracy: 0.915\n",
      "iteration 0 / 2000: loss 2.302642\n",
      "iteration 100 / 2000: loss 1.327446\n",
      "iteration 200 / 2000: loss 0.517840\n",
      "iteration 300 / 2000: loss 0.423225\n",
      "iteration 400 / 2000: loss 0.500248\n",
      "iteration 500 / 2000: loss 0.295546\n",
      "iteration 600 / 2000: loss 0.305678\n",
      "iteration 700 / 2000: loss 0.260354\n",
      "iteration 800 / 2000: loss 0.418174\n",
      "iteration 900 / 2000: loss 0.343033\n",
      "iteration 1000 / 2000: loss 0.354282\n",
      "iteration 1100 / 2000: loss 0.398219\n",
      "iteration 1200 / 2000: loss 0.304534\n",
      "iteration 1300 / 2000: loss 0.466847\n",
      "iteration 1400 / 2000: loss 0.313759\n",
      "iteration 1500 / 2000: loss 0.435160\n",
      "iteration 1600 / 2000: loss 0.284128\n",
      "iteration 1700 / 2000: loss 0.371708\n",
      "iteration 1800 / 2000: loss 0.316660\n",
      "iteration 1900 / 2000: loss 0.293219\n",
      "Hidden Size: 50, Learning Rate: 0.002500074999999999, LR Decay: 0.3, Regularization: 0.30000000000000004, Validation accuracy: 0.9138\n",
      "iteration 0 / 2000: loss 2.302662\n",
      "iteration 100 / 2000: loss 1.200859\n",
      "iteration 200 / 2000: loss 0.522395\n",
      "iteration 300 / 2000: loss 0.542177\n",
      "iteration 400 / 2000: loss 0.464008\n",
      "iteration 500 / 2000: loss 0.330755\n",
      "iteration 600 / 2000: loss 0.466493\n",
      "iteration 700 / 2000: loss 0.351520\n",
      "iteration 800 / 2000: loss 0.324651\n",
      "iteration 900 / 2000: loss 0.365675\n",
      "iteration 1000 / 2000: loss 0.395659\n",
      "iteration 1100 / 2000: loss 0.374601\n",
      "iteration 1200 / 2000: loss 0.379075\n",
      "iteration 1300 / 2000: loss 0.385641\n",
      "iteration 1400 / 2000: loss 0.346985\n",
      "iteration 1500 / 2000: loss 0.327982\n",
      "iteration 1600 / 2000: loss 0.369318\n",
      "iteration 1700 / 2000: loss 0.334023\n",
      "iteration 1800 / 2000: loss 0.328578\n",
      "iteration 1900 / 2000: loss 0.376926\n",
      "Hidden Size: 50, Learning Rate: 0.002500074999999999, LR Decay: 0.3, Regularization: 0.4, Validation accuracy: 0.916\n",
      "iteration 0 / 2000: loss 2.302594\n",
      "iteration 100 / 2000: loss 2.302601\n",
      "iteration 200 / 2000: loss 2.302598\n",
      "iteration 300 / 2000: loss 2.302577\n",
      "iteration 400 / 2000: loss 2.302592\n",
      "iteration 500 / 2000: loss 2.302573\n",
      "iteration 600 / 2000: loss 2.302604\n",
      "iteration 700 / 2000: loss 2.302585\n",
      "iteration 800 / 2000: loss 2.302592\n",
      "iteration 900 / 2000: loss 2.302583\n",
      "iteration 1000 / 2000: loss 2.302587\n",
      "iteration 1100 / 2000: loss 2.302609\n",
      "iteration 1200 / 2000: loss 2.302580\n",
      "iteration 1300 / 2000: loss 2.302592\n",
      "iteration 1400 / 2000: loss 2.302590\n",
      "iteration 1500 / 2000: loss 2.302591\n",
      "iteration 1600 / 2000: loss 2.302589\n",
      "iteration 1700 / 2000: loss 2.302590\n",
      "iteration 1800 / 2000: loss 2.302583\n",
      "iteration 1900 / 2000: loss 2.302592\n",
      "Hidden Size: 50, Learning Rate: 1e-07, LR Decay: 0.99, Regularization: 0.0, Validation accuracy: 0.111\n",
      "iteration 0 / 2000: loss 2.302615\n",
      "iteration 100 / 2000: loss 2.302603\n",
      "iteration 200 / 2000: loss 2.302608\n",
      "iteration 300 / 2000: loss 2.302612\n",
      "iteration 400 / 2000: loss 2.302606\n",
      "iteration 500 / 2000: loss 2.302606\n",
      "iteration 600 / 2000: loss 2.302607\n",
      "iteration 700 / 2000: loss 2.302608\n",
      "iteration 800 / 2000: loss 2.302614\n",
      "iteration 900 / 2000: loss 2.302598\n",
      "iteration 1000 / 2000: loss 2.302612\n",
      "iteration 1100 / 2000: loss 2.302611\n",
      "iteration 1200 / 2000: loss 2.302616\n",
      "iteration 1300 / 2000: loss 2.302605\n",
      "iteration 1400 / 2000: loss 2.302602\n",
      "iteration 1500 / 2000: loss 2.302597\n",
      "iteration 1600 / 2000: loss 2.302616\n",
      "iteration 1700 / 2000: loss 2.302598\n",
      "iteration 1800 / 2000: loss 2.302605\n",
      "iteration 1900 / 2000: loss 2.302599\n",
      "Hidden Size: 50, Learning Rate: 1e-07, LR Decay: 0.99, Regularization: 0.1, Validation accuracy: 0.0798\n",
      "iteration 0 / 2000: loss 2.302636\n",
      "iteration 100 / 2000: loss 2.302629\n",
      "iteration 200 / 2000: loss 2.302628\n",
      "iteration 300 / 2000: loss 2.302637\n",
      "iteration 400 / 2000: loss 2.302637\n",
      "iteration 500 / 2000: loss 2.302630\n",
      "iteration 600 / 2000: loss 2.302639\n",
      "iteration 700 / 2000: loss 2.302629\n",
      "iteration 800 / 2000: loss 2.302640\n",
      "iteration 900 / 2000: loss 2.302624\n",
      "iteration 1000 / 2000: loss 2.302624\n",
      "iteration 1100 / 2000: loss 2.302627\n",
      "iteration 1200 / 2000: loss 2.302614\n",
      "iteration 1300 / 2000: loss 2.302624\n",
      "iteration 1400 / 2000: loss 2.302633\n",
      "iteration 1500 / 2000: loss 2.302626\n",
      "iteration 1600 / 2000: loss 2.302623\n",
      "iteration 1700 / 2000: loss 2.302625\n",
      "iteration 1800 / 2000: loss 2.302623\n",
      "iteration 1900 / 2000: loss 2.302631\n",
      "Hidden Size: 50, Learning Rate: 1e-07, LR Decay: 0.99, Regularization: 0.2, Validation accuracy: 0.0796\n",
      "iteration 0 / 2000: loss 2.302657\n",
      "iteration 100 / 2000: loss 2.302648\n",
      "iteration 200 / 2000: loss 2.302652\n",
      "iteration 300 / 2000: loss 2.302652\n",
      "iteration 400 / 2000: loss 2.302644\n",
      "iteration 500 / 2000: loss 2.302656\n",
      "iteration 600 / 2000: loss 2.302642\n",
      "iteration 700 / 2000: loss 2.302638\n",
      "iteration 800 / 2000: loss 2.302640\n",
      "iteration 900 / 2000: loss 2.302642\n",
      "iteration 1000 / 2000: loss 2.302655\n",
      "iteration 1100 / 2000: loss 2.302650\n",
      "iteration 1200 / 2000: loss 2.302643\n",
      "iteration 1300 / 2000: loss 2.302641\n",
      "iteration 1400 / 2000: loss 2.302645\n",
      "iteration 1500 / 2000: loss 2.302638\n",
      "iteration 1600 / 2000: loss 2.302645\n",
      "iteration 1700 / 2000: loss 2.302642\n",
      "iteration 1800 / 2000: loss 2.302641\n",
      "iteration 1900 / 2000: loss 2.302648\n",
      "Hidden Size: 50, Learning Rate: 1e-07, LR Decay: 0.99, Regularization: 0.30000000000000004, Validation accuracy: 0.1056\n",
      "iteration 0 / 2000: loss 2.302687\n",
      "iteration 100 / 2000: loss 2.302678\n",
      "iteration 200 / 2000: loss 2.302679\n",
      "iteration 300 / 2000: loss 2.302673\n",
      "iteration 400 / 2000: loss 2.302677\n",
      "iteration 500 / 2000: loss 2.302677\n",
      "iteration 600 / 2000: loss 2.302687\n",
      "iteration 700 / 2000: loss 2.302672\n",
      "iteration 800 / 2000: loss 2.302676\n",
      "iteration 900 / 2000: loss 2.302679\n",
      "iteration 1000 / 2000: loss 2.302680\n",
      "iteration 1100 / 2000: loss 2.302681\n",
      "iteration 1200 / 2000: loss 2.302684\n",
      "iteration 1300 / 2000: loss 2.302666\n",
      "iteration 1400 / 2000: loss 2.302688\n",
      "iteration 1500 / 2000: loss 2.302676\n",
      "iteration 1600 / 2000: loss 2.302675\n",
      "iteration 1700 / 2000: loss 2.302684\n",
      "iteration 1800 / 2000: loss 2.302674\n",
      "iteration 1900 / 2000: loss 2.302670\n",
      "Hidden Size: 50, Learning Rate: 1e-07, LR Decay: 0.99, Regularization: 0.4, Validation accuracy: 0.0792\n",
      "iteration 0 / 2000: loss 2.302584\n",
      "iteration 100 / 2000: loss 2.302590\n",
      "iteration 200 / 2000: loss 2.302594\n",
      "iteration 300 / 2000: loss 2.302589\n",
      "iteration 400 / 2000: loss 2.302576\n",
      "iteration 500 / 2000: loss 2.302589\n",
      "iteration 600 / 2000: loss 2.302580\n",
      "iteration 700 / 2000: loss 2.302587\n",
      "iteration 800 / 2000: loss 2.302590\n",
      "iteration 900 / 2000: loss 2.302589\n",
      "iteration 1000 / 2000: loss 2.302583\n",
      "iteration 1100 / 2000: loss 2.302592\n",
      "iteration 1200 / 2000: loss 2.302595\n",
      "iteration 1300 / 2000: loss 2.302588\n",
      "iteration 1400 / 2000: loss 2.302577\n",
      "iteration 1500 / 2000: loss 2.302581\n",
      "iteration 1600 / 2000: loss 2.302586\n",
      "iteration 1700 / 2000: loss 2.302583\n",
      "iteration 1800 / 2000: loss 2.302577\n",
      "iteration 1900 / 2000: loss 2.302576\n",
      "Hidden Size: 50, Learning Rate: 1e-07, LR Decay: 0.8175, Regularization: 0.0, Validation accuracy: 0.1158\n",
      "iteration 0 / 2000: loss 2.302625\n",
      "iteration 100 / 2000: loss 2.302620\n",
      "iteration 200 / 2000: loss 2.302625\n",
      "iteration 300 / 2000: loss 2.302616\n",
      "iteration 400 / 2000: loss 2.302618\n",
      "iteration 500 / 2000: loss 2.302608\n",
      "iteration 600 / 2000: loss 2.302643\n",
      "iteration 700 / 2000: loss 2.302627\n",
      "iteration 800 / 2000: loss 2.302623\n",
      "iteration 900 / 2000: loss 2.302609\n",
      "iteration 1000 / 2000: loss 2.302616\n",
      "iteration 1100 / 2000: loss 2.302618\n",
      "iteration 1200 / 2000: loss 2.302625\n",
      "iteration 1300 / 2000: loss 2.302625\n",
      "iteration 1400 / 2000: loss 2.302622\n",
      "iteration 1500 / 2000: loss 2.302602\n",
      "iteration 1600 / 2000: loss 2.302629\n",
      "iteration 1700 / 2000: loss 2.302620\n",
      "iteration 1800 / 2000: loss 2.302622\n",
      "iteration 1900 / 2000: loss 2.302625\n",
      "Hidden Size: 50, Learning Rate: 1e-07, LR Decay: 0.8175, Regularization: 0.1, Validation accuracy: 0.094\n",
      "iteration 0 / 2000: loss 2.302593\n",
      "iteration 100 / 2000: loss 2.302608\n",
      "iteration 200 / 2000: loss 2.302601\n",
      "iteration 300 / 2000: loss 2.302602\n",
      "iteration 400 / 2000: loss 2.302613\n",
      "iteration 500 / 2000: loss 2.302603\n",
      "iteration 600 / 2000: loss 2.302610\n",
      "iteration 700 / 2000: loss 2.302606\n",
      "iteration 800 / 2000: loss 2.302612\n",
      "iteration 900 / 2000: loss 2.302602\n",
      "iteration 1000 / 2000: loss 2.302616\n",
      "iteration 1100 / 2000: loss 2.302606\n",
      "iteration 1200 / 2000: loss 2.302605\n",
      "iteration 1300 / 2000: loss 2.302609\n",
      "iteration 1400 / 2000: loss 2.302614\n",
      "iteration 1500 / 2000: loss 2.302611\n",
      "iteration 1600 / 2000: loss 2.302592\n",
      "iteration 1700 / 2000: loss 2.302603\n",
      "iteration 1800 / 2000: loss 2.302610\n",
      "iteration 1900 / 2000: loss 2.302613\n",
      "Hidden Size: 50, Learning Rate: 1e-07, LR Decay: 0.8175, Regularization: 0.2, Validation accuracy: 0.1172\n",
      "iteration 0 / 2000: loss 2.302645\n",
      "iteration 100 / 2000: loss 2.302652\n",
      "iteration 200 / 2000: loss 2.302655\n",
      "iteration 300 / 2000: loss 2.302652\n",
      "iteration 400 / 2000: loss 2.302661\n",
      "iteration 500 / 2000: loss 2.302653\n",
      "iteration 600 / 2000: loss 2.302650\n",
      "iteration 700 / 2000: loss 2.302668\n",
      "iteration 800 / 2000: loss 2.302663\n",
      "iteration 900 / 2000: loss 2.302653\n",
      "iteration 1000 / 2000: loss 2.302660\n",
      "iteration 1100 / 2000: loss 2.302654\n",
      "iteration 1200 / 2000: loss 2.302651\n",
      "iteration 1300 / 2000: loss 2.302670\n",
      "iteration 1400 / 2000: loss 2.302667\n",
      "iteration 1500 / 2000: loss 2.302659\n",
      "iteration 1600 / 2000: loss 2.302659\n",
      "iteration 1700 / 2000: loss 2.302664\n",
      "iteration 1800 / 2000: loss 2.302641\n",
      "iteration 1900 / 2000: loss 2.302663\n",
      "Hidden Size: 50, Learning Rate: 1e-07, LR Decay: 0.8175, Regularization: 0.30000000000000004, Validation accuracy: 0.0828\n",
      "iteration 0 / 2000: loss 2.302663\n",
      "iteration 100 / 2000: loss 2.302663\n",
      "iteration 200 / 2000: loss 2.302654\n",
      "iteration 300 / 2000: loss 2.302656\n",
      "iteration 400 / 2000: loss 2.302662\n",
      "iteration 500 / 2000: loss 2.302655\n",
      "iteration 600 / 2000: loss 2.302659\n",
      "iteration 700 / 2000: loss 2.302659\n",
      "iteration 800 / 2000: loss 2.302662\n",
      "iteration 900 / 2000: loss 2.302663\n",
      "iteration 1000 / 2000: loss 2.302650\n",
      "iteration 1100 / 2000: loss 2.302655\n",
      "iteration 1200 / 2000: loss 2.302650\n",
      "iteration 1300 / 2000: loss 2.302658\n",
      "iteration 1400 / 2000: loss 2.302654\n",
      "iteration 1500 / 2000: loss 2.302657\n",
      "iteration 1600 / 2000: loss 2.302655\n",
      "iteration 1700 / 2000: loss 2.302649\n",
      "iteration 1800 / 2000: loss 2.302659\n",
      "iteration 1900 / 2000: loss 2.302661\n",
      "Hidden Size: 50, Learning Rate: 1e-07, LR Decay: 0.8175, Regularization: 0.4, Validation accuracy: 0.0832\n",
      "iteration 0 / 2000: loss 2.302584\n",
      "iteration 100 / 2000: loss 2.302594\n",
      "iteration 200 / 2000: loss 2.302595\n",
      "iteration 300 / 2000: loss 2.302580\n",
      "iteration 400 / 2000: loss 2.302589\n",
      "iteration 500 / 2000: loss 2.302582\n",
      "iteration 600 / 2000: loss 2.302588\n",
      "iteration 700 / 2000: loss 2.302590\n",
      "iteration 800 / 2000: loss 2.302604\n",
      "iteration 900 / 2000: loss 2.302581\n",
      "iteration 1000 / 2000: loss 2.302583\n",
      "iteration 1100 / 2000: loss 2.302590\n",
      "iteration 1200 / 2000: loss 2.302589\n",
      "iteration 1300 / 2000: loss 2.302582\n",
      "iteration 1400 / 2000: loss 2.302576\n",
      "iteration 1500 / 2000: loss 2.302588\n",
      "iteration 1600 / 2000: loss 2.302590\n",
      "iteration 1700 / 2000: loss 2.302579\n",
      "iteration 1800 / 2000: loss 2.302593\n",
      "iteration 1900 / 2000: loss 2.302581\n",
      "Hidden Size: 50, Learning Rate: 1e-07, LR Decay: 0.645, Regularization: 0.0, Validation accuracy: 0.1086\n",
      "iteration 0 / 2000: loss 2.302606\n",
      "iteration 100 / 2000: loss 2.302613\n",
      "iteration 200 / 2000: loss 2.302605\n",
      "iteration 300 / 2000: loss 2.302595\n",
      "iteration 400 / 2000: loss 2.302610\n",
      "iteration 500 / 2000: loss 2.302607\n",
      "iteration 600 / 2000: loss 2.302601\n",
      "iteration 700 / 2000: loss 2.302601\n",
      "iteration 800 / 2000: loss 2.302606\n",
      "iteration 900 / 2000: loss 2.302607\n",
      "iteration 1000 / 2000: loss 2.302601\n",
      "iteration 1100 / 2000: loss 2.302608\n",
      "iteration 1200 / 2000: loss 2.302603\n",
      "iteration 1300 / 2000: loss 2.302599\n",
      "iteration 1400 / 2000: loss 2.302605\n",
      "iteration 1500 / 2000: loss 2.302607\n",
      "iteration 1600 / 2000: loss 2.302610\n",
      "iteration 1700 / 2000: loss 2.302607\n",
      "iteration 1800 / 2000: loss 2.302610\n",
      "iteration 1900 / 2000: loss 2.302605\n",
      "Hidden Size: 50, Learning Rate: 1e-07, LR Decay: 0.645, Regularization: 0.1, Validation accuracy: 0.0696\n",
      "iteration 0 / 2000: loss 2.302641\n",
      "iteration 100 / 2000: loss 2.302636\n",
      "iteration 200 / 2000: loss 2.302629\n",
      "iteration 300 / 2000: loss 2.302642\n",
      "iteration 400 / 2000: loss 2.302638\n",
      "iteration 500 / 2000: loss 2.302633\n",
      "iteration 600 / 2000: loss 2.302633\n",
      "iteration 700 / 2000: loss 2.302636\n",
      "iteration 800 / 2000: loss 2.302642\n",
      "iteration 900 / 2000: loss 2.302627\n",
      "iteration 1000 / 2000: loss 2.302643\n",
      "iteration 1100 / 2000: loss 2.302653\n",
      "iteration 1200 / 2000: loss 2.302631\n",
      "iteration 1300 / 2000: loss 2.302641\n",
      "iteration 1400 / 2000: loss 2.302643\n",
      "iteration 1500 / 2000: loss 2.302640\n",
      "iteration 1600 / 2000: loss 2.302652\n",
      "iteration 1700 / 2000: loss 2.302630\n",
      "iteration 1800 / 2000: loss 2.302635\n",
      "iteration 1900 / 2000: loss 2.302637\n",
      "Hidden Size: 50, Learning Rate: 1e-07, LR Decay: 0.645, Regularization: 0.2, Validation accuracy: 0.0798\n",
      "iteration 0 / 2000: loss 2.302657\n",
      "iteration 100 / 2000: loss 2.302665\n",
      "iteration 200 / 2000: loss 2.302657\n",
      "iteration 300 / 2000: loss 2.302655\n",
      "iteration 400 / 2000: loss 2.302666\n",
      "iteration 500 / 2000: loss 2.302650\n",
      "iteration 600 / 2000: loss 2.302666\n",
      "iteration 700 / 2000: loss 2.302667\n",
      "iteration 800 / 2000: loss 2.302651\n",
      "iteration 900 / 2000: loss 2.302672\n",
      "iteration 1000 / 2000: loss 2.302665\n",
      "iteration 1100 / 2000: loss 2.302660\n",
      "iteration 1200 / 2000: loss 2.302653\n",
      "iteration 1300 / 2000: loss 2.302663\n",
      "iteration 1400 / 2000: loss 2.302667\n",
      "iteration 1500 / 2000: loss 2.302660\n",
      "iteration 1600 / 2000: loss 2.302658\n",
      "iteration 1700 / 2000: loss 2.302657\n",
      "iteration 1800 / 2000: loss 2.302652\n",
      "iteration 1900 / 2000: loss 2.302658\n",
      "Hidden Size: 50, Learning Rate: 1e-07, LR Decay: 0.645, Regularization: 0.30000000000000004, Validation accuracy: 0.0884\n",
      "iteration 0 / 2000: loss 2.302672\n",
      "iteration 100 / 2000: loss 2.302666\n",
      "iteration 200 / 2000: loss 2.302668\n",
      "iteration 300 / 2000: loss 2.302662\n",
      "iteration 400 / 2000: loss 2.302674\n",
      "iteration 500 / 2000: loss 2.302670\n",
      "iteration 600 / 2000: loss 2.302678\n",
      "iteration 700 / 2000: loss 2.302676\n",
      "iteration 800 / 2000: loss 2.302670\n",
      "iteration 900 / 2000: loss 2.302665\n",
      "iteration 1000 / 2000: loss 2.302672\n",
      "iteration 1100 / 2000: loss 2.302666\n",
      "iteration 1200 / 2000: loss 2.302674\n",
      "iteration 1300 / 2000: loss 2.302671\n",
      "iteration 1400 / 2000: loss 2.302663\n",
      "iteration 1500 / 2000: loss 2.302680\n",
      "iteration 1600 / 2000: loss 2.302672\n",
      "iteration 1700 / 2000: loss 2.302674\n",
      "iteration 1800 / 2000: loss 2.302678\n",
      "iteration 1900 / 2000: loss 2.302667\n",
      "Hidden Size: 50, Learning Rate: 1e-07, LR Decay: 0.645, Regularization: 0.4, Validation accuracy: 0.1074\n",
      "iteration 0 / 2000: loss 2.302593\n",
      "iteration 100 / 2000: loss 2.302606\n",
      "iteration 200 / 2000: loss 2.302594\n",
      "iteration 300 / 2000: loss 2.302597\n",
      "iteration 400 / 2000: loss 2.302598\n",
      "iteration 500 / 2000: loss 2.302590\n",
      "iteration 600 / 2000: loss 2.302597\n",
      "iteration 700 / 2000: loss 2.302613\n",
      "iteration 800 / 2000: loss 2.302598\n",
      "iteration 900 / 2000: loss 2.302608\n",
      "iteration 1000 / 2000: loss 2.302597\n",
      "iteration 1100 / 2000: loss 2.302599\n",
      "iteration 1200 / 2000: loss 2.302603\n",
      "iteration 1300 / 2000: loss 2.302596\n",
      "iteration 1400 / 2000: loss 2.302595\n",
      "iteration 1500 / 2000: loss 2.302596\n",
      "iteration 1600 / 2000: loss 2.302590\n",
      "iteration 1700 / 2000: loss 2.302593\n",
      "iteration 1800 / 2000: loss 2.302586\n",
      "iteration 1900 / 2000: loss 2.302597\n",
      "Hidden Size: 50, Learning Rate: 1e-07, LR Decay: 0.47250000000000003, Regularization: 0.0, Validation accuracy: 0.0642\n",
      "iteration 0 / 2000: loss 2.302596\n",
      "iteration 100 / 2000: loss 2.302602\n",
      "iteration 200 / 2000: loss 2.302593\n",
      "iteration 300 / 2000: loss 2.302601\n",
      "iteration 400 / 2000: loss 2.302600\n",
      "iteration 500 / 2000: loss 2.302595\n",
      "iteration 600 / 2000: loss 2.302607\n",
      "iteration 700 / 2000: loss 2.302594\n",
      "iteration 800 / 2000: loss 2.302592\n",
      "iteration 900 / 2000: loss 2.302607\n",
      "iteration 1000 / 2000: loss 2.302598\n",
      "iteration 1100 / 2000: loss 2.302600\n",
      "iteration 1200 / 2000: loss 2.302597\n",
      "iteration 1300 / 2000: loss 2.302615\n",
      "iteration 1400 / 2000: loss 2.302596\n",
      "iteration 1500 / 2000: loss 2.302606\n",
      "iteration 1600 / 2000: loss 2.302606\n",
      "iteration 1700 / 2000: loss 2.302605\n",
      "iteration 1800 / 2000: loss 2.302591\n",
      "iteration 1900 / 2000: loss 2.302606\n",
      "Hidden Size: 50, Learning Rate: 1e-07, LR Decay: 0.47250000000000003, Regularization: 0.1, Validation accuracy: 0.098\n",
      "iteration 0 / 2000: loss 2.302638\n",
      "iteration 100 / 2000: loss 2.302620\n",
      "iteration 200 / 2000: loss 2.302629\n",
      "iteration 300 / 2000: loss 2.302625\n",
      "iteration 400 / 2000: loss 2.302627\n",
      "iteration 500 / 2000: loss 2.302646\n",
      "iteration 600 / 2000: loss 2.302642\n",
      "iteration 700 / 2000: loss 2.302630\n",
      "iteration 800 / 2000: loss 2.302642\n",
      "iteration 900 / 2000: loss 2.302628\n",
      "iteration 1000 / 2000: loss 2.302632\n",
      "iteration 1100 / 2000: loss 2.302629\n",
      "iteration 1200 / 2000: loss 2.302629\n",
      "iteration 1300 / 2000: loss 2.302628\n",
      "iteration 1400 / 2000: loss 2.302624\n",
      "iteration 1500 / 2000: loss 2.302629\n",
      "iteration 1600 / 2000: loss 2.302624\n",
      "iteration 1700 / 2000: loss 2.302630\n",
      "iteration 1800 / 2000: loss 2.302626\n",
      "iteration 1900 / 2000: loss 2.302629\n",
      "Hidden Size: 50, Learning Rate: 1e-07, LR Decay: 0.47250000000000003, Regularization: 0.2, Validation accuracy: 0.0898\n",
      "iteration 0 / 2000: loss 2.302643\n",
      "iteration 100 / 2000: loss 2.302647\n",
      "iteration 200 / 2000: loss 2.302661\n",
      "iteration 300 / 2000: loss 2.302648\n",
      "iteration 400 / 2000: loss 2.302640\n",
      "iteration 500 / 2000: loss 2.302651\n",
      "iteration 600 / 2000: loss 2.302644\n",
      "iteration 700 / 2000: loss 2.302648\n",
      "iteration 800 / 2000: loss 2.302647\n",
      "iteration 900 / 2000: loss 2.302657\n",
      "iteration 1000 / 2000: loss 2.302643\n",
      "iteration 1100 / 2000: loss 2.302646\n",
      "iteration 1200 / 2000: loss 2.302643\n",
      "iteration 1300 / 2000: loss 2.302640\n",
      "iteration 1400 / 2000: loss 2.302656\n",
      "iteration 1500 / 2000: loss 2.302647\n",
      "iteration 1600 / 2000: loss 2.302650\n",
      "iteration 1700 / 2000: loss 2.302639\n",
      "iteration 1800 / 2000: loss 2.302646\n",
      "iteration 1900 / 2000: loss 2.302643\n",
      "Hidden Size: 50, Learning Rate: 1e-07, LR Decay: 0.47250000000000003, Regularization: 0.30000000000000004, Validation accuracy: 0.1312\n",
      "iteration 0 / 2000: loss 2.302684\n",
      "iteration 100 / 2000: loss 2.302683\n",
      "iteration 200 / 2000: loss 2.302687\n",
      "iteration 300 / 2000: loss 2.302676\n",
      "iteration 400 / 2000: loss 2.302684\n",
      "iteration 500 / 2000: loss 2.302676\n",
      "iteration 600 / 2000: loss 2.302683\n",
      "iteration 700 / 2000: loss 2.302685\n",
      "iteration 800 / 2000: loss 2.302686\n",
      "iteration 900 / 2000: loss 2.302687\n",
      "iteration 1000 / 2000: loss 2.302684\n",
      "iteration 1100 / 2000: loss 2.302686\n",
      "iteration 1200 / 2000: loss 2.302683\n",
      "iteration 1300 / 2000: loss 2.302675\n",
      "iteration 1400 / 2000: loss 2.302683\n",
      "iteration 1500 / 2000: loss 2.302677\n",
      "iteration 1600 / 2000: loss 2.302677\n",
      "iteration 1700 / 2000: loss 2.302681\n",
      "iteration 1800 / 2000: loss 2.302683\n",
      "iteration 1900 / 2000: loss 2.302675\n",
      "Hidden Size: 50, Learning Rate: 1e-07, LR Decay: 0.47250000000000003, Regularization: 0.4, Validation accuracy: 0.0598\n",
      "iteration 0 / 2000: loss 2.302593\n",
      "iteration 100 / 2000: loss 2.302586\n",
      "iteration 200 / 2000: loss 2.302585\n",
      "iteration 300 / 2000: loss 2.302589\n",
      "iteration 400 / 2000: loss 2.302594\n",
      "iteration 500 / 2000: loss 2.302586\n",
      "iteration 600 / 2000: loss 2.302581\n",
      "iteration 700 / 2000: loss 2.302583\n",
      "iteration 800 / 2000: loss 2.302596\n",
      "iteration 900 / 2000: loss 2.302588\n",
      "iteration 1000 / 2000: loss 2.302594\n",
      "iteration 1100 / 2000: loss 2.302598\n",
      "iteration 1200 / 2000: loss 2.302584\n",
      "iteration 1300 / 2000: loss 2.302597\n",
      "iteration 1400 / 2000: loss 2.302594\n",
      "iteration 1500 / 2000: loss 2.302594\n",
      "iteration 1600 / 2000: loss 2.302595\n",
      "iteration 1700 / 2000: loss 2.302574\n",
      "iteration 1800 / 2000: loss 2.302581\n",
      "iteration 1900 / 2000: loss 2.302579\n",
      "Hidden Size: 50, Learning Rate: 1e-07, LR Decay: 0.3, Regularization: 0.0, Validation accuracy: 0.1396\n",
      "iteration 0 / 2000: loss 2.302623\n",
      "iteration 100 / 2000: loss 2.302608\n",
      "iteration 200 / 2000: loss 2.302628\n",
      "iteration 300 / 2000: loss 2.302616\n",
      "iteration 400 / 2000: loss 2.302615\n",
      "iteration 500 / 2000: loss 2.302612\n",
      "iteration 600 / 2000: loss 2.302619\n",
      "iteration 700 / 2000: loss 2.302607\n",
      "iteration 800 / 2000: loss 2.302621\n",
      "iteration 900 / 2000: loss 2.302622\n",
      "iteration 1000 / 2000: loss 2.302622\n",
      "iteration 1100 / 2000: loss 2.302614\n",
      "iteration 1200 / 2000: loss 2.302612\n",
      "iteration 1300 / 2000: loss 2.302622\n",
      "iteration 1400 / 2000: loss 2.302626\n",
      "iteration 1500 / 2000: loss 2.302615\n",
      "iteration 1600 / 2000: loss 2.302621\n",
      "iteration 1700 / 2000: loss 2.302618\n",
      "iteration 1800 / 2000: loss 2.302601\n",
      "iteration 1900 / 2000: loss 2.302614\n",
      "Hidden Size: 50, Learning Rate: 1e-07, LR Decay: 0.3, Regularization: 0.1, Validation accuracy: 0.0936\n",
      "iteration 0 / 2000: loss 2.302632\n",
      "iteration 100 / 2000: loss 2.302629\n",
      "iteration 200 / 2000: loss 2.302636\n",
      "iteration 300 / 2000: loss 2.302637\n",
      "iteration 400 / 2000: loss 2.302630\n",
      "iteration 500 / 2000: loss 2.302637\n",
      "iteration 600 / 2000: loss 2.302634\n",
      "iteration 700 / 2000: loss 2.302632\n",
      "iteration 800 / 2000: loss 2.302636\n",
      "iteration 900 / 2000: loss 2.302638\n",
      "iteration 1000 / 2000: loss 2.302632\n",
      "iteration 1100 / 2000: loss 2.302632\n",
      "iteration 1200 / 2000: loss 2.302625\n",
      "iteration 1300 / 2000: loss 2.302640\n",
      "iteration 1400 / 2000: loss 2.302618\n",
      "iteration 1500 / 2000: loss 2.302642\n",
      "iteration 1600 / 2000: loss 2.302635\n",
      "iteration 1700 / 2000: loss 2.302633\n",
      "iteration 1800 / 2000: loss 2.302640\n",
      "iteration 1900 / 2000: loss 2.302633\n",
      "Hidden Size: 50, Learning Rate: 1e-07, LR Decay: 0.3, Regularization: 0.2, Validation accuracy: 0.0728\n",
      "iteration 0 / 2000: loss 2.302643\n",
      "iteration 100 / 2000: loss 2.302652\n",
      "iteration 200 / 2000: loss 2.302646\n",
      "iteration 300 / 2000: loss 2.302651\n",
      "iteration 400 / 2000: loss 2.302647\n",
      "iteration 500 / 2000: loss 2.302644\n",
      "iteration 600 / 2000: loss 2.302641\n",
      "iteration 700 / 2000: loss 2.302648\n",
      "iteration 800 / 2000: loss 2.302637\n",
      "iteration 900 / 2000: loss 2.302645\n",
      "iteration 1000 / 2000: loss 2.302654\n",
      "iteration 1100 / 2000: loss 2.302647\n",
      "iteration 1200 / 2000: loss 2.302639\n",
      "iteration 1300 / 2000: loss 2.302654\n",
      "iteration 1400 / 2000: loss 2.302640\n",
      "iteration 1500 / 2000: loss 2.302658\n",
      "iteration 1600 / 2000: loss 2.302645\n",
      "iteration 1700 / 2000: loss 2.302635\n",
      "iteration 1800 / 2000: loss 2.302649\n",
      "iteration 1900 / 2000: loss 2.302636\n",
      "Hidden Size: 50, Learning Rate: 1e-07, LR Decay: 0.3, Regularization: 0.30000000000000004, Validation accuracy: 0.081\n",
      "iteration 0 / 2000: loss 2.302663\n",
      "iteration 100 / 2000: loss 2.302676\n",
      "iteration 200 / 2000: loss 2.302669\n",
      "iteration 300 / 2000: loss 2.302666\n",
      "iteration 400 / 2000: loss 2.302668\n",
      "iteration 500 / 2000: loss 2.302656\n",
      "iteration 600 / 2000: loss 2.302679\n",
      "iteration 700 / 2000: loss 2.302661\n",
      "iteration 800 / 2000: loss 2.302670\n",
      "iteration 900 / 2000: loss 2.302676\n",
      "iteration 1000 / 2000: loss 2.302673\n",
      "iteration 1100 / 2000: loss 2.302653\n",
      "iteration 1200 / 2000: loss 2.302664\n",
      "iteration 1300 / 2000: loss 2.302658\n",
      "iteration 1400 / 2000: loss 2.302658\n",
      "iteration 1500 / 2000: loss 2.302671\n",
      "iteration 1600 / 2000: loss 2.302674\n",
      "iteration 1700 / 2000: loss 2.302680\n",
      "iteration 1800 / 2000: loss 2.302666\n",
      "iteration 1900 / 2000: loss 2.302674\n",
      "Hidden Size: 50, Learning Rate: 1e-07, LR Decay: 0.3, Regularization: 0.4, Validation accuracy: 0.0986\n",
      "iteration 0 / 2000: loss 2.302584\n",
      "iteration 100 / 2000: loss 0.164019\n",
      "iteration 200 / 2000: loss 0.113048\n",
      "iteration 300 / 2000: loss 0.132625\n",
      "iteration 400 / 2000: loss 0.131315\n",
      "iteration 500 / 2000: loss 0.142272\n",
      "iteration 600 / 2000: loss 0.148073\n",
      "iteration 700 / 2000: loss 0.141560\n",
      "iteration 800 / 2000: loss 0.064455\n",
      "iteration 900 / 2000: loss 0.116577\n",
      "iteration 1000 / 2000: loss 0.044577\n",
      "iteration 1100 / 2000: loss 0.038568\n",
      "iteration 1200 / 2000: loss 0.043076\n",
      "iteration 1300 / 2000: loss 0.020150\n",
      "iteration 1400 / 2000: loss 0.035201\n",
      "iteration 1500 / 2000: loss 0.091926\n",
      "iteration 1600 / 2000: loss 0.084085\n",
      "iteration 1700 / 2000: loss 0.095178\n",
      "iteration 1800 / 2000: loss 0.032124\n",
      "iteration 1900 / 2000: loss 0.064876\n",
      "Hidden Size: 60, Learning Rate: 0.01, LR Decay: 0.99, Regularization: 0.0, Validation accuracy: 0.9744\n",
      "iteration 0 / 2000: loss 2.302602\n",
      "iteration 100 / 2000: loss 0.266782\n",
      "iteration 200 / 2000: loss 0.158604\n",
      "iteration 300 / 2000: loss 0.173245\n",
      "iteration 400 / 2000: loss 0.128066\n",
      "iteration 500 / 2000: loss 0.265012\n",
      "iteration 600 / 2000: loss 0.117053\n",
      "iteration 700 / 2000: loss 0.109644\n",
      "iteration 800 / 2000: loss 0.161442\n",
      "iteration 900 / 2000: loss 0.153899\n",
      "iteration 1000 / 2000: loss 0.146675\n",
      "iteration 1100 / 2000: loss 0.118791\n",
      "iteration 1200 / 2000: loss 0.121086\n",
      "iteration 1300 / 2000: loss 0.231434\n",
      "iteration 1400 / 2000: loss 0.126799\n",
      "iteration 1500 / 2000: loss 0.110340\n",
      "iteration 1600 / 2000: loss 0.142006\n",
      "iteration 1700 / 2000: loss 0.120445\n",
      "iteration 1800 / 2000: loss 0.107486\n",
      "iteration 1900 / 2000: loss 0.097428\n",
      "Hidden Size: 60, Learning Rate: 0.01, LR Decay: 0.99, Regularization: 0.1, Validation accuracy: 0.9758\n",
      "iteration 0 / 2000: loss 2.302631\n",
      "iteration 100 / 2000: loss 0.361323\n",
      "iteration 200 / 2000: loss 0.170774\n",
      "iteration 300 / 2000: loss 0.177242\n",
      "iteration 400 / 2000: loss 0.175964\n",
      "iteration 500 / 2000: loss 0.219700\n",
      "iteration 600 / 2000: loss 0.267195\n",
      "iteration 700 / 2000: loss 0.225072\n",
      "iteration 800 / 2000: loss 0.244277\n",
      "iteration 900 / 2000: loss 0.206076\n",
      "iteration 1000 / 2000: loss 0.217645\n",
      "iteration 1100 / 2000: loss 0.189339\n",
      "iteration 1200 / 2000: loss 0.213650\n",
      "iteration 1300 / 2000: loss 0.143430\n",
      "iteration 1400 / 2000: loss 0.156675\n",
      "iteration 1500 / 2000: loss 0.161854\n",
      "iteration 1600 / 2000: loss 0.182473\n",
      "iteration 1700 / 2000: loss 0.222548\n",
      "iteration 1800 / 2000: loss 0.137155\n",
      "iteration 1900 / 2000: loss 0.189229\n",
      "Hidden Size: 60, Learning Rate: 0.01, LR Decay: 0.99, Regularization: 0.2, Validation accuracy: 0.9668\n",
      "iteration 0 / 2000: loss 2.302639\n",
      "iteration 100 / 2000: loss 0.280887\n",
      "iteration 200 / 2000: loss 0.308040\n",
      "iteration 300 / 2000: loss 0.312654\n",
      "iteration 400 / 2000: loss 0.212584\n",
      "iteration 500 / 2000: loss 0.194953\n",
      "iteration 600 / 2000: loss 0.232821\n",
      "iteration 700 / 2000: loss 0.226039\n",
      "iteration 800 / 2000: loss 0.196254\n",
      "iteration 900 / 2000: loss 0.171232\n",
      "iteration 1000 / 2000: loss 0.166791\n",
      "iteration 1100 / 2000: loss 0.210733\n",
      "iteration 1200 / 2000: loss 0.193776\n",
      "iteration 1300 / 2000: loss 0.174879\n",
      "iteration 1400 / 2000: loss 0.199444\n",
      "iteration 1500 / 2000: loss 0.204250\n",
      "iteration 1600 / 2000: loss 0.214173\n",
      "iteration 1700 / 2000: loss 0.176578\n",
      "iteration 1800 / 2000: loss 0.248906\n",
      "iteration 1900 / 2000: loss 0.247091\n",
      "Hidden Size: 60, Learning Rate: 0.01, LR Decay: 0.99, Regularization: 0.30000000000000004, Validation accuracy: 0.9676\n",
      "iteration 0 / 2000: loss 2.302661\n",
      "iteration 100 / 2000: loss 0.413730\n",
      "iteration 200 / 2000: loss 0.251047\n",
      "iteration 300 / 2000: loss 0.301924\n",
      "iteration 400 / 2000: loss 0.190239\n",
      "iteration 500 / 2000: loss 0.300651\n",
      "iteration 600 / 2000: loss 0.382573\n",
      "iteration 700 / 2000: loss 0.220018\n",
      "iteration 800 / 2000: loss 0.323708\n",
      "iteration 900 / 2000: loss 0.298201\n",
      "iteration 1000 / 2000: loss 0.236572\n",
      "iteration 1100 / 2000: loss 0.247464\n",
      "iteration 1200 / 2000: loss 0.241901\n",
      "iteration 1300 / 2000: loss 0.258017\n",
      "iteration 1400 / 2000: loss 0.240065\n",
      "iteration 1500 / 2000: loss 0.266172\n",
      "iteration 1600 / 2000: loss 0.224571\n",
      "iteration 1700 / 2000: loss 0.235973\n",
      "iteration 1800 / 2000: loss 0.245789\n",
      "iteration 1900 / 2000: loss 0.271068\n",
      "Hidden Size: 60, Learning Rate: 0.01, LR Decay: 0.99, Regularization: 0.4, Validation accuracy: 0.9692\n",
      "iteration 0 / 2000: loss 2.302589\n",
      "iteration 100 / 2000: loss 0.258159\n",
      "iteration 200 / 2000: loss 0.226049\n",
      "iteration 300 / 2000: loss 0.068950\n",
      "iteration 400 / 2000: loss 0.140616\n",
      "iteration 500 / 2000: loss 0.120673\n",
      "iteration 600 / 2000: loss 0.151808\n",
      "iteration 700 / 2000: loss 0.081385\n",
      "iteration 800 / 2000: loss 0.091464\n",
      "iteration 900 / 2000: loss 0.100946\n",
      "iteration 1000 / 2000: loss 0.080644\n",
      "iteration 1100 / 2000: loss 0.037452\n",
      "iteration 1200 / 2000: loss 0.056208\n",
      "iteration 1300 / 2000: loss 0.042653\n",
      "iteration 1400 / 2000: loss 0.022315\n",
      "iteration 1500 / 2000: loss 0.024802\n",
      "iteration 1600 / 2000: loss 0.009901\n",
      "iteration 1700 / 2000: loss 0.033593\n",
      "iteration 1800 / 2000: loss 0.015234\n",
      "iteration 1900 / 2000: loss 0.024280\n",
      "Hidden Size: 60, Learning Rate: 0.01, LR Decay: 0.8175, Regularization: 0.0, Validation accuracy: 0.9752\n",
      "iteration 0 / 2000: loss 2.302618\n",
      "iteration 100 / 2000: loss 0.392263\n",
      "iteration 200 / 2000: loss 0.214022\n",
      "iteration 300 / 2000: loss 0.157068\n",
      "iteration 400 / 2000: loss 0.146806\n",
      "iteration 500 / 2000: loss 0.123859\n",
      "iteration 600 / 2000: loss 0.134568\n",
      "iteration 700 / 2000: loss 0.155089\n",
      "iteration 800 / 2000: loss 0.139523\n",
      "iteration 900 / 2000: loss 0.082641\n",
      "iteration 1000 / 2000: loss 0.097051\n",
      "iteration 1100 / 2000: loss 0.084319\n",
      "iteration 1200 / 2000: loss 0.126579\n",
      "iteration 1300 / 2000: loss 0.075384\n",
      "iteration 1400 / 2000: loss 0.085264\n",
      "iteration 1500 / 2000: loss 0.076051\n",
      "iteration 1600 / 2000: loss 0.085445\n",
      "iteration 1700 / 2000: loss 0.161132\n",
      "iteration 1800 / 2000: loss 0.138711\n",
      "iteration 1900 / 2000: loss 0.090741\n",
      "Hidden Size: 60, Learning Rate: 0.01, LR Decay: 0.8175, Regularization: 0.1, Validation accuracy: 0.9764\n",
      "iteration 0 / 2000: loss 2.302629\n",
      "iteration 100 / 2000: loss 0.375181\n",
      "iteration 200 / 2000: loss 0.242581\n",
      "iteration 300 / 2000: loss 0.166974\n",
      "iteration 400 / 2000: loss 0.139970\n",
      "iteration 500 / 2000: loss 0.136960\n",
      "iteration 600 / 2000: loss 0.215418\n",
      "iteration 700 / 2000: loss 0.150013\n",
      "iteration 800 / 2000: loss 0.170988\n",
      "iteration 900 / 2000: loss 0.197758\n",
      "iteration 1000 / 2000: loss 0.153394\n",
      "iteration 1100 / 2000: loss 0.138932\n",
      "iteration 1200 / 2000: loss 0.156776\n",
      "iteration 1300 / 2000: loss 0.150937\n",
      "iteration 1400 / 2000: loss 0.147654\n",
      "iteration 1500 / 2000: loss 0.110841\n",
      "iteration 1600 / 2000: loss 0.168693\n",
      "iteration 1700 / 2000: loss 0.115855\n",
      "iteration 1800 / 2000: loss 0.126032\n",
      "iteration 1900 / 2000: loss 0.177516\n",
      "Hidden Size: 60, Learning Rate: 0.01, LR Decay: 0.8175, Regularization: 0.2, Validation accuracy: 0.978\n",
      "iteration 0 / 2000: loss 2.302670\n",
      "iteration 100 / 2000: loss 0.396665\n",
      "iteration 200 / 2000: loss 0.262135\n",
      "iteration 300 / 2000: loss 0.205639\n",
      "iteration 400 / 2000: loss 0.162244\n",
      "iteration 500 / 2000: loss 0.229922\n",
      "iteration 600 / 2000: loss 0.250541\n",
      "iteration 700 / 2000: loss 0.235894\n",
      "iteration 800 / 2000: loss 0.201487\n",
      "iteration 900 / 2000: loss 0.199095\n",
      "iteration 1000 / 2000: loss 0.153718\n",
      "iteration 1100 / 2000: loss 0.212585\n",
      "iteration 1200 / 2000: loss 0.225107\n",
      "iteration 1300 / 2000: loss 0.197874\n",
      "iteration 1400 / 2000: loss 0.164665\n",
      "iteration 1500 / 2000: loss 0.149239\n",
      "iteration 1600 / 2000: loss 0.157850\n",
      "iteration 1700 / 2000: loss 0.144998\n",
      "iteration 1800 / 2000: loss 0.197364\n",
      "iteration 1900 / 2000: loss 0.157415\n",
      "Hidden Size: 60, Learning Rate: 0.01, LR Decay: 0.8175, Regularization: 0.30000000000000004, Validation accuracy: 0.9746\n",
      "iteration 0 / 2000: loss 2.302673\n",
      "iteration 100 / 2000: loss 0.280432\n",
      "iteration 200 / 2000: loss 0.268965\n",
      "iteration 300 / 2000: loss 0.294862\n",
      "iteration 400 / 2000: loss 0.222453\n",
      "iteration 500 / 2000: loss 0.286173\n",
      "iteration 600 / 2000: loss 0.210191\n",
      "iteration 700 / 2000: loss 0.225249\n",
      "iteration 800 / 2000: loss 0.214861\n",
      "iteration 900 / 2000: loss 0.221124\n",
      "iteration 1000 / 2000: loss 0.197866\n",
      "iteration 1100 / 2000: loss 0.295928\n",
      "iteration 1200 / 2000: loss 0.173670\n",
      "iteration 1300 / 2000: loss 0.243237\n",
      "iteration 1400 / 2000: loss 0.214760\n",
      "iteration 1500 / 2000: loss 0.208965\n",
      "iteration 1600 / 2000: loss 0.188326\n",
      "iteration 1700 / 2000: loss 0.186391\n",
      "iteration 1800 / 2000: loss 0.171352\n",
      "iteration 1900 / 2000: loss 0.185011\n",
      "Hidden Size: 60, Learning Rate: 0.01, LR Decay: 0.8175, Regularization: 0.4, Validation accuracy: 0.973\n",
      "iteration 0 / 2000: loss 2.302573\n",
      "iteration 100 / 2000: loss 0.290431\n",
      "iteration 200 / 2000: loss 0.196132\n",
      "iteration 300 / 2000: loss 0.186992\n",
      "iteration 400 / 2000: loss 0.079249\n",
      "iteration 500 / 2000: loss 0.037833\n",
      "iteration 600 / 2000: loss 0.083600\n",
      "iteration 700 / 2000: loss 0.117539\n",
      "iteration 800 / 2000: loss 0.050263\n",
      "iteration 900 / 2000: loss 0.053302\n",
      "iteration 1000 / 2000: loss 0.055214\n",
      "iteration 1100 / 2000: loss 0.059908\n",
      "iteration 1200 / 2000: loss 0.042194\n",
      "iteration 1300 / 2000: loss 0.070215\n",
      "iteration 1400 / 2000: loss 0.024677\n",
      "iteration 1500 / 2000: loss 0.038273\n",
      "iteration 1600 / 2000: loss 0.091168\n",
      "iteration 1700 / 2000: loss 0.028345\n",
      "iteration 1800 / 2000: loss 0.083276\n",
      "iteration 1900 / 2000: loss 0.069921\n",
      "Hidden Size: 60, Learning Rate: 0.01, LR Decay: 0.645, Regularization: 0.0, Validation accuracy: 0.9742\n",
      "iteration 0 / 2000: loss 2.302599\n",
      "iteration 100 / 2000: loss 0.218625\n",
      "iteration 200 / 2000: loss 0.249111\n",
      "iteration 300 / 2000: loss 0.208765\n",
      "iteration 400 / 2000: loss 0.202401\n",
      "iteration 500 / 2000: loss 0.123957\n",
      "iteration 600 / 2000: loss 0.161945\n",
      "iteration 700 / 2000: loss 0.116686\n",
      "iteration 800 / 2000: loss 0.154180\n",
      "iteration 900 / 2000: loss 0.142963\n",
      "iteration 1000 / 2000: loss 0.149541\n",
      "iteration 1100 / 2000: loss 0.100556\n",
      "iteration 1200 / 2000: loss 0.090690\n",
      "iteration 1300 / 2000: loss 0.142812\n",
      "iteration 1400 / 2000: loss 0.091933\n",
      "iteration 1500 / 2000: loss 0.096970\n",
      "iteration 1600 / 2000: loss 0.071627\n",
      "iteration 1700 / 2000: loss 0.085290\n",
      "iteration 1800 / 2000: loss 0.124640\n",
      "iteration 1900 / 2000: loss 0.101307\n",
      "Hidden Size: 60, Learning Rate: 0.01, LR Decay: 0.645, Regularization: 0.1, Validation accuracy: 0.9734\n",
      "iteration 0 / 2000: loss 2.302633\n",
      "iteration 100 / 2000: loss 0.274326\n",
      "iteration 200 / 2000: loss 0.248873\n",
      "iteration 300 / 2000: loss 0.253496\n",
      "iteration 400 / 2000: loss 0.118412\n",
      "iteration 500 / 2000: loss 0.182860\n",
      "iteration 600 / 2000: loss 0.125375\n",
      "iteration 700 / 2000: loss 0.120510\n",
      "iteration 800 / 2000: loss 0.135066\n",
      "iteration 900 / 2000: loss 0.162156\n",
      "iteration 1000 / 2000: loss 0.136500\n",
      "iteration 1100 / 2000: loss 0.176560\n",
      "iteration 1200 / 2000: loss 0.152073\n",
      "iteration 1300 / 2000: loss 0.135195\n",
      "iteration 1400 / 2000: loss 0.111750\n",
      "iteration 1500 / 2000: loss 0.140840\n",
      "iteration 1600 / 2000: loss 0.136932\n",
      "iteration 1700 / 2000: loss 0.135583\n",
      "iteration 1800 / 2000: loss 0.189888\n",
      "iteration 1900 / 2000: loss 0.147455\n",
      "Hidden Size: 60, Learning Rate: 0.01, LR Decay: 0.645, Regularization: 0.2, Validation accuracy: 0.9728\n",
      "iteration 0 / 2000: loss 2.302673\n",
      "iteration 100 / 2000: loss 0.321072\n",
      "iteration 200 / 2000: loss 0.226880\n",
      "iteration 300 / 2000: loss 0.316149\n",
      "iteration 400 / 2000: loss 0.202181\n",
      "iteration 500 / 2000: loss 0.224135\n",
      "iteration 600 / 2000: loss 0.280750\n",
      "iteration 700 / 2000: loss 0.146919\n",
      "iteration 800 / 2000: loss 0.156068\n",
      "iteration 900 / 2000: loss 0.205767\n",
      "iteration 1000 / 2000: loss 0.207636\n",
      "iteration 1100 / 2000: loss 0.182736\n",
      "iteration 1200 / 2000: loss 0.174840\n",
      "iteration 1300 / 2000: loss 0.212938\n",
      "iteration 1400 / 2000: loss 0.185025\n",
      "iteration 1500 / 2000: loss 0.154085\n",
      "iteration 1600 / 2000: loss 0.177734\n",
      "iteration 1700 / 2000: loss 0.157905\n",
      "iteration 1800 / 2000: loss 0.147781\n",
      "iteration 1900 / 2000: loss 0.179975\n",
      "Hidden Size: 60, Learning Rate: 0.01, LR Decay: 0.645, Regularization: 0.30000000000000004, Validation accuracy: 0.9708\n",
      "iteration 0 / 2000: loss 2.302682\n",
      "iteration 100 / 2000: loss 0.431367\n",
      "iteration 200 / 2000: loss 0.319246\n",
      "iteration 300 / 2000: loss 0.257526\n",
      "iteration 400 / 2000: loss 0.204094\n",
      "iteration 500 / 2000: loss 0.224430\n",
      "iteration 600 / 2000: loss 0.234332\n",
      "iteration 700 / 2000: loss 0.211439\n",
      "iteration 800 / 2000: loss 0.199982\n",
      "iteration 900 / 2000: loss 0.235993\n",
      "iteration 1000 / 2000: loss 0.164004\n",
      "iteration 1100 / 2000: loss 0.182501\n",
      "iteration 1200 / 2000: loss 0.192782\n",
      "iteration 1300 / 2000: loss 0.237778\n",
      "iteration 1400 / 2000: loss 0.192036\n",
      "iteration 1500 / 2000: loss 0.232839\n",
      "iteration 1600 / 2000: loss 0.200980\n",
      "iteration 1700 / 2000: loss 0.244280\n",
      "iteration 1800 / 2000: loss 0.229737\n",
      "iteration 1900 / 2000: loss 0.241409\n",
      "Hidden Size: 60, Learning Rate: 0.01, LR Decay: 0.645, Regularization: 0.4, Validation accuracy: 0.969\n",
      "iteration 0 / 2000: loss 2.302594\n",
      "iteration 100 / 2000: loss 0.207059\n",
      "iteration 200 / 2000: loss 0.227419\n",
      "iteration 300 / 2000: loss 0.228321\n",
      "iteration 400 / 2000: loss 0.124945\n",
      "iteration 500 / 2000: loss 0.084251\n",
      "iteration 600 / 2000: loss 0.113331\n",
      "iteration 700 / 2000: loss 0.113115\n",
      "iteration 800 / 2000: loss 0.089814\n",
      "iteration 900 / 2000: loss 0.127913\n",
      "iteration 1000 / 2000: loss 0.103863\n",
      "iteration 1100 / 2000: loss 0.143787\n",
      "iteration 1200 / 2000: loss 0.080687\n",
      "iteration 1300 / 2000: loss 0.086358\n",
      "iteration 1400 / 2000: loss 0.065947\n",
      "iteration 1500 / 2000: loss 0.096885\n",
      "iteration 1600 / 2000: loss 0.086233\n",
      "iteration 1700 / 2000: loss 0.127887\n",
      "iteration 1800 / 2000: loss 0.065481\n",
      "iteration 1900 / 2000: loss 0.129925\n",
      "Hidden Size: 60, Learning Rate: 0.01, LR Decay: 0.47250000000000003, Regularization: 0.0, Validation accuracy: 0.9674\n",
      "iteration 0 / 2000: loss 2.302632\n",
      "iteration 100 / 2000: loss 0.356178\n",
      "iteration 200 / 2000: loss 0.220098\n",
      "iteration 300 / 2000: loss 0.127588\n",
      "iteration 400 / 2000: loss 0.194121\n",
      "iteration 500 / 2000: loss 0.198656\n",
      "iteration 600 / 2000: loss 0.163115\n",
      "iteration 700 / 2000: loss 0.131587\n",
      "iteration 800 / 2000: loss 0.184608\n",
      "iteration 900 / 2000: loss 0.256939\n",
      "iteration 1000 / 2000: loss 0.113007\n",
      "iteration 1100 / 2000: loss 0.106976\n",
      "iteration 1200 / 2000: loss 0.152268\n",
      "iteration 1300 / 2000: loss 0.127137\n",
      "iteration 1400 / 2000: loss 0.115435\n",
      "iteration 1500 / 2000: loss 0.121640\n",
      "iteration 1600 / 2000: loss 0.162408\n",
      "iteration 1700 / 2000: loss 0.103272\n",
      "iteration 1800 / 2000: loss 0.218941\n",
      "iteration 1900 / 2000: loss 0.131005\n",
      "Hidden Size: 60, Learning Rate: 0.01, LR Decay: 0.47250000000000003, Regularization: 0.1, Validation accuracy: 0.967\n",
      "iteration 0 / 2000: loss 2.302623\n",
      "iteration 100 / 2000: loss 0.229728\n",
      "iteration 200 / 2000: loss 0.273280\n",
      "iteration 300 / 2000: loss 0.297239\n",
      "iteration 400 / 2000: loss 0.164994\n",
      "iteration 500 / 2000: loss 0.211791\n",
      "iteration 600 / 2000: loss 0.294965\n",
      "iteration 700 / 2000: loss 0.159820\n",
      "iteration 800 / 2000: loss 0.161195\n",
      "iteration 900 / 2000: loss 0.203453\n",
      "iteration 1000 / 2000: loss 0.123529\n",
      "iteration 1100 / 2000: loss 0.169112\n",
      "iteration 1200 / 2000: loss 0.221783\n",
      "iteration 1300 / 2000: loss 0.184320\n",
      "iteration 1400 / 2000: loss 0.131768\n",
      "iteration 1500 / 2000: loss 0.170828\n",
      "iteration 1600 / 2000: loss 0.191698\n",
      "iteration 1700 / 2000: loss 0.186063\n",
      "iteration 1800 / 2000: loss 0.179318\n",
      "iteration 1900 / 2000: loss 0.161268\n",
      "Hidden Size: 60, Learning Rate: 0.01, LR Decay: 0.47250000000000003, Regularization: 0.2, Validation accuracy: 0.9644\n",
      "iteration 0 / 2000: loss 2.302650\n",
      "iteration 100 / 2000: loss 0.351714\n",
      "iteration 200 / 2000: loss 0.197685\n",
      "iteration 300 / 2000: loss 0.169059\n",
      "iteration 400 / 2000: loss 0.196415\n",
      "iteration 500 / 2000: loss 0.170320\n",
      "iteration 600 / 2000: loss 0.175642\n",
      "iteration 700 / 2000: loss 0.229173\n",
      "iteration 800 / 2000: loss 0.182760\n",
      "iteration 900 / 2000: loss 0.188893\n",
      "iteration 1000 / 2000: loss 0.230510\n",
      "iteration 1100 / 2000: loss 0.294216\n",
      "iteration 1200 / 2000: loss 0.242861\n",
      "iteration 1300 / 2000: loss 0.238730\n",
      "iteration 1400 / 2000: loss 0.180122\n",
      "iteration 1500 / 2000: loss 0.146798\n",
      "iteration 1600 / 2000: loss 0.189327\n",
      "iteration 1700 / 2000: loss 0.182763\n",
      "iteration 1800 / 2000: loss 0.193137\n",
      "iteration 1900 / 2000: loss 0.184561\n",
      "Hidden Size: 60, Learning Rate: 0.01, LR Decay: 0.47250000000000003, Regularization: 0.30000000000000004, Validation accuracy: 0.9662\n",
      "iteration 0 / 2000: loss 2.302672\n",
      "iteration 100 / 2000: loss 0.387641\n",
      "iteration 200 / 2000: loss 0.270760\n",
      "iteration 300 / 2000: loss 0.318411\n",
      "iteration 400 / 2000: loss 0.209459\n",
      "iteration 500 / 2000: loss 0.261444\n",
      "iteration 600 / 2000: loss 0.177099\n",
      "iteration 700 / 2000: loss 0.215784\n",
      "iteration 800 / 2000: loss 0.251481\n",
      "iteration 900 / 2000: loss 0.214603\n",
      "iteration 1000 / 2000: loss 0.175962\n",
      "iteration 1100 / 2000: loss 0.265532\n",
      "iteration 1200 / 2000: loss 0.227083\n",
      "iteration 1300 / 2000: loss 0.260535\n",
      "iteration 1400 / 2000: loss 0.214891\n",
      "iteration 1500 / 2000: loss 0.247205\n",
      "iteration 1600 / 2000: loss 0.178242\n",
      "iteration 1700 / 2000: loss 0.213326\n",
      "iteration 1800 / 2000: loss 0.195913\n",
      "iteration 1900 / 2000: loss 0.285501\n",
      "Hidden Size: 60, Learning Rate: 0.01, LR Decay: 0.47250000000000003, Regularization: 0.4, Validation accuracy: 0.965\n",
      "iteration 0 / 2000: loss 2.302570\n",
      "iteration 100 / 2000: loss 0.388840\n",
      "iteration 200 / 2000: loss 0.237332\n",
      "iteration 300 / 2000: loss 0.162371\n",
      "iteration 400 / 2000: loss 0.200450\n",
      "iteration 500 / 2000: loss 0.204824\n",
      "iteration 600 / 2000: loss 0.195049\n",
      "iteration 700 / 2000: loss 0.133370\n",
      "iteration 800 / 2000: loss 0.189055\n",
      "iteration 900 / 2000: loss 0.132745\n",
      "iteration 1000 / 2000: loss 0.164091\n",
      "iteration 1100 / 2000: loss 0.180452\n",
      "iteration 1200 / 2000: loss 0.141011\n",
      "iteration 1300 / 2000: loss 0.099279\n",
      "iteration 1400 / 2000: loss 0.261250\n",
      "iteration 1500 / 2000: loss 0.225187\n",
      "iteration 1600 / 2000: loss 0.191902\n",
      "iteration 1700 / 2000: loss 0.122375\n",
      "iteration 1800 / 2000: loss 0.167332\n",
      "iteration 1900 / 2000: loss 0.093980\n",
      "Hidden Size: 60, Learning Rate: 0.01, LR Decay: 0.3, Regularization: 0.0, Validation accuracy: 0.954\n",
      "iteration 0 / 2000: loss 2.302627\n",
      "iteration 100 / 2000: loss 0.299840\n",
      "iteration 200 / 2000: loss 0.265550\n",
      "iteration 300 / 2000: loss 0.192084\n",
      "iteration 400 / 2000: loss 0.224939\n",
      "iteration 500 / 2000: loss 0.163967\n",
      "iteration 600 / 2000: loss 0.219187\n",
      "iteration 700 / 2000: loss 0.185578\n",
      "iteration 800 / 2000: loss 0.171587\n",
      "iteration 900 / 2000: loss 0.210058\n",
      "iteration 1000 / 2000: loss 0.272013\n",
      "iteration 1100 / 2000: loss 0.265766\n",
      "iteration 1200 / 2000: loss 0.216072\n",
      "iteration 1300 / 2000: loss 0.180613\n",
      "iteration 1400 / 2000: loss 0.157150\n",
      "iteration 1500 / 2000: loss 0.195274\n",
      "iteration 1600 / 2000: loss 0.219876\n",
      "iteration 1700 / 2000: loss 0.251604\n",
      "iteration 1800 / 2000: loss 0.150690\n",
      "iteration 1900 / 2000: loss 0.185003\n",
      "Hidden Size: 60, Learning Rate: 0.01, LR Decay: 0.3, Regularization: 0.1, Validation accuracy: 0.9568\n",
      "iteration 0 / 2000: loss 2.302622\n",
      "iteration 100 / 2000: loss 0.380132\n",
      "iteration 200 / 2000: loss 0.247214\n",
      "iteration 300 / 2000: loss 0.257387\n",
      "iteration 400 / 2000: loss 0.197869\n",
      "iteration 500 / 2000: loss 0.189939\n",
      "iteration 600 / 2000: loss 0.217104\n",
      "iteration 700 / 2000: loss 0.167940\n",
      "iteration 800 / 2000: loss 0.176027\n",
      "iteration 900 / 2000: loss 0.198540\n",
      "iteration 1000 / 2000: loss 0.184615\n",
      "iteration 1100 / 2000: loss 0.220499\n",
      "iteration 1200 / 2000: loss 0.253432\n",
      "iteration 1300 / 2000: loss 0.177190\n",
      "iteration 1400 / 2000: loss 0.140452\n",
      "iteration 1500 / 2000: loss 0.175838\n",
      "iteration 1600 / 2000: loss 0.152449\n",
      "iteration 1700 / 2000: loss 0.160952\n",
      "iteration 1800 / 2000: loss 0.249071\n",
      "iteration 1900 / 2000: loss 0.248230\n",
      "Hidden Size: 60, Learning Rate: 0.01, LR Decay: 0.3, Regularization: 0.2, Validation accuracy: 0.9552\n",
      "iteration 0 / 2000: loss 2.302668\n",
      "iteration 100 / 2000: loss 0.415784\n",
      "iteration 200 / 2000: loss 0.385966\n",
      "iteration 300 / 2000: loss 0.196674\n",
      "iteration 400 / 2000: loss 0.249450\n",
      "iteration 500 / 2000: loss 0.244546\n",
      "iteration 600 / 2000: loss 0.235110\n",
      "iteration 700 / 2000: loss 0.219917\n",
      "iteration 800 / 2000: loss 0.196422\n",
      "iteration 900 / 2000: loss 0.237806\n",
      "iteration 1000 / 2000: loss 0.260007\n",
      "iteration 1100 / 2000: loss 0.212591\n",
      "iteration 1200 / 2000: loss 0.247423\n",
      "iteration 1300 / 2000: loss 0.227089\n",
      "iteration 1400 / 2000: loss 0.281102\n",
      "iteration 1500 / 2000: loss 0.222640\n",
      "iteration 1600 / 2000: loss 0.205966\n",
      "iteration 1700 / 2000: loss 0.230790\n",
      "iteration 1800 / 2000: loss 0.142971\n",
      "iteration 1900 / 2000: loss 0.155396\n",
      "Hidden Size: 60, Learning Rate: 0.01, LR Decay: 0.3, Regularization: 0.30000000000000004, Validation accuracy: 0.954\n",
      "iteration 0 / 2000: loss 2.302683\n",
      "iteration 100 / 2000: loss 0.364507\n",
      "iteration 200 / 2000: loss 0.433093\n",
      "iteration 300 / 2000: loss 0.272897\n",
      "iteration 400 / 2000: loss 0.310231\n",
      "iteration 500 / 2000: loss 0.178669\n",
      "iteration 600 / 2000: loss 0.259029\n",
      "iteration 700 / 2000: loss 0.222440\n",
      "iteration 800 / 2000: loss 0.224437\n",
      "iteration 900 / 2000: loss 0.279613\n",
      "iteration 1000 / 2000: loss 0.320975\n",
      "iteration 1100 / 2000: loss 0.287447\n",
      "iteration 1200 / 2000: loss 0.230059\n",
      "iteration 1300 / 2000: loss 0.295575\n",
      "iteration 1400 / 2000: loss 0.182617\n",
      "iteration 1500 / 2000: loss 0.295067\n",
      "iteration 1600 / 2000: loss 0.251388\n",
      "iteration 1700 / 2000: loss 0.305639\n",
      "iteration 1800 / 2000: loss 0.261856\n",
      "iteration 1900 / 2000: loss 0.287261\n",
      "Hidden Size: 60, Learning Rate: 0.01, LR Decay: 0.3, Regularization: 0.4, Validation accuracy: 0.9508\n",
      "iteration 0 / 2000: loss 2.302561\n",
      "iteration 100 / 2000: loss 0.302247\n",
      "iteration 200 / 2000: loss 0.118036\n",
      "iteration 300 / 2000: loss 0.156450\n",
      "iteration 400 / 2000: loss 0.033484\n",
      "iteration 500 / 2000: loss 0.113330\n",
      "iteration 600 / 2000: loss 0.055387\n",
      "iteration 700 / 2000: loss 0.074728\n",
      "iteration 800 / 2000: loss 0.067742\n",
      "iteration 900 / 2000: loss 0.061906\n",
      "iteration 1000 / 2000: loss 0.096704\n",
      "iteration 1100 / 2000: loss 0.088424\n",
      "iteration 1200 / 2000: loss 0.043690\n",
      "iteration 1300 / 2000: loss 0.027010\n",
      "iteration 1400 / 2000: loss 0.019106\n",
      "iteration 1500 / 2000: loss 0.026350\n",
      "iteration 1600 / 2000: loss 0.037704\n",
      "iteration 1700 / 2000: loss 0.042026\n",
      "iteration 1800 / 2000: loss 0.010932\n",
      "iteration 1900 / 2000: loss 0.022283\n",
      "Hidden Size: 60, Learning Rate: 0.0075000250000000004, LR Decay: 0.99, Regularization: 0.0, Validation accuracy: 0.9752\n",
      "iteration 0 / 2000: loss 2.302613\n",
      "iteration 100 / 2000: loss 0.319136\n",
      "iteration 200 / 2000: loss 0.276168\n",
      "iteration 300 / 2000: loss 0.088547\n",
      "iteration 400 / 2000: loss 0.123360\n",
      "iteration 500 / 2000: loss 0.184765\n",
      "iteration 600 / 2000: loss 0.165183\n",
      "iteration 700 / 2000: loss 0.122670\n",
      "iteration 800 / 2000: loss 0.118240\n",
      "iteration 900 / 2000: loss 0.120095\n",
      "iteration 1000 / 2000: loss 0.095210\n",
      "iteration 1100 / 2000: loss 0.146095\n",
      "iteration 1200 / 2000: loss 0.088364\n",
      "iteration 1300 / 2000: loss 0.097444\n",
      "iteration 1400 / 2000: loss 0.112387\n",
      "iteration 1500 / 2000: loss 0.118406\n",
      "iteration 1600 / 2000: loss 0.113776\n",
      "iteration 1700 / 2000: loss 0.139542\n",
      "iteration 1800 / 2000: loss 0.093240\n",
      "iteration 1900 / 2000: loss 0.161781\n",
      "Hidden Size: 60, Learning Rate: 0.0075000250000000004, LR Decay: 0.99, Regularization: 0.1, Validation accuracy: 0.9732\n",
      "iteration 0 / 2000: loss 2.302632\n",
      "iteration 100 / 2000: loss 0.292708\n",
      "iteration 200 / 2000: loss 0.207819\n",
      "iteration 300 / 2000: loss 0.250537\n",
      "iteration 400 / 2000: loss 0.211593\n",
      "iteration 500 / 2000: loss 0.212917\n",
      "iteration 600 / 2000: loss 0.299208\n",
      "iteration 700 / 2000: loss 0.199503\n",
      "iteration 800 / 2000: loss 0.167564\n",
      "iteration 900 / 2000: loss 0.166278\n",
      "iteration 1000 / 2000: loss 0.161555\n",
      "iteration 1100 / 2000: loss 0.189398\n",
      "iteration 1200 / 2000: loss 0.167933\n",
      "iteration 1300 / 2000: loss 0.199571\n",
      "iteration 1400 / 2000: loss 0.147714\n",
      "iteration 1500 / 2000: loss 0.121557\n",
      "iteration 1600 / 2000: loss 0.120878\n",
      "iteration 1700 / 2000: loss 0.174653\n",
      "iteration 1800 / 2000: loss 0.198054\n",
      "iteration 1900 / 2000: loss 0.157492\n",
      "Hidden Size: 60, Learning Rate: 0.0075000250000000004, LR Decay: 0.99, Regularization: 0.2, Validation accuracy: 0.9736\n",
      "iteration 0 / 2000: loss 2.302668\n",
      "iteration 100 / 2000: loss 0.292354\n",
      "iteration 200 / 2000: loss 0.181875\n",
      "iteration 300 / 2000: loss 0.235179\n",
      "iteration 400 / 2000: loss 0.255411\n",
      "iteration 500 / 2000: loss 0.233751\n",
      "iteration 600 / 2000: loss 0.200052\n",
      "iteration 700 / 2000: loss 0.237007\n",
      "iteration 800 / 2000: loss 0.251942\n",
      "iteration 900 / 2000: loss 0.189529\n",
      "iteration 1000 / 2000: loss 0.187221\n",
      "iteration 1100 / 2000: loss 0.206180\n",
      "iteration 1200 / 2000: loss 0.181916\n",
      "iteration 1300 / 2000: loss 0.181422\n",
      "iteration 1400 / 2000: loss 0.171492\n",
      "iteration 1500 / 2000: loss 0.180520\n",
      "iteration 1600 / 2000: loss 0.211139\n",
      "iteration 1700 / 2000: loss 0.159672\n",
      "iteration 1800 / 2000: loss 0.168200\n",
      "iteration 1900 / 2000: loss 0.188367\n",
      "Hidden Size: 60, Learning Rate: 0.0075000250000000004, LR Decay: 0.99, Regularization: 0.30000000000000004, Validation accuracy: 0.9714\n",
      "iteration 0 / 2000: loss 2.302682\n",
      "iteration 100 / 2000: loss 0.263694\n",
      "iteration 200 / 2000: loss 0.260772\n",
      "iteration 300 / 2000: loss 0.234228\n",
      "iteration 400 / 2000: loss 0.198518\n",
      "iteration 500 / 2000: loss 0.225033\n",
      "iteration 600 / 2000: loss 0.225050\n",
      "iteration 700 / 2000: loss 0.220030\n",
      "iteration 800 / 2000: loss 0.298725\n",
      "iteration 900 / 2000: loss 0.255926\n",
      "iteration 1000 / 2000: loss 0.241748\n",
      "iteration 1100 / 2000: loss 0.185579\n",
      "iteration 1200 / 2000: loss 0.299936\n",
      "iteration 1300 / 2000: loss 0.221759\n",
      "iteration 1400 / 2000: loss 0.248085\n",
      "iteration 1500 / 2000: loss 0.214702\n",
      "iteration 1600 / 2000: loss 0.237634\n",
      "iteration 1700 / 2000: loss 0.234987\n",
      "iteration 1800 / 2000: loss 0.216513\n",
      "iteration 1900 / 2000: loss 0.204721\n",
      "Hidden Size: 60, Learning Rate: 0.0075000250000000004, LR Decay: 0.99, Regularization: 0.4, Validation accuracy: 0.965\n",
      "iteration 0 / 2000: loss 2.302588\n",
      "iteration 100 / 2000: loss 0.304262\n",
      "iteration 200 / 2000: loss 0.223941\n",
      "iteration 300 / 2000: loss 0.212502\n",
      "iteration 400 / 2000: loss 0.123013\n",
      "iteration 500 / 2000: loss 0.082982\n",
      "iteration 600 / 2000: loss 0.145024\n",
      "iteration 700 / 2000: loss 0.035475\n",
      "iteration 800 / 2000: loss 0.095962\n",
      "iteration 900 / 2000: loss 0.065144\n",
      "iteration 1000 / 2000: loss 0.025149\n",
      "iteration 1100 / 2000: loss 0.082440\n",
      "iteration 1200 / 2000: loss 0.040141\n",
      "iteration 1300 / 2000: loss 0.075319\n",
      "iteration 1400 / 2000: loss 0.053787\n",
      "iteration 1500 / 2000: loss 0.044121\n",
      "iteration 1600 / 2000: loss 0.020400\n",
      "iteration 1700 / 2000: loss 0.040844\n",
      "iteration 1800 / 2000: loss 0.026558\n",
      "iteration 1900 / 2000: loss 0.017678\n",
      "Hidden Size: 60, Learning Rate: 0.0075000250000000004, LR Decay: 0.8175, Regularization: 0.0, Validation accuracy: 0.975\n",
      "iteration 0 / 2000: loss 2.302600\n",
      "iteration 100 / 2000: loss 0.311250\n",
      "iteration 200 / 2000: loss 0.156662\n",
      "iteration 300 / 2000: loss 0.255407\n",
      "iteration 400 / 2000: loss 0.119603\n",
      "iteration 500 / 2000: loss 0.137170\n",
      "iteration 600 / 2000: loss 0.144615\n",
      "iteration 700 / 2000: loss 0.079718\n",
      "iteration 800 / 2000: loss 0.133444\n",
      "iteration 900 / 2000: loss 0.097572\n",
      "iteration 1000 / 2000: loss 0.136959\n",
      "iteration 1100 / 2000: loss 0.120013\n",
      "iteration 1200 / 2000: loss 0.094583\n",
      "iteration 1300 / 2000: loss 0.116452\n",
      "iteration 1400 / 2000: loss 0.108167\n",
      "iteration 1500 / 2000: loss 0.092594\n",
      "iteration 1600 / 2000: loss 0.118606\n",
      "iteration 1700 / 2000: loss 0.158316\n",
      "iteration 1800 / 2000: loss 0.103478\n",
      "iteration 1900 / 2000: loss 0.113030\n",
      "Hidden Size: 60, Learning Rate: 0.0075000250000000004, LR Decay: 0.8175, Regularization: 0.1, Validation accuracy: 0.9742\n",
      "iteration 0 / 2000: loss 2.302619\n",
      "iteration 100 / 2000: loss 0.283887\n",
      "iteration 200 / 2000: loss 0.322019\n",
      "iteration 300 / 2000: loss 0.168784\n",
      "iteration 400 / 2000: loss 0.218376\n",
      "iteration 500 / 2000: loss 0.199676\n",
      "iteration 600 / 2000: loss 0.190355\n",
      "iteration 700 / 2000: loss 0.218435\n",
      "iteration 800 / 2000: loss 0.172974\n",
      "iteration 900 / 2000: loss 0.210845\n",
      "iteration 1000 / 2000: loss 0.114384\n",
      "iteration 1100 / 2000: loss 0.135620\n",
      "iteration 1200 / 2000: loss 0.165068\n",
      "iteration 1300 / 2000: loss 0.134742\n",
      "iteration 1400 / 2000: loss 0.145343\n",
      "iteration 1500 / 2000: loss 0.180139\n",
      "iteration 1600 / 2000: loss 0.111170\n",
      "iteration 1700 / 2000: loss 0.119399\n",
      "iteration 1800 / 2000: loss 0.153504\n",
      "iteration 1900 / 2000: loss 0.128562\n",
      "Hidden Size: 60, Learning Rate: 0.0075000250000000004, LR Decay: 0.8175, Regularization: 0.2, Validation accuracy: 0.9744\n",
      "iteration 0 / 2000: loss 2.302663\n",
      "iteration 100 / 2000: loss 0.414232\n",
      "iteration 200 / 2000: loss 0.230320\n",
      "iteration 300 / 2000: loss 0.176964\n",
      "iteration 400 / 2000: loss 0.202594\n",
      "iteration 500 / 2000: loss 0.218040\n",
      "iteration 600 / 2000: loss 0.196818\n",
      "iteration 700 / 2000: loss 0.214125\n",
      "iteration 800 / 2000: loss 0.192363\n",
      "iteration 900 / 2000: loss 0.220480\n",
      "iteration 1000 / 2000: loss 0.216945\n",
      "iteration 1100 / 2000: loss 0.187913\n",
      "iteration 1200 / 2000: loss 0.139183\n",
      "iteration 1300 / 2000: loss 0.252857\n",
      "iteration 1400 / 2000: loss 0.166672\n",
      "iteration 1500 / 2000: loss 0.229830\n",
      "iteration 1600 / 2000: loss 0.165273\n",
      "iteration 1700 / 2000: loss 0.155964\n",
      "iteration 1800 / 2000: loss 0.173378\n",
      "iteration 1900 / 2000: loss 0.197395\n",
      "Hidden Size: 60, Learning Rate: 0.0075000250000000004, LR Decay: 0.8175, Regularization: 0.30000000000000004, Validation accuracy: 0.9724\n",
      "iteration 0 / 2000: loss 2.302672\n",
      "iteration 100 / 2000: loss 0.353696\n",
      "iteration 200 / 2000: loss 0.281652\n",
      "iteration 300 / 2000: loss 0.206317\n",
      "iteration 400 / 2000: loss 0.222396\n",
      "iteration 500 / 2000: loss 0.231538\n",
      "iteration 600 / 2000: loss 0.230314\n",
      "iteration 700 / 2000: loss 0.188426\n",
      "iteration 800 / 2000: loss 0.233881\n",
      "iteration 900 / 2000: loss 0.192987\n",
      "iteration 1000 / 2000: loss 0.193871\n",
      "iteration 1100 / 2000: loss 0.215045\n",
      "iteration 1200 / 2000: loss 0.234327\n",
      "iteration 1300 / 2000: loss 0.209858\n",
      "iteration 1400 / 2000: loss 0.241608\n",
      "iteration 1500 / 2000: loss 0.188634\n",
      "iteration 1600 / 2000: loss 0.218689\n",
      "iteration 1700 / 2000: loss 0.264298\n",
      "iteration 1800 / 2000: loss 0.201458\n",
      "iteration 1900 / 2000: loss 0.220715\n",
      "Hidden Size: 60, Learning Rate: 0.0075000250000000004, LR Decay: 0.8175, Regularization: 0.4, Validation accuracy: 0.973\n",
      "iteration 0 / 2000: loss 2.302581\n",
      "iteration 100 / 2000: loss 0.337324\n",
      "iteration 200 / 2000: loss 0.196825\n",
      "iteration 300 / 2000: loss 0.229378\n",
      "iteration 400 / 2000: loss 0.170257\n",
      "iteration 500 / 2000: loss 0.128054\n",
      "iteration 600 / 2000: loss 0.117319\n",
      "iteration 700 / 2000: loss 0.109276\n",
      "iteration 800 / 2000: loss 0.089552\n",
      "iteration 900 / 2000: loss 0.092588\n",
      "iteration 1000 / 2000: loss 0.052251\n",
      "iteration 1100 / 2000: loss 0.061276\n",
      "iteration 1200 / 2000: loss 0.080102\n",
      "iteration 1300 / 2000: loss 0.050230\n",
      "iteration 1400 / 2000: loss 0.049262\n",
      "iteration 1500 / 2000: loss 0.030753\n",
      "iteration 1600 / 2000: loss 0.050145\n",
      "iteration 1700 / 2000: loss 0.090315\n",
      "iteration 1800 / 2000: loss 0.067904\n",
      "iteration 1900 / 2000: loss 0.093263\n",
      "Hidden Size: 60, Learning Rate: 0.0075000250000000004, LR Decay: 0.645, Regularization: 0.0, Validation accuracy: 0.9718\n",
      "iteration 0 / 2000: loss 2.302601\n",
      "iteration 100 / 2000: loss 0.310392\n",
      "iteration 200 / 2000: loss 0.189606\n",
      "iteration 300 / 2000: loss 0.190934\n",
      "iteration 400 / 2000: loss 0.207251\n",
      "iteration 500 / 2000: loss 0.142863\n",
      "iteration 600 / 2000: loss 0.103028\n",
      "iteration 700 / 2000: loss 0.151501\n",
      "iteration 800 / 2000: loss 0.124452\n",
      "iteration 900 / 2000: loss 0.074223\n",
      "iteration 1000 / 2000: loss 0.128395\n",
      "iteration 1100 / 2000: loss 0.123977\n",
      "iteration 1200 / 2000: loss 0.093306\n",
      "iteration 1300 / 2000: loss 0.095099\n",
      "iteration 1400 / 2000: loss 0.171803\n",
      "iteration 1500 / 2000: loss 0.075544\n",
      "iteration 1600 / 2000: loss 0.134295\n",
      "iteration 1700 / 2000: loss 0.086775\n",
      "iteration 1800 / 2000: loss 0.076234\n",
      "iteration 1900 / 2000: loss 0.105692\n",
      "Hidden Size: 60, Learning Rate: 0.0075000250000000004, LR Decay: 0.645, Regularization: 0.1, Validation accuracy: 0.9708\n",
      "iteration 0 / 2000: loss 2.302627\n",
      "iteration 100 / 2000: loss 0.282311\n",
      "iteration 200 / 2000: loss 0.265668\n",
      "iteration 300 / 2000: loss 0.174296\n",
      "iteration 400 / 2000: loss 0.170646\n",
      "iteration 500 / 2000: loss 0.197767\n",
      "iteration 600 / 2000: loss 0.186639\n",
      "iteration 700 / 2000: loss 0.133985\n",
      "iteration 800 / 2000: loss 0.192162\n",
      "iteration 900 / 2000: loss 0.163183\n",
      "iteration 1000 / 2000: loss 0.145736\n",
      "iteration 1100 / 2000: loss 0.174494\n",
      "iteration 1200 / 2000: loss 0.122905\n",
      "iteration 1300 / 2000: loss 0.140352\n",
      "iteration 1400 / 2000: loss 0.138101\n",
      "iteration 1500 / 2000: loss 0.152939\n",
      "iteration 1600 / 2000: loss 0.178859\n",
      "iteration 1700 / 2000: loss 0.133297\n",
      "iteration 1800 / 2000: loss 0.185957\n",
      "iteration 1900 / 2000: loss 0.114154\n",
      "Hidden Size: 60, Learning Rate: 0.0075000250000000004, LR Decay: 0.645, Regularization: 0.2, Validation accuracy: 0.9696\n",
      "iteration 0 / 2000: loss 2.302665\n",
      "iteration 100 / 2000: loss 0.347436\n",
      "iteration 200 / 2000: loss 0.224708\n",
      "iteration 300 / 2000: loss 0.253239\n",
      "iteration 400 / 2000: loss 0.177153\n",
      "iteration 500 / 2000: loss 0.171579\n",
      "iteration 600 / 2000: loss 0.229984\n",
      "iteration 700 / 2000: loss 0.208240\n",
      "iteration 800 / 2000: loss 0.224212\n",
      "iteration 900 / 2000: loss 0.231359\n",
      "iteration 1000 / 2000: loss 0.176677\n",
      "iteration 1100 / 2000: loss 0.152742\n",
      "iteration 1200 / 2000: loss 0.189652\n",
      "iteration 1300 / 2000: loss 0.166245\n",
      "iteration 1400 / 2000: loss 0.157614\n",
      "iteration 1500 / 2000: loss 0.209047\n",
      "iteration 1600 / 2000: loss 0.194159\n",
      "iteration 1700 / 2000: loss 0.181093\n",
      "iteration 1800 / 2000: loss 0.197259\n",
      "iteration 1900 / 2000: loss 0.232277\n",
      "Hidden Size: 60, Learning Rate: 0.0075000250000000004, LR Decay: 0.645, Regularization: 0.30000000000000004, Validation accuracy: 0.9668\n",
      "iteration 0 / 2000: loss 2.302680\n",
      "iteration 100 / 2000: loss 0.343166\n",
      "iteration 200 / 2000: loss 0.351233\n",
      "iteration 300 / 2000: loss 0.323716\n",
      "iteration 400 / 2000: loss 0.331172\n",
      "iteration 500 / 2000: loss 0.192052\n",
      "iteration 600 / 2000: loss 0.196651\n",
      "iteration 700 / 2000: loss 0.172060\n",
      "iteration 800 / 2000: loss 0.215738\n",
      "iteration 900 / 2000: loss 0.183198\n",
      "iteration 1000 / 2000: loss 0.334380\n",
      "iteration 1100 / 2000: loss 0.224160\n",
      "iteration 1200 / 2000: loss 0.220234\n",
      "iteration 1300 / 2000: loss 0.249227\n",
      "iteration 1400 / 2000: loss 0.176551\n",
      "iteration 1500 / 2000: loss 0.248771\n",
      "iteration 1600 / 2000: loss 0.213213\n",
      "iteration 1700 / 2000: loss 0.224726\n",
      "iteration 1800 / 2000: loss 0.186087\n",
      "iteration 1900 / 2000: loss 0.183727\n",
      "Hidden Size: 60, Learning Rate: 0.0075000250000000004, LR Decay: 0.645, Regularization: 0.4, Validation accuracy: 0.967\n",
      "iteration 0 / 2000: loss 2.302590\n",
      "iteration 100 / 2000: loss 0.297551\n",
      "iteration 200 / 2000: loss 0.224504\n",
      "iteration 300 / 2000: loss 0.132716\n",
      "iteration 400 / 2000: loss 0.126210\n",
      "iteration 500 / 2000: loss 0.139483\n",
      "iteration 600 / 2000: loss 0.210632\n",
      "iteration 700 / 2000: loss 0.085929\n",
      "iteration 800 / 2000: loss 0.124622\n",
      "iteration 900 / 2000: loss 0.140318\n",
      "iteration 1000 / 2000: loss 0.156025\n",
      "iteration 1100 / 2000: loss 0.114905\n",
      "iteration 1200 / 2000: loss 0.159707\n",
      "iteration 1300 / 2000: loss 0.102366\n",
      "iteration 1400 / 2000: loss 0.134332\n",
      "iteration 1500 / 2000: loss 0.206110\n",
      "iteration 1600 / 2000: loss 0.063823\n",
      "iteration 1700 / 2000: loss 0.103878\n",
      "iteration 1800 / 2000: loss 0.163819\n",
      "iteration 1900 / 2000: loss 0.122635\n",
      "Hidden Size: 60, Learning Rate: 0.0075000250000000004, LR Decay: 0.47250000000000003, Regularization: 0.0, Validation accuracy: 0.9628\n",
      "iteration 0 / 2000: loss 2.302604\n",
      "iteration 100 / 2000: loss 0.189407\n",
      "iteration 200 / 2000: loss 0.319645\n",
      "iteration 300 / 2000: loss 0.266310\n",
      "iteration 400 / 2000: loss 0.208632\n",
      "iteration 500 / 2000: loss 0.204122\n",
      "iteration 600 / 2000: loss 0.123856\n",
      "iteration 700 / 2000: loss 0.146734\n",
      "iteration 800 / 2000: loss 0.156404\n",
      "iteration 900 / 2000: loss 0.107526\n",
      "iteration 1000 / 2000: loss 0.107640\n",
      "iteration 1100 / 2000: loss 0.137076\n",
      "iteration 1200 / 2000: loss 0.158686\n",
      "iteration 1300 / 2000: loss 0.096313\n",
      "iteration 1400 / 2000: loss 0.128419\n",
      "iteration 1500 / 2000: loss 0.143670\n",
      "iteration 1600 / 2000: loss 0.154453\n",
      "iteration 1700 / 2000: loss 0.132220\n",
      "iteration 1800 / 2000: loss 0.107196\n",
      "iteration 1900 / 2000: loss 0.130333\n",
      "Hidden Size: 60, Learning Rate: 0.0075000250000000004, LR Decay: 0.47250000000000003, Regularization: 0.1, Validation accuracy: 0.9616\n",
      "iteration 0 / 2000: loss 2.302643\n",
      "iteration 100 / 2000: loss 0.316253\n",
      "iteration 200 / 2000: loss 0.368359\n",
      "iteration 300 / 2000: loss 0.213109\n",
      "iteration 400 / 2000: loss 0.235869\n",
      "iteration 500 / 2000: loss 0.212917\n",
      "iteration 600 / 2000: loss 0.189398\n",
      "iteration 700 / 2000: loss 0.202923\n",
      "iteration 800 / 2000: loss 0.151588\n",
      "iteration 900 / 2000: loss 0.178651\n",
      "iteration 1000 / 2000: loss 0.193449\n",
      "iteration 1100 / 2000: loss 0.221364\n",
      "iteration 1200 / 2000: loss 0.154358\n",
      "iteration 1300 / 2000: loss 0.176582\n",
      "iteration 1400 / 2000: loss 0.152700\n",
      "iteration 1500 / 2000: loss 0.166171\n",
      "iteration 1600 / 2000: loss 0.145527\n",
      "iteration 1700 / 2000: loss 0.145419\n",
      "iteration 1800 / 2000: loss 0.180193\n",
      "iteration 1900 / 2000: loss 0.155883\n",
      "Hidden Size: 60, Learning Rate: 0.0075000250000000004, LR Decay: 0.47250000000000003, Regularization: 0.2, Validation accuracy: 0.961\n",
      "iteration 0 / 2000: loss 2.302671\n",
      "iteration 100 / 2000: loss 0.312733\n",
      "iteration 200 / 2000: loss 0.335331\n",
      "iteration 300 / 2000: loss 0.245055\n",
      "iteration 400 / 2000: loss 0.222883\n",
      "iteration 500 / 2000: loss 0.233487\n",
      "iteration 600 / 2000: loss 0.204468\n",
      "iteration 700 / 2000: loss 0.210797\n",
      "iteration 800 / 2000: loss 0.291527\n",
      "iteration 900 / 2000: loss 0.180111\n",
      "iteration 1000 / 2000: loss 0.181710\n",
      "iteration 1100 / 2000: loss 0.250452\n",
      "iteration 1200 / 2000: loss 0.199158\n",
      "iteration 1300 / 2000: loss 0.272862\n",
      "iteration 1400 / 2000: loss 0.226098\n",
      "iteration 1500 / 2000: loss 0.199941\n",
      "iteration 1600 / 2000: loss 0.216144\n",
      "iteration 1700 / 2000: loss 0.217888\n",
      "iteration 1800 / 2000: loss 0.221369\n",
      "iteration 1900 / 2000: loss 0.265258\n",
      "Hidden Size: 60, Learning Rate: 0.0075000250000000004, LR Decay: 0.47250000000000003, Regularization: 0.30000000000000004, Validation accuracy: 0.9598\n",
      "iteration 0 / 2000: loss 2.302689\n",
      "iteration 100 / 2000: loss 0.345857\n",
      "iteration 200 / 2000: loss 0.344942\n",
      "iteration 300 / 2000: loss 0.335493\n",
      "iteration 400 / 2000: loss 0.253829\n",
      "iteration 500 / 2000: loss 0.351401\n",
      "iteration 600 / 2000: loss 0.235701\n",
      "iteration 700 / 2000: loss 0.218849\n",
      "iteration 800 / 2000: loss 0.246838\n",
      "iteration 900 / 2000: loss 0.212835\n",
      "iteration 1000 / 2000: loss 0.232747\n",
      "iteration 1100 / 2000: loss 0.272020\n",
      "iteration 1200 / 2000: loss 0.256176\n",
      "iteration 1300 / 2000: loss 0.188058\n",
      "iteration 1400 / 2000: loss 0.260624\n",
      "iteration 1500 / 2000: loss 0.194549\n",
      "iteration 1600 / 2000: loss 0.210442\n",
      "iteration 1700 / 2000: loss 0.253217\n",
      "iteration 1800 / 2000: loss 0.202634\n",
      "iteration 1900 / 2000: loss 0.163085\n",
      "Hidden Size: 60, Learning Rate: 0.0075000250000000004, LR Decay: 0.47250000000000003, Regularization: 0.4, Validation accuracy: 0.9604\n",
      "iteration 0 / 2000: loss 2.302619\n",
      "iteration 100 / 2000: loss 0.281398\n",
      "iteration 200 / 2000: loss 0.286462\n",
      "iteration 300 / 2000: loss 0.257327\n",
      "iteration 400 / 2000: loss 0.239359\n",
      "iteration 500 / 2000: loss 0.234049\n",
      "iteration 600 / 2000: loss 0.185125\n",
      "iteration 700 / 2000: loss 0.223101\n",
      "iteration 800 / 2000: loss 0.236177\n",
      "iteration 900 / 2000: loss 0.227684\n",
      "iteration 1000 / 2000: loss 0.163016\n",
      "iteration 1100 / 2000: loss 0.152623\n",
      "iteration 1200 / 2000: loss 0.188618\n",
      "iteration 1300 / 2000: loss 0.184528\n",
      "iteration 1400 / 2000: loss 0.165165\n",
      "iteration 1500 / 2000: loss 0.185000\n",
      "iteration 1600 / 2000: loss 0.184458\n",
      "iteration 1700 / 2000: loss 0.198301\n",
      "iteration 1800 / 2000: loss 0.126087\n",
      "iteration 1900 / 2000: loss 0.175870\n",
      "Hidden Size: 60, Learning Rate: 0.0075000250000000004, LR Decay: 0.3, Regularization: 0.0, Validation accuracy: 0.9486\n",
      "iteration 0 / 2000: loss 2.302631\n",
      "iteration 100 / 2000: loss 0.342071\n",
      "iteration 200 / 2000: loss 0.257473\n",
      "iteration 300 / 2000: loss 0.219892\n",
      "iteration 400 / 2000: loss 0.283845\n",
      "iteration 500 / 2000: loss 0.244501\n",
      "iteration 600 / 2000: loss 0.272126\n",
      "iteration 700 / 2000: loss 0.169697\n",
      "iteration 800 / 2000: loss 0.292179\n",
      "iteration 900 / 2000: loss 0.184517\n",
      "iteration 1000 / 2000: loss 0.194663\n",
      "iteration 1100 / 2000: loss 0.216915\n",
      "iteration 1200 / 2000: loss 0.228497\n",
      "iteration 1300 / 2000: loss 0.251546\n",
      "iteration 1400 / 2000: loss 0.266116\n",
      "iteration 1500 / 2000: loss 0.204403\n",
      "iteration 1600 / 2000: loss 0.217584\n",
      "iteration 1700 / 2000: loss 0.164053\n",
      "iteration 1800 / 2000: loss 0.290483\n",
      "iteration 1900 / 2000: loss 0.199300\n",
      "Hidden Size: 60, Learning Rate: 0.0075000250000000004, LR Decay: 0.3, Regularization: 0.1, Validation accuracy: 0.947\n",
      "iteration 0 / 2000: loss 2.302642\n",
      "iteration 100 / 2000: loss 0.413773\n",
      "iteration 200 / 2000: loss 0.240597\n",
      "iteration 300 / 2000: loss 0.242390\n",
      "iteration 400 / 2000: loss 0.306890\n",
      "iteration 500 / 2000: loss 0.278154\n",
      "iteration 600 / 2000: loss 0.208989\n",
      "iteration 700 / 2000: loss 0.213280\n",
      "iteration 800 / 2000: loss 0.234168\n",
      "iteration 900 / 2000: loss 0.256972\n",
      "iteration 1000 / 2000: loss 0.184904\n",
      "iteration 1100 / 2000: loss 0.183249\n",
      "iteration 1200 / 2000: loss 0.207478\n",
      "iteration 1300 / 2000: loss 0.248861\n",
      "iteration 1400 / 2000: loss 0.184731\n",
      "iteration 1500 / 2000: loss 0.221606\n",
      "iteration 1600 / 2000: loss 0.203272\n",
      "iteration 1700 / 2000: loss 0.263256\n",
      "iteration 1800 / 2000: loss 0.164469\n",
      "iteration 1900 / 2000: loss 0.205976\n",
      "Hidden Size: 60, Learning Rate: 0.0075000250000000004, LR Decay: 0.3, Regularization: 0.2, Validation accuracy: 0.9452\n",
      "iteration 0 / 2000: loss 2.302639\n",
      "iteration 100 / 2000: loss 0.481048\n",
      "iteration 200 / 2000: loss 0.358776\n",
      "iteration 300 / 2000: loss 0.287130\n",
      "iteration 400 / 2000: loss 0.294365\n",
      "iteration 500 / 2000: loss 0.235572\n",
      "iteration 600 / 2000: loss 0.317714\n",
      "iteration 700 / 2000: loss 0.422417\n",
      "iteration 800 / 2000: loss 0.357155\n",
      "iteration 900 / 2000: loss 0.233190\n",
      "iteration 1000 / 2000: loss 0.444926\n",
      "iteration 1100 / 2000: loss 0.214420\n",
      "iteration 1200 / 2000: loss 0.234130\n",
      "iteration 1300 / 2000: loss 0.386108\n",
      "iteration 1400 / 2000: loss 0.321325\n",
      "iteration 1500 / 2000: loss 0.227673\n",
      "iteration 1600 / 2000: loss 0.346870\n",
      "iteration 1700 / 2000: loss 0.270002\n",
      "iteration 1800 / 2000: loss 0.250104\n",
      "iteration 1900 / 2000: loss 0.244196\n",
      "Hidden Size: 60, Learning Rate: 0.0075000250000000004, LR Decay: 0.3, Regularization: 0.30000000000000004, Validation accuracy: 0.9422\n",
      "iteration 0 / 2000: loss 2.302672\n",
      "iteration 100 / 2000: loss 0.367619\n",
      "iteration 200 / 2000: loss 0.318130\n",
      "iteration 300 / 2000: loss 0.355264\n",
      "iteration 400 / 2000: loss 0.266696\n",
      "iteration 500 / 2000: loss 0.262158\n",
      "iteration 600 / 2000: loss 0.246350\n",
      "iteration 700 / 2000: loss 0.256941\n",
      "iteration 800 / 2000: loss 0.182724\n",
      "iteration 900 / 2000: loss 0.263401\n",
      "iteration 1000 / 2000: loss 0.225745\n",
      "iteration 1100 / 2000: loss 0.236967\n",
      "iteration 1200 / 2000: loss 0.220845\n",
      "iteration 1300 / 2000: loss 0.303659\n",
      "iteration 1400 / 2000: loss 0.319339\n",
      "iteration 1500 / 2000: loss 0.309125\n",
      "iteration 1600 / 2000: loss 0.420543\n",
      "iteration 1700 / 2000: loss 0.222815\n",
      "iteration 1800 / 2000: loss 0.306693\n",
      "iteration 1900 / 2000: loss 0.289579\n",
      "Hidden Size: 60, Learning Rate: 0.0075000250000000004, LR Decay: 0.3, Regularization: 0.4, Validation accuracy: 0.9432\n",
      "iteration 0 / 2000: loss 2.302568\n",
      "iteration 100 / 2000: loss 0.296556\n",
      "iteration 200 / 2000: loss 0.174409\n",
      "iteration 300 / 2000: loss 0.137725\n",
      "iteration 400 / 2000: loss 0.101485\n",
      "iteration 500 / 2000: loss 0.180708\n",
      "iteration 600 / 2000: loss 0.201007\n",
      "iteration 700 / 2000: loss 0.140655\n",
      "iteration 800 / 2000: loss 0.053641\n",
      "iteration 900 / 2000: loss 0.108293\n",
      "iteration 1000 / 2000: loss 0.075925\n",
      "iteration 1100 / 2000: loss 0.070512\n",
      "iteration 1200 / 2000: loss 0.061644\n",
      "iteration 1300 / 2000: loss 0.041117\n",
      "iteration 1400 / 2000: loss 0.062915\n",
      "iteration 1500 / 2000: loss 0.075927\n",
      "iteration 1600 / 2000: loss 0.064165\n",
      "iteration 1700 / 2000: loss 0.014565\n",
      "iteration 1800 / 2000: loss 0.016194\n",
      "iteration 1900 / 2000: loss 0.037673\n",
      "Hidden Size: 60, Learning Rate: 0.00500005, LR Decay: 0.99, Regularization: 0.0, Validation accuracy: 0.974\n",
      "iteration 0 / 2000: loss 2.302614\n",
      "iteration 100 / 2000: loss 0.298200\n",
      "iteration 200 / 2000: loss 0.279167\n",
      "iteration 300 / 2000: loss 0.195733\n",
      "iteration 400 / 2000: loss 0.184011\n",
      "iteration 500 / 2000: loss 0.151075\n",
      "iteration 600 / 2000: loss 0.166045\n",
      "iteration 700 / 2000: loss 0.147132\n",
      "iteration 800 / 2000: loss 0.128228\n",
      "iteration 900 / 2000: loss 0.137954\n",
      "iteration 1000 / 2000: loss 0.111255\n",
      "iteration 1100 / 2000: loss 0.115032\n",
      "iteration 1200 / 2000: loss 0.103788\n",
      "iteration 1300 / 2000: loss 0.094170\n",
      "iteration 1400 / 2000: loss 0.184842\n",
      "iteration 1500 / 2000: loss 0.104421\n",
      "iteration 1600 / 2000: loss 0.084300\n",
      "iteration 1700 / 2000: loss 0.086636\n",
      "iteration 1800 / 2000: loss 0.109322\n",
      "iteration 1900 / 2000: loss 0.101110\n",
      "Hidden Size: 60, Learning Rate: 0.00500005, LR Decay: 0.99, Regularization: 0.1, Validation accuracy: 0.973\n",
      "iteration 0 / 2000: loss 2.302618\n",
      "iteration 100 / 2000: loss 0.234589\n",
      "iteration 200 / 2000: loss 0.306794\n",
      "iteration 300 / 2000: loss 0.304372\n",
      "iteration 400 / 2000: loss 0.161933\n",
      "iteration 500 / 2000: loss 0.179465\n",
      "iteration 600 / 2000: loss 0.187827\n",
      "iteration 700 / 2000: loss 0.145701\n",
      "iteration 800 / 2000: loss 0.137401\n",
      "iteration 900 / 2000: loss 0.193157\n",
      "iteration 1000 / 2000: loss 0.150392\n",
      "iteration 1100 / 2000: loss 0.163072\n",
      "iteration 1200 / 2000: loss 0.123680\n",
      "iteration 1300 / 2000: loss 0.151392\n",
      "iteration 1400 / 2000: loss 0.156255\n",
      "iteration 1500 / 2000: loss 0.133053\n",
      "iteration 1600 / 2000: loss 0.142232\n",
      "iteration 1700 / 2000: loss 0.118088\n",
      "iteration 1800 / 2000: loss 0.214701\n",
      "iteration 1900 / 2000: loss 0.116275\n",
      "Hidden Size: 60, Learning Rate: 0.00500005, LR Decay: 0.99, Regularization: 0.2, Validation accuracy: 0.9762\n",
      "iteration 0 / 2000: loss 2.302653\n",
      "iteration 100 / 2000: loss 0.348109\n",
      "iteration 200 / 2000: loss 0.315478\n",
      "iteration 300 / 2000: loss 0.354382\n",
      "iteration 400 / 2000: loss 0.255273\n",
      "iteration 500 / 2000: loss 0.190531\n",
      "iteration 600 / 2000: loss 0.179939\n",
      "iteration 700 / 2000: loss 0.164355\n",
      "iteration 800 / 2000: loss 0.243737\n",
      "iteration 900 / 2000: loss 0.234209\n",
      "iteration 1000 / 2000: loss 0.213618\n",
      "iteration 1100 / 2000: loss 0.179950\n",
      "iteration 1200 / 2000: loss 0.178937\n",
      "iteration 1300 / 2000: loss 0.226788\n",
      "iteration 1400 / 2000: loss 0.187050\n",
      "iteration 1500 / 2000: loss 0.176047\n",
      "iteration 1600 / 2000: loss 0.180266\n",
      "iteration 1700 / 2000: loss 0.238973\n",
      "iteration 1800 / 2000: loss 0.180110\n",
      "iteration 1900 / 2000: loss 0.155793\n",
      "Hidden Size: 60, Learning Rate: 0.00500005, LR Decay: 0.99, Regularization: 0.30000000000000004, Validation accuracy: 0.9718\n",
      "iteration 0 / 2000: loss 2.302683\n",
      "iteration 100 / 2000: loss 0.380144\n",
      "iteration 200 / 2000: loss 0.219256\n",
      "iteration 300 / 2000: loss 0.217010\n",
      "iteration 400 / 2000: loss 0.281277\n",
      "iteration 500 / 2000: loss 0.201071\n",
      "iteration 600 / 2000: loss 0.299049\n",
      "iteration 700 / 2000: loss 0.239900\n",
      "iteration 800 / 2000: loss 0.237856\n",
      "iteration 900 / 2000: loss 0.253627\n",
      "iteration 1000 / 2000: loss 0.228997\n",
      "iteration 1100 / 2000: loss 0.201208\n",
      "iteration 1200 / 2000: loss 0.213609\n",
      "iteration 1300 / 2000: loss 0.247086\n",
      "iteration 1400 / 2000: loss 0.242565\n",
      "iteration 1500 / 2000: loss 0.209205\n",
      "iteration 1600 / 2000: loss 0.196555\n",
      "iteration 1700 / 2000: loss 0.225798\n",
      "iteration 1800 / 2000: loss 0.214604\n",
      "iteration 1900 / 2000: loss 0.216110\n",
      "Hidden Size: 60, Learning Rate: 0.00500005, LR Decay: 0.99, Regularization: 0.4, Validation accuracy: 0.9688\n",
      "iteration 0 / 2000: loss 2.302597\n",
      "iteration 100 / 2000: loss 0.243048\n",
      "iteration 200 / 2000: loss 0.203245\n",
      "iteration 300 / 2000: loss 0.199315\n",
      "iteration 400 / 2000: loss 0.198067\n",
      "iteration 500 / 2000: loss 0.195772\n",
      "iteration 600 / 2000: loss 0.177648\n",
      "iteration 700 / 2000: loss 0.093244\n",
      "iteration 800 / 2000: loss 0.143224\n",
      "iteration 900 / 2000: loss 0.119831\n",
      "iteration 1000 / 2000: loss 0.114148\n",
      "iteration 1100 / 2000: loss 0.092856\n",
      "iteration 1200 / 2000: loss 0.149536\n",
      "iteration 1300 / 2000: loss 0.063467\n",
      "iteration 1400 / 2000: loss 0.048350\n",
      "iteration 1500 / 2000: loss 0.044593\n",
      "iteration 1600 / 2000: loss 0.038810\n",
      "iteration 1700 / 2000: loss 0.067580\n",
      "iteration 1800 / 2000: loss 0.035139\n",
      "iteration 1900 / 2000: loss 0.109604\n",
      "Hidden Size: 60, Learning Rate: 0.00500005, LR Decay: 0.8175, Regularization: 0.0, Validation accuracy: 0.9744\n",
      "iteration 0 / 2000: loss 2.302588\n",
      "iteration 100 / 2000: loss 0.306007\n",
      "iteration 200 / 2000: loss 0.246184\n",
      "iteration 300 / 2000: loss 0.201068\n",
      "iteration 400 / 2000: loss 0.234666\n",
      "iteration 500 / 2000: loss 0.136494\n",
      "iteration 600 / 2000: loss 0.130790\n",
      "iteration 700 / 2000: loss 0.135993\n",
      "iteration 800 / 2000: loss 0.141452\n",
      "iteration 900 / 2000: loss 0.161915\n",
      "iteration 1000 / 2000: loss 0.112230\n",
      "iteration 1100 / 2000: loss 0.156506\n",
      "iteration 1200 / 2000: loss 0.087761\n",
      "iteration 1300 / 2000: loss 0.097521\n",
      "iteration 1400 / 2000: loss 0.108582\n",
      "iteration 1500 / 2000: loss 0.113168\n",
      "iteration 1600 / 2000: loss 0.114546\n",
      "iteration 1700 / 2000: loss 0.101470\n",
      "iteration 1800 / 2000: loss 0.097837\n",
      "iteration 1900 / 2000: loss 0.134178\n",
      "Hidden Size: 60, Learning Rate: 0.00500005, LR Decay: 0.8175, Regularization: 0.1, Validation accuracy: 0.9728\n",
      "iteration 0 / 2000: loss 2.302628\n",
      "iteration 100 / 2000: loss 0.259433\n",
      "iteration 200 / 2000: loss 0.230389\n",
      "iteration 300 / 2000: loss 0.187311\n",
      "iteration 400 / 2000: loss 0.192816\n",
      "iteration 500 / 2000: loss 0.165159\n",
      "iteration 600 / 2000: loss 0.126142\n",
      "iteration 700 / 2000: loss 0.193580\n",
      "iteration 800 / 2000: loss 0.184855\n",
      "iteration 900 / 2000: loss 0.213971\n",
      "iteration 1000 / 2000: loss 0.148903\n",
      "iteration 1100 / 2000: loss 0.131722\n",
      "iteration 1200 / 2000: loss 0.124666\n",
      "iteration 1300 / 2000: loss 0.146068\n",
      "iteration 1400 / 2000: loss 0.156579\n",
      "iteration 1500 / 2000: loss 0.157778\n",
      "iteration 1600 / 2000: loss 0.169055\n",
      "iteration 1700 / 2000: loss 0.122404\n",
      "iteration 1800 / 2000: loss 0.118709\n",
      "iteration 1900 / 2000: loss 0.130576\n",
      "Hidden Size: 60, Learning Rate: 0.00500005, LR Decay: 0.8175, Regularization: 0.2, Validation accuracy: 0.972\n",
      "iteration 0 / 2000: loss 2.302659\n",
      "iteration 100 / 2000: loss 0.277887\n",
      "iteration 200 / 2000: loss 0.304295\n",
      "iteration 300 / 2000: loss 0.187496\n",
      "iteration 400 / 2000: loss 0.191725\n",
      "iteration 500 / 2000: loss 0.255174\n",
      "iteration 600 / 2000: loss 0.313890\n",
      "iteration 700 / 2000: loss 0.213839\n",
      "iteration 800 / 2000: loss 0.180142\n",
      "iteration 900 / 2000: loss 0.202205\n",
      "iteration 1000 / 2000: loss 0.190050\n",
      "iteration 1100 / 2000: loss 0.213982\n",
      "iteration 1200 / 2000: loss 0.227643\n",
      "iteration 1300 / 2000: loss 0.252547\n",
      "iteration 1400 / 2000: loss 0.180773\n",
      "iteration 1500 / 2000: loss 0.161224\n",
      "iteration 1600 / 2000: loss 0.174905\n",
      "iteration 1700 / 2000: loss 0.171097\n",
      "iteration 1800 / 2000: loss 0.147725\n",
      "iteration 1900 / 2000: loss 0.165126\n",
      "Hidden Size: 60, Learning Rate: 0.00500005, LR Decay: 0.8175, Regularization: 0.30000000000000004, Validation accuracy: 0.9728\n",
      "iteration 0 / 2000: loss 2.302669\n",
      "iteration 100 / 2000: loss 0.297299\n",
      "iteration 200 / 2000: loss 0.311289\n",
      "iteration 300 / 2000: loss 0.238400\n",
      "iteration 400 / 2000: loss 0.231538\n",
      "iteration 500 / 2000: loss 0.284515\n",
      "iteration 600 / 2000: loss 0.249793\n",
      "iteration 700 / 2000: loss 0.185283\n",
      "iteration 800 / 2000: loss 0.234402\n",
      "iteration 900 / 2000: loss 0.207423\n",
      "iteration 1000 / 2000: loss 0.265322\n",
      "iteration 1100 / 2000: loss 0.201687\n",
      "iteration 1200 / 2000: loss 0.194849\n",
      "iteration 1300 / 2000: loss 0.169133\n",
      "iteration 1400 / 2000: loss 0.249376\n",
      "iteration 1500 / 2000: loss 0.234266\n",
      "iteration 1600 / 2000: loss 0.254360\n",
      "iteration 1700 / 2000: loss 0.207818\n",
      "iteration 1800 / 2000: loss 0.186949\n",
      "iteration 1900 / 2000: loss 0.170027\n",
      "Hidden Size: 60, Learning Rate: 0.00500005, LR Decay: 0.8175, Regularization: 0.4, Validation accuracy: 0.968\n",
      "iteration 0 / 2000: loss 2.302570\n",
      "iteration 100 / 2000: loss 0.207474\n",
      "iteration 200 / 2000: loss 0.200355\n",
      "iteration 300 / 2000: loss 0.257936\n",
      "iteration 400 / 2000: loss 0.154091\n",
      "iteration 500 / 2000: loss 0.129070\n",
      "iteration 600 / 2000: loss 0.202223\n",
      "iteration 700 / 2000: loss 0.236260\n",
      "iteration 800 / 2000: loss 0.125112\n",
      "iteration 900 / 2000: loss 0.147474\n",
      "iteration 1000 / 2000: loss 0.089792\n",
      "iteration 1100 / 2000: loss 0.134380\n",
      "iteration 1200 / 2000: loss 0.102478\n",
      "iteration 1300 / 2000: loss 0.071128\n",
      "iteration 1400 / 2000: loss 0.057399\n",
      "iteration 1500 / 2000: loss 0.085232\n",
      "iteration 1600 / 2000: loss 0.071353\n",
      "iteration 1700 / 2000: loss 0.122000\n",
      "iteration 1800 / 2000: loss 0.057183\n",
      "iteration 1900 / 2000: loss 0.109397\n",
      "Hidden Size: 60, Learning Rate: 0.00500005, LR Decay: 0.645, Regularization: 0.0, Validation accuracy: 0.9662\n",
      "iteration 0 / 2000: loss 2.302609\n",
      "iteration 100 / 2000: loss 0.325374\n",
      "iteration 200 / 2000: loss 0.342302\n",
      "iteration 300 / 2000: loss 0.260756\n",
      "iteration 400 / 2000: loss 0.230413\n",
      "iteration 500 / 2000: loss 0.187277\n",
      "iteration 600 / 2000: loss 0.130536\n",
      "iteration 700 / 2000: loss 0.182266\n",
      "iteration 800 / 2000: loss 0.218361\n",
      "iteration 900 / 2000: loss 0.181029\n",
      "iteration 1000 / 2000: loss 0.169723\n",
      "iteration 1100 / 2000: loss 0.178963\n",
      "iteration 1200 / 2000: loss 0.150867\n",
      "iteration 1300 / 2000: loss 0.163124\n",
      "iteration 1400 / 2000: loss 0.122444\n",
      "iteration 1500 / 2000: loss 0.148314\n",
      "iteration 1600 / 2000: loss 0.113940\n",
      "iteration 1700 / 2000: loss 0.114018\n",
      "iteration 1800 / 2000: loss 0.110099\n",
      "iteration 1900 / 2000: loss 0.174010\n",
      "Hidden Size: 60, Learning Rate: 0.00500005, LR Decay: 0.645, Regularization: 0.1, Validation accuracy: 0.9656\n",
      "iteration 0 / 2000: loss 2.302639\n",
      "iteration 100 / 2000: loss 0.188917\n",
      "iteration 200 / 2000: loss 0.364955\n",
      "iteration 300 / 2000: loss 0.275767\n",
      "iteration 400 / 2000: loss 0.139342\n",
      "iteration 500 / 2000: loss 0.167677\n",
      "iteration 600 / 2000: loss 0.152703\n",
      "iteration 700 / 2000: loss 0.241232\n",
      "iteration 800 / 2000: loss 0.187722\n",
      "iteration 900 / 2000: loss 0.196833\n",
      "iteration 1000 / 2000: loss 0.195642\n",
      "iteration 1100 / 2000: loss 0.195317\n",
      "iteration 1200 / 2000: loss 0.151873\n",
      "iteration 1300 / 2000: loss 0.171431\n",
      "iteration 1400 / 2000: loss 0.163720\n",
      "iteration 1500 / 2000: loss 0.204035\n",
      "iteration 1600 / 2000: loss 0.204427\n",
      "iteration 1700 / 2000: loss 0.159627\n",
      "iteration 1800 / 2000: loss 0.152458\n",
      "iteration 1900 / 2000: loss 0.232614\n",
      "Hidden Size: 60, Learning Rate: 0.00500005, LR Decay: 0.645, Regularization: 0.2, Validation accuracy: 0.9626\n",
      "iteration 0 / 2000: loss 2.302661\n",
      "iteration 100 / 2000: loss 0.298408\n",
      "iteration 200 / 2000: loss 0.288568\n",
      "iteration 300 / 2000: loss 0.320575\n",
      "iteration 400 / 2000: loss 0.254597\n",
      "iteration 500 / 2000: loss 0.254410\n",
      "iteration 600 / 2000: loss 0.192095\n",
      "iteration 700 / 2000: loss 0.197648\n",
      "iteration 800 / 2000: loss 0.297741\n",
      "iteration 900 / 2000: loss 0.181551\n",
      "iteration 1000 / 2000: loss 0.247650\n",
      "iteration 1100 / 2000: loss 0.256249\n",
      "iteration 1200 / 2000: loss 0.182474\n",
      "iteration 1300 / 2000: loss 0.183420\n",
      "iteration 1400 / 2000: loss 0.171052\n",
      "iteration 1500 / 2000: loss 0.182473\n",
      "iteration 1600 / 2000: loss 0.182768\n",
      "iteration 1700 / 2000: loss 0.197280\n",
      "iteration 1800 / 2000: loss 0.171112\n",
      "iteration 1900 / 2000: loss 0.208554\n",
      "Hidden Size: 60, Learning Rate: 0.00500005, LR Decay: 0.645, Regularization: 0.30000000000000004, Validation accuracy: 0.9628\n",
      "iteration 0 / 2000: loss 2.302682\n",
      "iteration 100 / 2000: loss 0.331151\n",
      "iteration 200 / 2000: loss 0.314010\n",
      "iteration 300 / 2000: loss 0.286441\n",
      "iteration 400 / 2000: loss 0.285396\n",
      "iteration 500 / 2000: loss 0.251235\n",
      "iteration 600 / 2000: loss 0.232687\n",
      "iteration 700 / 2000: loss 0.189950\n",
      "iteration 800 / 2000: loss 0.282984\n",
      "iteration 900 / 2000: loss 0.203632\n",
      "iteration 1000 / 2000: loss 0.300420\n",
      "iteration 1100 / 2000: loss 0.226702\n",
      "iteration 1200 / 2000: loss 0.279431\n",
      "iteration 1300 / 2000: loss 0.165953\n",
      "iteration 1400 / 2000: loss 0.282766\n",
      "iteration 1500 / 2000: loss 0.225554\n",
      "iteration 1600 / 2000: loss 0.278668\n",
      "iteration 1700 / 2000: loss 0.227250\n",
      "iteration 1800 / 2000: loss 0.168854\n",
      "iteration 1900 / 2000: loss 0.208391\n",
      "Hidden Size: 60, Learning Rate: 0.00500005, LR Decay: 0.645, Regularization: 0.4, Validation accuracy: 0.9626\n",
      "iteration 0 / 2000: loss 2.302565\n",
      "iteration 100 / 2000: loss 0.379123\n",
      "iteration 200 / 2000: loss 0.276624\n",
      "iteration 300 / 2000: loss 0.157264\n",
      "iteration 400 / 2000: loss 0.151905\n",
      "iteration 500 / 2000: loss 0.166310\n",
      "iteration 600 / 2000: loss 0.175893\n",
      "iteration 700 / 2000: loss 0.212814\n",
      "iteration 800 / 2000: loss 0.110167\n",
      "iteration 900 / 2000: loss 0.146433\n",
      "iteration 1000 / 2000: loss 0.189860\n",
      "iteration 1100 / 2000: loss 0.231962\n",
      "iteration 1200 / 2000: loss 0.120306\n",
      "iteration 1300 / 2000: loss 0.188847\n",
      "iteration 1400 / 2000: loss 0.158442\n",
      "iteration 1500 / 2000: loss 0.226086\n",
      "iteration 1600 / 2000: loss 0.091868\n",
      "iteration 1700 / 2000: loss 0.133704\n",
      "iteration 1800 / 2000: loss 0.172007\n",
      "iteration 1900 / 2000: loss 0.098350\n",
      "Hidden Size: 60, Learning Rate: 0.00500005, LR Decay: 0.47250000000000003, Regularization: 0.0, Validation accuracy: 0.9574\n",
      "iteration 0 / 2000: loss 2.302622\n",
      "iteration 100 / 2000: loss 0.330733\n",
      "iteration 200 / 2000: loss 0.271984\n",
      "iteration 300 / 2000: loss 0.219361\n",
      "iteration 400 / 2000: loss 0.268060\n",
      "iteration 500 / 2000: loss 0.243540\n",
      "iteration 600 / 2000: loss 0.161384\n",
      "iteration 700 / 2000: loss 0.178392\n",
      "iteration 800 / 2000: loss 0.166740\n",
      "iteration 900 / 2000: loss 0.370901\n",
      "iteration 1000 / 2000: loss 0.176298\n",
      "iteration 1100 / 2000: loss 0.164143\n",
      "iteration 1200 / 2000: loss 0.191313\n",
      "iteration 1300 / 2000: loss 0.145278\n",
      "iteration 1400 / 2000: loss 0.206605\n",
      "iteration 1500 / 2000: loss 0.152881\n",
      "iteration 1600 / 2000: loss 0.151506\n",
      "iteration 1700 / 2000: loss 0.156470\n",
      "iteration 1800 / 2000: loss 0.154425\n",
      "iteration 1900 / 2000: loss 0.167657\n",
      "Hidden Size: 60, Learning Rate: 0.00500005, LR Decay: 0.47250000000000003, Regularization: 0.1, Validation accuracy: 0.9544\n",
      "iteration 0 / 2000: loss 2.302637\n",
      "iteration 100 / 2000: loss 0.356099\n",
      "iteration 200 / 2000: loss 0.324285\n",
      "iteration 300 / 2000: loss 0.284020\n",
      "iteration 400 / 2000: loss 0.179910\n",
      "iteration 500 / 2000: loss 0.208129\n",
      "iteration 600 / 2000: loss 0.252582\n",
      "iteration 700 / 2000: loss 0.156536\n",
      "iteration 800 / 2000: loss 0.180398\n",
      "iteration 900 / 2000: loss 0.201533\n",
      "iteration 1000 / 2000: loss 0.196497\n",
      "iteration 1100 / 2000: loss 0.255118\n",
      "iteration 1200 / 2000: loss 0.235459\n",
      "iteration 1300 / 2000: loss 0.231282\n",
      "iteration 1400 / 2000: loss 0.242916\n",
      "iteration 1500 / 2000: loss 0.254050\n",
      "iteration 1600 / 2000: loss 0.238288\n",
      "iteration 1700 / 2000: loss 0.203805\n",
      "iteration 1800 / 2000: loss 0.210840\n",
      "iteration 1900 / 2000: loss 0.170794\n",
      "Hidden Size: 60, Learning Rate: 0.00500005, LR Decay: 0.47250000000000003, Regularization: 0.2, Validation accuracy: 0.955\n",
      "iteration 0 / 2000: loss 2.302657\n",
      "iteration 100 / 2000: loss 0.370926\n",
      "iteration 200 / 2000: loss 0.274922\n",
      "iteration 300 / 2000: loss 0.306164\n",
      "iteration 400 / 2000: loss 0.262118\n",
      "iteration 500 / 2000: loss 0.279453\n",
      "iteration 600 / 2000: loss 0.232741\n",
      "iteration 700 / 2000: loss 0.228691\n",
      "iteration 800 / 2000: loss 0.308745\n",
      "iteration 900 / 2000: loss 0.177314\n",
      "iteration 1000 / 2000: loss 0.194130\n",
      "iteration 1100 / 2000: loss 0.283043\n",
      "iteration 1200 / 2000: loss 0.227404\n",
      "iteration 1300 / 2000: loss 0.205271\n",
      "iteration 1400 / 2000: loss 0.277519\n",
      "iteration 1500 / 2000: loss 0.185921\n",
      "iteration 1600 / 2000: loss 0.237129\n",
      "iteration 1700 / 2000: loss 0.255481\n",
      "iteration 1800 / 2000: loss 0.247600\n",
      "iteration 1900 / 2000: loss 0.196919\n",
      "Hidden Size: 60, Learning Rate: 0.00500005, LR Decay: 0.47250000000000003, Regularization: 0.30000000000000004, Validation accuracy: 0.9528\n",
      "iteration 0 / 2000: loss 2.302658\n",
      "iteration 100 / 2000: loss 0.368820\n",
      "iteration 200 / 2000: loss 0.315751\n",
      "iteration 300 / 2000: loss 0.255445\n",
      "iteration 400 / 2000: loss 0.287741\n",
      "iteration 500 / 2000: loss 0.294794\n",
      "iteration 600 / 2000: loss 0.317424\n",
      "iteration 700 / 2000: loss 0.294840\n",
      "iteration 800 / 2000: loss 0.355065\n",
      "iteration 900 / 2000: loss 0.213035\n",
      "iteration 1000 / 2000: loss 0.235048\n",
      "iteration 1100 / 2000: loss 0.253210\n",
      "iteration 1200 / 2000: loss 0.232898\n",
      "iteration 1300 / 2000: loss 0.195204\n",
      "iteration 1400 / 2000: loss 0.251289\n",
      "iteration 1500 / 2000: loss 0.319146\n",
      "iteration 1600 / 2000: loss 0.268529\n",
      "iteration 1700 / 2000: loss 0.290516\n",
      "iteration 1800 / 2000: loss 0.162093\n",
      "iteration 1900 / 2000: loss 0.236047\n",
      "Hidden Size: 60, Learning Rate: 0.00500005, LR Decay: 0.47250000000000003, Regularization: 0.4, Validation accuracy: 0.9516\n",
      "iteration 0 / 2000: loss 2.302594\n",
      "iteration 100 / 2000: loss 0.431693\n",
      "iteration 200 / 2000: loss 0.303853\n",
      "iteration 300 / 2000: loss 0.269512\n",
      "iteration 400 / 2000: loss 0.326493\n",
      "iteration 500 / 2000: loss 0.215308\n",
      "iteration 600 / 2000: loss 0.213188\n",
      "iteration 700 / 2000: loss 0.232388\n",
      "iteration 800 / 2000: loss 0.227193\n",
      "iteration 900 / 2000: loss 0.289971\n",
      "iteration 1000 / 2000: loss 0.183385\n",
      "iteration 1100 / 2000: loss 0.181942\n",
      "iteration 1200 / 2000: loss 0.226103\n",
      "iteration 1300 / 2000: loss 0.261003\n",
      "iteration 1400 / 2000: loss 0.301589\n",
      "iteration 1500 / 2000: loss 0.223741\n",
      "iteration 1600 / 2000: loss 0.264054\n",
      "iteration 1700 / 2000: loss 0.195470\n",
      "iteration 1800 / 2000: loss 0.356581\n",
      "iteration 1900 / 2000: loss 0.285367\n",
      "Hidden Size: 60, Learning Rate: 0.00500005, LR Decay: 0.3, Regularization: 0.0, Validation accuracy: 0.9356\n",
      "iteration 0 / 2000: loss 2.302616\n",
      "iteration 100 / 2000: loss 0.426039\n",
      "iteration 200 / 2000: loss 0.400818\n",
      "iteration 300 / 2000: loss 0.239553\n",
      "iteration 400 / 2000: loss 0.348139\n",
      "iteration 500 / 2000: loss 0.154287\n",
      "iteration 600 / 2000: loss 0.272020\n",
      "iteration 700 / 2000: loss 0.254199\n",
      "iteration 800 / 2000: loss 0.198476\n",
      "iteration 900 / 2000: loss 0.266155\n",
      "iteration 1000 / 2000: loss 0.187496\n",
      "iteration 1100 / 2000: loss 0.209371\n",
      "iteration 1200 / 2000: loss 0.256029\n",
      "iteration 1300 / 2000: loss 0.254026\n",
      "iteration 1400 / 2000: loss 0.269032\n",
      "iteration 1500 / 2000: loss 0.158939\n",
      "iteration 1600 / 2000: loss 0.221889\n",
      "iteration 1700 / 2000: loss 0.224660\n",
      "iteration 1800 / 2000: loss 0.302387\n",
      "iteration 1900 / 2000: loss 0.249889\n",
      "Hidden Size: 60, Learning Rate: 0.00500005, LR Decay: 0.3, Regularization: 0.1, Validation accuracy: 0.9362\n",
      "iteration 0 / 2000: loss 2.302631\n",
      "iteration 100 / 2000: loss 0.497701\n",
      "iteration 200 / 2000: loss 0.387121\n",
      "iteration 300 / 2000: loss 0.291836\n",
      "iteration 400 / 2000: loss 0.237398\n",
      "iteration 500 / 2000: loss 0.195889\n",
      "iteration 600 / 2000: loss 0.294313\n",
      "iteration 700 / 2000: loss 0.244420\n",
      "iteration 800 / 2000: loss 0.377945\n",
      "iteration 900 / 2000: loss 0.240952\n",
      "iteration 1000 / 2000: loss 0.258356\n",
      "iteration 1100 / 2000: loss 0.255215\n",
      "iteration 1200 / 2000: loss 0.204027\n",
      "iteration 1300 / 2000: loss 0.273071\n",
      "iteration 1400 / 2000: loss 0.282945\n",
      "iteration 1500 / 2000: loss 0.363092\n",
      "iteration 1600 / 2000: loss 0.308318\n",
      "iteration 1700 / 2000: loss 0.291478\n",
      "iteration 1800 / 2000: loss 0.293431\n",
      "iteration 1900 / 2000: loss 0.236630\n",
      "Hidden Size: 60, Learning Rate: 0.00500005, LR Decay: 0.3, Regularization: 0.2, Validation accuracy: 0.9354\n",
      "iteration 0 / 2000: loss 2.302661\n",
      "iteration 100 / 2000: loss 0.447449\n",
      "iteration 200 / 2000: loss 0.356885\n",
      "iteration 300 / 2000: loss 0.277892\n",
      "iteration 400 / 2000: loss 0.322207\n",
      "iteration 500 / 2000: loss 0.274183\n",
      "iteration 600 / 2000: loss 0.351883\n",
      "iteration 700 / 2000: loss 0.243581\n",
      "iteration 800 / 2000: loss 0.267758\n",
      "iteration 900 / 2000: loss 0.197808\n",
      "iteration 1000 / 2000: loss 0.282085\n",
      "iteration 1100 / 2000: loss 0.323351\n",
      "iteration 1200 / 2000: loss 0.239858\n",
      "iteration 1300 / 2000: loss 0.288972\n",
      "iteration 1400 / 2000: loss 0.242419\n",
      "iteration 1500 / 2000: loss 0.335506\n",
      "iteration 1600 / 2000: loss 0.417665\n",
      "iteration 1700 / 2000: loss 0.320085\n",
      "iteration 1800 / 2000: loss 0.306668\n",
      "iteration 1900 / 2000: loss 0.323505\n",
      "Hidden Size: 60, Learning Rate: 0.00500005, LR Decay: 0.3, Regularization: 0.30000000000000004, Validation accuracy: 0.9342\n",
      "iteration 0 / 2000: loss 2.302685\n",
      "iteration 100 / 2000: loss 0.531316\n",
      "iteration 200 / 2000: loss 0.372941\n",
      "iteration 300 / 2000: loss 0.270259\n",
      "iteration 400 / 2000: loss 0.370036\n",
      "iteration 500 / 2000: loss 0.309827\n",
      "iteration 600 / 2000: loss 0.379371\n",
      "iteration 700 / 2000: loss 0.318642\n",
      "iteration 800 / 2000: loss 0.383000\n",
      "iteration 900 / 2000: loss 0.278078\n",
      "iteration 1000 / 2000: loss 0.354645\n",
      "iteration 1100 / 2000: loss 0.225764\n",
      "iteration 1200 / 2000: loss 0.290921\n",
      "iteration 1300 / 2000: loss 0.293791\n",
      "iteration 1400 / 2000: loss 0.276708\n",
      "iteration 1500 / 2000: loss 0.334812\n",
      "iteration 1600 / 2000: loss 0.337089\n",
      "iteration 1700 / 2000: loss 0.329981\n",
      "iteration 1800 / 2000: loss 0.254405\n",
      "iteration 1900 / 2000: loss 0.394172\n",
      "Hidden Size: 60, Learning Rate: 0.00500005, LR Decay: 0.3, Regularization: 0.4, Validation accuracy: 0.933\n",
      "iteration 0 / 2000: loss 2.302579\n",
      "iteration 100 / 2000: loss 0.450817\n",
      "iteration 200 / 2000: loss 0.225172\n",
      "iteration 300 / 2000: loss 0.189468\n",
      "iteration 400 / 2000: loss 0.184845\n",
      "iteration 500 / 2000: loss 0.169679\n",
      "iteration 600 / 2000: loss 0.234325\n",
      "iteration 700 / 2000: loss 0.148343\n",
      "iteration 800 / 2000: loss 0.065790\n",
      "iteration 900 / 2000: loss 0.125528\n",
      "iteration 1000 / 2000: loss 0.128380\n",
      "iteration 1100 / 2000: loss 0.073900\n",
      "iteration 1200 / 2000: loss 0.062807\n",
      "iteration 1300 / 2000: loss 0.160312\n",
      "iteration 1400 / 2000: loss 0.039555\n",
      "iteration 1500 / 2000: loss 0.073568\n",
      "iteration 1600 / 2000: loss 0.065592\n",
      "iteration 1700 / 2000: loss 0.042870\n",
      "iteration 1800 / 2000: loss 0.069146\n",
      "iteration 1900 / 2000: loss 0.047457\n",
      "Hidden Size: 60, Learning Rate: 0.002500074999999999, LR Decay: 0.99, Regularization: 0.0, Validation accuracy: 0.9694\n",
      "iteration 0 / 2000: loss 2.302593\n",
      "iteration 100 / 2000: loss 0.533248\n",
      "iteration 200 / 2000: loss 0.441154\n",
      "iteration 300 / 2000: loss 0.254404\n",
      "iteration 400 / 2000: loss 0.228513\n",
      "iteration 500 / 2000: loss 0.216405\n",
      "iteration 600 / 2000: loss 0.126314\n",
      "iteration 700 / 2000: loss 0.188086\n",
      "iteration 800 / 2000: loss 0.121973\n",
      "iteration 900 / 2000: loss 0.135685\n",
      "iteration 1000 / 2000: loss 0.144586\n",
      "iteration 1100 / 2000: loss 0.119803\n",
      "iteration 1200 / 2000: loss 0.141159\n",
      "iteration 1300 / 2000: loss 0.188971\n",
      "iteration 1400 / 2000: loss 0.102443\n",
      "iteration 1500 / 2000: loss 0.100314\n",
      "iteration 1600 / 2000: loss 0.112585\n",
      "iteration 1700 / 2000: loss 0.157510\n",
      "iteration 1800 / 2000: loss 0.125262\n",
      "iteration 1900 / 2000: loss 0.105549\n",
      "Hidden Size: 60, Learning Rate: 0.002500074999999999, LR Decay: 0.99, Regularization: 0.1, Validation accuracy: 0.9714\n",
      "iteration 0 / 2000: loss 2.302642\n",
      "iteration 100 / 2000: loss 0.377290\n",
      "iteration 200 / 2000: loss 0.175788\n",
      "iteration 300 / 2000: loss 0.343415\n",
      "iteration 400 / 2000: loss 0.198990\n",
      "iteration 500 / 2000: loss 0.291532\n",
      "iteration 600 / 2000: loss 0.290183\n",
      "iteration 700 / 2000: loss 0.181796\n",
      "iteration 800 / 2000: loss 0.243436\n",
      "iteration 900 / 2000: loss 0.206771\n",
      "iteration 1000 / 2000: loss 0.202512\n",
      "iteration 1100 / 2000: loss 0.176192\n",
      "iteration 1200 / 2000: loss 0.160064\n",
      "iteration 1300 / 2000: loss 0.164900\n",
      "iteration 1400 / 2000: loss 0.155475\n",
      "iteration 1500 / 2000: loss 0.160108\n",
      "iteration 1600 / 2000: loss 0.150200\n",
      "iteration 1700 / 2000: loss 0.169679\n",
      "iteration 1800 / 2000: loss 0.153671\n",
      "iteration 1900 / 2000: loss 0.158134\n",
      "Hidden Size: 60, Learning Rate: 0.002500074999999999, LR Decay: 0.99, Regularization: 0.2, Validation accuracy: 0.9708\n",
      "iteration 0 / 2000: loss 2.302647\n",
      "iteration 100 / 2000: loss 0.404751\n",
      "iteration 200 / 2000: loss 0.279598\n",
      "iteration 300 / 2000: loss 0.318138\n",
      "iteration 400 / 2000: loss 0.286850\n",
      "iteration 500 / 2000: loss 0.228076\n",
      "iteration 600 / 2000: loss 0.192023\n",
      "iteration 700 / 2000: loss 0.228520\n",
      "iteration 800 / 2000: loss 0.274116\n",
      "iteration 900 / 2000: loss 0.205990\n",
      "iteration 1000 / 2000: loss 0.237568\n",
      "iteration 1100 / 2000: loss 0.206517\n",
      "iteration 1200 / 2000: loss 0.193199\n",
      "iteration 1300 / 2000: loss 0.239552\n",
      "iteration 1400 / 2000: loss 0.172018\n",
      "iteration 1500 / 2000: loss 0.164221\n",
      "iteration 1600 / 2000: loss 0.162290\n",
      "iteration 1700 / 2000: loss 0.167735\n",
      "iteration 1800 / 2000: loss 0.202666\n",
      "iteration 1900 / 2000: loss 0.167295\n",
      "Hidden Size: 60, Learning Rate: 0.002500074999999999, LR Decay: 0.99, Regularization: 0.30000000000000004, Validation accuracy: 0.9676\n",
      "iteration 0 / 2000: loss 2.302691\n",
      "iteration 100 / 2000: loss 0.371056\n",
      "iteration 200 / 2000: loss 0.293665\n",
      "iteration 300 / 2000: loss 0.234462\n",
      "iteration 400 / 2000: loss 0.297134\n",
      "iteration 500 / 2000: loss 0.270114\n",
      "iteration 600 / 2000: loss 0.331602\n",
      "iteration 700 / 2000: loss 0.311271\n",
      "iteration 800 / 2000: loss 0.200034\n",
      "iteration 900 / 2000: loss 0.239087\n",
      "iteration 1000 / 2000: loss 0.202317\n",
      "iteration 1100 / 2000: loss 0.217692\n",
      "iteration 1200 / 2000: loss 0.178601\n",
      "iteration 1300 / 2000: loss 0.192294\n",
      "iteration 1400 / 2000: loss 0.178111\n",
      "iteration 1500 / 2000: loss 0.208730\n",
      "iteration 1600 / 2000: loss 0.219687\n",
      "iteration 1700 / 2000: loss 0.186441\n",
      "iteration 1800 / 2000: loss 0.262084\n",
      "iteration 1900 / 2000: loss 0.193334\n",
      "Hidden Size: 60, Learning Rate: 0.002500074999999999, LR Decay: 0.99, Regularization: 0.4, Validation accuracy: 0.968\n",
      "iteration 0 / 2000: loss 2.302584\n",
      "iteration 100 / 2000: loss 0.345827\n",
      "iteration 200 / 2000: loss 0.374903\n",
      "iteration 300 / 2000: loss 0.280818\n",
      "iteration 400 / 2000: loss 0.224926\n",
      "iteration 500 / 2000: loss 0.251441\n",
      "iteration 600 / 2000: loss 0.174329\n",
      "iteration 700 / 2000: loss 0.157629\n",
      "iteration 800 / 2000: loss 0.142652\n",
      "iteration 900 / 2000: loss 0.128112\n",
      "iteration 1000 / 2000: loss 0.141194\n",
      "iteration 1100 / 2000: loss 0.139037\n",
      "iteration 1200 / 2000: loss 0.071050\n",
      "iteration 1300 / 2000: loss 0.101671\n",
      "iteration 1400 / 2000: loss 0.087256\n",
      "iteration 1500 / 2000: loss 0.162661\n",
      "iteration 1600 / 2000: loss 0.088585\n",
      "iteration 1700 / 2000: loss 0.067747\n",
      "iteration 1800 / 2000: loss 0.138770\n",
      "iteration 1900 / 2000: loss 0.142297\n",
      "Hidden Size: 60, Learning Rate: 0.002500074999999999, LR Decay: 0.8175, Regularization: 0.0, Validation accuracy: 0.9656\n",
      "iteration 0 / 2000: loss 2.302611\n",
      "iteration 100 / 2000: loss 0.392913\n",
      "iteration 200 / 2000: loss 0.269422\n",
      "iteration 300 / 2000: loss 0.341968\n",
      "iteration 400 / 2000: loss 0.179343\n",
      "iteration 500 / 2000: loss 0.277492\n",
      "iteration 600 / 2000: loss 0.213181\n",
      "iteration 700 / 2000: loss 0.146465\n",
      "iteration 800 / 2000: loss 0.161170\n",
      "iteration 900 / 2000: loss 0.196129\n",
      "iteration 1000 / 2000: loss 0.154704\n",
      "iteration 1100 / 2000: loss 0.149721\n",
      "iteration 1200 / 2000: loss 0.173474\n",
      "iteration 1300 / 2000: loss 0.145657\n",
      "iteration 1400 / 2000: loss 0.114287\n",
      "iteration 1500 / 2000: loss 0.161985\n",
      "iteration 1600 / 2000: loss 0.123806\n",
      "iteration 1700 / 2000: loss 0.132933\n",
      "iteration 1800 / 2000: loss 0.159711\n",
      "iteration 1900 / 2000: loss 0.173064\n",
      "Hidden Size: 60, Learning Rate: 0.002500074999999999, LR Decay: 0.8175, Regularization: 0.1, Validation accuracy: 0.9654\n",
      "iteration 0 / 2000: loss 2.302625\n",
      "iteration 100 / 2000: loss 0.350868\n",
      "iteration 200 / 2000: loss 0.340570\n",
      "iteration 300 / 2000: loss 0.272893\n",
      "iteration 400 / 2000: loss 0.234890\n",
      "iteration 500 / 2000: loss 0.160922\n",
      "iteration 600 / 2000: loss 0.210928\n",
      "iteration 700 / 2000: loss 0.222201\n",
      "iteration 800 / 2000: loss 0.169066\n",
      "iteration 900 / 2000: loss 0.182582\n",
      "iteration 1000 / 2000: loss 0.215039\n",
      "iteration 1100 / 2000: loss 0.194460\n",
      "iteration 1200 / 2000: loss 0.135204\n",
      "iteration 1300 / 2000: loss 0.202577\n",
      "iteration 1400 / 2000: loss 0.172619\n",
      "iteration 1500 / 2000: loss 0.165474\n",
      "iteration 1600 / 2000: loss 0.202640\n",
      "iteration 1700 / 2000: loss 0.177830\n",
      "iteration 1800 / 2000: loss 0.173378\n",
      "iteration 1900 / 2000: loss 0.239700\n",
      "Hidden Size: 60, Learning Rate: 0.002500074999999999, LR Decay: 0.8175, Regularization: 0.2, Validation accuracy: 0.9644\n",
      "iteration 0 / 2000: loss 2.302654\n",
      "iteration 100 / 2000: loss 0.510757\n",
      "iteration 200 / 2000: loss 0.284889\n",
      "iteration 300 / 2000: loss 0.231170\n",
      "iteration 400 / 2000: loss 0.197041\n",
      "iteration 500 / 2000: loss 0.218603\n",
      "iteration 600 / 2000: loss 0.211312\n",
      "iteration 700 / 2000: loss 0.185533\n",
      "iteration 800 / 2000: loss 0.229349\n",
      "iteration 900 / 2000: loss 0.235350\n",
      "iteration 1000 / 2000: loss 0.195108\n",
      "iteration 1100 / 2000: loss 0.208143\n",
      "iteration 1200 / 2000: loss 0.228831\n",
      "iteration 1300 / 2000: loss 0.228077\n",
      "iteration 1400 / 2000: loss 0.332916\n",
      "iteration 1500 / 2000: loss 0.222429\n",
      "iteration 1600 / 2000: loss 0.188687\n",
      "iteration 1700 / 2000: loss 0.167992\n",
      "iteration 1800 / 2000: loss 0.147224\n",
      "iteration 1900 / 2000: loss 0.207494\n",
      "Hidden Size: 60, Learning Rate: 0.002500074999999999, LR Decay: 0.8175, Regularization: 0.30000000000000004, Validation accuracy: 0.9638\n",
      "iteration 0 / 2000: loss 2.302689\n",
      "iteration 100 / 2000: loss 0.506598\n",
      "iteration 200 / 2000: loss 0.357157\n",
      "iteration 300 / 2000: loss 0.428333\n",
      "iteration 400 / 2000: loss 0.322106\n",
      "iteration 500 / 2000: loss 0.227093\n",
      "iteration 600 / 2000: loss 0.281672\n",
      "iteration 700 / 2000: loss 0.215223\n",
      "iteration 800 / 2000: loss 0.308121\n",
      "iteration 900 / 2000: loss 0.245069\n",
      "iteration 1000 / 2000: loss 0.309975\n",
      "iteration 1100 / 2000: loss 0.260412\n",
      "iteration 1200 / 2000: loss 0.242831\n",
      "iteration 1300 / 2000: loss 0.222116\n",
      "iteration 1400 / 2000: loss 0.216505\n",
      "iteration 1500 / 2000: loss 0.294383\n",
      "iteration 1600 / 2000: loss 0.274310\n",
      "iteration 1700 / 2000: loss 0.182754\n",
      "iteration 1800 / 2000: loss 0.221532\n",
      "iteration 1900 / 2000: loss 0.201493\n",
      "Hidden Size: 60, Learning Rate: 0.002500074999999999, LR Decay: 0.8175, Regularization: 0.4, Validation accuracy: 0.9624\n",
      "iteration 0 / 2000: loss 2.302574\n",
      "iteration 100 / 2000: loss 0.519969\n",
      "iteration 200 / 2000: loss 0.378260\n",
      "iteration 300 / 2000: loss 0.380293\n",
      "iteration 400 / 2000: loss 0.244440\n",
      "iteration 500 / 2000: loss 0.161351\n",
      "iteration 600 / 2000: loss 0.227447\n",
      "iteration 700 / 2000: loss 0.180460\n",
      "iteration 800 / 2000: loss 0.209221\n",
      "iteration 900 / 2000: loss 0.161588\n",
      "iteration 1000 / 2000: loss 0.101034\n",
      "iteration 1100 / 2000: loss 0.154296\n",
      "iteration 1200 / 2000: loss 0.190570\n",
      "iteration 1300 / 2000: loss 0.131112\n",
      "iteration 1400 / 2000: loss 0.145351\n",
      "iteration 1500 / 2000: loss 0.158180\n",
      "iteration 1600 / 2000: loss 0.115558\n",
      "iteration 1700 / 2000: loss 0.169321\n",
      "iteration 1800 / 2000: loss 0.104499\n",
      "iteration 1900 / 2000: loss 0.148116\n",
      "Hidden Size: 60, Learning Rate: 0.002500074999999999, LR Decay: 0.645, Regularization: 0.0, Validation accuracy: 0.9542\n",
      "iteration 0 / 2000: loss 2.302639\n",
      "iteration 100 / 2000: loss 0.409494\n",
      "iteration 200 / 2000: loss 0.305536\n",
      "iteration 300 / 2000: loss 0.228378\n",
      "iteration 400 / 2000: loss 0.344017\n",
      "iteration 500 / 2000: loss 0.210608\n",
      "iteration 600 / 2000: loss 0.155568\n",
      "iteration 700 / 2000: loss 0.220092\n",
      "iteration 800 / 2000: loss 0.174818\n",
      "iteration 900 / 2000: loss 0.205826\n",
      "iteration 1000 / 2000: loss 0.237637\n",
      "iteration 1100 / 2000: loss 0.116056\n",
      "iteration 1200 / 2000: loss 0.226885\n",
      "iteration 1300 / 2000: loss 0.236651\n",
      "iteration 1400 / 2000: loss 0.133101\n",
      "iteration 1500 / 2000: loss 0.255795\n",
      "iteration 1600 / 2000: loss 0.215930\n",
      "iteration 1700 / 2000: loss 0.190618\n",
      "iteration 1800 / 2000: loss 0.166831\n",
      "iteration 1900 / 2000: loss 0.139138\n",
      "Hidden Size: 60, Learning Rate: 0.002500074999999999, LR Decay: 0.645, Regularization: 0.1, Validation accuracy: 0.957\n",
      "iteration 0 / 2000: loss 2.302638\n",
      "iteration 100 / 2000: loss 0.459914\n",
      "iteration 200 / 2000: loss 0.439306\n",
      "iteration 300 / 2000: loss 0.364195\n",
      "iteration 400 / 2000: loss 0.282149\n",
      "iteration 500 / 2000: loss 0.279566\n",
      "iteration 600 / 2000: loss 0.219455\n",
      "iteration 700 / 2000: loss 0.219808\n",
      "iteration 800 / 2000: loss 0.244701\n",
      "iteration 900 / 2000: loss 0.169287\n",
      "iteration 1000 / 2000: loss 0.204437\n",
      "iteration 1100 / 2000: loss 0.268238\n",
      "iteration 1200 / 2000: loss 0.189829\n",
      "iteration 1300 / 2000: loss 0.196379\n",
      "iteration 1400 / 2000: loss 0.178541\n",
      "iteration 1500 / 2000: loss 0.194415\n",
      "iteration 1600 / 2000: loss 0.186505\n",
      "iteration 1700 / 2000: loss 0.235833\n",
      "iteration 1800 / 2000: loss 0.162427\n",
      "iteration 1900 / 2000: loss 0.267457\n",
      "Hidden Size: 60, Learning Rate: 0.002500074999999999, LR Decay: 0.645, Regularization: 0.2, Validation accuracy: 0.9534\n",
      "iteration 0 / 2000: loss 2.302664\n",
      "iteration 100 / 2000: loss 0.561175\n",
      "iteration 200 / 2000: loss 0.374989\n",
      "iteration 300 / 2000: loss 0.355800\n",
      "iteration 400 / 2000: loss 0.293665\n",
      "iteration 500 / 2000: loss 0.329240\n",
      "iteration 600 / 2000: loss 0.204929\n",
      "iteration 700 / 2000: loss 0.278494\n",
      "iteration 800 / 2000: loss 0.199307\n",
      "iteration 900 / 2000: loss 0.213495\n",
      "iteration 1000 / 2000: loss 0.327800\n",
      "iteration 1100 / 2000: loss 0.215129\n",
      "iteration 1200 / 2000: loss 0.234789\n",
      "iteration 1300 / 2000: loss 0.254377\n",
      "iteration 1400 / 2000: loss 0.263777\n",
      "iteration 1500 / 2000: loss 0.211056\n",
      "iteration 1600 / 2000: loss 0.283196\n",
      "iteration 1700 / 2000: loss 0.154891\n",
      "iteration 1800 / 2000: loss 0.218223\n",
      "iteration 1900 / 2000: loss 0.183998\n",
      "Hidden Size: 60, Learning Rate: 0.002500074999999999, LR Decay: 0.645, Regularization: 0.30000000000000004, Validation accuracy: 0.9532\n",
      "iteration 0 / 2000: loss 2.302658\n",
      "iteration 100 / 2000: loss 0.434997\n",
      "iteration 200 / 2000: loss 0.361516\n",
      "iteration 300 / 2000: loss 0.320414\n",
      "iteration 400 / 2000: loss 0.400555\n",
      "iteration 500 / 2000: loss 0.296835\n",
      "iteration 600 / 2000: loss 0.241289\n",
      "iteration 700 / 2000: loss 0.232647\n",
      "iteration 800 / 2000: loss 0.298977\n",
      "iteration 900 / 2000: loss 0.304266\n",
      "iteration 1000 / 2000: loss 0.277921\n",
      "iteration 1100 / 2000: loss 0.207840\n",
      "iteration 1200 / 2000: loss 0.231962\n",
      "iteration 1300 / 2000: loss 0.185960\n",
      "iteration 1400 / 2000: loss 0.220548\n",
      "iteration 1500 / 2000: loss 0.278412\n",
      "iteration 1600 / 2000: loss 0.209186\n",
      "iteration 1700 / 2000: loss 0.237903\n",
      "iteration 1800 / 2000: loss 0.206974\n",
      "iteration 1900 / 2000: loss 0.258839\n",
      "Hidden Size: 60, Learning Rate: 0.002500074999999999, LR Decay: 0.645, Regularization: 0.4, Validation accuracy: 0.9514\n",
      "iteration 0 / 2000: loss 2.302600\n",
      "iteration 100 / 2000: loss 0.562279\n",
      "iteration 200 / 2000: loss 0.330190\n",
      "iteration 300 / 2000: loss 0.257675\n",
      "iteration 400 / 2000: loss 0.335195\n",
      "iteration 500 / 2000: loss 0.279457\n",
      "iteration 600 / 2000: loss 0.304087\n",
      "iteration 700 / 2000: loss 0.283964\n",
      "iteration 800 / 2000: loss 0.152709\n",
      "iteration 900 / 2000: loss 0.246725\n",
      "iteration 1000 / 2000: loss 0.248608\n",
      "iteration 1100 / 2000: loss 0.334179\n",
      "iteration 1200 / 2000: loss 0.173069\n",
      "iteration 1300 / 2000: loss 0.166860\n",
      "iteration 1400 / 2000: loss 0.199189\n",
      "iteration 1500 / 2000: loss 0.247340\n",
      "iteration 1600 / 2000: loss 0.309121\n",
      "iteration 1700 / 2000: loss 0.214175\n",
      "iteration 1800 / 2000: loss 0.268938\n",
      "iteration 1900 / 2000: loss 0.143955\n",
      "Hidden Size: 60, Learning Rate: 0.002500074999999999, LR Decay: 0.47250000000000003, Regularization: 0.0, Validation accuracy: 0.936\n",
      "iteration 0 / 2000: loss 2.302604\n",
      "iteration 100 / 2000: loss 0.554969\n",
      "iteration 200 / 2000: loss 0.417007\n",
      "iteration 300 / 2000: loss 0.282124\n",
      "iteration 400 / 2000: loss 0.250460\n",
      "iteration 500 / 2000: loss 0.293048\n",
      "iteration 600 / 2000: loss 0.272862\n",
      "iteration 700 / 2000: loss 0.285879\n",
      "iteration 800 / 2000: loss 0.318377\n",
      "iteration 900 / 2000: loss 0.345163\n",
      "iteration 1000 / 2000: loss 0.282780\n",
      "iteration 1100 / 2000: loss 0.480626\n",
      "iteration 1200 / 2000: loss 0.328990\n",
      "iteration 1300 / 2000: loss 0.162100\n",
      "iteration 1400 / 2000: loss 0.306766\n",
      "iteration 1500 / 2000: loss 0.323229\n",
      "iteration 1600 / 2000: loss 0.258182\n",
      "iteration 1700 / 2000: loss 0.244466\n",
      "iteration 1800 / 2000: loss 0.209499\n",
      "iteration 1900 / 2000: loss 0.306184\n",
      "Hidden Size: 60, Learning Rate: 0.002500074999999999, LR Decay: 0.47250000000000003, Regularization: 0.1, Validation accuracy: 0.9342\n",
      "iteration 0 / 2000: loss 2.302605\n",
      "iteration 100 / 2000: loss 0.683210\n",
      "iteration 200 / 2000: loss 0.385951\n",
      "iteration 300 / 2000: loss 0.340825\n",
      "iteration 400 / 2000: loss 0.301515\n",
      "iteration 500 / 2000: loss 0.245793\n",
      "iteration 600 / 2000: loss 0.240642\n",
      "iteration 700 / 2000: loss 0.230933\n",
      "iteration 800 / 2000: loss 0.284960\n",
      "iteration 900 / 2000: loss 0.257545\n",
      "iteration 1000 / 2000: loss 0.214361\n",
      "iteration 1100 / 2000: loss 0.257073\n",
      "iteration 1200 / 2000: loss 0.230734\n",
      "iteration 1300 / 2000: loss 0.308422\n",
      "iteration 1400 / 2000: loss 0.248012\n",
      "iteration 1500 / 2000: loss 0.293844\n",
      "iteration 1600 / 2000: loss 0.215910\n",
      "iteration 1700 / 2000: loss 0.351402\n",
      "iteration 1800 / 2000: loss 0.289521\n",
      "iteration 1900 / 2000: loss 0.263860\n",
      "Hidden Size: 60, Learning Rate: 0.002500074999999999, LR Decay: 0.47250000000000003, Regularization: 0.2, Validation accuracy: 0.9344\n",
      "iteration 0 / 2000: loss 2.302666\n",
      "iteration 100 / 2000: loss 0.693962\n",
      "iteration 200 / 2000: loss 0.323721\n",
      "iteration 300 / 2000: loss 0.347901\n",
      "iteration 400 / 2000: loss 0.232871\n",
      "iteration 500 / 2000: loss 0.323384\n",
      "iteration 600 / 2000: loss 0.286518\n",
      "iteration 700 / 2000: loss 0.284516\n",
      "iteration 800 / 2000: loss 0.240053\n",
      "iteration 900 / 2000: loss 0.269578\n",
      "iteration 1000 / 2000: loss 0.329200\n",
      "iteration 1100 / 2000: loss 0.320271\n",
      "iteration 1200 / 2000: loss 0.285228\n",
      "iteration 1300 / 2000: loss 0.295515\n",
      "iteration 1400 / 2000: loss 0.321226\n",
      "iteration 1500 / 2000: loss 0.320132\n",
      "iteration 1600 / 2000: loss 0.276315\n",
      "iteration 1700 / 2000: loss 0.342242\n",
      "iteration 1800 / 2000: loss 0.232313\n",
      "iteration 1900 / 2000: loss 0.261086\n",
      "Hidden Size: 60, Learning Rate: 0.002500074999999999, LR Decay: 0.47250000000000003, Regularization: 0.30000000000000004, Validation accuracy: 0.9326\n",
      "iteration 0 / 2000: loss 2.302664\n",
      "iteration 100 / 2000: loss 0.591958\n",
      "iteration 200 / 2000: loss 0.514550\n",
      "iteration 300 / 2000: loss 0.372355\n",
      "iteration 400 / 2000: loss 0.358852\n",
      "iteration 500 / 2000: loss 0.332482\n",
      "iteration 600 / 2000: loss 0.263742\n",
      "iteration 700 / 2000: loss 0.256257\n",
      "iteration 800 / 2000: loss 0.243542\n",
      "iteration 900 / 2000: loss 0.198493\n",
      "iteration 1000 / 2000: loss 0.258296\n",
      "iteration 1100 / 2000: loss 0.330391\n",
      "iteration 1200 / 2000: loss 0.298290\n",
      "iteration 1300 / 2000: loss 0.346361\n",
      "iteration 1400 / 2000: loss 0.302805\n",
      "iteration 1500 / 2000: loss 0.267589\n",
      "iteration 1600 / 2000: loss 0.222581\n",
      "iteration 1700 / 2000: loss 0.246495\n",
      "iteration 1800 / 2000: loss 0.303122\n",
      "iteration 1900 / 2000: loss 0.299925\n",
      "Hidden Size: 60, Learning Rate: 0.002500074999999999, LR Decay: 0.47250000000000003, Regularization: 0.4, Validation accuracy: 0.9338\n",
      "iteration 0 / 2000: loss 2.302608\n",
      "iteration 100 / 2000: loss 1.276160\n",
      "iteration 200 / 2000: loss 0.430166\n",
      "iteration 300 / 2000: loss 0.319723\n",
      "iteration 400 / 2000: loss 0.290900\n",
      "iteration 500 / 2000: loss 0.377011\n",
      "iteration 600 / 2000: loss 0.307278\n",
      "iteration 700 / 2000: loss 0.330837\n",
      "iteration 800 / 2000: loss 0.266727\n",
      "iteration 900 / 2000: loss 0.310535\n",
      "iteration 1000 / 2000: loss 0.336844\n",
      "iteration 1100 / 2000: loss 0.323237\n",
      "iteration 1200 / 2000: loss 0.174327\n",
      "iteration 1300 / 2000: loss 0.298761\n",
      "iteration 1400 / 2000: loss 0.253424\n",
      "iteration 1500 / 2000: loss 0.295392\n",
      "iteration 1600 / 2000: loss 0.306784\n",
      "iteration 1700 / 2000: loss 0.339560\n",
      "iteration 1800 / 2000: loss 0.382788\n",
      "iteration 1900 / 2000: loss 0.295058\n",
      "Hidden Size: 60, Learning Rate: 0.002500074999999999, LR Decay: 0.3, Regularization: 0.0, Validation accuracy: 0.9164\n",
      "iteration 0 / 2000: loss 2.302609\n",
      "iteration 100 / 2000: loss 1.268417\n",
      "iteration 200 / 2000: loss 0.429611\n",
      "iteration 300 / 2000: loss 0.262191\n",
      "iteration 400 / 2000: loss 0.311153\n",
      "iteration 500 / 2000: loss 0.320657\n",
      "iteration 600 / 2000: loss 0.392095\n",
      "iteration 700 / 2000: loss 0.314794\n",
      "iteration 800 / 2000: loss 0.392738\n",
      "iteration 900 / 2000: loss 0.342624\n",
      "iteration 1000 / 2000: loss 0.368144\n",
      "iteration 1100 / 2000: loss 0.358353\n",
      "iteration 1200 / 2000: loss 0.257994\n",
      "iteration 1300 / 2000: loss 0.387432\n",
      "iteration 1400 / 2000: loss 0.225581\n",
      "iteration 1500 / 2000: loss 0.378340\n",
      "iteration 1600 / 2000: loss 0.207308\n",
      "iteration 1700 / 2000: loss 0.328931\n",
      "iteration 1800 / 2000: loss 0.250377\n",
      "iteration 1900 / 2000: loss 0.292752\n",
      "Hidden Size: 60, Learning Rate: 0.002500074999999999, LR Decay: 0.3, Regularization: 0.1, Validation accuracy: 0.9134\n",
      "iteration 0 / 2000: loss 2.302655\n",
      "iteration 100 / 2000: loss 1.271019\n",
      "iteration 200 / 2000: loss 0.466907\n",
      "iteration 300 / 2000: loss 0.422374\n",
      "iteration 400 / 2000: loss 0.438517\n",
      "iteration 500 / 2000: loss 0.440421\n",
      "iteration 600 / 2000: loss 0.405335\n",
      "iteration 700 / 2000: loss 0.391630\n",
      "iteration 800 / 2000: loss 0.379422\n",
      "iteration 900 / 2000: loss 0.343526\n",
      "iteration 1000 / 2000: loss 0.306420\n",
      "iteration 1100 / 2000: loss 0.323197\n",
      "iteration 1200 / 2000: loss 0.382283\n",
      "iteration 1300 / 2000: loss 0.360989\n",
      "iteration 1400 / 2000: loss 0.308063\n",
      "iteration 1500 / 2000: loss 0.432075\n",
      "iteration 1600 / 2000: loss 0.407915\n",
      "iteration 1700 / 2000: loss 0.337911\n",
      "iteration 1800 / 2000: loss 0.364838\n",
      "iteration 1900 / 2000: loss 0.448354\n",
      "Hidden Size: 60, Learning Rate: 0.002500074999999999, LR Decay: 0.3, Regularization: 0.2, Validation accuracy: 0.9134\n",
      "iteration 0 / 2000: loss 2.302638\n",
      "iteration 100 / 2000: loss 1.294702\n",
      "iteration 200 / 2000: loss 0.502070\n",
      "iteration 300 / 2000: loss 0.376899\n",
      "iteration 400 / 2000: loss 0.450133\n",
      "iteration 500 / 2000: loss 0.412751\n",
      "iteration 600 / 2000: loss 0.428963\n",
      "iteration 700 / 2000: loss 0.338699\n",
      "iteration 800 / 2000: loss 0.325494\n",
      "iteration 900 / 2000: loss 0.386831\n",
      "iteration 1000 / 2000: loss 0.429168\n",
      "iteration 1100 / 2000: loss 0.333161\n",
      "iteration 1200 / 2000: loss 0.361172\n",
      "iteration 1300 / 2000: loss 0.412554\n",
      "iteration 1400 / 2000: loss 0.445534\n",
      "iteration 1500 / 2000: loss 0.356160\n",
      "iteration 1600 / 2000: loss 0.470436\n",
      "iteration 1700 / 2000: loss 0.279317\n",
      "iteration 1800 / 2000: loss 0.284597\n",
      "iteration 1900 / 2000: loss 0.404546\n",
      "Hidden Size: 60, Learning Rate: 0.002500074999999999, LR Decay: 0.3, Regularization: 0.30000000000000004, Validation accuracy: 0.915\n",
      "iteration 0 / 2000: loss 2.302689\n",
      "iteration 100 / 2000: loss 1.327178\n",
      "iteration 200 / 2000: loss 0.579454\n",
      "iteration 300 / 2000: loss 0.292329\n",
      "iteration 400 / 2000: loss 0.274224\n",
      "iteration 500 / 2000: loss 0.350466\n",
      "iteration 600 / 2000: loss 0.432800\n",
      "iteration 700 / 2000: loss 0.282861\n",
      "iteration 800 / 2000: loss 0.389490\n",
      "iteration 900 / 2000: loss 0.334283\n",
      "iteration 1000 / 2000: loss 0.346962\n",
      "iteration 1100 / 2000: loss 0.444980\n",
      "iteration 1200 / 2000: loss 0.298189\n",
      "iteration 1300 / 2000: loss 0.338117\n",
      "iteration 1400 / 2000: loss 0.352410\n",
      "iteration 1500 / 2000: loss 0.329838\n",
      "iteration 1600 / 2000: loss 0.329815\n",
      "iteration 1700 / 2000: loss 0.418980\n",
      "iteration 1800 / 2000: loss 0.434873\n",
      "iteration 1900 / 2000: loss 0.387333\n",
      "Hidden Size: 60, Learning Rate: 0.002500074999999999, LR Decay: 0.3, Regularization: 0.4, Validation accuracy: 0.9154\n",
      "iteration 0 / 2000: loss 2.302580\n",
      "iteration 100 / 2000: loss 2.302582\n",
      "iteration 200 / 2000: loss 2.302588\n",
      "iteration 300 / 2000: loss 2.302573\n",
      "iteration 400 / 2000: loss 2.302580\n",
      "iteration 500 / 2000: loss 2.302574\n",
      "iteration 600 / 2000: loss 2.302584\n",
      "iteration 700 / 2000: loss 2.302591\n",
      "iteration 800 / 2000: loss 2.302585\n",
      "iteration 900 / 2000: loss 2.302579\n",
      "iteration 1000 / 2000: loss 2.302586\n",
      "iteration 1100 / 2000: loss 2.302575\n",
      "iteration 1200 / 2000: loss 2.302580\n",
      "iteration 1300 / 2000: loss 2.302582\n",
      "iteration 1400 / 2000: loss 2.302586\n",
      "iteration 1500 / 2000: loss 2.302574\n",
      "iteration 1600 / 2000: loss 2.302579\n",
      "iteration 1700 / 2000: loss 2.302585\n",
      "iteration 1800 / 2000: loss 2.302583\n",
      "iteration 1900 / 2000: loss 2.302584\n",
      "Hidden Size: 60, Learning Rate: 1e-07, LR Decay: 0.99, Regularization: 0.0, Validation accuracy: 0.0984\n",
      "iteration 0 / 2000: loss 2.302613\n",
      "iteration 100 / 2000: loss 2.302611\n",
      "iteration 200 / 2000: loss 2.302627\n",
      "iteration 300 / 2000: loss 2.302613\n",
      "iteration 400 / 2000: loss 2.302619\n",
      "iteration 500 / 2000: loss 2.302612\n",
      "iteration 600 / 2000: loss 2.302613\n",
      "iteration 700 / 2000: loss 2.302622\n",
      "iteration 800 / 2000: loss 2.302619\n",
      "iteration 900 / 2000: loss 2.302610\n",
      "iteration 1000 / 2000: loss 2.302619\n",
      "iteration 1100 / 2000: loss 2.302608\n",
      "iteration 1200 / 2000: loss 2.302626\n",
      "iteration 1300 / 2000: loss 2.302609\n",
      "iteration 1400 / 2000: loss 2.302609\n",
      "iteration 1500 / 2000: loss 2.302616\n",
      "iteration 1600 / 2000: loss 2.302617\n",
      "iteration 1700 / 2000: loss 2.302605\n",
      "iteration 1800 / 2000: loss 2.302600\n",
      "iteration 1900 / 2000: loss 2.302616\n",
      "Hidden Size: 60, Learning Rate: 1e-07, LR Decay: 0.99, Regularization: 0.1, Validation accuracy: 0.0736\n",
      "iteration 0 / 2000: loss 2.302643\n",
      "iteration 100 / 2000: loss 2.302639\n",
      "iteration 200 / 2000: loss 2.302644\n",
      "iteration 300 / 2000: loss 2.302643\n",
      "iteration 400 / 2000: loss 2.302627\n",
      "iteration 500 / 2000: loss 2.302646\n",
      "iteration 600 / 2000: loss 2.302634\n",
      "iteration 700 / 2000: loss 2.302642\n",
      "iteration 800 / 2000: loss 2.302629\n",
      "iteration 900 / 2000: loss 2.302645\n",
      "iteration 1000 / 2000: loss 2.302629\n",
      "iteration 1100 / 2000: loss 2.302643\n",
      "iteration 1200 / 2000: loss 2.302631\n",
      "iteration 1300 / 2000: loss 2.302649\n",
      "iteration 1400 / 2000: loss 2.302641\n",
      "iteration 1500 / 2000: loss 2.302636\n",
      "iteration 1600 / 2000: loss 2.302633\n",
      "iteration 1700 / 2000: loss 2.302636\n",
      "iteration 1800 / 2000: loss 2.302642\n",
      "iteration 1900 / 2000: loss 2.302630\n",
      "Hidden Size: 60, Learning Rate: 1e-07, LR Decay: 0.99, Regularization: 0.2, Validation accuracy: 0.1216\n",
      "iteration 0 / 2000: loss 2.302665\n",
      "iteration 100 / 2000: loss 2.302658\n",
      "iteration 200 / 2000: loss 2.302661\n",
      "iteration 300 / 2000: loss 2.302656\n",
      "iteration 400 / 2000: loss 2.302670\n",
      "iteration 500 / 2000: loss 2.302666\n",
      "iteration 600 / 2000: loss 2.302666\n",
      "iteration 700 / 2000: loss 2.302652\n",
      "iteration 800 / 2000: loss 2.302671\n",
      "iteration 900 / 2000: loss 2.302666\n",
      "iteration 1000 / 2000: loss 2.302651\n",
      "iteration 1100 / 2000: loss 2.302676\n",
      "iteration 1200 / 2000: loss 2.302672\n",
      "iteration 1300 / 2000: loss 2.302663\n",
      "iteration 1400 / 2000: loss 2.302669\n",
      "iteration 1500 / 2000: loss 2.302657\n",
      "iteration 1600 / 2000: loss 2.302665\n",
      "iteration 1700 / 2000: loss 2.302665\n",
      "iteration 1800 / 2000: loss 2.302653\n",
      "iteration 1900 / 2000: loss 2.302659\n",
      "Hidden Size: 60, Learning Rate: 1e-07, LR Decay: 0.99, Regularization: 0.30000000000000004, Validation accuracy: 0.0994\n",
      "iteration 0 / 2000: loss 2.302662\n",
      "iteration 100 / 2000: loss 2.302652\n",
      "iteration 200 / 2000: loss 2.302671\n",
      "iteration 300 / 2000: loss 2.302657\n",
      "iteration 400 / 2000: loss 2.302656\n",
      "iteration 500 / 2000: loss 2.302652\n",
      "iteration 600 / 2000: loss 2.302668\n",
      "iteration 700 / 2000: loss 2.302667\n",
      "iteration 800 / 2000: loss 2.302667\n",
      "iteration 900 / 2000: loss 2.302645\n",
      "iteration 1000 / 2000: loss 2.302665\n",
      "iteration 1100 / 2000: loss 2.302649\n",
      "iteration 1200 / 2000: loss 2.302670\n",
      "iteration 1300 / 2000: loss 2.302645\n",
      "iteration 1400 / 2000: loss 2.302661\n",
      "iteration 1500 / 2000: loss 2.302660\n",
      "iteration 1600 / 2000: loss 2.302659\n",
      "iteration 1700 / 2000: loss 2.302642\n",
      "iteration 1800 / 2000: loss 2.302654\n",
      "iteration 1900 / 2000: loss 2.302643\n",
      "Hidden Size: 60, Learning Rate: 1e-07, LR Decay: 0.99, Regularization: 0.4, Validation accuracy: 0.1694\n",
      "iteration 0 / 2000: loss 2.302557\n",
      "iteration 100 / 2000: loss 2.302567\n",
      "iteration 200 / 2000: loss 2.302562\n",
      "iteration 300 / 2000: loss 2.302561\n",
      "iteration 400 / 2000: loss 2.302559\n",
      "iteration 500 / 2000: loss 2.302568\n",
      "iteration 600 / 2000: loss 2.302561\n",
      "iteration 700 / 2000: loss 2.302559\n",
      "iteration 800 / 2000: loss 2.302575\n",
      "iteration 900 / 2000: loss 2.302564\n",
      "iteration 1000 / 2000: loss 2.302558\n",
      "iteration 1100 / 2000: loss 2.302560\n",
      "iteration 1200 / 2000: loss 2.302562\n",
      "iteration 1300 / 2000: loss 2.302560\n",
      "iteration 1400 / 2000: loss 2.302560\n",
      "iteration 1500 / 2000: loss 2.302559\n",
      "iteration 1600 / 2000: loss 2.302566\n",
      "iteration 1700 / 2000: loss 2.302561\n",
      "iteration 1800 / 2000: loss 2.302553\n",
      "iteration 1900 / 2000: loss 2.302562\n",
      "Hidden Size: 60, Learning Rate: 1e-07, LR Decay: 0.8175, Regularization: 0.0, Validation accuracy: 0.1358\n",
      "iteration 0 / 2000: loss 2.302596\n",
      "iteration 100 / 2000: loss 2.302596\n",
      "iteration 200 / 2000: loss 2.302595\n",
      "iteration 300 / 2000: loss 2.302596\n",
      "iteration 400 / 2000: loss 2.302605\n",
      "iteration 500 / 2000: loss 2.302597\n",
      "iteration 600 / 2000: loss 2.302602\n",
      "iteration 700 / 2000: loss 2.302596\n",
      "iteration 800 / 2000: loss 2.302596\n",
      "iteration 900 / 2000: loss 2.302602\n",
      "iteration 1000 / 2000: loss 2.302613\n",
      "iteration 1100 / 2000: loss 2.302598\n",
      "iteration 1200 / 2000: loss 2.302595\n",
      "iteration 1300 / 2000: loss 2.302590\n",
      "iteration 1400 / 2000: loss 2.302601\n",
      "iteration 1500 / 2000: loss 2.302597\n",
      "iteration 1600 / 2000: loss 2.302604\n",
      "iteration 1700 / 2000: loss 2.302599\n",
      "iteration 1800 / 2000: loss 2.302600\n",
      "iteration 1900 / 2000: loss 2.302594\n",
      "Hidden Size: 60, Learning Rate: 1e-07, LR Decay: 0.8175, Regularization: 0.1, Validation accuracy: 0.109\n",
      "iteration 0 / 2000: loss 2.302636\n",
      "iteration 100 / 2000: loss 2.302631\n",
      "iteration 200 / 2000: loss 2.302634\n",
      "iteration 300 / 2000: loss 2.302635\n",
      "iteration 400 / 2000: loss 2.302646\n",
      "iteration 500 / 2000: loss 2.302642\n",
      "iteration 600 / 2000: loss 2.302638\n",
      "iteration 700 / 2000: loss 2.302626\n",
      "iteration 800 / 2000: loss 2.302651\n",
      "iteration 900 / 2000: loss 2.302650\n",
      "iteration 1000 / 2000: loss 2.302627\n",
      "iteration 1100 / 2000: loss 2.302625\n",
      "iteration 1200 / 2000: loss 2.302648\n",
      "iteration 1300 / 2000: loss 2.302620\n",
      "iteration 1400 / 2000: loss 2.302645\n",
      "iteration 1500 / 2000: loss 2.302643\n",
      "iteration 1600 / 2000: loss 2.302633\n",
      "iteration 1700 / 2000: loss 2.302645\n",
      "iteration 1800 / 2000: loss 2.302636\n",
      "iteration 1900 / 2000: loss 2.302632\n",
      "Hidden Size: 60, Learning Rate: 1e-07, LR Decay: 0.8175, Regularization: 0.2, Validation accuracy: 0.1072\n",
      "iteration 0 / 2000: loss 2.302680\n",
      "iteration 100 / 2000: loss 2.302684\n",
      "iteration 200 / 2000: loss 2.302679\n",
      "iteration 300 / 2000: loss 2.302678\n",
      "iteration 400 / 2000: loss 2.302660\n",
      "iteration 500 / 2000: loss 2.302675\n",
      "iteration 600 / 2000: loss 2.302667\n",
      "iteration 700 / 2000: loss 2.302662\n",
      "iteration 800 / 2000: loss 2.302668\n",
      "iteration 900 / 2000: loss 2.302669\n",
      "iteration 1000 / 2000: loss 2.302669\n",
      "iteration 1100 / 2000: loss 2.302672\n",
      "iteration 1200 / 2000: loss 2.302676\n",
      "iteration 1300 / 2000: loss 2.302677\n",
      "iteration 1400 / 2000: loss 2.302666\n",
      "iteration 1500 / 2000: loss 2.302676\n",
      "iteration 1600 / 2000: loss 2.302670\n",
      "iteration 1700 / 2000: loss 2.302672\n",
      "iteration 1800 / 2000: loss 2.302670\n",
      "iteration 1900 / 2000: loss 2.302669\n",
      "Hidden Size: 60, Learning Rate: 1e-07, LR Decay: 0.8175, Regularization: 0.30000000000000004, Validation accuracy: 0.0644\n",
      "iteration 0 / 2000: loss 2.302691\n",
      "iteration 100 / 2000: loss 2.302679\n",
      "iteration 200 / 2000: loss 2.302679\n",
      "iteration 300 / 2000: loss 2.302673\n",
      "iteration 400 / 2000: loss 2.302675\n",
      "iteration 500 / 2000: loss 2.302669\n",
      "iteration 600 / 2000: loss 2.302676\n",
      "iteration 700 / 2000: loss 2.302673\n",
      "iteration 800 / 2000: loss 2.302664\n",
      "iteration 900 / 2000: loss 2.302678\n",
      "iteration 1000 / 2000: loss 2.302669\n",
      "iteration 1100 / 2000: loss 2.302671\n",
      "iteration 1200 / 2000: loss 2.302672\n",
      "iteration 1300 / 2000: loss 2.302668\n",
      "iteration 1400 / 2000: loss 2.302676\n",
      "iteration 1500 / 2000: loss 2.302677\n",
      "iteration 1600 / 2000: loss 2.302669\n",
      "iteration 1700 / 2000: loss 2.302674\n",
      "iteration 1800 / 2000: loss 2.302678\n",
      "iteration 1900 / 2000: loss 2.302680\n",
      "Hidden Size: 60, Learning Rate: 1e-07, LR Decay: 0.8175, Regularization: 0.4, Validation accuracy: 0.107\n",
      "iteration 0 / 2000: loss 2.302579\n",
      "iteration 100 / 2000: loss 2.302571\n",
      "iteration 200 / 2000: loss 2.302562\n",
      "iteration 300 / 2000: loss 2.302558\n",
      "iteration 400 / 2000: loss 2.302583\n",
      "iteration 500 / 2000: loss 2.302575\n",
      "iteration 600 / 2000: loss 2.302566\n",
      "iteration 700 / 2000: loss 2.302555\n",
      "iteration 800 / 2000: loss 2.302584\n",
      "iteration 900 / 2000: loss 2.302563\n",
      "iteration 1000 / 2000: loss 2.302570\n",
      "iteration 1100 / 2000: loss 2.302564\n",
      "iteration 1200 / 2000: loss 2.302569\n",
      "iteration 1300 / 2000: loss 2.302565\n",
      "iteration 1400 / 2000: loss 2.302561\n",
      "iteration 1500 / 2000: loss 2.302576\n",
      "iteration 1600 / 2000: loss 2.302552\n",
      "iteration 1700 / 2000: loss 2.302574\n",
      "iteration 1800 / 2000: loss 2.302578\n",
      "iteration 1900 / 2000: loss 2.302581\n",
      "Hidden Size: 60, Learning Rate: 1e-07, LR Decay: 0.645, Regularization: 0.0, Validation accuracy: 0.1538\n",
      "iteration 0 / 2000: loss 2.302616\n",
      "iteration 100 / 2000: loss 2.302612\n",
      "iteration 200 / 2000: loss 2.302604\n",
      "iteration 300 / 2000: loss 2.302611\n",
      "iteration 400 / 2000: loss 2.302600\n",
      "iteration 500 / 2000: loss 2.302601\n",
      "iteration 600 / 2000: loss 2.302609\n",
      "iteration 700 / 2000: loss 2.302613\n",
      "iteration 800 / 2000: loss 2.302623\n",
      "iteration 900 / 2000: loss 2.302613\n",
      "iteration 1000 / 2000: loss 2.302614\n",
      "iteration 1100 / 2000: loss 2.302615\n",
      "iteration 1200 / 2000: loss 2.302615\n",
      "iteration 1300 / 2000: loss 2.302609\n",
      "iteration 1400 / 2000: loss 2.302610\n",
      "iteration 1500 / 2000: loss 2.302601\n",
      "iteration 1600 / 2000: loss 2.302614\n",
      "iteration 1700 / 2000: loss 2.302620\n",
      "iteration 1800 / 2000: loss 2.302610\n",
      "iteration 1900 / 2000: loss 2.302610\n",
      "Hidden Size: 60, Learning Rate: 1e-07, LR Decay: 0.645, Regularization: 0.1, Validation accuracy: 0.1432\n",
      "iteration 0 / 2000: loss 2.302628\n",
      "iteration 100 / 2000: loss 2.302627\n",
      "iteration 200 / 2000: loss 2.302622\n",
      "iteration 300 / 2000: loss 2.302632\n",
      "iteration 400 / 2000: loss 2.302631\n",
      "iteration 500 / 2000: loss 2.302620\n",
      "iteration 600 / 2000: loss 2.302626\n",
      "iteration 700 / 2000: loss 2.302623\n",
      "iteration 800 / 2000: loss 2.302628\n",
      "iteration 900 / 2000: loss 2.302636\n",
      "iteration 1000 / 2000: loss 2.302618\n",
      "iteration 1100 / 2000: loss 2.302631\n",
      "iteration 1200 / 2000: loss 2.302627\n",
      "iteration 1300 / 2000: loss 2.302632\n",
      "iteration 1400 / 2000: loss 2.302630\n",
      "iteration 1500 / 2000: loss 2.302620\n",
      "iteration 1600 / 2000: loss 2.302627\n",
      "iteration 1700 / 2000: loss 2.302626\n",
      "iteration 1800 / 2000: loss 2.302624\n",
      "iteration 1900 / 2000: loss 2.302625\n",
      "Hidden Size: 60, Learning Rate: 1e-07, LR Decay: 0.645, Regularization: 0.2, Validation accuracy: 0.1042\n",
      "iteration 0 / 2000: loss 2.302663\n",
      "iteration 100 / 2000: loss 2.302679\n",
      "iteration 200 / 2000: loss 2.302668\n",
      "iteration 300 / 2000: loss 2.302664\n",
      "iteration 400 / 2000: loss 2.302663\n",
      "iteration 500 / 2000: loss 2.302669\n",
      "iteration 600 / 2000: loss 2.302677\n",
      "iteration 700 / 2000: loss 2.302668\n",
      "iteration 800 / 2000: loss 2.302677\n",
      "iteration 900 / 2000: loss 2.302672\n",
      "iteration 1000 / 2000: loss 2.302662\n",
      "iteration 1100 / 2000: loss 2.302655\n",
      "iteration 1200 / 2000: loss 2.302660\n",
      "iteration 1300 / 2000: loss 2.302669\n",
      "iteration 1400 / 2000: loss 2.302674\n",
      "iteration 1500 / 2000: loss 2.302662\n",
      "iteration 1600 / 2000: loss 2.302682\n",
      "iteration 1700 / 2000: loss 2.302669\n",
      "iteration 1800 / 2000: loss 2.302672\n",
      "iteration 1900 / 2000: loss 2.302675\n",
      "Hidden Size: 60, Learning Rate: 1e-07, LR Decay: 0.645, Regularization: 0.30000000000000004, Validation accuracy: 0.067\n",
      "iteration 0 / 2000: loss 2.302686\n",
      "iteration 100 / 2000: loss 2.302676\n",
      "iteration 200 / 2000: loss 2.302676\n",
      "iteration 300 / 2000: loss 2.302682\n",
      "iteration 400 / 2000: loss 2.302675\n",
      "iteration 500 / 2000: loss 2.302660\n",
      "iteration 600 / 2000: loss 2.302673\n",
      "iteration 700 / 2000: loss 2.302669\n",
      "iteration 800 / 2000: loss 2.302669\n",
      "iteration 900 / 2000: loss 2.302668\n",
      "iteration 1000 / 2000: loss 2.302664\n",
      "iteration 1100 / 2000: loss 2.302671\n",
      "iteration 1200 / 2000: loss 2.302678\n",
      "iteration 1300 / 2000: loss 2.302668\n",
      "iteration 1400 / 2000: loss 2.302679\n",
      "iteration 1500 / 2000: loss 2.302674\n",
      "iteration 1600 / 2000: loss 2.302675\n",
      "iteration 1700 / 2000: loss 2.302680\n",
      "iteration 1800 / 2000: loss 2.302673\n",
      "iteration 1900 / 2000: loss 2.302683\n",
      "Hidden Size: 60, Learning Rate: 1e-07, LR Decay: 0.645, Regularization: 0.4, Validation accuracy: 0.0944\n",
      "iteration 0 / 2000: loss 2.302568\n",
      "iteration 100 / 2000: loss 2.302569\n",
      "iteration 200 / 2000: loss 2.302578\n",
      "iteration 300 / 2000: loss 2.302573\n",
      "iteration 400 / 2000: loss 2.302577\n",
      "iteration 500 / 2000: loss 2.302573\n",
      "iteration 600 / 2000: loss 2.302558\n",
      "iteration 700 / 2000: loss 2.302572\n",
      "iteration 800 / 2000: loss 2.302576\n",
      "iteration 900 / 2000: loss 2.302565\n",
      "iteration 1000 / 2000: loss 2.302565\n",
      "iteration 1100 / 2000: loss 2.302562\n",
      "iteration 1200 / 2000: loss 2.302575\n",
      "iteration 1300 / 2000: loss 2.302565\n",
      "iteration 1400 / 2000: loss 2.302567\n",
      "iteration 1500 / 2000: loss 2.302570\n",
      "iteration 1600 / 2000: loss 2.302563\n",
      "iteration 1700 / 2000: loss 2.302569\n",
      "iteration 1800 / 2000: loss 2.302561\n",
      "iteration 1900 / 2000: loss 2.302571\n",
      "Hidden Size: 60, Learning Rate: 1e-07, LR Decay: 0.47250000000000003, Regularization: 0.0, Validation accuracy: 0.0958\n",
      "iteration 0 / 2000: loss 2.302608\n",
      "iteration 100 / 2000: loss 2.302597\n",
      "iteration 200 / 2000: loss 2.302605\n",
      "iteration 300 / 2000: loss 2.302603\n",
      "iteration 400 / 2000: loss 2.302614\n",
      "iteration 500 / 2000: loss 2.302614\n",
      "iteration 600 / 2000: loss 2.302617\n",
      "iteration 700 / 2000: loss 2.302617\n",
      "iteration 800 / 2000: loss 2.302602\n",
      "iteration 900 / 2000: loss 2.302607\n",
      "iteration 1000 / 2000: loss 2.302604\n",
      "iteration 1100 / 2000: loss 2.302603\n",
      "iteration 1200 / 2000: loss 2.302611\n",
      "iteration 1300 / 2000: loss 2.302606\n",
      "iteration 1400 / 2000: loss 2.302600\n",
      "iteration 1500 / 2000: loss 2.302615\n",
      "iteration 1600 / 2000: loss 2.302610\n",
      "iteration 1700 / 2000: loss 2.302603\n",
      "iteration 1800 / 2000: loss 2.302601\n",
      "iteration 1900 / 2000: loss 2.302607\n",
      "Hidden Size: 60, Learning Rate: 1e-07, LR Decay: 0.47250000000000003, Regularization: 0.1, Validation accuracy: 0.0802\n",
      "iteration 0 / 2000: loss 2.302624\n",
      "iteration 100 / 2000: loss 2.302623\n",
      "iteration 200 / 2000: loss 2.302621\n",
      "iteration 300 / 2000: loss 2.302631\n",
      "iteration 400 / 2000: loss 2.302631\n",
      "iteration 500 / 2000: loss 2.302622\n",
      "iteration 600 / 2000: loss 2.302632\n",
      "iteration 700 / 2000: loss 2.302636\n",
      "iteration 800 / 2000: loss 2.302631\n",
      "iteration 900 / 2000: loss 2.302627\n",
      "iteration 1000 / 2000: loss 2.302619\n",
      "iteration 1100 / 2000: loss 2.302624\n",
      "iteration 1200 / 2000: loss 2.302621\n",
      "iteration 1300 / 2000: loss 2.302629\n",
      "iteration 1400 / 2000: loss 2.302620\n",
      "iteration 1500 / 2000: loss 2.302625\n",
      "iteration 1600 / 2000: loss 2.302627\n",
      "iteration 1700 / 2000: loss 2.302626\n",
      "iteration 1800 / 2000: loss 2.302625\n",
      "iteration 1900 / 2000: loss 2.302625\n",
      "Hidden Size: 60, Learning Rate: 1e-07, LR Decay: 0.47250000000000003, Regularization: 0.2, Validation accuracy: 0.1218\n",
      "iteration 0 / 2000: loss 2.302646\n",
      "iteration 100 / 2000: loss 2.302674\n",
      "iteration 200 / 2000: loss 2.302666\n",
      "iteration 300 / 2000: loss 2.302668\n",
      "iteration 400 / 2000: loss 2.302674\n",
      "iteration 500 / 2000: loss 2.302674\n",
      "iteration 600 / 2000: loss 2.302679\n",
      "iteration 700 / 2000: loss 2.302674\n",
      "iteration 800 / 2000: loss 2.302681\n",
      "iteration 900 / 2000: loss 2.302663\n",
      "iteration 1000 / 2000: loss 2.302663\n",
      "iteration 1100 / 2000: loss 2.302676\n",
      "iteration 1200 / 2000: loss 2.302655\n",
      "iteration 1300 / 2000: loss 2.302658\n",
      "iteration 1400 / 2000: loss 2.302675\n",
      "iteration 1500 / 2000: loss 2.302673\n",
      "iteration 1600 / 2000: loss 2.302675\n",
      "iteration 1700 / 2000: loss 2.302669\n",
      "iteration 1800 / 2000: loss 2.302670\n",
      "iteration 1900 / 2000: loss 2.302673\n",
      "Hidden Size: 60, Learning Rate: 1e-07, LR Decay: 0.47250000000000003, Regularization: 0.30000000000000004, Validation accuracy: 0.0896\n",
      "iteration 0 / 2000: loss 2.302699\n",
      "iteration 100 / 2000: loss 2.302698\n",
      "iteration 200 / 2000: loss 2.302697\n",
      "iteration 300 / 2000: loss 2.302692\n",
      "iteration 400 / 2000: loss 2.302707\n",
      "iteration 500 / 2000: loss 2.302703\n",
      "iteration 600 / 2000: loss 2.302701\n",
      "iteration 700 / 2000: loss 2.302713\n",
      "iteration 800 / 2000: loss 2.302706\n",
      "iteration 900 / 2000: loss 2.302711\n",
      "iteration 1000 / 2000: loss 2.302700\n",
      "iteration 1100 / 2000: loss 2.302697\n",
      "iteration 1200 / 2000: loss 2.302697\n",
      "iteration 1300 / 2000: loss 2.302688\n",
      "iteration 1400 / 2000: loss 2.302705\n",
      "iteration 1500 / 2000: loss 2.302689\n",
      "iteration 1600 / 2000: loss 2.302696\n",
      "iteration 1700 / 2000: loss 2.302698\n",
      "iteration 1800 / 2000: loss 2.302708\n",
      "iteration 1900 / 2000: loss 2.302700\n",
      "Hidden Size: 60, Learning Rate: 1e-07, LR Decay: 0.47250000000000003, Regularization: 0.4, Validation accuracy: 0.0558\n",
      "iteration 0 / 2000: loss 2.302591\n",
      "iteration 100 / 2000: loss 2.302594\n",
      "iteration 200 / 2000: loss 2.302603\n",
      "iteration 300 / 2000: loss 2.302586\n",
      "iteration 400 / 2000: loss 2.302588\n",
      "iteration 500 / 2000: loss 2.302597\n",
      "iteration 600 / 2000: loss 2.302585\n",
      "iteration 700 / 2000: loss 2.302590\n",
      "iteration 800 / 2000: loss 2.302593\n",
      "iteration 900 / 2000: loss 2.302585\n",
      "iteration 1000 / 2000: loss 2.302588\n",
      "iteration 1100 / 2000: loss 2.302594\n",
      "iteration 1200 / 2000: loss 2.302600\n",
      "iteration 1300 / 2000: loss 2.302591\n",
      "iteration 1400 / 2000: loss 2.302586\n",
      "iteration 1500 / 2000: loss 2.302588\n",
      "iteration 1600 / 2000: loss 2.302587\n",
      "iteration 1700 / 2000: loss 2.302586\n",
      "iteration 1800 / 2000: loss 2.302603\n",
      "iteration 1900 / 2000: loss 2.302596\n",
      "Hidden Size: 60, Learning Rate: 1e-07, LR Decay: 0.3, Regularization: 0.0, Validation accuracy: 0.078\n",
      "iteration 0 / 2000: loss 2.302623\n",
      "iteration 100 / 2000: loss 2.302623\n",
      "iteration 200 / 2000: loss 2.302618\n",
      "iteration 300 / 2000: loss 2.302628\n",
      "iteration 400 / 2000: loss 2.302615\n",
      "iteration 500 / 2000: loss 2.302622\n",
      "iteration 600 / 2000: loss 2.302619\n",
      "iteration 700 / 2000: loss 2.302618\n",
      "iteration 800 / 2000: loss 2.302630\n",
      "iteration 900 / 2000: loss 2.302610\n",
      "iteration 1000 / 2000: loss 2.302632\n",
      "iteration 1100 / 2000: loss 2.302604\n",
      "iteration 1200 / 2000: loss 2.302622\n",
      "iteration 1300 / 2000: loss 2.302614\n",
      "iteration 1400 / 2000: loss 2.302616\n",
      "iteration 1500 / 2000: loss 2.302624\n",
      "iteration 1600 / 2000: loss 2.302626\n",
      "iteration 1700 / 2000: loss 2.302624\n",
      "iteration 1800 / 2000: loss 2.302614\n",
      "iteration 1900 / 2000: loss 2.302619\n",
      "Hidden Size: 60, Learning Rate: 1e-07, LR Decay: 0.3, Regularization: 0.1, Validation accuracy: 0.0626\n",
      "iteration 0 / 2000: loss 2.302630\n",
      "iteration 100 / 2000: loss 2.302647\n",
      "iteration 200 / 2000: loss 2.302652\n",
      "iteration 300 / 2000: loss 2.302638\n",
      "iteration 400 / 2000: loss 2.302628\n",
      "iteration 500 / 2000: loss 2.302626\n",
      "iteration 600 / 2000: loss 2.302631\n",
      "iteration 700 / 2000: loss 2.302641\n",
      "iteration 800 / 2000: loss 2.302630\n",
      "iteration 900 / 2000: loss 2.302629\n",
      "iteration 1000 / 2000: loss 2.302631\n",
      "iteration 1100 / 2000: loss 2.302634\n",
      "iteration 1200 / 2000: loss 2.302628\n",
      "iteration 1300 / 2000: loss 2.302631\n",
      "iteration 1400 / 2000: loss 2.302628\n",
      "iteration 1500 / 2000: loss 2.302633\n",
      "iteration 1600 / 2000: loss 2.302634\n",
      "iteration 1700 / 2000: loss 2.302635\n",
      "iteration 1800 / 2000: loss 2.302634\n",
      "iteration 1900 / 2000: loss 2.302629\n",
      "Hidden Size: 60, Learning Rate: 1e-07, LR Decay: 0.3, Regularization: 0.2, Validation accuracy: 0.0754\n",
      "iteration 0 / 2000: loss 2.302650\n",
      "iteration 100 / 2000: loss 2.302651\n",
      "iteration 200 / 2000: loss 2.302668\n",
      "iteration 300 / 2000: loss 2.302661\n",
      "iteration 400 / 2000: loss 2.302650\n",
      "iteration 500 / 2000: loss 2.302654\n",
      "iteration 600 / 2000: loss 2.302656\n",
      "iteration 700 / 2000: loss 2.302643\n",
      "iteration 800 / 2000: loss 2.302658\n",
      "iteration 900 / 2000: loss 2.302661\n",
      "iteration 1000 / 2000: loss 2.302664\n",
      "iteration 1100 / 2000: loss 2.302645\n",
      "iteration 1200 / 2000: loss 2.302670\n",
      "iteration 1300 / 2000: loss 2.302672\n",
      "iteration 1400 / 2000: loss 2.302640\n",
      "iteration 1500 / 2000: loss 2.302628\n",
      "iteration 1600 / 2000: loss 2.302655\n",
      "iteration 1700 / 2000: loss 2.302660\n",
      "iteration 1800 / 2000: loss 2.302654\n",
      "iteration 1900 / 2000: loss 2.302667\n",
      "Hidden Size: 60, Learning Rate: 1e-07, LR Decay: 0.3, Regularization: 0.30000000000000004, Validation accuracy: 0.165\n",
      "iteration 0 / 2000: loss 2.302644\n",
      "iteration 100 / 2000: loss 2.302664\n",
      "iteration 200 / 2000: loss 2.302662\n",
      "iteration 300 / 2000: loss 2.302651\n",
      "iteration 400 / 2000: loss 2.302657\n",
      "iteration 500 / 2000: loss 2.302683\n",
      "iteration 600 / 2000: loss 2.302658\n",
      "iteration 700 / 2000: loss 2.302658\n",
      "iteration 800 / 2000: loss 2.302653\n",
      "iteration 900 / 2000: loss 2.302653\n",
      "iteration 1000 / 2000: loss 2.302654\n",
      "iteration 1100 / 2000: loss 2.302666\n",
      "iteration 1200 / 2000: loss 2.302665\n",
      "iteration 1300 / 2000: loss 2.302668\n",
      "iteration 1400 / 2000: loss 2.302658\n",
      "iteration 1500 / 2000: loss 2.302681\n",
      "iteration 1600 / 2000: loss 2.302666\n",
      "iteration 1700 / 2000: loss 2.302659\n",
      "iteration 1800 / 2000: loss 2.302650\n",
      "iteration 1900 / 2000: loss 2.302656\n",
      "Hidden Size: 60, Learning Rate: 1e-07, LR Decay: 0.3, Regularization: 0.4, Validation accuracy: 0.1386\n",
      "iteration 0 / 2000: loss 2.302571\n",
      "iteration 100 / 2000: loss 0.131766\n",
      "iteration 200 / 2000: loss 0.137628\n",
      "iteration 300 / 2000: loss 0.124220\n",
      "iteration 400 / 2000: loss 0.133066\n",
      "iteration 500 / 2000: loss 0.074783\n",
      "iteration 600 / 2000: loss 0.059693\n",
      "iteration 700 / 2000: loss 0.095222\n",
      "iteration 800 / 2000: loss 0.077358\n",
      "iteration 900 / 2000: loss 0.178091\n",
      "iteration 1000 / 2000: loss 0.096840\n",
      "iteration 1100 / 2000: loss 0.085263\n",
      "iteration 1200 / 2000: loss 0.049516\n",
      "iteration 1300 / 2000: loss 0.075923\n",
      "iteration 1400 / 2000: loss 0.083993\n",
      "iteration 1500 / 2000: loss 0.028904\n",
      "iteration 1600 / 2000: loss 0.011733\n",
      "iteration 1700 / 2000: loss 0.070170\n",
      "iteration 1800 / 2000: loss 0.053215\n",
      "iteration 1900 / 2000: loss 0.035766\n",
      "Hidden Size: 70, Learning Rate: 0.01, LR Decay: 0.99, Regularization: 0.0, Validation accuracy: 0.9724\n",
      "iteration 0 / 2000: loss 2.302592\n",
      "iteration 100 / 2000: loss 0.406857\n",
      "iteration 200 / 2000: loss 0.349334\n",
      "iteration 300 / 2000: loss 0.166867\n",
      "iteration 400 / 2000: loss 0.154294\n",
      "iteration 500 / 2000: loss 0.199702\n",
      "iteration 600 / 2000: loss 0.193759\n",
      "iteration 700 / 2000: loss 0.633270\n",
      "iteration 800 / 2000: loss 0.107787\n",
      "iteration 900 / 2000: loss 0.142293\n",
      "iteration 1000 / 2000: loss 0.112066\n",
      "iteration 1100 / 2000: loss 0.115221\n",
      "iteration 1200 / 2000: loss 0.107770\n",
      "iteration 1300 / 2000: loss 0.150459\n",
      "iteration 1400 / 2000: loss 0.097544\n",
      "iteration 1500 / 2000: loss 0.103868\n",
      "iteration 1600 / 2000: loss 0.131852\n",
      "iteration 1700 / 2000: loss 0.136262\n",
      "iteration 1800 / 2000: loss 0.099637\n",
      "iteration 1900 / 2000: loss 0.114757\n",
      "Hidden Size: 70, Learning Rate: 0.01, LR Decay: 0.99, Regularization: 0.1, Validation accuracy: 0.9744\n",
      "iteration 0 / 2000: loss 2.302650\n",
      "iteration 100 / 2000: loss 0.370688\n",
      "iteration 200 / 2000: loss 0.259246\n",
      "iteration 300 / 2000: loss 0.171193\n",
      "iteration 400 / 2000: loss 0.194580\n",
      "iteration 500 / 2000: loss 0.179004\n",
      "iteration 600 / 2000: loss 0.188851\n",
      "iteration 700 / 2000: loss 0.160715\n",
      "iteration 800 / 2000: loss 0.215044\n",
      "iteration 900 / 2000: loss 0.183747\n",
      "iteration 1000 / 2000: loss 0.161820\n",
      "iteration 1100 / 2000: loss 0.163614\n",
      "iteration 1200 / 2000: loss 0.199318\n",
      "iteration 1300 / 2000: loss 0.192285\n",
      "iteration 1400 / 2000: loss 0.157879\n",
      "iteration 1500 / 2000: loss 0.173089\n",
      "iteration 1600 / 2000: loss 0.178482\n",
      "iteration 1700 / 2000: loss 0.140430\n",
      "iteration 1800 / 2000: loss 0.200542\n",
      "iteration 1900 / 2000: loss 0.238232\n",
      "Hidden Size: 70, Learning Rate: 0.01, LR Decay: 0.99, Regularization: 0.2, Validation accuracy: 0.9718\n",
      "iteration 0 / 2000: loss 2.302670\n",
      "iteration 100 / 2000: loss 0.509662\n",
      "iteration 200 / 2000: loss 0.237773\n",
      "iteration 300 / 2000: loss 0.214118\n",
      "iteration 400 / 2000: loss 0.297037\n",
      "iteration 500 / 2000: loss 0.236301\n",
      "iteration 600 / 2000: loss 0.233972\n",
      "iteration 700 / 2000: loss 0.170314\n",
      "iteration 800 / 2000: loss 0.281109\n",
      "iteration 900 / 2000: loss 0.222666\n",
      "iteration 1000 / 2000: loss 0.249117\n",
      "iteration 1100 / 2000: loss 0.222420\n",
      "iteration 1200 / 2000: loss 0.220313\n",
      "iteration 1300 / 2000: loss 0.180713\n",
      "iteration 1400 / 2000: loss 0.202655\n",
      "iteration 1500 / 2000: loss 0.212946\n",
      "iteration 1600 / 2000: loss 0.194013\n",
      "iteration 1700 / 2000: loss 0.213180\n",
      "iteration 1800 / 2000: loss 0.242417\n",
      "iteration 1900 / 2000: loss 0.194862\n",
      "Hidden Size: 70, Learning Rate: 0.01, LR Decay: 0.99, Regularization: 0.30000000000000004, Validation accuracy: 0.9698\n",
      "iteration 0 / 2000: loss 2.302723\n",
      "iteration 100 / 2000: loss 0.271205\n",
      "iteration 200 / 2000: loss 0.427204\n",
      "iteration 300 / 2000: loss 0.228523\n",
      "iteration 400 / 2000: loss 0.242102\n",
      "iteration 500 / 2000: loss 0.267076\n",
      "iteration 600 / 2000: loss 0.190496\n",
      "iteration 700 / 2000: loss 0.193700\n",
      "iteration 800 / 2000: loss 0.273936\n",
      "iteration 900 / 2000: loss 0.241401\n",
      "iteration 1000 / 2000: loss 0.194587\n",
      "iteration 1100 / 2000: loss 0.321105\n",
      "iteration 1200 / 2000: loss 0.231111\n",
      "iteration 1300 / 2000: loss 0.214255\n",
      "iteration 1400 / 2000: loss 0.238426\n",
      "iteration 1500 / 2000: loss 0.267472\n",
      "iteration 1600 / 2000: loss 0.230438\n",
      "iteration 1700 / 2000: loss 0.215684\n",
      "iteration 1800 / 2000: loss 0.216803\n",
      "iteration 1900 / 2000: loss 0.250091\n",
      "Hidden Size: 70, Learning Rate: 0.01, LR Decay: 0.99, Regularization: 0.4, Validation accuracy: 0.9656\n",
      "iteration 0 / 2000: loss 2.302598\n",
      "iteration 100 / 2000: loss 0.343026\n",
      "iteration 200 / 2000: loss 0.142324\n",
      "iteration 300 / 2000: loss 0.108561\n",
      "iteration 400 / 2000: loss 0.066907\n",
      "iteration 500 / 2000: loss 0.054187\n",
      "iteration 600 / 2000: loss 0.053535\n",
      "iteration 700 / 2000: loss 0.077454\n",
      "iteration 800 / 2000: loss 0.056112\n",
      "iteration 900 / 2000: loss 0.046955\n",
      "iteration 1000 / 2000: loss 0.074146\n",
      "iteration 1100 / 2000: loss 0.022117\n",
      "iteration 1200 / 2000: loss 0.042314\n",
      "iteration 1300 / 2000: loss 0.051831\n",
      "iteration 1400 / 2000: loss 0.043496\n",
      "iteration 1500 / 2000: loss 0.036986\n",
      "iteration 1600 / 2000: loss 0.073620\n",
      "iteration 1700 / 2000: loss 0.015946\n",
      "iteration 1800 / 2000: loss 0.027688\n",
      "iteration 1900 / 2000: loss 0.022499\n",
      "Hidden Size: 70, Learning Rate: 0.01, LR Decay: 0.8175, Regularization: 0.0, Validation accuracy: 0.9778\n",
      "iteration 0 / 2000: loss 2.302597\n",
      "iteration 100 / 2000: loss 0.220032\n",
      "iteration 200 / 2000: loss 0.136375\n",
      "iteration 300 / 2000: loss 0.197976\n",
      "iteration 400 / 2000: loss 0.235565\n",
      "iteration 500 / 2000: loss 0.085683\n",
      "iteration 600 / 2000: loss 0.143503\n",
      "iteration 700 / 2000: loss 0.108709\n",
      "iteration 800 / 2000: loss 0.107702\n",
      "iteration 900 / 2000: loss 0.144885\n",
      "iteration 1000 / 2000: loss 0.083808\n",
      "iteration 1100 / 2000: loss 0.128186\n",
      "iteration 1200 / 2000: loss 0.119461\n",
      "iteration 1300 / 2000: loss 0.077594\n",
      "iteration 1400 / 2000: loss 0.085591\n",
      "iteration 1500 / 2000: loss 0.128705\n",
      "iteration 1600 / 2000: loss 0.084926\n",
      "iteration 1700 / 2000: loss 0.112514\n",
      "iteration 1800 / 2000: loss 0.089039\n",
      "iteration 1900 / 2000: loss 0.071135\n",
      "Hidden Size: 70, Learning Rate: 0.01, LR Decay: 0.8175, Regularization: 0.1, Validation accuracy: 0.9774\n",
      "iteration 0 / 2000: loss 2.302638\n",
      "iteration 100 / 2000: loss 0.297547\n",
      "iteration 200 / 2000: loss 0.258242\n",
      "iteration 300 / 2000: loss 0.231929\n",
      "iteration 400 / 2000: loss 0.160281\n",
      "iteration 500 / 2000: loss 0.164997\n",
      "iteration 600 / 2000: loss 0.192409\n",
      "iteration 700 / 2000: loss 0.187932\n",
      "iteration 800 / 2000: loss 0.141421\n",
      "iteration 900 / 2000: loss 0.196405\n",
      "iteration 1000 / 2000: loss 0.150250\n",
      "iteration 1100 / 2000: loss 0.121654\n",
      "iteration 1200 / 2000: loss 0.202034\n",
      "iteration 1300 / 2000: loss 0.187393\n",
      "iteration 1400 / 2000: loss 0.129453\n",
      "iteration 1500 / 2000: loss 0.124181\n",
      "iteration 1600 / 2000: loss 0.174730\n",
      "iteration 1700 / 2000: loss 0.135510\n",
      "iteration 1800 / 2000: loss 0.139422\n",
      "iteration 1900 / 2000: loss 0.118099\n",
      "Hidden Size: 70, Learning Rate: 0.01, LR Decay: 0.8175, Regularization: 0.2, Validation accuracy: 0.976\n",
      "iteration 0 / 2000: loss 2.302666\n",
      "iteration 100 / 2000: loss 0.343419\n",
      "iteration 200 / 2000: loss 0.259104\n",
      "iteration 300 / 2000: loss 0.234996\n",
      "iteration 400 / 2000: loss 0.155202\n",
      "iteration 500 / 2000: loss 0.225671\n",
      "iteration 600 / 2000: loss 0.211665\n",
      "iteration 700 / 2000: loss 0.210305\n",
      "iteration 800 / 2000: loss 0.213639\n",
      "iteration 900 / 2000: loss 0.163631\n",
      "iteration 1000 / 2000: loss 0.192460\n",
      "iteration 1100 / 2000: loss 0.173200\n",
      "iteration 1200 / 2000: loss 0.177546\n",
      "iteration 1300 / 2000: loss 0.156553\n",
      "iteration 1400 / 2000: loss 0.183761\n",
      "iteration 1500 / 2000: loss 0.198640\n",
      "iteration 1600 / 2000: loss 0.157095\n",
      "iteration 1700 / 2000: loss 0.188481\n",
      "iteration 1800 / 2000: loss 0.184818\n",
      "iteration 1900 / 2000: loss 0.160232\n",
      "Hidden Size: 70, Learning Rate: 0.01, LR Decay: 0.8175, Regularization: 0.30000000000000004, Validation accuracy: 0.9746\n",
      "iteration 0 / 2000: loss 2.302719\n",
      "iteration 100 / 2000: loss 0.214762\n",
      "iteration 200 / 2000: loss 0.346913\n",
      "iteration 300 / 2000: loss 0.218759\n",
      "iteration 400 / 2000: loss 0.269536\n",
      "iteration 500 / 2000: loss 0.238544\n",
      "iteration 600 / 2000: loss 0.221646\n",
      "iteration 700 / 2000: loss 0.178718\n",
      "iteration 800 / 2000: loss 0.203614\n",
      "iteration 900 / 2000: loss 0.270935\n",
      "iteration 1000 / 2000: loss 0.200044\n",
      "iteration 1100 / 2000: loss 0.223982\n",
      "iteration 1200 / 2000: loss 0.192400\n",
      "iteration 1300 / 2000: loss 0.175939\n",
      "iteration 1400 / 2000: loss 0.208259\n",
      "iteration 1500 / 2000: loss 0.225005\n",
      "iteration 1600 / 2000: loss 0.199324\n",
      "iteration 1700 / 2000: loss 0.171636\n",
      "iteration 1800 / 2000: loss 0.181403\n",
      "iteration 1900 / 2000: loss 0.209573\n",
      "Hidden Size: 70, Learning Rate: 0.01, LR Decay: 0.8175, Regularization: 0.4, Validation accuracy: 0.974\n",
      "iteration 0 / 2000: loss 2.302583\n",
      "iteration 100 / 2000: loss 0.097859\n",
      "iteration 200 / 2000: loss 0.141065\n",
      "iteration 300 / 2000: loss 0.137660\n",
      "iteration 400 / 2000: loss 0.140011\n",
      "iteration 500 / 2000: loss 0.114498\n",
      "iteration 600 / 2000: loss 0.075993\n",
      "iteration 700 / 2000: loss 0.040197\n",
      "iteration 800 / 2000: loss 0.063434\n",
      "iteration 900 / 2000: loss 0.100783\n",
      "iteration 1000 / 2000: loss 0.039930\n",
      "iteration 1100 / 2000: loss 0.110665\n",
      "iteration 1200 / 2000: loss 0.048643\n",
      "iteration 1300 / 2000: loss 0.063933\n",
      "iteration 1400 / 2000: loss 0.063920\n",
      "iteration 1500 / 2000: loss 0.038592\n",
      "iteration 1600 / 2000: loss 0.072639\n",
      "iteration 1700 / 2000: loss 0.096625\n",
      "iteration 1800 / 2000: loss 0.064590\n",
      "iteration 1900 / 2000: loss 0.050146\n",
      "Hidden Size: 70, Learning Rate: 0.01, LR Decay: 0.645, Regularization: 0.0, Validation accuracy: 0.9718\n",
      "iteration 0 / 2000: loss 2.302618\n",
      "iteration 100 / 2000: loss 0.258584\n",
      "iteration 200 / 2000: loss 0.194948\n",
      "iteration 300 / 2000: loss 0.121183\n",
      "iteration 400 / 2000: loss 0.161522\n",
      "iteration 500 / 2000: loss 0.134819\n",
      "iteration 600 / 2000: loss 0.109775\n",
      "iteration 700 / 2000: loss 0.191007\n",
      "iteration 800 / 2000: loss 0.166407\n",
      "iteration 900 / 2000: loss 0.212046\n",
      "iteration 1000 / 2000: loss 0.119666\n",
      "iteration 1100 / 2000: loss 0.106798\n",
      "iteration 1200 / 2000: loss 0.088131\n",
      "iteration 1300 / 2000: loss 0.128012\n",
      "iteration 1400 / 2000: loss 0.084105\n",
      "iteration 1500 / 2000: loss 0.168563\n",
      "iteration 1600 / 2000: loss 0.104926\n",
      "iteration 1700 / 2000: loss 0.068228\n",
      "iteration 1800 / 2000: loss 0.094134\n",
      "iteration 1900 / 2000: loss 0.100644\n",
      "Hidden Size: 70, Learning Rate: 0.01, LR Decay: 0.645, Regularization: 0.1, Validation accuracy: 0.9758\n",
      "iteration 0 / 2000: loss 2.302629\n",
      "iteration 100 / 2000: loss 0.275972\n",
      "iteration 200 / 2000: loss 0.198500\n",
      "iteration 300 / 2000: loss 0.186150\n",
      "iteration 400 / 2000: loss 0.178364\n",
      "iteration 500 / 2000: loss 0.206784\n",
      "iteration 600 / 2000: loss 0.136297\n",
      "iteration 700 / 2000: loss 0.228478\n",
      "iteration 800 / 2000: loss 0.153098\n",
      "iteration 900 / 2000: loss 0.143731\n",
      "iteration 1000 / 2000: loss 0.216772\n",
      "iteration 1100 / 2000: loss 0.145647\n",
      "iteration 1200 / 2000: loss 0.173468\n",
      "iteration 1300 / 2000: loss 0.148884\n",
      "iteration 1400 / 2000: loss 0.156006\n",
      "iteration 1500 / 2000: loss 0.113028\n",
      "iteration 1600 / 2000: loss 0.171202\n",
      "iteration 1700 / 2000: loss 0.136719\n",
      "iteration 1800 / 2000: loss 0.155159\n",
      "iteration 1900 / 2000: loss 0.148746\n",
      "Hidden Size: 70, Learning Rate: 0.01, LR Decay: 0.645, Regularization: 0.2, Validation accuracy: 0.9746\n",
      "iteration 0 / 2000: loss 2.302688\n",
      "iteration 100 / 2000: loss 0.352536\n",
      "iteration 200 / 2000: loss 0.279729\n",
      "iteration 300 / 2000: loss 0.264192\n",
      "iteration 400 / 2000: loss 0.184720\n",
      "iteration 500 / 2000: loss 0.174246\n",
      "iteration 600 / 2000: loss 0.139178\n",
      "iteration 700 / 2000: loss 0.223426\n",
      "iteration 800 / 2000: loss 0.169728\n",
      "iteration 900 / 2000: loss 0.163202\n",
      "iteration 1000 / 2000: loss 0.191033\n",
      "iteration 1100 / 2000: loss 0.202821\n",
      "iteration 1200 / 2000: loss 0.149460\n",
      "iteration 1300 / 2000: loss 0.172412\n",
      "iteration 1400 / 2000: loss 0.204153\n",
      "iteration 1500 / 2000: loss 0.160122\n",
      "iteration 1600 / 2000: loss 0.176788\n",
      "iteration 1700 / 2000: loss 0.194902\n",
      "iteration 1800 / 2000: loss 0.161908\n",
      "iteration 1900 / 2000: loss 0.195913\n",
      "Hidden Size: 70, Learning Rate: 0.01, LR Decay: 0.645, Regularization: 0.30000000000000004, Validation accuracy: 0.9714\n",
      "iteration 0 / 2000: loss 2.302710\n",
      "iteration 100 / 2000: loss 0.330286\n",
      "iteration 200 / 2000: loss 0.244790\n",
      "iteration 300 / 2000: loss 0.223541\n",
      "iteration 400 / 2000: loss 0.222020\n",
      "iteration 500 / 2000: loss 0.176951\n",
      "iteration 600 / 2000: loss 0.179581\n",
      "iteration 700 / 2000: loss 0.195518\n",
      "iteration 800 / 2000: loss 0.225364\n",
      "iteration 900 / 2000: loss 0.170356\n",
      "iteration 1000 / 2000: loss 0.212603\n",
      "iteration 1100 / 2000: loss 0.163962\n",
      "iteration 1200 / 2000: loss 0.231329\n",
      "iteration 1300 / 2000: loss 0.183557\n",
      "iteration 1400 / 2000: loss 0.204593\n",
      "iteration 1500 / 2000: loss 0.179116\n",
      "iteration 1600 / 2000: loss 0.218514\n",
      "iteration 1700 / 2000: loss 0.194324\n",
      "iteration 1800 / 2000: loss 0.182510\n",
      "iteration 1900 / 2000: loss 0.183834\n",
      "Hidden Size: 70, Learning Rate: 0.01, LR Decay: 0.645, Regularization: 0.4, Validation accuracy: 0.9702\n",
      "iteration 0 / 2000: loss 2.302582\n",
      "iteration 100 / 2000: loss 0.306353\n",
      "iteration 200 / 2000: loss 0.309371\n",
      "iteration 300 / 2000: loss 0.155142\n",
      "iteration 400 / 2000: loss 0.173236\n",
      "iteration 500 / 2000: loss 0.102512\n",
      "iteration 600 / 2000: loss 0.102587\n",
      "iteration 700 / 2000: loss 0.131454\n",
      "iteration 800 / 2000: loss 0.101008\n",
      "iteration 900 / 2000: loss 0.065282\n",
      "iteration 1000 / 2000: loss 0.060655\n",
      "iteration 1100 / 2000: loss 0.079177\n",
      "iteration 1200 / 2000: loss 0.175864\n",
      "iteration 1300 / 2000: loss 0.111704\n",
      "iteration 1400 / 2000: loss 0.162938\n",
      "iteration 1500 / 2000: loss 0.079857\n",
      "iteration 1600 / 2000: loss 0.152885\n",
      "iteration 1700 / 2000: loss 0.058854\n",
      "iteration 1800 / 2000: loss 0.127347\n",
      "iteration 1900 / 2000: loss 0.101761\n",
      "Hidden Size: 70, Learning Rate: 0.01, LR Decay: 0.47250000000000003, Regularization: 0.0, Validation accuracy: 0.9694\n",
      "iteration 0 / 2000: loss 2.302593\n",
      "iteration 100 / 2000: loss 0.344808\n",
      "iteration 200 / 2000: loss 0.182826\n",
      "iteration 300 / 2000: loss 0.154188\n",
      "iteration 400 / 2000: loss 0.186141\n",
      "iteration 500 / 2000: loss 0.202836\n",
      "iteration 600 / 2000: loss 0.096449\n",
      "iteration 700 / 2000: loss 0.143406\n",
      "iteration 800 / 2000: loss 0.171843\n",
      "iteration 900 / 2000: loss 0.131287\n",
      "iteration 1000 / 2000: loss 0.201642\n",
      "iteration 1100 / 2000: loss 0.130096\n",
      "iteration 1200 / 2000: loss 0.158736\n",
      "iteration 1300 / 2000: loss 0.150733\n",
      "iteration 1400 / 2000: loss 0.081026\n",
      "iteration 1500 / 2000: loss 0.117336\n",
      "iteration 1600 / 2000: loss 0.157528\n",
      "iteration 1700 / 2000: loss 0.115278\n",
      "iteration 1800 / 2000: loss 0.153513\n",
      "iteration 1900 / 2000: loss 0.231414\n",
      "Hidden Size: 70, Learning Rate: 0.01, LR Decay: 0.47250000000000003, Regularization: 0.1, Validation accuracy: 0.967\n",
      "iteration 0 / 2000: loss 2.302644\n",
      "iteration 100 / 2000: loss 0.343198\n",
      "iteration 200 / 2000: loss 0.257940\n",
      "iteration 300 / 2000: loss 0.222753\n",
      "iteration 400 / 2000: loss 0.161781\n",
      "iteration 500 / 2000: loss 0.219133\n",
      "iteration 600 / 2000: loss 0.191863\n",
      "iteration 700 / 2000: loss 0.133605\n",
      "iteration 800 / 2000: loss 0.126439\n",
      "iteration 900 / 2000: loss 0.158112\n",
      "iteration 1000 / 2000: loss 0.176959\n",
      "iteration 1100 / 2000: loss 0.164123\n",
      "iteration 1200 / 2000: loss 0.114261\n",
      "iteration 1300 / 2000: loss 0.200093\n",
      "iteration 1400 / 2000: loss 0.157052\n",
      "iteration 1500 / 2000: loss 0.158911\n",
      "iteration 1600 / 2000: loss 0.185530\n",
      "iteration 1700 / 2000: loss 0.149739\n",
      "iteration 1800 / 2000: loss 0.175627\n",
      "iteration 1900 / 2000: loss 0.145402\n",
      "Hidden Size: 70, Learning Rate: 0.01, LR Decay: 0.47250000000000003, Regularization: 0.2, Validation accuracy: 0.967\n",
      "iteration 0 / 2000: loss 2.302695\n",
      "iteration 100 / 2000: loss 0.279126\n",
      "iteration 200 / 2000: loss 0.295005\n",
      "iteration 300 / 2000: loss 0.214022\n",
      "iteration 400 / 2000: loss 0.262897\n",
      "iteration 500 / 2000: loss 0.194362\n",
      "iteration 600 / 2000: loss 0.182012\n",
      "iteration 700 / 2000: loss 0.220401\n",
      "iteration 800 / 2000: loss 0.182080\n",
      "iteration 900 / 2000: loss 0.164272\n",
      "iteration 1000 / 2000: loss 0.168441\n",
      "iteration 1100 / 2000: loss 0.187525\n",
      "iteration 1200 / 2000: loss 0.148617\n",
      "iteration 1300 / 2000: loss 0.236527\n",
      "iteration 1400 / 2000: loss 0.220854\n",
      "iteration 1500 / 2000: loss 0.203469\n",
      "iteration 1600 / 2000: loss 0.197380\n",
      "iteration 1700 / 2000: loss 0.205898\n",
      "iteration 1800 / 2000: loss 0.159045\n",
      "iteration 1900 / 2000: loss 0.203963\n",
      "Hidden Size: 70, Learning Rate: 0.01, LR Decay: 0.47250000000000003, Regularization: 0.30000000000000004, Validation accuracy: 0.9646\n",
      "iteration 0 / 2000: loss 2.302707\n",
      "iteration 100 / 2000: loss 0.383619\n",
      "iteration 200 / 2000: loss 0.398194\n",
      "iteration 300 / 2000: loss 0.316874\n",
      "iteration 400 / 2000: loss 0.240521\n",
      "iteration 500 / 2000: loss 0.248790\n",
      "iteration 600 / 2000: loss 0.215347\n",
      "iteration 700 / 2000: loss 0.293855\n",
      "iteration 800 / 2000: loss 0.234608\n",
      "iteration 900 / 2000: loss 0.186805\n",
      "iteration 1000 / 2000: loss 0.226630\n",
      "iteration 1100 / 2000: loss 0.175752\n",
      "iteration 1200 / 2000: loss 0.218918\n",
      "iteration 1300 / 2000: loss 0.238511\n",
      "iteration 1400 / 2000: loss 0.217262\n",
      "iteration 1500 / 2000: loss 0.215959\n",
      "iteration 1600 / 2000: loss 0.231209\n",
      "iteration 1700 / 2000: loss 0.198300\n",
      "iteration 1800 / 2000: loss 0.231125\n",
      "iteration 1900 / 2000: loss 0.209800\n",
      "Hidden Size: 70, Learning Rate: 0.01, LR Decay: 0.47250000000000003, Regularization: 0.4, Validation accuracy: 0.965\n",
      "iteration 0 / 2000: loss 2.302580\n",
      "iteration 100 / 2000: loss 0.296194\n",
      "iteration 200 / 2000: loss 0.260929\n",
      "iteration 300 / 2000: loss 0.159563\n",
      "iteration 400 / 2000: loss 0.212914\n",
      "iteration 500 / 2000: loss 0.164122\n",
      "iteration 600 / 2000: loss 0.160512\n",
      "iteration 700 / 2000: loss 0.162969\n",
      "iteration 800 / 2000: loss 0.133482\n",
      "iteration 900 / 2000: loss 0.140789\n",
      "iteration 1000 / 2000: loss 0.123129\n",
      "iteration 1100 / 2000: loss 0.231737\n",
      "iteration 1200 / 2000: loss 0.153327\n",
      "iteration 1300 / 2000: loss 0.173715\n",
      "iteration 1400 / 2000: loss 0.250683\n",
      "iteration 1500 / 2000: loss 0.189700\n",
      "iteration 1600 / 2000: loss 0.170838\n",
      "iteration 1700 / 2000: loss 0.122305\n",
      "iteration 1800 / 2000: loss 0.160875\n",
      "iteration 1900 / 2000: loss 0.131875\n",
      "Hidden Size: 70, Learning Rate: 0.01, LR Decay: 0.3, Regularization: 0.0, Validation accuracy: 0.954\n",
      "iteration 0 / 2000: loss 2.302624\n",
      "iteration 100 / 2000: loss 0.410819\n",
      "iteration 200 / 2000: loss 0.379377\n",
      "iteration 300 / 2000: loss 0.169019\n",
      "iteration 400 / 2000: loss 0.207934\n",
      "iteration 500 / 2000: loss 0.207284\n",
      "iteration 600 / 2000: loss 0.159638\n",
      "iteration 700 / 2000: loss 0.161268\n",
      "iteration 800 / 2000: loss 0.123769\n",
      "iteration 900 / 2000: loss 0.214029\n",
      "iteration 1000 / 2000: loss 0.147601\n",
      "iteration 1100 / 2000: loss 0.173441\n",
      "iteration 1200 / 2000: loss 0.158820\n",
      "iteration 1300 / 2000: loss 0.203216\n",
      "iteration 1400 / 2000: loss 0.129476\n",
      "iteration 1500 / 2000: loss 0.218143\n",
      "iteration 1600 / 2000: loss 0.114742\n",
      "iteration 1700 / 2000: loss 0.186911\n",
      "iteration 1800 / 2000: loss 0.278742\n",
      "iteration 1900 / 2000: loss 0.127237\n",
      "Hidden Size: 70, Learning Rate: 0.01, LR Decay: 0.3, Regularization: 0.1, Validation accuracy: 0.954\n",
      "iteration 0 / 2000: loss 2.302637\n",
      "iteration 100 / 2000: loss 0.369402\n",
      "iteration 200 / 2000: loss 0.309781\n",
      "iteration 300 / 2000: loss 0.259106\n",
      "iteration 400 / 2000: loss 0.228913\n",
      "iteration 500 / 2000: loss 0.190419\n",
      "iteration 600 / 2000: loss 0.157603\n",
      "iteration 700 / 2000: loss 0.175874\n",
      "iteration 800 / 2000: loss 0.215700\n",
      "iteration 900 / 2000: loss 0.195356\n",
      "iteration 1000 / 2000: loss 0.195532\n",
      "iteration 1100 / 2000: loss 0.224468\n",
      "iteration 1200 / 2000: loss 0.211832\n",
      "iteration 1300 / 2000: loss 0.148778\n",
      "iteration 1400 / 2000: loss 0.167477\n",
      "iteration 1500 / 2000: loss 0.232867\n",
      "iteration 1600 / 2000: loss 0.256408\n",
      "iteration 1700 / 2000: loss 0.194356\n",
      "iteration 1800 / 2000: loss 0.171964\n",
      "iteration 1900 / 2000: loss 0.170280\n",
      "Hidden Size: 70, Learning Rate: 0.01, LR Decay: 0.3, Regularization: 0.2, Validation accuracy: 0.9534\n",
      "iteration 0 / 2000: loss 2.302663\n",
      "iteration 100 / 2000: loss 0.390871\n",
      "iteration 200 / 2000: loss 0.225739\n",
      "iteration 300 / 2000: loss 0.290581\n",
      "iteration 400 / 2000: loss 0.259602\n",
      "iteration 500 / 2000: loss 0.183559\n",
      "iteration 600 / 2000: loss 0.320585\n",
      "iteration 700 / 2000: loss 0.225513\n",
      "iteration 800 / 2000: loss 0.236738\n",
      "iteration 900 / 2000: loss 0.272512\n",
      "iteration 1000 / 2000: loss 0.223298\n",
      "iteration 1100 / 2000: loss 0.247541\n",
      "iteration 1200 / 2000: loss 0.190509\n",
      "iteration 1300 / 2000: loss 0.207592\n",
      "iteration 1400 / 2000: loss 0.213740\n",
      "iteration 1500 / 2000: loss 0.212657\n",
      "iteration 1600 / 2000: loss 0.230695\n",
      "iteration 1700 / 2000: loss 0.191010\n",
      "iteration 1800 / 2000: loss 0.250320\n",
      "iteration 1900 / 2000: loss 0.173947\n",
      "Hidden Size: 70, Learning Rate: 0.01, LR Decay: 0.3, Regularization: 0.30000000000000004, Validation accuracy: 0.954\n",
      "iteration 0 / 2000: loss 2.302697\n",
      "iteration 100 / 2000: loss 0.325514\n",
      "iteration 200 / 2000: loss 0.337201\n",
      "iteration 300 / 2000: loss 0.299277\n",
      "iteration 400 / 2000: loss 0.282104\n",
      "iteration 500 / 2000: loss 0.171088\n",
      "iteration 600 / 2000: loss 0.286201\n",
      "iteration 700 / 2000: loss 0.246860\n",
      "iteration 800 / 2000: loss 0.218144\n",
      "iteration 900 / 2000: loss 0.250178\n",
      "iteration 1000 / 2000: loss 0.215327\n",
      "iteration 1100 / 2000: loss 0.350382\n",
      "iteration 1200 / 2000: loss 0.281283\n",
      "iteration 1300 / 2000: loss 0.244219\n",
      "iteration 1400 / 2000: loss 0.219627\n",
      "iteration 1500 / 2000: loss 0.251770\n",
      "iteration 1600 / 2000: loss 0.238681\n",
      "iteration 1700 / 2000: loss 0.205852\n",
      "iteration 1800 / 2000: loss 0.285501\n",
      "iteration 1900 / 2000: loss 0.221579\n",
      "Hidden Size: 70, Learning Rate: 0.01, LR Decay: 0.3, Regularization: 0.4, Validation accuracy: 0.9526\n",
      "iteration 0 / 2000: loss 2.302591\n",
      "iteration 100 / 2000: loss 0.197967\n",
      "iteration 200 / 2000: loss 0.207878\n",
      "iteration 300 / 2000: loss 0.121410\n",
      "iteration 400 / 2000: loss 0.175256\n",
      "iteration 500 / 2000: loss 0.113745\n",
      "iteration 600 / 2000: loss 0.130938\n",
      "iteration 700 / 2000: loss 0.107147\n",
      "iteration 800 / 2000: loss 0.118022\n",
      "iteration 900 / 2000: loss 0.045324\n",
      "iteration 1000 / 2000: loss 0.127450\n",
      "iteration 1100 / 2000: loss 0.056225\n",
      "iteration 1200 / 2000: loss 0.021140\n",
      "iteration 1300 / 2000: loss 0.038823\n",
      "iteration 1400 / 2000: loss 0.039148\n",
      "iteration 1500 / 2000: loss 0.046982\n",
      "iteration 1600 / 2000: loss 0.040840\n",
      "iteration 1700 / 2000: loss 0.017975\n",
      "iteration 1800 / 2000: loss 0.009736\n",
      "iteration 1900 / 2000: loss 0.023104\n",
      "Hidden Size: 70, Learning Rate: 0.0075000250000000004, LR Decay: 0.99, Regularization: 0.0, Validation accuracy: 0.976\n",
      "iteration 0 / 2000: loss 2.302623\n",
      "iteration 100 / 2000: loss 0.249582\n",
      "iteration 200 / 2000: loss 0.220286\n",
      "iteration 300 / 2000: loss 0.206696\n",
      "iteration 400 / 2000: loss 0.215939\n",
      "iteration 500 / 2000: loss 0.117328\n",
      "iteration 600 / 2000: loss 0.130864\n",
      "iteration 700 / 2000: loss 0.119691\n",
      "iteration 800 / 2000: loss 0.102582\n",
      "iteration 900 / 2000: loss 0.133370\n",
      "iteration 1000 / 2000: loss 0.104216\n",
      "iteration 1100 / 2000: loss 0.113189\n",
      "iteration 1200 / 2000: loss 0.113904\n",
      "iteration 1300 / 2000: loss 0.156680\n",
      "iteration 1400 / 2000: loss 0.128738\n",
      "iteration 1500 / 2000: loss 0.105640\n",
      "iteration 1600 / 2000: loss 0.116557\n",
      "iteration 1700 / 2000: loss 0.124811\n",
      "iteration 1800 / 2000: loss 0.087552\n",
      "iteration 1900 / 2000: loss 0.142126\n",
      "Hidden Size: 70, Learning Rate: 0.0075000250000000004, LR Decay: 0.99, Regularization: 0.1, Validation accuracy: 0.9766\n",
      "iteration 0 / 2000: loss 2.302625\n",
      "iteration 100 / 2000: loss 0.366903\n",
      "iteration 200 / 2000: loss 0.156640\n",
      "iteration 300 / 2000: loss 0.204548\n",
      "iteration 400 / 2000: loss 0.166917\n",
      "iteration 500 / 2000: loss 0.203089\n",
      "iteration 600 / 2000: loss 0.198048\n",
      "iteration 700 / 2000: loss 0.177462\n",
      "iteration 800 / 2000: loss 0.161708\n",
      "iteration 900 / 2000: loss 0.159518\n",
      "iteration 1000 / 2000: loss 0.173450\n",
      "iteration 1100 / 2000: loss 0.136701\n",
      "iteration 1200 / 2000: loss 0.201259\n",
      "iteration 1300 / 2000: loss 0.169214\n",
      "iteration 1400 / 2000: loss 0.193951\n",
      "iteration 1500 / 2000: loss 0.151740\n",
      "iteration 1600 / 2000: loss 0.128559\n",
      "iteration 1700 / 2000: loss 0.167308\n",
      "iteration 1800 / 2000: loss 0.138088\n",
      "iteration 1900 / 2000: loss 0.143822\n",
      "Hidden Size: 70, Learning Rate: 0.0075000250000000004, LR Decay: 0.99, Regularization: 0.2, Validation accuracy: 0.9732\n",
      "iteration 0 / 2000: loss 2.302656\n",
      "iteration 100 / 2000: loss 0.357821\n",
      "iteration 200 / 2000: loss 0.285382\n",
      "iteration 300 / 2000: loss 0.242875\n",
      "iteration 400 / 2000: loss 0.231490\n",
      "iteration 500 / 2000: loss 0.232282\n",
      "iteration 600 / 2000: loss 0.158675\n",
      "iteration 700 / 2000: loss 0.195436\n",
      "iteration 800 / 2000: loss 0.234288\n",
      "iteration 900 / 2000: loss 0.241524\n",
      "iteration 1000 / 2000: loss 0.215265\n",
      "iteration 1100 / 2000: loss 0.193416\n",
      "iteration 1200 / 2000: loss 0.169388\n",
      "iteration 1300 / 2000: loss 0.233723\n",
      "iteration 1400 / 2000: loss 0.225140\n",
      "iteration 1500 / 2000: loss 0.175376\n",
      "iteration 1600 / 2000: loss 0.163010\n",
      "iteration 1700 / 2000: loss 0.175255\n",
      "iteration 1800 / 2000: loss 0.229360\n",
      "iteration 1900 / 2000: loss 0.169042\n",
      "Hidden Size: 70, Learning Rate: 0.0075000250000000004, LR Decay: 0.99, Regularization: 0.30000000000000004, Validation accuracy: 0.9736\n",
      "iteration 0 / 2000: loss 2.302693\n",
      "iteration 100 / 2000: loss 0.274568\n",
      "iteration 200 / 2000: loss 0.221999\n",
      "iteration 300 / 2000: loss 0.250892\n",
      "iteration 400 / 2000: loss 0.290486\n",
      "iteration 500 / 2000: loss 0.252716\n",
      "iteration 600 / 2000: loss 0.228480\n",
      "iteration 700 / 2000: loss 0.222390\n",
      "iteration 800 / 2000: loss 0.197007\n",
      "iteration 900 / 2000: loss 0.261969\n",
      "iteration 1000 / 2000: loss 0.263800\n",
      "iteration 1100 / 2000: loss 0.225466\n",
      "iteration 1200 / 2000: loss 0.206152\n",
      "iteration 1300 / 2000: loss 0.237072\n",
      "iteration 1400 / 2000: loss 0.270720\n",
      "iteration 1500 / 2000: loss 0.233296\n",
      "iteration 1600 / 2000: loss 0.185725\n",
      "iteration 1700 / 2000: loss 0.194909\n",
      "iteration 1800 / 2000: loss 0.215950\n",
      "iteration 1900 / 2000: loss 0.219231\n",
      "Hidden Size: 70, Learning Rate: 0.0075000250000000004, LR Decay: 0.99, Regularization: 0.4, Validation accuracy: 0.9724\n",
      "iteration 0 / 2000: loss 2.302565\n",
      "iteration 100 / 2000: loss 0.236862\n",
      "iteration 200 / 2000: loss 0.200747\n",
      "iteration 300 / 2000: loss 0.136760\n",
      "iteration 400 / 2000: loss 0.069721\n",
      "iteration 500 / 2000: loss 0.200542\n",
      "iteration 600 / 2000: loss 0.067010\n",
      "iteration 700 / 2000: loss 0.071229\n",
      "iteration 800 / 2000: loss 0.065387\n",
      "iteration 900 / 2000: loss 0.061041\n",
      "iteration 1000 / 2000: loss 0.122902\n",
      "iteration 1100 / 2000: loss 0.045626\n",
      "iteration 1200 / 2000: loss 0.058053\n",
      "iteration 1300 / 2000: loss 0.048518\n",
      "iteration 1400 / 2000: loss 0.019953\n",
      "iteration 1500 / 2000: loss 0.055798\n",
      "iteration 1600 / 2000: loss 0.049188\n",
      "iteration 1700 / 2000: loss 0.023539\n",
      "iteration 1800 / 2000: loss 0.033045\n",
      "iteration 1900 / 2000: loss 0.044695\n",
      "Hidden Size: 70, Learning Rate: 0.0075000250000000004, LR Decay: 0.8175, Regularization: 0.0, Validation accuracy: 0.978\n",
      "iteration 0 / 2000: loss 2.302613\n",
      "iteration 100 / 2000: loss 0.246953\n",
      "iteration 200 / 2000: loss 0.181774\n",
      "iteration 300 / 2000: loss 0.332598\n",
      "iteration 400 / 2000: loss 0.132227\n",
      "iteration 500 / 2000: loss 0.120489\n",
      "iteration 600 / 2000: loss 0.140299\n",
      "iteration 700 / 2000: loss 0.135552\n",
      "iteration 800 / 2000: loss 0.098663\n",
      "iteration 900 / 2000: loss 0.099176\n",
      "iteration 1000 / 2000: loss 0.114271\n",
      "iteration 1100 / 2000: loss 0.119038\n",
      "iteration 1200 / 2000: loss 0.117060\n",
      "iteration 1300 / 2000: loss 0.117257\n",
      "iteration 1400 / 2000: loss 0.086449\n",
      "iteration 1500 / 2000: loss 0.114323\n",
      "iteration 1600 / 2000: loss 0.076401\n",
      "iteration 1700 / 2000: loss 0.105065\n",
      "iteration 1800 / 2000: loss 0.086578\n",
      "iteration 1900 / 2000: loss 0.079637\n",
      "Hidden Size: 70, Learning Rate: 0.0075000250000000004, LR Decay: 0.8175, Regularization: 0.1, Validation accuracy: 0.9748\n",
      "iteration 0 / 2000: loss 2.302635\n",
      "iteration 100 / 2000: loss 0.267261\n",
      "iteration 200 / 2000: loss 0.202063\n",
      "iteration 300 / 2000: loss 0.222288\n",
      "iteration 400 / 2000: loss 0.153384\n",
      "iteration 500 / 2000: loss 0.153695\n",
      "iteration 600 / 2000: loss 0.165329\n",
      "iteration 700 / 2000: loss 0.203705\n",
      "iteration 800 / 2000: loss 0.175620\n",
      "iteration 900 / 2000: loss 0.126170\n",
      "iteration 1000 / 2000: loss 0.152448\n",
      "iteration 1100 / 2000: loss 0.146938\n",
      "iteration 1200 / 2000: loss 0.156341\n",
      "iteration 1300 / 2000: loss 0.191384\n",
      "iteration 1400 / 2000: loss 0.131526\n",
      "iteration 1500 / 2000: loss 0.130646\n",
      "iteration 1600 / 2000: loss 0.187347\n",
      "iteration 1700 / 2000: loss 0.127255\n",
      "iteration 1800 / 2000: loss 0.140011\n",
      "iteration 1900 / 2000: loss 0.154574\n",
      "Hidden Size: 70, Learning Rate: 0.0075000250000000004, LR Decay: 0.8175, Regularization: 0.2, Validation accuracy: 0.9764\n",
      "iteration 0 / 2000: loss 2.302684\n",
      "iteration 100 / 2000: loss 0.325570\n",
      "iteration 200 / 2000: loss 0.231894\n",
      "iteration 300 / 2000: loss 0.181182\n",
      "iteration 400 / 2000: loss 0.293029\n",
      "iteration 500 / 2000: loss 0.205510\n",
      "iteration 600 / 2000: loss 0.161303\n",
      "iteration 700 / 2000: loss 0.159538\n",
      "iteration 800 / 2000: loss 0.171915\n",
      "iteration 900 / 2000: loss 0.166008\n",
      "iteration 1000 / 2000: loss 0.185005\n",
      "iteration 1100 / 2000: loss 0.186515\n",
      "iteration 1200 / 2000: loss 0.155058\n",
      "iteration 1300 / 2000: loss 0.218419\n",
      "iteration 1400 / 2000: loss 0.135325\n",
      "iteration 1500 / 2000: loss 0.158208\n",
      "iteration 1600 / 2000: loss 0.139203\n",
      "iteration 1700 / 2000: loss 0.219838\n",
      "iteration 1800 / 2000: loss 0.149502\n",
      "iteration 1900 / 2000: loss 0.174972\n",
      "Hidden Size: 70, Learning Rate: 0.0075000250000000004, LR Decay: 0.8175, Regularization: 0.30000000000000004, Validation accuracy: 0.9708\n",
      "iteration 0 / 2000: loss 2.302703\n",
      "iteration 100 / 2000: loss 0.400098\n",
      "iteration 200 / 2000: loss 0.340069\n",
      "iteration 300 / 2000: loss 0.184137\n",
      "iteration 400 / 2000: loss 0.244519\n",
      "iteration 500 / 2000: loss 0.292091\n",
      "iteration 600 / 2000: loss 0.278052\n",
      "iteration 700 / 2000: loss 0.218793\n",
      "iteration 800 / 2000: loss 0.172202\n",
      "iteration 900 / 2000: loss 0.186244\n",
      "iteration 1000 / 2000: loss 0.230776\n",
      "iteration 1100 / 2000: loss 0.223274\n",
      "iteration 1200 / 2000: loss 0.256980\n",
      "iteration 1300 / 2000: loss 0.173379\n",
      "iteration 1400 / 2000: loss 0.186421\n",
      "iteration 1500 / 2000: loss 0.182336\n",
      "iteration 1600 / 2000: loss 0.242715\n",
      "iteration 1700 / 2000: loss 0.242502\n",
      "iteration 1800 / 2000: loss 0.223442\n",
      "iteration 1900 / 2000: loss 0.216701\n",
      "Hidden Size: 70, Learning Rate: 0.0075000250000000004, LR Decay: 0.8175, Regularization: 0.4, Validation accuracy: 0.9708\n",
      "iteration 0 / 2000: loss 2.302590\n",
      "iteration 100 / 2000: loss 0.297564\n",
      "iteration 200 / 2000: loss 0.213306\n",
      "iteration 300 / 2000: loss 0.172503\n",
      "iteration 400 / 2000: loss 0.066244\n",
      "iteration 500 / 2000: loss 0.100726\n",
      "iteration 600 / 2000: loss 0.099557\n",
      "iteration 700 / 2000: loss 0.092557\n",
      "iteration 800 / 2000: loss 0.043964\n",
      "iteration 900 / 2000: loss 0.083191\n",
      "iteration 1000 / 2000: loss 0.076063\n",
      "iteration 1100 / 2000: loss 0.093413\n",
      "iteration 1200 / 2000: loss 0.087878\n",
      "iteration 1300 / 2000: loss 0.081851\n",
      "iteration 1400 / 2000: loss 0.058406\n",
      "iteration 1500 / 2000: loss 0.091593\n",
      "iteration 1600 / 2000: loss 0.112979\n",
      "iteration 1700 / 2000: loss 0.102918\n",
      "iteration 1800 / 2000: loss 0.083386\n",
      "iteration 1900 / 2000: loss 0.028929\n",
      "Hidden Size: 70, Learning Rate: 0.0075000250000000004, LR Decay: 0.645, Regularization: 0.0, Validation accuracy: 0.9704\n",
      "iteration 0 / 2000: loss 2.302640\n",
      "iteration 100 / 2000: loss 0.335605\n",
      "iteration 200 / 2000: loss 0.167900\n",
      "iteration 300 / 2000: loss 0.234080\n",
      "iteration 400 / 2000: loss 0.137332\n",
      "iteration 500 / 2000: loss 0.133532\n",
      "iteration 600 / 2000: loss 0.119205\n",
      "iteration 700 / 2000: loss 0.138827\n",
      "iteration 800 / 2000: loss 0.181923\n",
      "iteration 900 / 2000: loss 0.187400\n",
      "iteration 1000 / 2000: loss 0.079595\n",
      "iteration 1100 / 2000: loss 0.093919\n",
      "iteration 1200 / 2000: loss 0.102149\n",
      "iteration 1300 / 2000: loss 0.141259\n",
      "iteration 1400 / 2000: loss 0.096549\n",
      "iteration 1500 / 2000: loss 0.132777\n",
      "iteration 1600 / 2000: loss 0.122442\n",
      "iteration 1700 / 2000: loss 0.150982\n",
      "iteration 1800 / 2000: loss 0.128354\n",
      "iteration 1900 / 2000: loss 0.091374\n",
      "Hidden Size: 70, Learning Rate: 0.0075000250000000004, LR Decay: 0.645, Regularization: 0.1, Validation accuracy: 0.9722\n",
      "iteration 0 / 2000: loss 2.302646\n",
      "iteration 100 / 2000: loss 0.295269\n",
      "iteration 200 / 2000: loss 0.278295\n",
      "iteration 300 / 2000: loss 0.258996\n",
      "iteration 400 / 2000: loss 0.148659\n",
      "iteration 500 / 2000: loss 0.136135\n",
      "iteration 600 / 2000: loss 0.138363\n",
      "iteration 700 / 2000: loss 0.202829\n",
      "iteration 800 / 2000: loss 0.144432\n",
      "iteration 900 / 2000: loss 0.137322\n",
      "iteration 1000 / 2000: loss 0.185481\n",
      "iteration 1100 / 2000: loss 0.137428\n",
      "iteration 1200 / 2000: loss 0.159039\n",
      "iteration 1300 / 2000: loss 0.139980\n",
      "iteration 1400 / 2000: loss 0.180951\n",
      "iteration 1500 / 2000: loss 0.163481\n",
      "iteration 1600 / 2000: loss 0.147970\n",
      "iteration 1700 / 2000: loss 0.148750\n",
      "iteration 1800 / 2000: loss 0.159608\n",
      "iteration 1900 / 2000: loss 0.156414\n",
      "Hidden Size: 70, Learning Rate: 0.0075000250000000004, LR Decay: 0.645, Regularization: 0.2, Validation accuracy: 0.9722\n",
      "iteration 0 / 2000: loss 2.302662\n",
      "iteration 100 / 2000: loss 0.351315\n",
      "iteration 200 / 2000: loss 0.190049\n",
      "iteration 300 / 2000: loss 0.221467\n",
      "iteration 400 / 2000: loss 0.296941\n",
      "iteration 500 / 2000: loss 0.201819\n",
      "iteration 600 / 2000: loss 0.164837\n",
      "iteration 700 / 2000: loss 0.213932\n",
      "iteration 800 / 2000: loss 0.173463\n",
      "iteration 900 / 2000: loss 0.165751\n",
      "iteration 1000 / 2000: loss 0.194346\n",
      "iteration 1100 / 2000: loss 0.203322\n",
      "iteration 1200 / 2000: loss 0.182852\n",
      "iteration 1300 / 2000: loss 0.185077\n",
      "iteration 1400 / 2000: loss 0.184722\n",
      "iteration 1500 / 2000: loss 0.175129\n",
      "iteration 1600 / 2000: loss 0.163831\n",
      "iteration 1700 / 2000: loss 0.186365\n",
      "iteration 1800 / 2000: loss 0.158904\n",
      "iteration 1900 / 2000: loss 0.184760\n",
      "Hidden Size: 70, Learning Rate: 0.0075000250000000004, LR Decay: 0.645, Regularization: 0.30000000000000004, Validation accuracy: 0.9706\n",
      "iteration 0 / 2000: loss 2.302681\n",
      "iteration 100 / 2000: loss 0.339599\n",
      "iteration 200 / 2000: loss 0.347370\n",
      "iteration 300 / 2000: loss 0.278679\n",
      "iteration 400 / 2000: loss 0.249373\n",
      "iteration 500 / 2000: loss 0.215552\n",
      "iteration 600 / 2000: loss 0.264088\n",
      "iteration 700 / 2000: loss 0.234653\n",
      "iteration 800 / 2000: loss 0.212839\n",
      "iteration 900 / 2000: loss 0.287566\n",
      "iteration 1000 / 2000: loss 0.223784\n",
      "iteration 1100 / 2000: loss 0.207888\n",
      "iteration 1200 / 2000: loss 0.266495\n",
      "iteration 1300 / 2000: loss 0.227354\n",
      "iteration 1400 / 2000: loss 0.177802\n",
      "iteration 1500 / 2000: loss 0.225969\n",
      "iteration 1600 / 2000: loss 0.231608\n",
      "iteration 1700 / 2000: loss 0.172730\n",
      "iteration 1800 / 2000: loss 0.172646\n",
      "iteration 1900 / 2000: loss 0.178444\n",
      "Hidden Size: 70, Learning Rate: 0.0075000250000000004, LR Decay: 0.645, Regularization: 0.4, Validation accuracy: 0.967\n",
      "iteration 0 / 2000: loss 2.302604\n",
      "iteration 100 / 2000: loss 0.321949\n",
      "iteration 200 / 2000: loss 0.341734\n",
      "iteration 300 / 2000: loss 0.132058\n",
      "iteration 400 / 2000: loss 0.148667\n",
      "iteration 500 / 2000: loss 0.140068\n",
      "iteration 600 / 2000: loss 0.157637\n",
      "iteration 700 / 2000: loss 0.147239\n",
      "iteration 800 / 2000: loss 0.088365\n",
      "iteration 900 / 2000: loss 0.117470\n",
      "iteration 1000 / 2000: loss 0.100536\n",
      "iteration 1100 / 2000: loss 0.194869\n",
      "iteration 1200 / 2000: loss 0.164203\n",
      "iteration 1300 / 2000: loss 0.102649\n",
      "iteration 1400 / 2000: loss 0.139394\n",
      "iteration 1500 / 2000: loss 0.097487\n",
      "iteration 1600 / 2000: loss 0.087530\n",
      "iteration 1700 / 2000: loss 0.089315\n",
      "iteration 1800 / 2000: loss 0.162750\n",
      "iteration 1900 / 2000: loss 0.122425\n",
      "Hidden Size: 70, Learning Rate: 0.0075000250000000004, LR Decay: 0.47250000000000003, Regularization: 0.0, Validation accuracy: 0.9642\n",
      "iteration 0 / 2000: loss 2.302613\n",
      "iteration 100 / 2000: loss 0.345712\n",
      "iteration 200 / 2000: loss 0.206120\n",
      "iteration 300 / 2000: loss 0.212101\n",
      "iteration 400 / 2000: loss 0.205428\n",
      "iteration 500 / 2000: loss 0.138440\n",
      "iteration 600 / 2000: loss 0.156230\n",
      "iteration 700 / 2000: loss 0.237526\n",
      "iteration 800 / 2000: loss 0.129327\n",
      "iteration 900 / 2000: loss 0.210411\n",
      "iteration 1000 / 2000: loss 0.104236\n",
      "iteration 1100 / 2000: loss 0.092103\n",
      "iteration 1200 / 2000: loss 0.158863\n",
      "iteration 1300 / 2000: loss 0.204335\n",
      "iteration 1400 / 2000: loss 0.092867\n",
      "iteration 1500 / 2000: loss 0.143243\n",
      "iteration 1600 / 2000: loss 0.146781\n",
      "iteration 1700 / 2000: loss 0.164820\n",
      "iteration 1800 / 2000: loss 0.129172\n",
      "iteration 1900 / 2000: loss 0.117403\n",
      "Hidden Size: 70, Learning Rate: 0.0075000250000000004, LR Decay: 0.47250000000000003, Regularization: 0.1, Validation accuracy: 0.9622\n",
      "iteration 0 / 2000: loss 2.302620\n",
      "iteration 100 / 2000: loss 0.337539\n",
      "iteration 200 / 2000: loss 0.237170\n",
      "iteration 300 / 2000: loss 0.185147\n",
      "iteration 400 / 2000: loss 0.166440\n",
      "iteration 500 / 2000: loss 0.208936\n",
      "iteration 600 / 2000: loss 0.154393\n",
      "iteration 700 / 2000: loss 0.195265\n",
      "iteration 800 / 2000: loss 0.151175\n",
      "iteration 900 / 2000: loss 0.142260\n",
      "iteration 1000 / 2000: loss 0.195244\n",
      "iteration 1100 / 2000: loss 0.250052\n",
      "iteration 1200 / 2000: loss 0.200398\n",
      "iteration 1300 / 2000: loss 0.176608\n",
      "iteration 1400 / 2000: loss 0.134780\n",
      "iteration 1500 / 2000: loss 0.244695\n",
      "iteration 1600 / 2000: loss 0.148517\n",
      "iteration 1700 / 2000: loss 0.128579\n",
      "iteration 1800 / 2000: loss 0.198758\n",
      "iteration 1900 / 2000: loss 0.208108\n",
      "Hidden Size: 70, Learning Rate: 0.0075000250000000004, LR Decay: 0.47250000000000003, Regularization: 0.2, Validation accuracy: 0.9626\n",
      "iteration 0 / 2000: loss 2.302678\n",
      "iteration 100 / 2000: loss 0.369429\n",
      "iteration 200 / 2000: loss 0.250487\n",
      "iteration 300 / 2000: loss 0.205521\n",
      "iteration 400 / 2000: loss 0.240852\n",
      "iteration 500 / 2000: loss 0.201345\n",
      "iteration 600 / 2000: loss 0.249221\n",
      "iteration 700 / 2000: loss 0.184395\n",
      "iteration 800 / 2000: loss 0.172353\n",
      "iteration 900 / 2000: loss 0.197715\n",
      "iteration 1000 / 2000: loss 0.175037\n",
      "iteration 1100 / 2000: loss 0.250447\n",
      "iteration 1200 / 2000: loss 0.185103\n",
      "iteration 1300 / 2000: loss 0.185717\n",
      "iteration 1400 / 2000: loss 0.182962\n",
      "iteration 1500 / 2000: loss 0.169914\n",
      "iteration 1600 / 2000: loss 0.193135\n",
      "iteration 1700 / 2000: loss 0.277673\n",
      "iteration 1800 / 2000: loss 0.193067\n",
      "iteration 1900 / 2000: loss 0.182399\n",
      "Hidden Size: 70, Learning Rate: 0.0075000250000000004, LR Decay: 0.47250000000000003, Regularization: 0.30000000000000004, Validation accuracy: 0.9628\n",
      "iteration 0 / 2000: loss 2.302708\n",
      "iteration 100 / 2000: loss 0.308466\n",
      "iteration 200 / 2000: loss 0.333146\n",
      "iteration 300 / 2000: loss 0.309824\n",
      "iteration 400 / 2000: loss 0.238116\n",
      "iteration 500 / 2000: loss 0.216008\n",
      "iteration 600 / 2000: loss 0.208125\n",
      "iteration 700 / 2000: loss 0.193446\n",
      "iteration 800 / 2000: loss 0.209067\n",
      "iteration 900 / 2000: loss 0.195288\n",
      "iteration 1000 / 2000: loss 0.261942\n",
      "iteration 1100 / 2000: loss 0.249502\n",
      "iteration 1200 / 2000: loss 0.273582\n",
      "iteration 1300 / 2000: loss 0.212054\n",
      "iteration 1400 / 2000: loss 0.209981\n",
      "iteration 1500 / 2000: loss 0.266805\n",
      "iteration 1600 / 2000: loss 0.187928\n",
      "iteration 1700 / 2000: loss 0.202741\n",
      "iteration 1800 / 2000: loss 0.237914\n",
      "iteration 1900 / 2000: loss 0.274819\n",
      "Hidden Size: 70, Learning Rate: 0.0075000250000000004, LR Decay: 0.47250000000000003, Regularization: 0.4, Validation accuracy: 0.9622\n",
      "iteration 0 / 2000: loss 2.302578\n",
      "iteration 100 / 2000: loss 0.328469\n",
      "iteration 200 / 2000: loss 0.227485\n",
      "iteration 300 / 2000: loss 0.378995\n",
      "iteration 400 / 2000: loss 0.158446\n",
      "iteration 500 / 2000: loss 0.223804\n",
      "iteration 600 / 2000: loss 0.180309\n",
      "iteration 700 / 2000: loss 0.232767\n",
      "iteration 800 / 2000: loss 0.111198\n",
      "iteration 900 / 2000: loss 0.142825\n",
      "iteration 1000 / 2000: loss 0.145899\n",
      "iteration 1100 / 2000: loss 0.201366\n",
      "iteration 1200 / 2000: loss 0.274034\n",
      "iteration 1300 / 2000: loss 0.185221\n",
      "iteration 1400 / 2000: loss 0.227502\n",
      "iteration 1500 / 2000: loss 0.121775\n",
      "iteration 1600 / 2000: loss 0.118791\n",
      "iteration 1700 / 2000: loss 0.193954\n",
      "iteration 1800 / 2000: loss 0.165645\n",
      "iteration 1900 / 2000: loss 0.159681\n",
      "Hidden Size: 70, Learning Rate: 0.0075000250000000004, LR Decay: 0.3, Regularization: 0.0, Validation accuracy: 0.9492\n",
      "iteration 0 / 2000: loss 2.302621\n",
      "iteration 100 / 2000: loss 0.438690\n",
      "iteration 200 / 2000: loss 0.386148\n",
      "iteration 300 / 2000: loss 0.298602\n",
      "iteration 400 / 2000: loss 0.233903\n",
      "iteration 500 / 2000: loss 0.167073\n",
      "iteration 600 / 2000: loss 0.243765\n",
      "iteration 700 / 2000: loss 0.130382\n",
      "iteration 800 / 2000: loss 0.196522\n",
      "iteration 900 / 2000: loss 0.282285\n",
      "iteration 1000 / 2000: loss 0.266146\n",
      "iteration 1100 / 2000: loss 0.213060\n",
      "iteration 1200 / 2000: loss 0.248301\n",
      "iteration 1300 / 2000: loss 0.198863\n",
      "iteration 1400 / 2000: loss 0.175776\n",
      "iteration 1500 / 2000: loss 0.310336\n",
      "iteration 1600 / 2000: loss 0.137289\n",
      "iteration 1700 / 2000: loss 0.272243\n",
      "iteration 1800 / 2000: loss 0.223349\n",
      "iteration 1900 / 2000: loss 0.244901\n",
      "Hidden Size: 70, Learning Rate: 0.0075000250000000004, LR Decay: 0.3, Regularization: 0.1, Validation accuracy: 0.9492\n",
      "iteration 0 / 2000: loss 2.302648\n",
      "iteration 100 / 2000: loss 0.446303\n",
      "iteration 200 / 2000: loss 0.323625\n",
      "iteration 300 / 2000: loss 0.347185\n",
      "iteration 400 / 2000: loss 0.238301\n",
      "iteration 500 / 2000: loss 0.172962\n",
      "iteration 600 / 2000: loss 0.218751\n",
      "iteration 700 / 2000: loss 0.235488\n",
      "iteration 800 / 2000: loss 0.220133\n",
      "iteration 900 / 2000: loss 0.237383\n",
      "iteration 1000 / 2000: loss 0.197899\n",
      "iteration 1100 / 2000: loss 0.246350\n",
      "iteration 1200 / 2000: loss 0.243638\n",
      "iteration 1300 / 2000: loss 0.237937\n",
      "iteration 1400 / 2000: loss 0.220015\n",
      "iteration 1500 / 2000: loss 0.198179\n",
      "iteration 1600 / 2000: loss 0.252400\n",
      "iteration 1700 / 2000: loss 0.264946\n",
      "iteration 1800 / 2000: loss 0.310834\n",
      "iteration 1900 / 2000: loss 0.249221\n",
      "Hidden Size: 70, Learning Rate: 0.0075000250000000004, LR Decay: 0.3, Regularization: 0.2, Validation accuracy: 0.9466\n",
      "iteration 0 / 2000: loss 2.302668\n",
      "iteration 100 / 2000: loss 0.260141\n",
      "iteration 200 / 2000: loss 0.290573\n",
      "iteration 300 / 2000: loss 0.346757\n",
      "iteration 400 / 2000: loss 0.299472\n",
      "iteration 500 / 2000: loss 0.261505\n",
      "iteration 600 / 2000: loss 0.341259\n",
      "iteration 700 / 2000: loss 0.281508\n",
      "iteration 800 / 2000: loss 0.274021\n",
      "iteration 900 / 2000: loss 0.225466\n",
      "iteration 1000 / 2000: loss 0.231946\n",
      "iteration 1100 / 2000: loss 0.238032\n",
      "iteration 1200 / 2000: loss 0.278624\n",
      "iteration 1300 / 2000: loss 0.284240\n",
      "iteration 1400 / 2000: loss 0.243164\n",
      "iteration 1500 / 2000: loss 0.206430\n",
      "iteration 1600 / 2000: loss 0.220836\n",
      "iteration 1700 / 2000: loss 0.249106\n",
      "iteration 1800 / 2000: loss 0.206449\n",
      "iteration 1900 / 2000: loss 0.261344\n",
      "Hidden Size: 70, Learning Rate: 0.0075000250000000004, LR Decay: 0.3, Regularization: 0.30000000000000004, Validation accuracy: 0.944\n",
      "iteration 0 / 2000: loss 2.302686\n",
      "iteration 100 / 2000: loss 0.419859\n",
      "iteration 200 / 2000: loss 0.313614\n",
      "iteration 300 / 2000: loss 0.279411\n",
      "iteration 400 / 2000: loss 0.268731\n",
      "iteration 500 / 2000: loss 0.214675\n",
      "iteration 600 / 2000: loss 0.231879\n",
      "iteration 700 / 2000: loss 0.278470\n",
      "iteration 800 / 2000: loss 0.251491\n",
      "iteration 900 / 2000: loss 0.177856\n",
      "iteration 1000 / 2000: loss 0.336758\n",
      "iteration 1100 / 2000: loss 0.341137\n",
      "iteration 1200 / 2000: loss 0.281305\n",
      "iteration 1300 / 2000: loss 0.374945\n",
      "iteration 1400 / 2000: loss 0.227759\n",
      "iteration 1500 / 2000: loss 0.235668\n",
      "iteration 1600 / 2000: loss 0.272444\n",
      "iteration 1700 / 2000: loss 0.284439\n",
      "iteration 1800 / 2000: loss 0.296179\n",
      "iteration 1900 / 2000: loss 0.283113\n",
      "Hidden Size: 70, Learning Rate: 0.0075000250000000004, LR Decay: 0.3, Regularization: 0.4, Validation accuracy: 0.9422\n",
      "iteration 0 / 2000: loss 2.302591\n",
      "iteration 100 / 2000: loss 0.382708\n",
      "iteration 200 / 2000: loss 0.200317\n",
      "iteration 300 / 2000: loss 0.102583\n",
      "iteration 400 / 2000: loss 0.127080\n",
      "iteration 500 / 2000: loss 0.142648\n",
      "iteration 600 / 2000: loss 0.139876\n",
      "iteration 700 / 2000: loss 0.175831\n",
      "iteration 800 / 2000: loss 0.082311\n",
      "iteration 900 / 2000: loss 0.061156\n",
      "iteration 1000 / 2000: loss 0.062855\n",
      "iteration 1100 / 2000: loss 0.071704\n",
      "iteration 1200 / 2000: loss 0.056992\n",
      "iteration 1300 / 2000: loss 0.021265\n",
      "iteration 1400 / 2000: loss 0.022439\n",
      "iteration 1500 / 2000: loss 0.046101\n",
      "iteration 1600 / 2000: loss 0.065308\n",
      "iteration 1700 / 2000: loss 0.022964\n",
      "iteration 1800 / 2000: loss 0.025791\n",
      "iteration 1900 / 2000: loss 0.026274\n",
      "Hidden Size: 70, Learning Rate: 0.00500005, LR Decay: 0.99, Regularization: 0.0, Validation accuracy: 0.9764\n",
      "iteration 0 / 2000: loss 2.302604\n",
      "iteration 100 / 2000: loss 0.379360\n",
      "iteration 200 / 2000: loss 0.203575\n",
      "iteration 300 / 2000: loss 0.160805\n",
      "iteration 400 / 2000: loss 0.205387\n",
      "iteration 500 / 2000: loss 0.184883\n",
      "iteration 600 / 2000: loss 0.167891\n",
      "iteration 700 / 2000: loss 0.109454\n",
      "iteration 800 / 2000: loss 0.127302\n",
      "iteration 900 / 2000: loss 0.132062\n",
      "iteration 1000 / 2000: loss 0.163695\n",
      "iteration 1100 / 2000: loss 0.082409\n",
      "iteration 1200 / 2000: loss 0.120291\n",
      "iteration 1300 / 2000: loss 0.102785\n",
      "iteration 1400 / 2000: loss 0.098879\n",
      "iteration 1500 / 2000: loss 0.119526\n",
      "iteration 1600 / 2000: loss 0.147364\n",
      "iteration 1700 / 2000: loss 0.106514\n",
      "iteration 1800 / 2000: loss 0.099065\n",
      "iteration 1900 / 2000: loss 0.099609\n",
      "Hidden Size: 70, Learning Rate: 0.00500005, LR Decay: 0.99, Regularization: 0.1, Validation accuracy: 0.9744\n",
      "iteration 0 / 2000: loss 2.302641\n",
      "iteration 100 / 2000: loss 0.305259\n",
      "iteration 200 / 2000: loss 0.295410\n",
      "iteration 300 / 2000: loss 0.270491\n",
      "iteration 400 / 2000: loss 0.149968\n",
      "iteration 500 / 2000: loss 0.179066\n",
      "iteration 600 / 2000: loss 0.180769\n",
      "iteration 700 / 2000: loss 0.190359\n",
      "iteration 800 / 2000: loss 0.164072\n",
      "iteration 900 / 2000: loss 0.163606\n",
      "iteration 1000 / 2000: loss 0.138319\n",
      "iteration 1100 / 2000: loss 0.140305\n",
      "iteration 1200 / 2000: loss 0.164292\n",
      "iteration 1300 / 2000: loss 0.174754\n",
      "iteration 1400 / 2000: loss 0.175327\n",
      "iteration 1500 / 2000: loss 0.163861\n",
      "iteration 1600 / 2000: loss 0.165787\n",
      "iteration 1700 / 2000: loss 0.140491\n",
      "iteration 1800 / 2000: loss 0.165329\n",
      "iteration 1900 / 2000: loss 0.121160\n",
      "Hidden Size: 70, Learning Rate: 0.00500005, LR Decay: 0.99, Regularization: 0.2, Validation accuracy: 0.972\n",
      "iteration 0 / 2000: loss 2.302660\n",
      "iteration 100 / 2000: loss 0.371277\n",
      "iteration 200 / 2000: loss 0.355065\n",
      "iteration 300 / 2000: loss 0.302652\n",
      "iteration 400 / 2000: loss 0.179562\n",
      "iteration 500 / 2000: loss 0.226686\n",
      "iteration 600 / 2000: loss 0.211773\n",
      "iteration 700 / 2000: loss 0.202478\n",
      "iteration 800 / 2000: loss 0.204352\n",
      "iteration 900 / 2000: loss 0.202042\n",
      "iteration 1000 / 2000: loss 0.224332\n",
      "iteration 1100 / 2000: loss 0.153863\n",
      "iteration 1200 / 2000: loss 0.215658\n",
      "iteration 1300 / 2000: loss 0.208479\n",
      "iteration 1400 / 2000: loss 0.222765\n",
      "iteration 1500 / 2000: loss 0.198803\n",
      "iteration 1600 / 2000: loss 0.177692\n",
      "iteration 1700 / 2000: loss 0.224519\n",
      "iteration 1800 / 2000: loss 0.163850\n",
      "iteration 1900 / 2000: loss 0.187034\n",
      "Hidden Size: 70, Learning Rate: 0.00500005, LR Decay: 0.99, Regularization: 0.30000000000000004, Validation accuracy: 0.9748\n",
      "iteration 0 / 2000: loss 2.302714\n",
      "iteration 100 / 2000: loss 0.335195\n",
      "iteration 200 / 2000: loss 0.357226\n",
      "iteration 300 / 2000: loss 0.209971\n",
      "iteration 400 / 2000: loss 0.262387\n",
      "iteration 500 / 2000: loss 0.191499\n",
      "iteration 600 / 2000: loss 0.229804\n",
      "iteration 700 / 2000: loss 0.210671\n",
      "iteration 800 / 2000: loss 0.194514\n",
      "iteration 900 / 2000: loss 0.222646\n",
      "iteration 1000 / 2000: loss 0.214355\n",
      "iteration 1100 / 2000: loss 0.208361\n",
      "iteration 1200 / 2000: loss 0.245815\n",
      "iteration 1300 / 2000: loss 0.254547\n",
      "iteration 1400 / 2000: loss 0.244776\n",
      "iteration 1500 / 2000: loss 0.282843\n",
      "iteration 1600 / 2000: loss 0.216705\n",
      "iteration 1700 / 2000: loss 0.216631\n",
      "iteration 1800 / 2000: loss 0.202504\n",
      "iteration 1900 / 2000: loss 0.218354\n",
      "Hidden Size: 70, Learning Rate: 0.00500005, LR Decay: 0.99, Regularization: 0.4, Validation accuracy: 0.972\n",
      "iteration 0 / 2000: loss 2.302587\n",
      "iteration 100 / 2000: loss 0.228346\n",
      "iteration 200 / 2000: loss 0.305795\n",
      "iteration 300 / 2000: loss 0.209511\n",
      "iteration 400 / 2000: loss 0.129584\n",
      "iteration 500 / 2000: loss 0.162943\n",
      "iteration 600 / 2000: loss 0.062639\n",
      "iteration 700 / 2000: loss 0.066919\n",
      "iteration 800 / 2000: loss 0.070722\n",
      "iteration 900 / 2000: loss 0.090058\n",
      "iteration 1000 / 2000: loss 0.138447\n",
      "iteration 1100 / 2000: loss 0.112008\n",
      "iteration 1200 / 2000: loss 0.058548\n",
      "iteration 1300 / 2000: loss 0.073605\n",
      "iteration 1400 / 2000: loss 0.053003\n",
      "iteration 1500 / 2000: loss 0.082682\n",
      "iteration 1600 / 2000: loss 0.082450\n",
      "iteration 1700 / 2000: loss 0.060930\n",
      "iteration 1800 / 2000: loss 0.072302\n",
      "iteration 1900 / 2000: loss 0.054305\n",
      "Hidden Size: 70, Learning Rate: 0.00500005, LR Decay: 0.8175, Regularization: 0.0, Validation accuracy: 0.9742\n",
      "iteration 0 / 2000: loss 2.302599\n",
      "iteration 100 / 2000: loss 0.335996\n",
      "iteration 200 / 2000: loss 0.203978\n",
      "iteration 300 / 2000: loss 0.246785\n",
      "iteration 400 / 2000: loss 0.121333\n",
      "iteration 500 / 2000: loss 0.300714\n",
      "iteration 600 / 2000: loss 0.119218\n",
      "iteration 700 / 2000: loss 0.135121\n",
      "iteration 800 / 2000: loss 0.102902\n",
      "iteration 900 / 2000: loss 0.118776\n",
      "iteration 1000 / 2000: loss 0.144978\n",
      "iteration 1100 / 2000: loss 0.131725\n",
      "iteration 1200 / 2000: loss 0.092616\n",
      "iteration 1300 / 2000: loss 0.098989\n",
      "iteration 1400 / 2000: loss 0.096985\n",
      "iteration 1500 / 2000: loss 0.128790\n",
      "iteration 1600 / 2000: loss 0.143943\n",
      "iteration 1700 / 2000: loss 0.090373\n",
      "iteration 1800 / 2000: loss 0.109681\n",
      "iteration 1900 / 2000: loss 0.103761\n",
      "Hidden Size: 70, Learning Rate: 0.00500005, LR Decay: 0.8175, Regularization: 0.1, Validation accuracy: 0.9756\n",
      "iteration 0 / 2000: loss 2.302624\n",
      "iteration 100 / 2000: loss 0.365276\n",
      "iteration 200 / 2000: loss 0.264826\n",
      "iteration 300 / 2000: loss 0.182539\n",
      "iteration 400 / 2000: loss 0.207573\n",
      "iteration 500 / 2000: loss 0.222162\n",
      "iteration 600 / 2000: loss 0.164843\n",
      "iteration 700 / 2000: loss 0.194101\n",
      "iteration 800 / 2000: loss 0.170756\n",
      "iteration 900 / 2000: loss 0.142527\n",
      "iteration 1000 / 2000: loss 0.195280\n",
      "iteration 1100 / 2000: loss 0.164599\n",
      "iteration 1200 / 2000: loss 0.148867\n",
      "iteration 1300 / 2000: loss 0.148957\n",
      "iteration 1400 / 2000: loss 0.162600\n",
      "iteration 1500 / 2000: loss 0.194295\n",
      "iteration 1600 / 2000: loss 0.134183\n",
      "iteration 1700 / 2000: loss 0.150491\n",
      "iteration 1800 / 2000: loss 0.153145\n",
      "iteration 1900 / 2000: loss 0.170795\n",
      "Hidden Size: 70, Learning Rate: 0.00500005, LR Decay: 0.8175, Regularization: 0.2, Validation accuracy: 0.9718\n",
      "iteration 0 / 2000: loss 2.302684\n",
      "iteration 100 / 2000: loss 0.372876\n",
      "iteration 200 / 2000: loss 0.267859\n",
      "iteration 300 / 2000: loss 0.171067\n",
      "iteration 400 / 2000: loss 0.267868\n",
      "iteration 500 / 2000: loss 0.216783\n",
      "iteration 600 / 2000: loss 0.167796\n",
      "iteration 700 / 2000: loss 0.187980\n",
      "iteration 800 / 2000: loss 0.210268\n",
      "iteration 900 / 2000: loss 0.187820\n",
      "iteration 1000 / 2000: loss 0.159373\n",
      "iteration 1100 / 2000: loss 0.221172\n",
      "iteration 1200 / 2000: loss 0.206107\n",
      "iteration 1300 / 2000: loss 0.229550\n",
      "iteration 1400 / 2000: loss 0.142531\n",
      "iteration 1500 / 2000: loss 0.153708\n",
      "iteration 1600 / 2000: loss 0.162130\n",
      "iteration 1700 / 2000: loss 0.207365\n",
      "iteration 1800 / 2000: loss 0.206133\n",
      "iteration 1900 / 2000: loss 0.211860\n",
      "Hidden Size: 70, Learning Rate: 0.00500005, LR Decay: 0.8175, Regularization: 0.30000000000000004, Validation accuracy: 0.9714\n",
      "iteration 0 / 2000: loss 2.302698\n",
      "iteration 100 / 2000: loss 0.375662\n",
      "iteration 200 / 2000: loss 0.306391\n",
      "iteration 300 / 2000: loss 0.288655\n",
      "iteration 400 / 2000: loss 0.287744\n",
      "iteration 500 / 2000: loss 0.240297\n",
      "iteration 600 / 2000: loss 0.215473\n",
      "iteration 700 / 2000: loss 0.184197\n",
      "iteration 800 / 2000: loss 0.222434\n",
      "iteration 900 / 2000: loss 0.239314\n",
      "iteration 1000 / 2000: loss 0.261355\n",
      "iteration 1100 / 2000: loss 0.187075\n",
      "iteration 1200 / 2000: loss 0.240507\n",
      "iteration 1300 / 2000: loss 0.212972\n",
      "iteration 1400 / 2000: loss 0.196180\n",
      "iteration 1500 / 2000: loss 0.224473\n",
      "iteration 1600 / 2000: loss 0.154300\n",
      "iteration 1700 / 2000: loss 0.179189\n",
      "iteration 1800 / 2000: loss 0.241655\n",
      "iteration 1900 / 2000: loss 0.216889\n",
      "Hidden Size: 70, Learning Rate: 0.00500005, LR Decay: 0.8175, Regularization: 0.4, Validation accuracy: 0.97\n",
      "iteration 0 / 2000: loss 2.302575\n",
      "iteration 100 / 2000: loss 0.459638\n",
      "iteration 200 / 2000: loss 0.273076\n",
      "iteration 300 / 2000: loss 0.246292\n",
      "iteration 400 / 2000: loss 0.144421\n",
      "iteration 500 / 2000: loss 0.209247\n",
      "iteration 600 / 2000: loss 0.131011\n",
      "iteration 700 / 2000: loss 0.129795\n",
      "iteration 800 / 2000: loss 0.092936\n",
      "iteration 900 / 2000: loss 0.166805\n",
      "iteration 1000 / 2000: loss 0.104309\n",
      "iteration 1100 / 2000: loss 0.078557\n",
      "iteration 1200 / 2000: loss 0.069875\n",
      "iteration 1300 / 2000: loss 0.082676\n",
      "iteration 1400 / 2000: loss 0.073322\n",
      "iteration 1500 / 2000: loss 0.076635\n",
      "iteration 1600 / 2000: loss 0.066126\n",
      "iteration 1700 / 2000: loss 0.090925\n",
      "iteration 1800 / 2000: loss 0.146163\n",
      "iteration 1900 / 2000: loss 0.091093\n",
      "Hidden Size: 70, Learning Rate: 0.00500005, LR Decay: 0.645, Regularization: 0.0, Validation accuracy: 0.9694\n",
      "iteration 0 / 2000: loss 2.302601\n",
      "iteration 100 / 2000: loss 0.278694\n",
      "iteration 200 / 2000: loss 0.284554\n",
      "iteration 300 / 2000: loss 0.206691\n",
      "iteration 400 / 2000: loss 0.305382\n",
      "iteration 500 / 2000: loss 0.168737\n",
      "iteration 600 / 2000: loss 0.189625\n",
      "iteration 700 / 2000: loss 0.110453\n",
      "iteration 800 / 2000: loss 0.122206\n",
      "iteration 900 / 2000: loss 0.162127\n",
      "iteration 1000 / 2000: loss 0.118234\n",
      "iteration 1100 / 2000: loss 0.170123\n",
      "iteration 1200 / 2000: loss 0.141499\n",
      "iteration 1300 / 2000: loss 0.161325\n",
      "iteration 1400 / 2000: loss 0.171765\n",
      "iteration 1500 / 2000: loss 0.119404\n",
      "iteration 1600 / 2000: loss 0.099737\n",
      "iteration 1700 / 2000: loss 0.123060\n",
      "iteration 1800 / 2000: loss 0.114108\n",
      "iteration 1900 / 2000: loss 0.129388\n",
      "Hidden Size: 70, Learning Rate: 0.00500005, LR Decay: 0.645, Regularization: 0.1, Validation accuracy: 0.9686\n",
      "iteration 0 / 2000: loss 2.302628\n",
      "iteration 100 / 2000: loss 0.301095\n",
      "iteration 200 / 2000: loss 0.289253\n",
      "iteration 300 / 2000: loss 0.237575\n",
      "iteration 400 / 2000: loss 0.162874\n",
      "iteration 500 / 2000: loss 0.221902\n",
      "iteration 600 / 2000: loss 0.203922\n",
      "iteration 700 / 2000: loss 0.171230\n",
      "iteration 800 / 2000: loss 0.155339\n",
      "iteration 900 / 2000: loss 0.165086\n",
      "iteration 1000 / 2000: loss 0.206958\n",
      "iteration 1100 / 2000: loss 0.112098\n",
      "iteration 1200 / 2000: loss 0.231030\n",
      "iteration 1300 / 2000: loss 0.141219\n",
      "iteration 1400 / 2000: loss 0.176672\n",
      "iteration 1500 / 2000: loss 0.218284\n",
      "iteration 1600 / 2000: loss 0.209678\n",
      "iteration 1700 / 2000: loss 0.216964\n",
      "iteration 1800 / 2000: loss 0.198800\n",
      "iteration 1900 / 2000: loss 0.123699\n",
      "Hidden Size: 70, Learning Rate: 0.00500005, LR Decay: 0.645, Regularization: 0.2, Validation accuracy: 0.9644\n",
      "iteration 0 / 2000: loss 2.302632\n",
      "iteration 100 / 2000: loss 0.395244\n",
      "iteration 200 / 2000: loss 0.337089\n",
      "iteration 300 / 2000: loss 0.264581\n",
      "iteration 400 / 2000: loss 0.270658\n",
      "iteration 500 / 2000: loss 0.249867\n",
      "iteration 600 / 2000: loss 0.212677\n",
      "iteration 700 / 2000: loss 0.218234\n",
      "iteration 800 / 2000: loss 0.263404\n",
      "iteration 900 / 2000: loss 0.219742\n",
      "iteration 1000 / 2000: loss 0.162947\n",
      "iteration 1100 / 2000: loss 0.197026\n",
      "iteration 1200 / 2000: loss 0.206048\n",
      "iteration 1300 / 2000: loss 0.179748\n",
      "iteration 1400 / 2000: loss 0.168980\n",
      "iteration 1500 / 2000: loss 0.192634\n",
      "iteration 1600 / 2000: loss 0.188229\n",
      "iteration 1700 / 2000: loss 0.162174\n",
      "iteration 1800 / 2000: loss 0.181177\n",
      "iteration 1900 / 2000: loss 0.198044\n",
      "Hidden Size: 70, Learning Rate: 0.00500005, LR Decay: 0.645, Regularization: 0.30000000000000004, Validation accuracy: 0.9654\n",
      "iteration 0 / 2000: loss 2.302704\n",
      "iteration 100 / 2000: loss 0.317088\n",
      "iteration 200 / 2000: loss 0.217693\n",
      "iteration 300 / 2000: loss 0.301742\n",
      "iteration 400 / 2000: loss 0.260549\n",
      "iteration 500 / 2000: loss 0.238572\n",
      "iteration 600 / 2000: loss 0.267302\n",
      "iteration 700 / 2000: loss 0.230739\n",
      "iteration 800 / 2000: loss 0.214467\n",
      "iteration 900 / 2000: loss 0.253903\n",
      "iteration 1000 / 2000: loss 0.237107\n",
      "iteration 1100 / 2000: loss 0.230483\n",
      "iteration 1200 / 2000: loss 0.224284\n",
      "iteration 1300 / 2000: loss 0.253600\n",
      "iteration 1400 / 2000: loss 0.170587\n",
      "iteration 1500 / 2000: loss 0.224486\n",
      "iteration 1600 / 2000: loss 0.193609\n",
      "iteration 1700 / 2000: loss 0.219227\n",
      "iteration 1800 / 2000: loss 0.230437\n",
      "iteration 1900 / 2000: loss 0.239709\n",
      "Hidden Size: 70, Learning Rate: 0.00500005, LR Decay: 0.645, Regularization: 0.4, Validation accuracy: 0.9626\n",
      "iteration 0 / 2000: loss 2.302598\n",
      "iteration 100 / 2000: loss 0.316751\n",
      "iteration 200 / 2000: loss 0.312740\n",
      "iteration 300 / 2000: loss 0.202264\n",
      "iteration 400 / 2000: loss 0.250026\n",
      "iteration 500 / 2000: loss 0.234496\n",
      "iteration 600 / 2000: loss 0.144149\n",
      "iteration 700 / 2000: loss 0.263065\n",
      "iteration 800 / 2000: loss 0.187213\n",
      "iteration 900 / 2000: loss 0.186359\n",
      "iteration 1000 / 2000: loss 0.231665\n",
      "iteration 1100 / 2000: loss 0.174012\n",
      "iteration 1200 / 2000: loss 0.120828\n",
      "iteration 1300 / 2000: loss 0.136317\n",
      "iteration 1400 / 2000: loss 0.114416\n",
      "iteration 1500 / 2000: loss 0.266582\n",
      "iteration 1600 / 2000: loss 0.106883\n",
      "iteration 1700 / 2000: loss 0.126518\n",
      "iteration 1800 / 2000: loss 0.128856\n",
      "iteration 1900 / 2000: loss 0.126314\n",
      "Hidden Size: 70, Learning Rate: 0.00500005, LR Decay: 0.47250000000000003, Regularization: 0.0, Validation accuracy: 0.959\n",
      "iteration 0 / 2000: loss 2.302615\n",
      "iteration 100 / 2000: loss 0.438603\n",
      "iteration 200 / 2000: loss 0.336780\n",
      "iteration 300 / 2000: loss 0.234267\n",
      "iteration 400 / 2000: loss 0.145752\n",
      "iteration 500 / 2000: loss 0.149930\n",
      "iteration 600 / 2000: loss 0.237865\n",
      "iteration 700 / 2000: loss 0.243286\n",
      "iteration 800 / 2000: loss 0.223351\n",
      "iteration 900 / 2000: loss 0.173905\n",
      "iteration 1000 / 2000: loss 0.235235\n",
      "iteration 1100 / 2000: loss 0.211388\n",
      "iteration 1200 / 2000: loss 0.125698\n",
      "iteration 1300 / 2000: loss 0.159550\n",
      "iteration 1400 / 2000: loss 0.186144\n",
      "iteration 1500 / 2000: loss 0.187634\n",
      "iteration 1600 / 2000: loss 0.209261\n",
      "iteration 1700 / 2000: loss 0.185921\n",
      "iteration 1800 / 2000: loss 0.111609\n",
      "iteration 1900 / 2000: loss 0.191398\n",
      "Hidden Size: 70, Learning Rate: 0.00500005, LR Decay: 0.47250000000000003, Regularization: 0.1, Validation accuracy: 0.9522\n",
      "iteration 0 / 2000: loss 2.302625\n",
      "iteration 100 / 2000: loss 0.272615\n",
      "iteration 200 / 2000: loss 0.343366\n",
      "iteration 300 / 2000: loss 0.275081\n",
      "iteration 400 / 2000: loss 0.217602\n",
      "iteration 500 / 2000: loss 0.279220\n",
      "iteration 600 / 2000: loss 0.257121\n",
      "iteration 700 / 2000: loss 0.226964\n",
      "iteration 800 / 2000: loss 0.183097\n",
      "iteration 900 / 2000: loss 0.241447\n",
      "iteration 1000 / 2000: loss 0.179555\n",
      "iteration 1100 / 2000: loss 0.179344\n",
      "iteration 1200 / 2000: loss 0.213917\n",
      "iteration 1300 / 2000: loss 0.210378\n",
      "iteration 1400 / 2000: loss 0.297312\n",
      "iteration 1500 / 2000: loss 0.132606\n",
      "iteration 1600 / 2000: loss 0.195409\n",
      "iteration 1700 / 2000: loss 0.201271\n",
      "iteration 1800 / 2000: loss 0.175518\n",
      "iteration 1900 / 2000: loss 0.129798\n",
      "Hidden Size: 70, Learning Rate: 0.00500005, LR Decay: 0.47250000000000003, Regularization: 0.2, Validation accuracy: 0.9566\n",
      "iteration 0 / 2000: loss 2.302686\n",
      "iteration 100 / 2000: loss 0.499377\n",
      "iteration 200 / 2000: loss 0.360871\n",
      "iteration 300 / 2000: loss 0.238256\n",
      "iteration 400 / 2000: loss 0.323511\n",
      "iteration 500 / 2000: loss 0.297157\n",
      "iteration 600 / 2000: loss 0.267118\n",
      "iteration 700 / 2000: loss 0.170362\n",
      "iteration 800 / 2000: loss 0.232543\n",
      "iteration 900 / 2000: loss 0.249443\n",
      "iteration 1000 / 2000: loss 0.259455\n",
      "iteration 1100 / 2000: loss 0.239577\n",
      "iteration 1200 / 2000: loss 0.357364\n",
      "iteration 1300 / 2000: loss 0.234386\n",
      "iteration 1400 / 2000: loss 0.225486\n",
      "iteration 1500 / 2000: loss 0.329340\n",
      "iteration 1600 / 2000: loss 0.167322\n",
      "iteration 1700 / 2000: loss 0.221517\n",
      "iteration 1800 / 2000: loss 0.262705\n",
      "iteration 1900 / 2000: loss 0.207472\n",
      "Hidden Size: 70, Learning Rate: 0.00500005, LR Decay: 0.47250000000000003, Regularization: 0.30000000000000004, Validation accuracy: 0.9538\n",
      "iteration 0 / 2000: loss 2.302686\n",
      "iteration 100 / 2000: loss 0.404369\n",
      "iteration 200 / 2000: loss 0.436339\n",
      "iteration 300 / 2000: loss 0.262208\n",
      "iteration 400 / 2000: loss 0.273293\n",
      "iteration 500 / 2000: loss 0.226162\n",
      "iteration 600 / 2000: loss 0.251595\n",
      "iteration 700 / 2000: loss 0.297081\n",
      "iteration 800 / 2000: loss 0.228169\n",
      "iteration 900 / 2000: loss 0.243033\n",
      "iteration 1000 / 2000: loss 0.260639\n",
      "iteration 1100 / 2000: loss 0.200900\n",
      "iteration 1200 / 2000: loss 0.239697\n",
      "iteration 1300 / 2000: loss 0.261313\n",
      "iteration 1400 / 2000: loss 0.267909\n",
      "iteration 1500 / 2000: loss 0.193466\n",
      "iteration 1600 / 2000: loss 0.174194\n",
      "iteration 1700 / 2000: loss 0.210033\n",
      "iteration 1800 / 2000: loss 0.334055\n",
      "iteration 1900 / 2000: loss 0.295586\n",
      "Hidden Size: 70, Learning Rate: 0.00500005, LR Decay: 0.47250000000000003, Regularization: 0.4, Validation accuracy: 0.952\n",
      "iteration 0 / 2000: loss 2.302592\n",
      "iteration 100 / 2000: loss 0.496885\n",
      "iteration 200 / 2000: loss 0.278565\n",
      "iteration 300 / 2000: loss 0.261902\n",
      "iteration 400 / 2000: loss 0.225959\n",
      "iteration 500 / 2000: loss 0.263701\n",
      "iteration 600 / 2000: loss 0.160585\n",
      "iteration 700 / 2000: loss 0.297902\n",
      "iteration 800 / 2000: loss 0.256559\n",
      "iteration 900 / 2000: loss 0.234837\n",
      "iteration 1000 / 2000: loss 0.248007\n",
      "iteration 1100 / 2000: loss 0.220315\n",
      "iteration 1200 / 2000: loss 0.201378\n",
      "iteration 1300 / 2000: loss 0.230036\n",
      "iteration 1400 / 2000: loss 0.236219\n",
      "iteration 1500 / 2000: loss 0.188838\n",
      "iteration 1600 / 2000: loss 0.250487\n",
      "iteration 1700 / 2000: loss 0.214226\n",
      "iteration 1800 / 2000: loss 0.287341\n",
      "iteration 1900 / 2000: loss 0.136296\n",
      "Hidden Size: 70, Learning Rate: 0.00500005, LR Decay: 0.3, Regularization: 0.0, Validation accuracy: 0.9368\n",
      "iteration 0 / 2000: loss 2.302625\n",
      "iteration 100 / 2000: loss 0.507889\n",
      "iteration 200 / 2000: loss 0.341843\n",
      "iteration 300 / 2000: loss 0.455185\n",
      "iteration 400 / 2000: loss 0.215817\n",
      "iteration 500 / 2000: loss 0.215161\n",
      "iteration 600 / 2000: loss 0.269680\n",
      "iteration 700 / 2000: loss 0.254524\n",
      "iteration 800 / 2000: loss 0.219013\n",
      "iteration 900 / 2000: loss 0.262352\n",
      "iteration 1000 / 2000: loss 0.260413\n",
      "iteration 1100 / 2000: loss 0.255084\n",
      "iteration 1200 / 2000: loss 0.243549\n",
      "iteration 1300 / 2000: loss 0.217865\n",
      "iteration 1400 / 2000: loss 0.236569\n",
      "iteration 1500 / 2000: loss 0.283542\n",
      "iteration 1600 / 2000: loss 0.295063\n",
      "iteration 1700 / 2000: loss 0.236807\n",
      "iteration 1800 / 2000: loss 0.200004\n",
      "iteration 1900 / 2000: loss 0.254969\n",
      "Hidden Size: 70, Learning Rate: 0.00500005, LR Decay: 0.3, Regularization: 0.1, Validation accuracy: 0.9348\n",
      "iteration 0 / 2000: loss 2.302628\n",
      "iteration 100 / 2000: loss 0.474249\n",
      "iteration 200 / 2000: loss 0.408244\n",
      "iteration 300 / 2000: loss 0.495998\n",
      "iteration 400 / 2000: loss 0.238253\n",
      "iteration 500 / 2000: loss 0.288286\n",
      "iteration 600 / 2000: loss 0.239510\n",
      "iteration 700 / 2000: loss 0.237660\n",
      "iteration 800 / 2000: loss 0.235548\n",
      "iteration 900 / 2000: loss 0.436018\n",
      "iteration 1000 / 2000: loss 0.281632\n",
      "iteration 1100 / 2000: loss 0.282477\n",
      "iteration 1200 / 2000: loss 0.329010\n",
      "iteration 1300 / 2000: loss 0.221622\n",
      "iteration 1400 / 2000: loss 0.297440\n",
      "iteration 1500 / 2000: loss 0.319213\n",
      "iteration 1600 / 2000: loss 0.237855\n",
      "iteration 1700 / 2000: loss 0.301033\n",
      "iteration 1800 / 2000: loss 0.253575\n",
      "iteration 1900 / 2000: loss 0.356331\n",
      "Hidden Size: 70, Learning Rate: 0.00500005, LR Decay: 0.3, Regularization: 0.2, Validation accuracy: 0.9328\n",
      "iteration 0 / 2000: loss 2.302666\n",
      "iteration 100 / 2000: loss 0.579139\n",
      "iteration 200 / 2000: loss 0.325354\n",
      "iteration 300 / 2000: loss 0.254307\n",
      "iteration 400 / 2000: loss 0.291511\n",
      "iteration 500 / 2000: loss 0.243336\n",
      "iteration 600 / 2000: loss 0.277244\n",
      "iteration 700 / 2000: loss 0.308872\n",
      "iteration 800 / 2000: loss 0.192690\n",
      "iteration 900 / 2000: loss 0.308761\n",
      "iteration 1000 / 2000: loss 0.360316\n",
      "iteration 1100 / 2000: loss 0.252512\n",
      "iteration 1200 / 2000: loss 0.315577\n",
      "iteration 1300 / 2000: loss 0.382816\n",
      "iteration 1400 / 2000: loss 0.252882\n",
      "iteration 1500 / 2000: loss 0.347453\n",
      "iteration 1600 / 2000: loss 0.216898\n",
      "iteration 1700 / 2000: loss 0.266217\n",
      "iteration 1800 / 2000: loss 0.399870\n",
      "iteration 1900 / 2000: loss 0.353256\n",
      "Hidden Size: 70, Learning Rate: 0.00500005, LR Decay: 0.3, Regularization: 0.30000000000000004, Validation accuracy: 0.9324\n",
      "iteration 0 / 2000: loss 2.302714\n",
      "iteration 100 / 2000: loss 0.493619\n",
      "iteration 200 / 2000: loss 0.403385\n",
      "iteration 300 / 2000: loss 0.423587\n",
      "iteration 400 / 2000: loss 0.254959\n",
      "iteration 500 / 2000: loss 0.345114\n",
      "iteration 600 / 2000: loss 0.245149\n",
      "iteration 700 / 2000: loss 0.313295\n",
      "iteration 800 / 2000: loss 0.256972\n",
      "iteration 900 / 2000: loss 0.317587\n",
      "iteration 1000 / 2000: loss 0.185664\n",
      "iteration 1100 / 2000: loss 0.300519\n",
      "iteration 1200 / 2000: loss 0.286261\n",
      "iteration 1300 / 2000: loss 0.280997\n",
      "iteration 1400 / 2000: loss 0.241880\n",
      "iteration 1500 / 2000: loss 0.318396\n",
      "iteration 1600 / 2000: loss 0.262614\n",
      "iteration 1700 / 2000: loss 0.278109\n",
      "iteration 1800 / 2000: loss 0.274351\n",
      "iteration 1900 / 2000: loss 0.333977\n",
      "Hidden Size: 70, Learning Rate: 0.00500005, LR Decay: 0.3, Regularization: 0.4, Validation accuracy: 0.9352\n",
      "iteration 0 / 2000: loss 2.302576\n",
      "iteration 100 / 2000: loss 0.469514\n",
      "iteration 200 / 2000: loss 0.254503\n",
      "iteration 300 / 2000: loss 0.217914\n",
      "iteration 400 / 2000: loss 0.343858\n",
      "iteration 500 / 2000: loss 0.153348\n",
      "iteration 600 / 2000: loss 0.194932\n",
      "iteration 700 / 2000: loss 0.175305\n",
      "iteration 800 / 2000: loss 0.101051\n",
      "iteration 900 / 2000: loss 0.118864\n",
      "iteration 1000 / 2000: loss 0.099850\n",
      "iteration 1100 / 2000: loss 0.163562\n",
      "iteration 1200 / 2000: loss 0.121660\n",
      "iteration 1300 / 2000: loss 0.095033\n",
      "iteration 1400 / 2000: loss 0.081235\n",
      "iteration 1500 / 2000: loss 0.117389\n",
      "iteration 1600 / 2000: loss 0.083159\n",
      "iteration 1700 / 2000: loss 0.103137\n",
      "iteration 1800 / 2000: loss 0.063338\n",
      "iteration 1900 / 2000: loss 0.025125\n",
      "Hidden Size: 70, Learning Rate: 0.002500074999999999, LR Decay: 0.99, Regularization: 0.0, Validation accuracy: 0.9752\n",
      "iteration 0 / 2000: loss 2.302619\n",
      "iteration 100 / 2000: loss 0.383117\n",
      "iteration 200 / 2000: loss 0.300058\n",
      "iteration 300 / 2000: loss 0.239865\n",
      "iteration 400 / 2000: loss 0.197842\n",
      "iteration 500 / 2000: loss 0.234907\n",
      "iteration 600 / 2000: loss 0.153492\n",
      "iteration 700 / 2000: loss 0.164557\n",
      "iteration 800 / 2000: loss 0.107616\n",
      "iteration 900 / 2000: loss 0.119933\n",
      "iteration 1000 / 2000: loss 0.159821\n",
      "iteration 1100 / 2000: loss 0.128902\n",
      "iteration 1200 / 2000: loss 0.147010\n",
      "iteration 1300 / 2000: loss 0.159369\n",
      "iteration 1400 / 2000: loss 0.115328\n",
      "iteration 1500 / 2000: loss 0.127476\n",
      "iteration 1600 / 2000: loss 0.136524\n",
      "iteration 1700 / 2000: loss 0.091062\n",
      "iteration 1800 / 2000: loss 0.092885\n",
      "iteration 1900 / 2000: loss 0.100055\n",
      "Hidden Size: 70, Learning Rate: 0.002500074999999999, LR Decay: 0.99, Regularization: 0.1, Validation accuracy: 0.9716\n",
      "iteration 0 / 2000: loss 2.302639\n",
      "iteration 100 / 2000: loss 0.310291\n",
      "iteration 200 / 2000: loss 0.328038\n",
      "iteration 300 / 2000: loss 0.297284\n",
      "iteration 400 / 2000: loss 0.259501\n",
      "iteration 500 / 2000: loss 0.230060\n",
      "iteration 600 / 2000: loss 0.198684\n",
      "iteration 700 / 2000: loss 0.205151\n",
      "iteration 800 / 2000: loss 0.147942\n",
      "iteration 900 / 2000: loss 0.187061\n",
      "iteration 1000 / 2000: loss 0.145151\n",
      "iteration 1100 / 2000: loss 0.165835\n",
      "iteration 1200 / 2000: loss 0.149149\n",
      "iteration 1300 / 2000: loss 0.154074\n",
      "iteration 1400 / 2000: loss 0.231246\n",
      "iteration 1500 / 2000: loss 0.179234\n",
      "iteration 1600 / 2000: loss 0.151156\n",
      "iteration 1700 / 2000: loss 0.198996\n",
      "iteration 1800 / 2000: loss 0.154544\n",
      "iteration 1900 / 2000: loss 0.119101\n",
      "Hidden Size: 70, Learning Rate: 0.002500074999999999, LR Decay: 0.99, Regularization: 0.2, Validation accuracy: 0.971\n",
      "iteration 0 / 2000: loss 2.302684\n",
      "iteration 100 / 2000: loss 0.428043\n",
      "iteration 200 / 2000: loss 0.288067\n",
      "iteration 300 / 2000: loss 0.250412\n",
      "iteration 400 / 2000: loss 0.277431\n",
      "iteration 500 / 2000: loss 0.213143\n",
      "iteration 600 / 2000: loss 0.314473\n",
      "iteration 700 / 2000: loss 0.213996\n",
      "iteration 800 / 2000: loss 0.191116\n",
      "iteration 900 / 2000: loss 0.222570\n",
      "iteration 1000 / 2000: loss 0.302536\n",
      "iteration 1100 / 2000: loss 0.222512\n",
      "iteration 1200 / 2000: loss 0.243584\n",
      "iteration 1300 / 2000: loss 0.208272\n",
      "iteration 1400 / 2000: loss 0.226233\n",
      "iteration 1500 / 2000: loss 0.172080\n",
      "iteration 1600 / 2000: loss 0.147830\n",
      "iteration 1700 / 2000: loss 0.170712\n",
      "iteration 1800 / 2000: loss 0.187285\n",
      "iteration 1900 / 2000: loss 0.169679\n",
      "Hidden Size: 70, Learning Rate: 0.002500074999999999, LR Decay: 0.99, Regularization: 0.30000000000000004, Validation accuracy: 0.9694\n",
      "iteration 0 / 2000: loss 2.302708\n",
      "iteration 100 / 2000: loss 0.561038\n",
      "iteration 200 / 2000: loss 0.327567\n",
      "iteration 300 / 2000: loss 0.259977\n",
      "iteration 400 / 2000: loss 0.179361\n",
      "iteration 500 / 2000: loss 0.282848\n",
      "iteration 600 / 2000: loss 0.226345\n",
      "iteration 700 / 2000: loss 0.203938\n",
      "iteration 800 / 2000: loss 0.198862\n",
      "iteration 900 / 2000: loss 0.264161\n",
      "iteration 1000 / 2000: loss 0.207921\n",
      "iteration 1100 / 2000: loss 0.328903\n",
      "iteration 1200 / 2000: loss 0.212385\n",
      "iteration 1300 / 2000: loss 0.202474\n",
      "iteration 1400 / 2000: loss 0.250722\n",
      "iteration 1500 / 2000: loss 0.185962\n",
      "iteration 1600 / 2000: loss 0.238513\n",
      "iteration 1700 / 2000: loss 0.229862\n",
      "iteration 1800 / 2000: loss 0.203773\n",
      "iteration 1900 / 2000: loss 0.191595\n",
      "Hidden Size: 70, Learning Rate: 0.002500074999999999, LR Decay: 0.99, Regularization: 0.4, Validation accuracy: 0.9708\n",
      "iteration 0 / 2000: loss 2.302580\n",
      "iteration 100 / 2000: loss 0.368846\n",
      "iteration 200 / 2000: loss 0.297197\n",
      "iteration 300 / 2000: loss 0.192321\n",
      "iteration 400 / 2000: loss 0.214718\n",
      "iteration 500 / 2000: loss 0.232336\n",
      "iteration 600 / 2000: loss 0.128666\n",
      "iteration 700 / 2000: loss 0.232855\n",
      "iteration 800 / 2000: loss 0.170854\n",
      "iteration 900 / 2000: loss 0.122914\n",
      "iteration 1000 / 2000: loss 0.146261\n",
      "iteration 1100 / 2000: loss 0.169427\n",
      "iteration 1200 / 2000: loss 0.094695\n",
      "iteration 1300 / 2000: loss 0.146392\n",
      "iteration 1400 / 2000: loss 0.113558\n",
      "iteration 1500 / 2000: loss 0.105129\n",
      "iteration 1600 / 2000: loss 0.162521\n",
      "iteration 1700 / 2000: loss 0.065689\n",
      "iteration 1800 / 2000: loss 0.149730\n",
      "iteration 1900 / 2000: loss 0.088648\n",
      "Hidden Size: 70, Learning Rate: 0.002500074999999999, LR Decay: 0.8175, Regularization: 0.0, Validation accuracy: 0.9662\n",
      "iteration 0 / 2000: loss 2.302609\n",
      "iteration 100 / 2000: loss 0.403212\n",
      "iteration 200 / 2000: loss 0.276214\n",
      "iteration 300 / 2000: loss 0.263349\n",
      "iteration 400 / 2000: loss 0.195827\n",
      "iteration 500 / 2000: loss 0.162323\n",
      "iteration 600 / 2000: loss 0.230123\n",
      "iteration 700 / 2000: loss 0.214794\n",
      "iteration 800 / 2000: loss 0.179405\n",
      "iteration 900 / 2000: loss 0.239779\n",
      "iteration 1000 / 2000: loss 0.190857\n",
      "iteration 1100 / 2000: loss 0.196017\n",
      "iteration 1200 / 2000: loss 0.166058\n",
      "iteration 1300 / 2000: loss 0.099599\n",
      "iteration 1400 / 2000: loss 0.106989\n",
      "iteration 1500 / 2000: loss 0.141527\n",
      "iteration 1600 / 2000: loss 0.169389\n",
      "iteration 1700 / 2000: loss 0.113644\n",
      "iteration 1800 / 2000: loss 0.200461\n",
      "iteration 1900 / 2000: loss 0.137689\n",
      "Hidden Size: 70, Learning Rate: 0.002500074999999999, LR Decay: 0.8175, Regularization: 0.1, Validation accuracy: 0.9658\n",
      "iteration 0 / 2000: loss 2.302662\n",
      "iteration 100 / 2000: loss 0.395451\n",
      "iteration 200 / 2000: loss 0.280304\n",
      "iteration 300 / 2000: loss 0.260096\n",
      "iteration 400 / 2000: loss 0.299966\n",
      "iteration 500 / 2000: loss 0.226517\n",
      "iteration 600 / 2000: loss 0.157444\n",
      "iteration 700 / 2000: loss 0.215523\n",
      "iteration 800 / 2000: loss 0.303045\n",
      "iteration 900 / 2000: loss 0.201906\n",
      "iteration 1000 / 2000: loss 0.156451\n",
      "iteration 1100 / 2000: loss 0.153969\n",
      "iteration 1200 / 2000: loss 0.174774\n",
      "iteration 1300 / 2000: loss 0.162274\n",
      "iteration 1400 / 2000: loss 0.171373\n",
      "iteration 1500 / 2000: loss 0.191375\n",
      "iteration 1600 / 2000: loss 0.196336\n",
      "iteration 1700 / 2000: loss 0.218350\n",
      "iteration 1800 / 2000: loss 0.145622\n",
      "iteration 1900 / 2000: loss 0.186957\n",
      "Hidden Size: 70, Learning Rate: 0.002500074999999999, LR Decay: 0.8175, Regularization: 0.2, Validation accuracy: 0.9644\n",
      "iteration 0 / 2000: loss 2.302677\n",
      "iteration 100 / 2000: loss 0.350402\n",
      "iteration 200 / 2000: loss 0.387020\n",
      "iteration 300 / 2000: loss 0.291800\n",
      "iteration 400 / 2000: loss 0.307201\n",
      "iteration 500 / 2000: loss 0.231682\n",
      "iteration 600 / 2000: loss 0.242261\n",
      "iteration 700 / 2000: loss 0.233079\n",
      "iteration 800 / 2000: loss 0.215798\n",
      "iteration 900 / 2000: loss 0.227600\n",
      "iteration 1000 / 2000: loss 0.191848\n",
      "iteration 1100 / 2000: loss 0.248625\n",
      "iteration 1200 / 2000: loss 0.201296\n",
      "iteration 1300 / 2000: loss 0.162345\n",
      "iteration 1400 / 2000: loss 0.190202\n",
      "iteration 1500 / 2000: loss 0.231066\n",
      "iteration 1600 / 2000: loss 0.231654\n",
      "iteration 1700 / 2000: loss 0.244315\n",
      "iteration 1800 / 2000: loss 0.173065\n",
      "iteration 1900 / 2000: loss 0.190437\n",
      "Hidden Size: 70, Learning Rate: 0.002500074999999999, LR Decay: 0.8175, Regularization: 0.30000000000000004, Validation accuracy: 0.9634\n",
      "iteration 0 / 2000: loss 2.302673\n",
      "iteration 100 / 2000: loss 0.321703\n",
      "iteration 200 / 2000: loss 0.301761\n",
      "iteration 300 / 2000: loss 0.456694\n",
      "iteration 400 / 2000: loss 0.248149\n",
      "iteration 500 / 2000: loss 0.318110\n",
      "iteration 600 / 2000: loss 0.246656\n",
      "iteration 700 / 2000: loss 0.288210\n",
      "iteration 800 / 2000: loss 0.232587\n",
      "iteration 900 / 2000: loss 0.252830\n",
      "iteration 1000 / 2000: loss 0.203226\n",
      "iteration 1100 / 2000: loss 0.232598\n",
      "iteration 1200 / 2000: loss 0.205684\n",
      "iteration 1300 / 2000: loss 0.195482\n",
      "iteration 1400 / 2000: loss 0.229191\n",
      "iteration 1500 / 2000: loss 0.228866\n",
      "iteration 1600 / 2000: loss 0.279186\n",
      "iteration 1700 / 2000: loss 0.324023\n",
      "iteration 1800 / 2000: loss 0.226669\n",
      "iteration 1900 / 2000: loss 0.207218\n",
      "Hidden Size: 70, Learning Rate: 0.002500074999999999, LR Decay: 0.8175, Regularization: 0.4, Validation accuracy: 0.9652\n",
      "iteration 0 / 2000: loss 2.302588\n",
      "iteration 100 / 2000: loss 0.454691\n",
      "iteration 200 / 2000: loss 0.297183\n",
      "iteration 300 / 2000: loss 0.288129\n",
      "iteration 400 / 2000: loss 0.262596\n",
      "iteration 500 / 2000: loss 0.216638\n",
      "iteration 600 / 2000: loss 0.185021\n",
      "iteration 700 / 2000: loss 0.155113\n",
      "iteration 800 / 2000: loss 0.260859\n",
      "iteration 900 / 2000: loss 0.192002\n",
      "iteration 1000 / 2000: loss 0.156412\n",
      "iteration 1100 / 2000: loss 0.178819\n",
      "iteration 1200 / 2000: loss 0.188362\n",
      "iteration 1300 / 2000: loss 0.207930\n",
      "iteration 1400 / 2000: loss 0.256290\n",
      "iteration 1500 / 2000: loss 0.123761\n",
      "iteration 1600 / 2000: loss 0.221394\n",
      "iteration 1700 / 2000: loss 0.192707\n",
      "iteration 1800 / 2000: loss 0.083645\n",
      "iteration 1900 / 2000: loss 0.175177\n",
      "Hidden Size: 70, Learning Rate: 0.002500074999999999, LR Decay: 0.645, Regularization: 0.0, Validation accuracy: 0.9558\n",
      "iteration 0 / 2000: loss 2.302619\n",
      "iteration 100 / 2000: loss 0.433561\n",
      "iteration 200 / 2000: loss 0.299017\n",
      "iteration 300 / 2000: loss 0.208888\n",
      "iteration 400 / 2000: loss 0.328486\n",
      "iteration 500 / 2000: loss 0.273582\n",
      "iteration 600 / 2000: loss 0.174973\n",
      "iteration 700 / 2000: loss 0.118385\n",
      "iteration 800 / 2000: loss 0.183933\n",
      "iteration 900 / 2000: loss 0.163848\n",
      "iteration 1000 / 2000: loss 0.281040\n",
      "iteration 1100 / 2000: loss 0.249693\n",
      "iteration 1200 / 2000: loss 0.158236\n",
      "iteration 1300 / 2000: loss 0.087509\n",
      "iteration 1400 / 2000: loss 0.139507\n",
      "iteration 1500 / 2000: loss 0.281441\n",
      "iteration 1600 / 2000: loss 0.158304\n",
      "iteration 1700 / 2000: loss 0.225941\n",
      "iteration 1800 / 2000: loss 0.166550\n",
      "iteration 1900 / 2000: loss 0.148130\n",
      "Hidden Size: 70, Learning Rate: 0.002500074999999999, LR Decay: 0.645, Regularization: 0.1, Validation accuracy: 0.9562\n",
      "iteration 0 / 2000: loss 2.302649\n",
      "iteration 100 / 2000: loss 0.534658\n",
      "iteration 200 / 2000: loss 0.349835\n",
      "iteration 300 / 2000: loss 0.275856\n",
      "iteration 400 / 2000: loss 0.350644\n",
      "iteration 500 / 2000: loss 0.277662\n",
      "iteration 600 / 2000: loss 0.193318\n",
      "iteration 700 / 2000: loss 0.212077\n",
      "iteration 800 / 2000: loss 0.164922\n",
      "iteration 900 / 2000: loss 0.163662\n",
      "iteration 1000 / 2000: loss 0.232901\n",
      "iteration 1100 / 2000: loss 0.254327\n",
      "iteration 1200 / 2000: loss 0.171879\n",
      "iteration 1300 / 2000: loss 0.257904\n",
      "iteration 1400 / 2000: loss 0.183264\n",
      "iteration 1500 / 2000: loss 0.169246\n",
      "iteration 1600 / 2000: loss 0.227679\n",
      "iteration 1700 / 2000: loss 0.292421\n",
      "iteration 1800 / 2000: loss 0.198283\n",
      "iteration 1900 / 2000: loss 0.188963\n",
      "Hidden Size: 70, Learning Rate: 0.002500074999999999, LR Decay: 0.645, Regularization: 0.2, Validation accuracy: 0.9536\n",
      "iteration 0 / 2000: loss 2.302661\n",
      "iteration 100 / 2000: loss 0.477665\n",
      "iteration 200 / 2000: loss 0.376933\n",
      "iteration 300 / 2000: loss 0.242545\n",
      "iteration 400 / 2000: loss 0.334663\n",
      "iteration 500 / 2000: loss 0.328112\n",
      "iteration 600 / 2000: loss 0.356763\n",
      "iteration 700 / 2000: loss 0.259590\n",
      "iteration 800 / 2000: loss 0.298710\n",
      "iteration 900 / 2000: loss 0.314621\n",
      "iteration 1000 / 2000: loss 0.210080\n",
      "iteration 1100 / 2000: loss 0.261143\n",
      "iteration 1200 / 2000: loss 0.182197\n",
      "iteration 1300 / 2000: loss 0.225113\n",
      "iteration 1400 / 2000: loss 0.245299\n",
      "iteration 1500 / 2000: loss 0.201392\n",
      "iteration 1600 / 2000: loss 0.205763\n",
      "iteration 1700 / 2000: loss 0.170700\n",
      "iteration 1800 / 2000: loss 0.195401\n",
      "iteration 1900 / 2000: loss 0.232357\n",
      "Hidden Size: 70, Learning Rate: 0.002500074999999999, LR Decay: 0.645, Regularization: 0.30000000000000004, Validation accuracy: 0.9546\n",
      "iteration 0 / 2000: loss 2.302682\n",
      "iteration 100 / 2000: loss 0.538731\n",
      "iteration 200 / 2000: loss 0.371166\n",
      "iteration 300 / 2000: loss 0.371086\n",
      "iteration 400 / 2000: loss 0.337776\n",
      "iteration 500 / 2000: loss 0.229215\n",
      "iteration 600 / 2000: loss 0.249487\n",
      "iteration 700 / 2000: loss 0.288801\n",
      "iteration 800 / 2000: loss 0.243541\n",
      "iteration 900 / 2000: loss 0.304858\n",
      "iteration 1000 / 2000: loss 0.285366\n",
      "iteration 1100 / 2000: loss 0.309390\n",
      "iteration 1200 / 2000: loss 0.277050\n",
      "iteration 1300 / 2000: loss 0.221772\n",
      "iteration 1400 / 2000: loss 0.260048\n",
      "iteration 1500 / 2000: loss 0.300031\n",
      "iteration 1600 / 2000: loss 0.263481\n",
      "iteration 1700 / 2000: loss 0.278416\n",
      "iteration 1800 / 2000: loss 0.224483\n",
      "iteration 1900 / 2000: loss 0.272046\n",
      "Hidden Size: 70, Learning Rate: 0.002500074999999999, LR Decay: 0.645, Regularization: 0.4, Validation accuracy: 0.9528\n",
      "iteration 0 / 2000: loss 2.302580\n",
      "iteration 100 / 2000: loss 0.565572\n",
      "iteration 200 / 2000: loss 0.412701\n",
      "iteration 300 / 2000: loss 0.307700\n",
      "iteration 400 / 2000: loss 0.210397\n",
      "iteration 500 / 2000: loss 0.277683\n",
      "iteration 600 / 2000: loss 0.215169\n",
      "iteration 700 / 2000: loss 0.208897\n",
      "iteration 800 / 2000: loss 0.358006\n",
      "iteration 900 / 2000: loss 0.277577\n",
      "iteration 1000 / 2000: loss 0.237688\n",
      "iteration 1100 / 2000: loss 0.219537\n",
      "iteration 1200 / 2000: loss 0.243807\n",
      "iteration 1300 / 2000: loss 0.244720\n",
      "iteration 1400 / 2000: loss 0.182782\n",
      "iteration 1500 / 2000: loss 0.198829\n",
      "iteration 1600 / 2000: loss 0.206893\n",
      "iteration 1700 / 2000: loss 0.184398\n",
      "iteration 1800 / 2000: loss 0.246059\n",
      "iteration 1900 / 2000: loss 0.239541\n",
      "Hidden Size: 70, Learning Rate: 0.002500074999999999, LR Decay: 0.47250000000000003, Regularization: 0.0, Validation accuracy: 0.935\n",
      "iteration 0 / 2000: loss 2.302609\n",
      "iteration 100 / 2000: loss 0.646839\n",
      "iteration 200 / 2000: loss 0.465112\n",
      "iteration 300 / 2000: loss 0.400859\n",
      "iteration 400 / 2000: loss 0.273967\n",
      "iteration 500 / 2000: loss 0.228826\n",
      "iteration 600 / 2000: loss 0.265464\n",
      "iteration 700 / 2000: loss 0.201942\n",
      "iteration 800 / 2000: loss 0.258859\n",
      "iteration 900 / 2000: loss 0.226593\n",
      "iteration 1000 / 2000: loss 0.264438\n",
      "iteration 1100 / 2000: loss 0.186156\n",
      "iteration 1200 / 2000: loss 0.247199\n",
      "iteration 1300 / 2000: loss 0.214552\n",
      "iteration 1400 / 2000: loss 0.350643\n",
      "iteration 1500 / 2000: loss 0.332986\n",
      "iteration 1600 / 2000: loss 0.242773\n",
      "iteration 1700 / 2000: loss 0.202722\n",
      "iteration 1800 / 2000: loss 0.243017\n",
      "iteration 1900 / 2000: loss 0.211620\n",
      "Hidden Size: 70, Learning Rate: 0.002500074999999999, LR Decay: 0.47250000000000003, Regularization: 0.1, Validation accuracy: 0.9358\n",
      "iteration 0 / 2000: loss 2.302647\n",
      "iteration 100 / 2000: loss 0.636397\n",
      "iteration 200 / 2000: loss 0.314372\n",
      "iteration 300 / 2000: loss 0.343963\n",
      "iteration 400 / 2000: loss 0.193736\n",
      "iteration 500 / 2000: loss 0.289513\n",
      "iteration 600 / 2000: loss 0.212263\n",
      "iteration 700 / 2000: loss 0.254279\n",
      "iteration 800 / 2000: loss 0.254069\n",
      "iteration 900 / 2000: loss 0.218368\n",
      "iteration 1000 / 2000: loss 0.328799\n",
      "iteration 1100 / 2000: loss 0.302120\n",
      "iteration 1200 / 2000: loss 0.328827\n",
      "iteration 1300 / 2000: loss 0.280934\n",
      "iteration 1400 / 2000: loss 0.247782\n",
      "iteration 1500 / 2000: loss 0.262646\n",
      "iteration 1600 / 2000: loss 0.305518\n",
      "iteration 1700 / 2000: loss 0.239000\n",
      "iteration 1800 / 2000: loss 0.240530\n",
      "iteration 1900 / 2000: loss 0.318493\n",
      "Hidden Size: 70, Learning Rate: 0.002500074999999999, LR Decay: 0.47250000000000003, Regularization: 0.2, Validation accuracy: 0.9348\n",
      "iteration 0 / 2000: loss 2.302681\n",
      "iteration 100 / 2000: loss 0.648155\n",
      "iteration 200 / 2000: loss 0.352264\n",
      "iteration 300 / 2000: loss 0.389124\n",
      "iteration 400 / 2000: loss 0.411247\n",
      "iteration 500 / 2000: loss 0.414380\n",
      "iteration 600 / 2000: loss 0.319162\n",
      "iteration 700 / 2000: loss 0.267135\n",
      "iteration 800 / 2000: loss 0.224633\n",
      "iteration 900 / 2000: loss 0.269674\n",
      "iteration 1000 / 2000: loss 0.306535\n",
      "iteration 1100 / 2000: loss 0.244239\n",
      "iteration 1200 / 2000: loss 0.294468\n",
      "iteration 1300 / 2000: loss 0.241247\n",
      "iteration 1400 / 2000: loss 0.277790\n",
      "iteration 1500 / 2000: loss 0.311483\n",
      "iteration 1600 / 2000: loss 0.326945\n",
      "iteration 1700 / 2000: loss 0.352299\n",
      "iteration 1800 / 2000: loss 0.277686\n",
      "iteration 1900 / 2000: loss 0.251214\n",
      "Hidden Size: 70, Learning Rate: 0.002500074999999999, LR Decay: 0.47250000000000003, Regularization: 0.30000000000000004, Validation accuracy: 0.9328\n",
      "iteration 0 / 2000: loss 2.302684\n",
      "iteration 100 / 2000: loss 0.641314\n",
      "iteration 200 / 2000: loss 0.432874\n",
      "iteration 300 / 2000: loss 0.246192\n",
      "iteration 400 / 2000: loss 0.372182\n",
      "iteration 500 / 2000: loss 0.319535\n",
      "iteration 600 / 2000: loss 0.339818\n",
      "iteration 700 / 2000: loss 0.383627\n",
      "iteration 800 / 2000: loss 0.285531\n",
      "iteration 900 / 2000: loss 0.264178\n",
      "iteration 1000 / 2000: loss 0.282376\n",
      "iteration 1100 / 2000: loss 0.219886\n",
      "iteration 1200 / 2000: loss 0.328424\n",
      "iteration 1300 / 2000: loss 0.331229\n",
      "iteration 1400 / 2000: loss 0.343422\n",
      "iteration 1500 / 2000: loss 0.246405\n",
      "iteration 1600 / 2000: loss 0.345381\n",
      "iteration 1700 / 2000: loss 0.250081\n",
      "iteration 1800 / 2000: loss 0.301403\n",
      "iteration 1900 / 2000: loss 0.293375\n",
      "Hidden Size: 70, Learning Rate: 0.002500074999999999, LR Decay: 0.47250000000000003, Regularization: 0.4, Validation accuracy: 0.934\n",
      "iteration 0 / 2000: loss 2.302561\n",
      "iteration 100 / 2000: loss 1.237990\n",
      "iteration 200 / 2000: loss 0.469715\n",
      "iteration 300 / 2000: loss 0.238845\n",
      "iteration 400 / 2000: loss 0.375761\n",
      "iteration 500 / 2000: loss 0.230330\n",
      "iteration 600 / 2000: loss 0.358263\n",
      "iteration 700 / 2000: loss 0.249475\n",
      "iteration 800 / 2000: loss 0.315906\n",
      "iteration 900 / 2000: loss 0.373172\n",
      "iteration 1000 / 2000: loss 0.312698\n",
      "iteration 1100 / 2000: loss 0.249862\n",
      "iteration 1200 / 2000: loss 0.238354\n",
      "iteration 1300 / 2000: loss 0.370797\n",
      "iteration 1400 / 2000: loss 0.343011\n",
      "iteration 1500 / 2000: loss 0.274857\n",
      "iteration 1600 / 2000: loss 0.290356\n",
      "iteration 1700 / 2000: loss 0.318566\n",
      "iteration 1800 / 2000: loss 0.232686\n",
      "iteration 1900 / 2000: loss 0.374479\n",
      "Hidden Size: 70, Learning Rate: 0.002500074999999999, LR Decay: 0.3, Regularization: 0.0, Validation accuracy: 0.916\n",
      "iteration 0 / 2000: loss 2.302615\n",
      "iteration 100 / 2000: loss 1.220244\n",
      "iteration 200 / 2000: loss 0.431639\n",
      "iteration 300 / 2000: loss 0.378328\n",
      "iteration 400 / 2000: loss 0.385148\n",
      "iteration 500 / 2000: loss 0.355526\n",
      "iteration 600 / 2000: loss 0.387482\n",
      "iteration 700 / 2000: loss 0.294210\n",
      "iteration 800 / 2000: loss 0.317663\n",
      "iteration 900 / 2000: loss 0.385446\n",
      "iteration 1000 / 2000: loss 0.338923\n",
      "iteration 1100 / 2000: loss 0.292970\n",
      "iteration 1200 / 2000: loss 0.321301\n",
      "iteration 1300 / 2000: loss 0.337026\n",
      "iteration 1400 / 2000: loss 0.258611\n",
      "iteration 1500 / 2000: loss 0.355040\n",
      "iteration 1600 / 2000: loss 0.318630\n",
      "iteration 1700 / 2000: loss 0.349663\n",
      "iteration 1800 / 2000: loss 0.304379\n",
      "iteration 1900 / 2000: loss 0.341440\n",
      "Hidden Size: 70, Learning Rate: 0.002500074999999999, LR Decay: 0.3, Regularization: 0.1, Validation accuracy: 0.9146\n",
      "iteration 0 / 2000: loss 2.302651\n",
      "iteration 100 / 2000: loss 1.141045\n",
      "iteration 200 / 2000: loss 0.585119\n",
      "iteration 300 / 2000: loss 0.382096\n",
      "iteration 400 / 2000: loss 0.322948\n",
      "iteration 500 / 2000: loss 0.385527\n",
      "iteration 600 / 2000: loss 0.330198\n",
      "iteration 700 / 2000: loss 0.331825\n",
      "iteration 800 / 2000: loss 0.362628\n",
      "iteration 900 / 2000: loss 0.345829\n",
      "iteration 1000 / 2000: loss 0.401161\n",
      "iteration 1100 / 2000: loss 0.333378\n",
      "iteration 1200 / 2000: loss 0.368870\n",
      "iteration 1300 / 2000: loss 0.280911\n",
      "iteration 1400 / 2000: loss 0.290547\n",
      "iteration 1500 / 2000: loss 0.324187\n",
      "iteration 1600 / 2000: loss 0.337456\n",
      "iteration 1700 / 2000: loss 0.314495\n",
      "iteration 1800 / 2000: loss 0.335638\n",
      "iteration 1900 / 2000: loss 0.321471\n",
      "Hidden Size: 70, Learning Rate: 0.002500074999999999, LR Decay: 0.3, Regularization: 0.2, Validation accuracy: 0.9152\n",
      "iteration 0 / 2000: loss 2.302674\n",
      "iteration 100 / 2000: loss 1.401669\n",
      "iteration 200 / 2000: loss 0.497769\n",
      "iteration 300 / 2000: loss 0.390830\n",
      "iteration 400 / 2000: loss 0.357962\n",
      "iteration 500 / 2000: loss 0.369793\n",
      "iteration 600 / 2000: loss 0.297631\n",
      "iteration 700 / 2000: loss 0.384705\n",
      "iteration 800 / 2000: loss 0.320641\n",
      "iteration 900 / 2000: loss 0.376859\n",
      "iteration 1000 / 2000: loss 0.322148\n",
      "iteration 1100 / 2000: loss 0.405754\n",
      "iteration 1200 / 2000: loss 0.380463\n",
      "iteration 1300 / 2000: loss 0.371882\n",
      "iteration 1400 / 2000: loss 0.402766\n",
      "iteration 1500 / 2000: loss 0.405023\n",
      "iteration 1600 / 2000: loss 0.366252\n",
      "iteration 1700 / 2000: loss 0.311735\n",
      "iteration 1800 / 2000: loss 0.377518\n",
      "iteration 1900 / 2000: loss 0.381113\n",
      "Hidden Size: 70, Learning Rate: 0.002500074999999999, LR Decay: 0.3, Regularization: 0.30000000000000004, Validation accuracy: 0.9152\n",
      "iteration 0 / 2000: loss 2.302685\n",
      "iteration 100 / 2000: loss 1.156822\n",
      "iteration 200 / 2000: loss 0.522631\n",
      "iteration 300 / 2000: loss 0.408309\n",
      "iteration 400 / 2000: loss 0.344264\n",
      "iteration 500 / 2000: loss 0.383741\n",
      "iteration 600 / 2000: loss 0.448750\n",
      "iteration 700 / 2000: loss 0.334101\n",
      "iteration 800 / 2000: loss 0.499483\n",
      "iteration 900 / 2000: loss 0.298188\n",
      "iteration 1000 / 2000: loss 0.369932\n",
      "iteration 1100 / 2000: loss 0.473937\n",
      "iteration 1200 / 2000: loss 0.313505\n",
      "iteration 1300 / 2000: loss 0.412309\n",
      "iteration 1400 / 2000: loss 0.387644\n",
      "iteration 1500 / 2000: loss 0.321406\n",
      "iteration 1600 / 2000: loss 0.351610\n",
      "iteration 1700 / 2000: loss 0.324013\n",
      "iteration 1800 / 2000: loss 0.284879\n",
      "iteration 1900 / 2000: loss 0.386990\n",
      "Hidden Size: 70, Learning Rate: 0.002500074999999999, LR Decay: 0.3, Regularization: 0.4, Validation accuracy: 0.9162\n",
      "iteration 0 / 2000: loss 2.302574\n",
      "iteration 100 / 2000: loss 2.302585\n",
      "iteration 200 / 2000: loss 2.302583\n",
      "iteration 300 / 2000: loss 2.302579\n",
      "iteration 400 / 2000: loss 2.302582\n",
      "iteration 500 / 2000: loss 2.302579\n",
      "iteration 600 / 2000: loss 2.302572\n",
      "iteration 700 / 2000: loss 2.302574\n",
      "iteration 800 / 2000: loss 2.302572\n",
      "iteration 900 / 2000: loss 2.302576\n",
      "iteration 1000 / 2000: loss 2.302590\n",
      "iteration 1100 / 2000: loss 2.302581\n",
      "iteration 1200 / 2000: loss 2.302575\n",
      "iteration 1300 / 2000: loss 2.302581\n",
      "iteration 1400 / 2000: loss 2.302579\n",
      "iteration 1500 / 2000: loss 2.302586\n",
      "iteration 1600 / 2000: loss 2.302594\n",
      "iteration 1700 / 2000: loss 2.302575\n",
      "iteration 1800 / 2000: loss 2.302584\n",
      "iteration 1900 / 2000: loss 2.302579\n",
      "Hidden Size: 70, Learning Rate: 1e-07, LR Decay: 0.99, Regularization: 0.0, Validation accuracy: 0.1236\n",
      "iteration 0 / 2000: loss 2.302644\n",
      "iteration 100 / 2000: loss 2.302637\n",
      "iteration 200 / 2000: loss 2.302633\n",
      "iteration 300 / 2000: loss 2.302641\n",
      "iteration 400 / 2000: loss 2.302644\n",
      "iteration 500 / 2000: loss 2.302629\n",
      "iteration 600 / 2000: loss 2.302615\n",
      "iteration 700 / 2000: loss 2.302634\n",
      "iteration 800 / 2000: loss 2.302644\n",
      "iteration 900 / 2000: loss 2.302632\n",
      "iteration 1000 / 2000: loss 2.302633\n",
      "iteration 1100 / 2000: loss 2.302634\n",
      "iteration 1200 / 2000: loss 2.302643\n",
      "iteration 1300 / 2000: loss 2.302639\n",
      "iteration 1400 / 2000: loss 2.302639\n",
      "iteration 1500 / 2000: loss 2.302640\n",
      "iteration 1600 / 2000: loss 2.302639\n",
      "iteration 1700 / 2000: loss 2.302631\n",
      "iteration 1800 / 2000: loss 2.302641\n",
      "iteration 1900 / 2000: loss 2.302628\n",
      "Hidden Size: 70, Learning Rate: 1e-07, LR Decay: 0.99, Regularization: 0.1, Validation accuracy: 0.0634\n",
      "iteration 0 / 2000: loss 2.302648\n",
      "iteration 100 / 2000: loss 2.302645\n",
      "iteration 200 / 2000: loss 2.302635\n",
      "iteration 300 / 2000: loss 2.302635\n",
      "iteration 400 / 2000: loss 2.302635\n",
      "iteration 500 / 2000: loss 2.302626\n",
      "iteration 600 / 2000: loss 2.302636\n",
      "iteration 700 / 2000: loss 2.302643\n",
      "iteration 800 / 2000: loss 2.302646\n",
      "iteration 900 / 2000: loss 2.302640\n",
      "iteration 1000 / 2000: loss 2.302628\n",
      "iteration 1100 / 2000: loss 2.302645\n",
      "iteration 1200 / 2000: loss 2.302640\n",
      "iteration 1300 / 2000: loss 2.302634\n",
      "iteration 1400 / 2000: loss 2.302644\n",
      "iteration 1500 / 2000: loss 2.302640\n",
      "iteration 1600 / 2000: loss 2.302634\n",
      "iteration 1700 / 2000: loss 2.302638\n",
      "iteration 1800 / 2000: loss 2.302653\n",
      "iteration 1900 / 2000: loss 2.302643\n",
      "Hidden Size: 70, Learning Rate: 1e-07, LR Decay: 0.99, Regularization: 0.2, Validation accuracy: 0.138\n",
      "iteration 0 / 2000: loss 2.302666\n",
      "iteration 100 / 2000: loss 2.302667\n",
      "iteration 200 / 2000: loss 2.302643\n",
      "iteration 300 / 2000: loss 2.302658\n",
      "iteration 400 / 2000: loss 2.302642\n",
      "iteration 500 / 2000: loss 2.302641\n",
      "iteration 600 / 2000: loss 2.302653\n",
      "iteration 700 / 2000: loss 2.302659\n",
      "iteration 800 / 2000: loss 2.302668\n",
      "iteration 900 / 2000: loss 2.302657\n",
      "iteration 1000 / 2000: loss 2.302656\n",
      "iteration 1100 / 2000: loss 2.302654\n",
      "iteration 1200 / 2000: loss 2.302661\n",
      "iteration 1300 / 2000: loss 2.302656\n",
      "iteration 1400 / 2000: loss 2.302651\n",
      "iteration 1500 / 2000: loss 2.302643\n",
      "iteration 1600 / 2000: loss 2.302649\n",
      "iteration 1700 / 2000: loss 2.302661\n",
      "iteration 1800 / 2000: loss 2.302636\n",
      "iteration 1900 / 2000: loss 2.302641\n",
      "Hidden Size: 70, Learning Rate: 1e-07, LR Decay: 0.99, Regularization: 0.30000000000000004, Validation accuracy: 0.1282\n",
      "iteration 0 / 2000: loss 2.302703\n",
      "iteration 100 / 2000: loss 2.302698\n",
      "iteration 200 / 2000: loss 2.302715\n",
      "iteration 300 / 2000: loss 2.302705\n",
      "iteration 400 / 2000: loss 2.302694\n",
      "iteration 500 / 2000: loss 2.302691\n",
      "iteration 600 / 2000: loss 2.302704\n",
      "iteration 700 / 2000: loss 2.302703\n",
      "iteration 800 / 2000: loss 2.302710\n",
      "iteration 900 / 2000: loss 2.302706\n",
      "iteration 1000 / 2000: loss 2.302694\n",
      "iteration 1100 / 2000: loss 2.302716\n",
      "iteration 1200 / 2000: loss 2.302701\n",
      "iteration 1300 / 2000: loss 2.302706\n",
      "iteration 1400 / 2000: loss 2.302684\n",
      "iteration 1500 / 2000: loss 2.302699\n",
      "iteration 1600 / 2000: loss 2.302693\n",
      "iteration 1700 / 2000: loss 2.302701\n",
      "iteration 1800 / 2000: loss 2.302706\n",
      "iteration 1900 / 2000: loss 2.302702\n",
      "Hidden Size: 70, Learning Rate: 1e-07, LR Decay: 0.99, Regularization: 0.4, Validation accuracy: 0.1132\n",
      "iteration 0 / 2000: loss 2.302591\n",
      "iteration 100 / 2000: loss 2.302581\n",
      "iteration 200 / 2000: loss 2.302594\n",
      "iteration 300 / 2000: loss 2.302585\n",
      "iteration 400 / 2000: loss 2.302588\n",
      "iteration 500 / 2000: loss 2.302588\n",
      "iteration 600 / 2000: loss 2.302583\n",
      "iteration 700 / 2000: loss 2.302600\n",
      "iteration 800 / 2000: loss 2.302587\n",
      "iteration 900 / 2000: loss 2.302586\n",
      "iteration 1000 / 2000: loss 2.302583\n",
      "iteration 1100 / 2000: loss 2.302576\n",
      "iteration 1200 / 2000: loss 2.302584\n",
      "iteration 1300 / 2000: loss 2.302590\n",
      "iteration 1400 / 2000: loss 2.302603\n",
      "iteration 1500 / 2000: loss 2.302581\n",
      "iteration 1600 / 2000: loss 2.302587\n",
      "iteration 1700 / 2000: loss 2.302587\n",
      "iteration 1800 / 2000: loss 2.302577\n",
      "iteration 1900 / 2000: loss 2.302605\n",
      "Hidden Size: 70, Learning Rate: 1e-07, LR Decay: 0.8175, Regularization: 0.0, Validation accuracy: 0.1202\n",
      "iteration 0 / 2000: loss 2.302602\n",
      "iteration 100 / 2000: loss 2.302602\n",
      "iteration 200 / 2000: loss 2.302611\n",
      "iteration 300 / 2000: loss 2.302619\n",
      "iteration 400 / 2000: loss 2.302612\n",
      "iteration 500 / 2000: loss 2.302611\n",
      "iteration 600 / 2000: loss 2.302610\n",
      "iteration 700 / 2000: loss 2.302597\n",
      "iteration 800 / 2000: loss 2.302617\n",
      "iteration 900 / 2000: loss 2.302620\n",
      "iteration 1000 / 2000: loss 2.302595\n",
      "iteration 1100 / 2000: loss 2.302605\n",
      "iteration 1200 / 2000: loss 2.302602\n",
      "iteration 1300 / 2000: loss 2.302617\n",
      "iteration 1400 / 2000: loss 2.302609\n",
      "iteration 1500 / 2000: loss 2.302618\n",
      "iteration 1600 / 2000: loss 2.302616\n",
      "iteration 1700 / 2000: loss 2.302624\n",
      "iteration 1800 / 2000: loss 2.302620\n",
      "iteration 1900 / 2000: loss 2.302624\n",
      "Hidden Size: 70, Learning Rate: 1e-07, LR Decay: 0.8175, Regularization: 0.1, Validation accuracy: 0.1218\n",
      "iteration 0 / 2000: loss 2.302648\n",
      "iteration 100 / 2000: loss 2.302659\n",
      "iteration 200 / 2000: loss 2.302638\n",
      "iteration 300 / 2000: loss 2.302649\n",
      "iteration 400 / 2000: loss 2.302631\n",
      "iteration 500 / 2000: loss 2.302649\n",
      "iteration 600 / 2000: loss 2.302633\n",
      "iteration 700 / 2000: loss 2.302662\n",
      "iteration 800 / 2000: loss 2.302643\n",
      "iteration 900 / 2000: loss 2.302648\n",
      "iteration 1000 / 2000: loss 2.302645\n",
      "iteration 1100 / 2000: loss 2.302655\n",
      "iteration 1200 / 2000: loss 2.302648\n",
      "iteration 1300 / 2000: loss 2.302628\n",
      "iteration 1400 / 2000: loss 2.302621\n",
      "iteration 1500 / 2000: loss 2.302644\n",
      "iteration 1600 / 2000: loss 2.302635\n",
      "iteration 1700 / 2000: loss 2.302636\n",
      "iteration 1800 / 2000: loss 2.302638\n",
      "iteration 1900 / 2000: loss 2.302639\n",
      "Hidden Size: 70, Learning Rate: 1e-07, LR Decay: 0.8175, Regularization: 0.2, Validation accuracy: 0.128\n",
      "iteration 0 / 2000: loss 2.302662\n",
      "iteration 100 / 2000: loss 2.302659\n",
      "iteration 200 / 2000: loss 2.302667\n",
      "iteration 300 / 2000: loss 2.302674\n",
      "iteration 400 / 2000: loss 2.302666\n",
      "iteration 500 / 2000: loss 2.302687\n",
      "iteration 600 / 2000: loss 2.302680\n",
      "iteration 700 / 2000: loss 2.302657\n",
      "iteration 800 / 2000: loss 2.302668\n",
      "iteration 900 / 2000: loss 2.302683\n",
      "iteration 1000 / 2000: loss 2.302664\n",
      "iteration 1100 / 2000: loss 2.302649\n",
      "iteration 1200 / 2000: loss 2.302664\n",
      "iteration 1300 / 2000: loss 2.302670\n",
      "iteration 1400 / 2000: loss 2.302648\n",
      "iteration 1500 / 2000: loss 2.302669\n",
      "iteration 1600 / 2000: loss 2.302666\n",
      "iteration 1700 / 2000: loss 2.302664\n",
      "iteration 1800 / 2000: loss 2.302666\n",
      "iteration 1900 / 2000: loss 2.302657\n",
      "Hidden Size: 70, Learning Rate: 1e-07, LR Decay: 0.8175, Regularization: 0.30000000000000004, Validation accuracy: 0.11\n",
      "iteration 0 / 2000: loss 2.302716\n",
      "iteration 100 / 2000: loss 2.302712\n",
      "iteration 200 / 2000: loss 2.302701\n",
      "iteration 300 / 2000: loss 2.302714\n",
      "iteration 400 / 2000: loss 2.302719\n",
      "iteration 500 / 2000: loss 2.302702\n",
      "iteration 600 / 2000: loss 2.302712\n",
      "iteration 700 / 2000: loss 2.302696\n",
      "iteration 800 / 2000: loss 2.302719\n",
      "iteration 900 / 2000: loss 2.302731\n",
      "iteration 1000 / 2000: loss 2.302706\n",
      "iteration 1100 / 2000: loss 2.302711\n",
      "iteration 1200 / 2000: loss 2.302719\n",
      "iteration 1300 / 2000: loss 2.302706\n",
      "iteration 1400 / 2000: loss 2.302698\n",
      "iteration 1500 / 2000: loss 2.302717\n",
      "iteration 1600 / 2000: loss 2.302716\n",
      "iteration 1700 / 2000: loss 2.302711\n",
      "iteration 1800 / 2000: loss 2.302717\n",
      "iteration 1900 / 2000: loss 2.302699\n",
      "Hidden Size: 70, Learning Rate: 1e-07, LR Decay: 0.8175, Regularization: 0.4, Validation accuracy: 0.1028\n",
      "iteration 0 / 2000: loss 2.302587\n",
      "iteration 100 / 2000: loss 2.302582\n",
      "iteration 200 / 2000: loss 2.302571\n",
      "iteration 300 / 2000: loss 2.302564\n",
      "iteration 400 / 2000: loss 2.302579\n",
      "iteration 500 / 2000: loss 2.302574\n",
      "iteration 600 / 2000: loss 2.302579\n",
      "iteration 700 / 2000: loss 2.302589\n",
      "iteration 800 / 2000: loss 2.302575\n",
      "iteration 900 / 2000: loss 2.302578\n",
      "iteration 1000 / 2000: loss 2.302570\n",
      "iteration 1100 / 2000: loss 2.302574\n",
      "iteration 1200 / 2000: loss 2.302584\n",
      "iteration 1300 / 2000: loss 2.302579\n",
      "iteration 1400 / 2000: loss 2.302561\n",
      "iteration 1500 / 2000: loss 2.302574\n",
      "iteration 1600 / 2000: loss 2.302577\n",
      "iteration 1700 / 2000: loss 2.302582\n",
      "iteration 1800 / 2000: loss 2.302591\n",
      "iteration 1900 / 2000: loss 2.302569\n",
      "Hidden Size: 70, Learning Rate: 1e-07, LR Decay: 0.645, Regularization: 0.0, Validation accuracy: 0.1494\n",
      "iteration 0 / 2000: loss 2.302598\n",
      "iteration 100 / 2000: loss 2.302586\n",
      "iteration 200 / 2000: loss 2.302598\n",
      "iteration 300 / 2000: loss 2.302586\n",
      "iteration 400 / 2000: loss 2.302592\n",
      "iteration 500 / 2000: loss 2.302590\n",
      "iteration 600 / 2000: loss 2.302589\n",
      "iteration 700 / 2000: loss 2.302589\n",
      "iteration 800 / 2000: loss 2.302594\n",
      "iteration 900 / 2000: loss 2.302585\n",
      "iteration 1000 / 2000: loss 2.302601\n",
      "iteration 1100 / 2000: loss 2.302594\n",
      "iteration 1200 / 2000: loss 2.302596\n",
      "iteration 1300 / 2000: loss 2.302589\n",
      "iteration 1400 / 2000: loss 2.302597\n",
      "iteration 1500 / 2000: loss 2.302586\n",
      "iteration 1600 / 2000: loss 2.302595\n",
      "iteration 1700 / 2000: loss 2.302602\n",
      "iteration 1800 / 2000: loss 2.302592\n",
      "iteration 1900 / 2000: loss 2.302599\n",
      "Hidden Size: 70, Learning Rate: 1e-07, LR Decay: 0.645, Regularization: 0.1, Validation accuracy: 0.1444\n",
      "iteration 0 / 2000: loss 2.302658\n",
      "iteration 100 / 2000: loss 2.302657\n",
      "iteration 200 / 2000: loss 2.302656\n",
      "iteration 300 / 2000: loss 2.302659\n",
      "iteration 400 / 2000: loss 2.302651\n",
      "iteration 500 / 2000: loss 2.302649\n",
      "iteration 600 / 2000: loss 2.302655\n",
      "iteration 700 / 2000: loss 2.302660\n",
      "iteration 800 / 2000: loss 2.302654\n",
      "iteration 900 / 2000: loss 2.302647\n",
      "iteration 1000 / 2000: loss 2.302651\n",
      "iteration 1100 / 2000: loss 2.302649\n",
      "iteration 1200 / 2000: loss 2.302651\n",
      "iteration 1300 / 2000: loss 2.302646\n",
      "iteration 1400 / 2000: loss 2.302651\n",
      "iteration 1500 / 2000: loss 2.302655\n",
      "iteration 1600 / 2000: loss 2.302659\n",
      "iteration 1700 / 2000: loss 2.302652\n",
      "iteration 1800 / 2000: loss 2.302648\n",
      "iteration 1900 / 2000: loss 2.302652\n",
      "Hidden Size: 70, Learning Rate: 1e-07, LR Decay: 0.645, Regularization: 0.2, Validation accuracy: 0.0702\n",
      "iteration 0 / 2000: loss 2.302664\n",
      "iteration 100 / 2000: loss 2.302679\n",
      "iteration 200 / 2000: loss 2.302663\n",
      "iteration 300 / 2000: loss 2.302662\n",
      "iteration 400 / 2000: loss 2.302680\n",
      "iteration 500 / 2000: loss 2.302658\n",
      "iteration 600 / 2000: loss 2.302669\n",
      "iteration 700 / 2000: loss 2.302664\n",
      "iteration 800 / 2000: loss 2.302649\n",
      "iteration 900 / 2000: loss 2.302667\n",
      "iteration 1000 / 2000: loss 2.302666\n",
      "iteration 1100 / 2000: loss 2.302676\n",
      "iteration 1200 / 2000: loss 2.302660\n",
      "iteration 1300 / 2000: loss 2.302668\n",
      "iteration 1400 / 2000: loss 2.302669\n",
      "iteration 1500 / 2000: loss 2.302670\n",
      "iteration 1600 / 2000: loss 2.302663\n",
      "iteration 1700 / 2000: loss 2.302664\n",
      "iteration 1800 / 2000: loss 2.302664\n",
      "iteration 1900 / 2000: loss 2.302662\n",
      "Hidden Size: 70, Learning Rate: 1e-07, LR Decay: 0.645, Regularization: 0.30000000000000004, Validation accuracy: 0.1062\n",
      "iteration 0 / 2000: loss 2.302718\n",
      "iteration 100 / 2000: loss 2.302710\n",
      "iteration 200 / 2000: loss 2.302710\n",
      "iteration 300 / 2000: loss 2.302710\n",
      "iteration 400 / 2000: loss 2.302713\n",
      "iteration 500 / 2000: loss 2.302711\n",
      "iteration 600 / 2000: loss 2.302710\n",
      "iteration 700 / 2000: loss 2.302706\n",
      "iteration 800 / 2000: loss 2.302708\n",
      "iteration 900 / 2000: loss 2.302713\n",
      "iteration 1000 / 2000: loss 2.302720\n",
      "iteration 1100 / 2000: loss 2.302704\n",
      "iteration 1200 / 2000: loss 2.302716\n",
      "iteration 1300 / 2000: loss 2.302701\n",
      "iteration 1400 / 2000: loss 2.302725\n",
      "iteration 1500 / 2000: loss 2.302711\n",
      "iteration 1600 / 2000: loss 2.302709\n",
      "iteration 1700 / 2000: loss 2.302701\n",
      "iteration 1800 / 2000: loss 2.302695\n",
      "iteration 1900 / 2000: loss 2.302714\n",
      "Hidden Size: 70, Learning Rate: 1e-07, LR Decay: 0.645, Regularization: 0.4, Validation accuracy: 0.0726\n",
      "iteration 0 / 2000: loss 2.302595\n",
      "iteration 100 / 2000: loss 2.302590\n",
      "iteration 200 / 2000: loss 2.302578\n",
      "iteration 300 / 2000: loss 2.302597\n",
      "iteration 400 / 2000: loss 2.302584\n",
      "iteration 500 / 2000: loss 2.302601\n",
      "iteration 600 / 2000: loss 2.302596\n",
      "iteration 700 / 2000: loss 2.302589\n",
      "iteration 800 / 2000: loss 2.302600\n",
      "iteration 900 / 2000: loss 2.302586\n",
      "iteration 1000 / 2000: loss 2.302586\n",
      "iteration 1100 / 2000: loss 2.302587\n",
      "iteration 1200 / 2000: loss 2.302594\n",
      "iteration 1300 / 2000: loss 2.302600\n",
      "iteration 1400 / 2000: loss 2.302584\n",
      "iteration 1500 / 2000: loss 2.302590\n",
      "iteration 1600 / 2000: loss 2.302588\n",
      "iteration 1700 / 2000: loss 2.302588\n",
      "iteration 1800 / 2000: loss 2.302591\n",
      "iteration 1900 / 2000: loss 2.302601\n",
      "Hidden Size: 70, Learning Rate: 1e-07, LR Decay: 0.47250000000000003, Regularization: 0.0, Validation accuracy: 0.0754\n",
      "iteration 0 / 2000: loss 2.302615\n",
      "iteration 100 / 2000: loss 2.302605\n",
      "iteration 200 / 2000: loss 2.302600\n",
      "iteration 300 / 2000: loss 2.302613\n",
      "iteration 400 / 2000: loss 2.302619\n",
      "iteration 500 / 2000: loss 2.302600\n",
      "iteration 600 / 2000: loss 2.302606\n",
      "iteration 700 / 2000: loss 2.302614\n",
      "iteration 800 / 2000: loss 2.302599\n",
      "iteration 900 / 2000: loss 2.302608\n",
      "iteration 1000 / 2000: loss 2.302613\n",
      "iteration 1100 / 2000: loss 2.302607\n",
      "iteration 1200 / 2000: loss 2.302606\n",
      "iteration 1300 / 2000: loss 2.302605\n",
      "iteration 1400 / 2000: loss 2.302602\n",
      "iteration 1500 / 2000: loss 2.302600\n",
      "iteration 1600 / 2000: loss 2.302609\n",
      "iteration 1700 / 2000: loss 2.302599\n",
      "iteration 1800 / 2000: loss 2.302609\n",
      "iteration 1900 / 2000: loss 2.302602\n",
      "Hidden Size: 70, Learning Rate: 1e-07, LR Decay: 0.47250000000000003, Regularization: 0.1, Validation accuracy: 0.1126\n",
      "iteration 0 / 2000: loss 2.302590\n",
      "iteration 100 / 2000: loss 2.302612\n",
      "iteration 200 / 2000: loss 2.302601\n",
      "iteration 300 / 2000: loss 2.302599\n",
      "iteration 400 / 2000: loss 2.302619\n",
      "iteration 500 / 2000: loss 2.302611\n",
      "iteration 600 / 2000: loss 2.302617\n",
      "iteration 700 / 2000: loss 2.302592\n",
      "iteration 800 / 2000: loss 2.302612\n",
      "iteration 900 / 2000: loss 2.302616\n",
      "iteration 1000 / 2000: loss 2.302602\n",
      "iteration 1100 / 2000: loss 2.302606\n",
      "iteration 1200 / 2000: loss 2.302598\n",
      "iteration 1300 / 2000: loss 2.302594\n",
      "iteration 1400 / 2000: loss 2.302607\n",
      "iteration 1500 / 2000: loss 2.302597\n",
      "iteration 1600 / 2000: loss 2.302596\n",
      "iteration 1700 / 2000: loss 2.302594\n",
      "iteration 1800 / 2000: loss 2.302604\n",
      "iteration 1900 / 2000: loss 2.302598\n",
      "Hidden Size: 70, Learning Rate: 1e-07, LR Decay: 0.47250000000000003, Regularization: 0.2, Validation accuracy: 0.1566\n",
      "iteration 0 / 2000: loss 2.302671\n",
      "iteration 100 / 2000: loss 2.302654\n",
      "iteration 200 / 2000: loss 2.302666\n",
      "iteration 300 / 2000: loss 2.302672\n",
      "iteration 400 / 2000: loss 2.302661\n",
      "iteration 500 / 2000: loss 2.302665\n",
      "iteration 600 / 2000: loss 2.302664\n",
      "iteration 700 / 2000: loss 2.302651\n",
      "iteration 800 / 2000: loss 2.302662\n",
      "iteration 900 / 2000: loss 2.302667\n",
      "iteration 1000 / 2000: loss 2.302655\n",
      "iteration 1100 / 2000: loss 2.302662\n",
      "iteration 1200 / 2000: loss 2.302665\n",
      "iteration 1300 / 2000: loss 2.302663\n",
      "iteration 1400 / 2000: loss 2.302667\n",
      "iteration 1500 / 2000: loss 2.302659\n",
      "iteration 1600 / 2000: loss 2.302656\n",
      "iteration 1700 / 2000: loss 2.302660\n",
      "iteration 1800 / 2000: loss 2.302665\n",
      "iteration 1900 / 2000: loss 2.302664\n",
      "Hidden Size: 70, Learning Rate: 1e-07, LR Decay: 0.47250000000000003, Regularization: 0.30000000000000004, Validation accuracy: 0.1294\n",
      "iteration 0 / 2000: loss 2.302719\n",
      "iteration 100 / 2000: loss 2.302709\n",
      "iteration 200 / 2000: loss 2.302711\n",
      "iteration 300 / 2000: loss 2.302724\n",
      "iteration 400 / 2000: loss 2.302713\n",
      "iteration 500 / 2000: loss 2.302720\n",
      "iteration 600 / 2000: loss 2.302712\n",
      "iteration 700 / 2000: loss 2.302713\n",
      "iteration 800 / 2000: loss 2.302705\n",
      "iteration 900 / 2000: loss 2.302716\n",
      "iteration 1000 / 2000: loss 2.302712\n",
      "iteration 1100 / 2000: loss 2.302712\n",
      "iteration 1200 / 2000: loss 2.302698\n",
      "iteration 1300 / 2000: loss 2.302712\n",
      "iteration 1400 / 2000: loss 2.302713\n",
      "iteration 1500 / 2000: loss 2.302727\n",
      "iteration 1600 / 2000: loss 2.302718\n",
      "iteration 1700 / 2000: loss 2.302710\n",
      "iteration 1800 / 2000: loss 2.302714\n",
      "iteration 1900 / 2000: loss 2.302714\n",
      "Hidden Size: 70, Learning Rate: 1e-07, LR Decay: 0.47250000000000003, Regularization: 0.4, Validation accuracy: 0.0662\n",
      "iteration 0 / 2000: loss 2.302589\n",
      "iteration 100 / 2000: loss 2.302582\n",
      "iteration 200 / 2000: loss 2.302583\n",
      "iteration 300 / 2000: loss 2.302577\n",
      "iteration 400 / 2000: loss 2.302567\n",
      "iteration 500 / 2000: loss 2.302597\n",
      "iteration 600 / 2000: loss 2.302581\n",
      "iteration 700 / 2000: loss 2.302571\n",
      "iteration 800 / 2000: loss 2.302576\n",
      "iteration 900 / 2000: loss 2.302587\n",
      "iteration 1000 / 2000: loss 2.302575\n",
      "iteration 1100 / 2000: loss 2.302584\n",
      "iteration 1200 / 2000: loss 2.302569\n",
      "iteration 1300 / 2000: loss 2.302569\n",
      "iteration 1400 / 2000: loss 2.302574\n",
      "iteration 1500 / 2000: loss 2.302595\n",
      "iteration 1600 / 2000: loss 2.302580\n",
      "iteration 1700 / 2000: loss 2.302589\n",
      "iteration 1800 / 2000: loss 2.302571\n",
      "iteration 1900 / 2000: loss 2.302579\n",
      "Hidden Size: 70, Learning Rate: 1e-07, LR Decay: 0.3, Regularization: 0.0, Validation accuracy: 0.0732\n",
      "iteration 0 / 2000: loss 2.302607\n",
      "iteration 100 / 2000: loss 2.302594\n",
      "iteration 200 / 2000: loss 2.302608\n",
      "iteration 300 / 2000: loss 2.302603\n",
      "iteration 400 / 2000: loss 2.302599\n",
      "iteration 500 / 2000: loss 2.302607\n",
      "iteration 600 / 2000: loss 2.302612\n",
      "iteration 700 / 2000: loss 2.302593\n",
      "iteration 800 / 2000: loss 2.302615\n",
      "iteration 900 / 2000: loss 2.302602\n",
      "iteration 1000 / 2000: loss 2.302596\n",
      "iteration 1100 / 2000: loss 2.302610\n",
      "iteration 1200 / 2000: loss 2.302611\n",
      "iteration 1300 / 2000: loss 2.302608\n",
      "iteration 1400 / 2000: loss 2.302591\n",
      "iteration 1500 / 2000: loss 2.302621\n",
      "iteration 1600 / 2000: loss 2.302608\n",
      "iteration 1700 / 2000: loss 2.302606\n",
      "iteration 1800 / 2000: loss 2.302615\n",
      "iteration 1900 / 2000: loss 2.302614\n",
      "Hidden Size: 70, Learning Rate: 1e-07, LR Decay: 0.3, Regularization: 0.1, Validation accuracy: 0.1172\n",
      "iteration 0 / 2000: loss 2.302634\n",
      "iteration 100 / 2000: loss 2.302635\n",
      "iteration 200 / 2000: loss 2.302641\n",
      "iteration 300 / 2000: loss 2.302639\n",
      "iteration 400 / 2000: loss 2.302648\n",
      "iteration 500 / 2000: loss 2.302642\n",
      "iteration 600 / 2000: loss 2.302635\n",
      "iteration 700 / 2000: loss 2.302647\n",
      "iteration 800 / 2000: loss 2.302643\n",
      "iteration 900 / 2000: loss 2.302637\n",
      "iteration 1000 / 2000: loss 2.302641\n",
      "iteration 1100 / 2000: loss 2.302645\n",
      "iteration 1200 / 2000: loss 2.302628\n",
      "iteration 1300 / 2000: loss 2.302640\n",
      "iteration 1400 / 2000: loss 2.302636\n",
      "iteration 1500 / 2000: loss 2.302626\n",
      "iteration 1600 / 2000: loss 2.302642\n",
      "iteration 1700 / 2000: loss 2.302656\n",
      "iteration 1800 / 2000: loss 2.302646\n",
      "iteration 1900 / 2000: loss 2.302634\n",
      "Hidden Size: 70, Learning Rate: 1e-07, LR Decay: 0.3, Regularization: 0.2, Validation accuracy: 0.1056\n",
      "iteration 0 / 2000: loss 2.302694\n",
      "iteration 100 / 2000: loss 2.302684\n",
      "iteration 200 / 2000: loss 2.302685\n",
      "iteration 300 / 2000: loss 2.302683\n",
      "iteration 400 / 2000: loss 2.302695\n",
      "iteration 500 / 2000: loss 2.302684\n",
      "iteration 600 / 2000: loss 2.302692\n",
      "iteration 700 / 2000: loss 2.302691\n",
      "iteration 800 / 2000: loss 2.302682\n",
      "iteration 900 / 2000: loss 2.302708\n",
      "iteration 1000 / 2000: loss 2.302687\n",
      "iteration 1100 / 2000: loss 2.302681\n",
      "iteration 1200 / 2000: loss 2.302679\n",
      "iteration 1300 / 2000: loss 2.302674\n",
      "iteration 1400 / 2000: loss 2.302682\n",
      "iteration 1500 / 2000: loss 2.302682\n",
      "iteration 1600 / 2000: loss 2.302673\n",
      "iteration 1700 / 2000: loss 2.302672\n",
      "iteration 1800 / 2000: loss 2.302694\n",
      "iteration 1900 / 2000: loss 2.302681\n",
      "Hidden Size: 70, Learning Rate: 1e-07, LR Decay: 0.3, Regularization: 0.30000000000000004, Validation accuracy: 0.057\n",
      "iteration 0 / 2000: loss 2.302690\n",
      "iteration 100 / 2000: loss 2.302691\n",
      "iteration 200 / 2000: loss 2.302693\n",
      "iteration 300 / 2000: loss 2.302702\n",
      "iteration 400 / 2000: loss 2.302691\n",
      "iteration 500 / 2000: loss 2.302685\n",
      "iteration 600 / 2000: loss 2.302696\n",
      "iteration 700 / 2000: loss 2.302688\n",
      "iteration 800 / 2000: loss 2.302693\n",
      "iteration 900 / 2000: loss 2.302692\n",
      "iteration 1000 / 2000: loss 2.302681\n",
      "iteration 1100 / 2000: loss 2.302688\n",
      "iteration 1200 / 2000: loss 2.302694\n",
      "iteration 1300 / 2000: loss 2.302688\n",
      "iteration 1400 / 2000: loss 2.302683\n",
      "iteration 1500 / 2000: loss 2.302683\n",
      "iteration 1600 / 2000: loss 2.302695\n",
      "iteration 1700 / 2000: loss 2.302685\n",
      "iteration 1800 / 2000: loss 2.302688\n",
      "iteration 1900 / 2000: loss 2.302689\n",
      "Hidden Size: 70, Learning Rate: 1e-07, LR Decay: 0.3, Regularization: 0.4, Validation accuracy: 0.0826\n",
      "iteration 0 / 2000: loss 2.302612\n",
      "iteration 100 / 2000: loss 0.249590\n",
      "iteration 200 / 2000: loss 0.267548\n",
      "iteration 300 / 2000: loss 0.097403\n",
      "iteration 400 / 2000: loss 0.103493\n",
      "iteration 500 / 2000: loss 0.130073\n",
      "iteration 600 / 2000: loss 0.042722\n",
      "iteration 700 / 2000: loss 0.056268\n",
      "iteration 800 / 2000: loss 0.186174\n",
      "iteration 900 / 2000: loss 0.059370\n",
      "iteration 1000 / 2000: loss 0.027784\n",
      "iteration 1100 / 2000: loss 0.038600\n",
      "iteration 1200 / 2000: loss 0.046692\n",
      "iteration 1300 / 2000: loss 0.018210\n",
      "iteration 1400 / 2000: loss 0.095253\n",
      "iteration 1500 / 2000: loss 0.078696\n",
      "iteration 1600 / 2000: loss 0.069437\n",
      "iteration 1700 / 2000: loss 0.044724\n",
      "iteration 1800 / 2000: loss 0.022350\n",
      "iteration 1900 / 2000: loss 0.057902\n",
      "Hidden Size: 80, Learning Rate: 0.01, LR Decay: 0.99, Regularization: 0.0, Validation accuracy: 0.9758\n",
      "iteration 0 / 2000: loss 2.302602\n",
      "iteration 100 / 2000: loss 0.372592\n",
      "iteration 200 / 2000: loss 0.189022\n",
      "iteration 300 / 2000: loss 0.133008\n",
      "iteration 400 / 2000: loss 0.090536\n",
      "iteration 500 / 2000: loss 0.186280\n",
      "iteration 600 / 2000: loss 0.200559\n",
      "iteration 700 / 2000: loss 0.120992\n",
      "iteration 800 / 2000: loss 0.152638\n",
      "iteration 900 / 2000: loss 0.126783\n",
      "iteration 1000 / 2000: loss 0.143095\n",
      "iteration 1100 / 2000: loss 0.124200\n",
      "iteration 1200 / 2000: loss 0.131349\n",
      "iteration 1300 / 2000: loss 0.134848\n",
      "iteration 1400 / 2000: loss 0.127378\n",
      "iteration 1500 / 2000: loss 0.134441\n",
      "iteration 1600 / 2000: loss 0.141862\n",
      "iteration 1700 / 2000: loss 0.108687\n",
      "iteration 1800 / 2000: loss 0.105707\n",
      "iteration 1900 / 2000: loss 0.113942\n",
      "Hidden Size: 80, Learning Rate: 0.01, LR Decay: 0.99, Regularization: 0.1, Validation accuracy: 0.977\n",
      "iteration 0 / 2000: loss 2.302632\n",
      "iteration 100 / 2000: loss 0.293996\n",
      "iteration 200 / 2000: loss 0.320831\n",
      "iteration 300 / 2000: loss 0.135209\n",
      "iteration 400 / 2000: loss 0.216518\n",
      "iteration 500 / 2000: loss 0.180002\n",
      "iteration 600 / 2000: loss 0.170220\n",
      "iteration 700 / 2000: loss 0.163970\n",
      "iteration 800 / 2000: loss 0.171288\n",
      "iteration 900 / 2000: loss 0.132135\n",
      "iteration 1000 / 2000: loss 0.158836\n",
      "iteration 1100 / 2000: loss 0.134592\n",
      "iteration 1200 / 2000: loss 0.159842\n",
      "iteration 1300 / 2000: loss 0.151663\n",
      "iteration 1400 / 2000: loss 0.184425\n",
      "iteration 1500 / 2000: loss 0.168023\n",
      "iteration 1600 / 2000: loss 0.215615\n",
      "iteration 1700 / 2000: loss 0.162527\n",
      "iteration 1800 / 2000: loss 0.150995\n",
      "iteration 1900 / 2000: loss 0.248072\n",
      "Hidden Size: 80, Learning Rate: 0.01, LR Decay: 0.99, Regularization: 0.2, Validation accuracy: 0.9702\n",
      "iteration 0 / 2000: loss 2.302709\n",
      "iteration 100 / 2000: loss 0.236111\n",
      "iteration 200 / 2000: loss 0.249319\n",
      "iteration 300 / 2000: loss 0.238781\n",
      "iteration 400 / 2000: loss 0.328340\n",
      "iteration 500 / 2000: loss 0.203625\n",
      "iteration 600 / 2000: loss 0.260117\n",
      "iteration 700 / 2000: loss 0.193635\n",
      "iteration 800 / 2000: loss 0.234398\n",
      "iteration 900 / 2000: loss 0.206914\n",
      "iteration 1000 / 2000: loss 0.282799\n",
      "iteration 1100 / 2000: loss 0.242340\n",
      "iteration 1200 / 2000: loss 0.187699\n",
      "iteration 1300 / 2000: loss 0.177886\n",
      "iteration 1400 / 2000: loss 0.197444\n",
      "iteration 1500 / 2000: loss 0.191635\n",
      "iteration 1600 / 2000: loss 0.208374\n",
      "iteration 1700 / 2000: loss 0.182281\n",
      "iteration 1800 / 2000: loss 0.241426\n",
      "iteration 1900 / 2000: loss 0.187515\n",
      "Hidden Size: 80, Learning Rate: 0.01, LR Decay: 0.99, Regularization: 0.30000000000000004, Validation accuracy: 0.9594\n",
      "iteration 0 / 2000: loss 2.302713\n",
      "iteration 100 / 2000: loss 0.351440\n",
      "iteration 200 / 2000: loss 0.306859\n",
      "iteration 300 / 2000: loss 0.219152\n",
      "iteration 400 / 2000: loss 0.211016\n",
      "iteration 500 / 2000: loss 0.276659\n",
      "iteration 600 / 2000: loss 0.278559\n",
      "iteration 700 / 2000: loss 0.266833\n",
      "iteration 800 / 2000: loss 0.216495\n",
      "iteration 900 / 2000: loss 0.308008\n",
      "iteration 1000 / 2000: loss 0.180888\n",
      "iteration 1100 / 2000: loss 0.267827\n",
      "iteration 1200 / 2000: loss 0.210723\n",
      "iteration 1300 / 2000: loss 0.239816\n",
      "iteration 1400 / 2000: loss 0.215623\n",
      "iteration 1500 / 2000: loss 0.248399\n",
      "iteration 1600 / 2000: loss 0.201978\n",
      "iteration 1700 / 2000: loss 0.240867\n",
      "iteration 1800 / 2000: loss 0.277349\n",
      "iteration 1900 / 2000: loss 0.235208\n",
      "Hidden Size: 80, Learning Rate: 0.01, LR Decay: 0.99, Regularization: 0.4, Validation accuracy: 0.9636\n",
      "iteration 0 / 2000: loss 2.302568\n",
      "iteration 100 / 2000: loss 0.213336\n",
      "iteration 200 / 2000: loss 0.139968\n",
      "iteration 300 / 2000: loss 0.080980\n",
      "iteration 400 / 2000: loss 0.157906\n",
      "iteration 500 / 2000: loss 0.156232\n",
      "iteration 600 / 2000: loss 0.029335\n",
      "iteration 700 / 2000: loss 0.069107\n",
      "iteration 800 / 2000: loss 0.076927\n",
      "iteration 900 / 2000: loss 0.082521\n",
      "iteration 1000 / 2000: loss 0.018660\n",
      "iteration 1100 / 2000: loss 0.049638\n",
      "iteration 1200 / 2000: loss 0.031231\n",
      "iteration 1300 / 2000: loss 0.033802\n",
      "iteration 1400 / 2000: loss 0.034174\n",
      "iteration 1500 / 2000: loss 0.028798\n",
      "iteration 1600 / 2000: loss 0.025687\n",
      "iteration 1700 / 2000: loss 0.027224\n",
      "iteration 1800 / 2000: loss 0.049101\n",
      "iteration 1900 / 2000: loss 0.015555\n",
      "Hidden Size: 80, Learning Rate: 0.01, LR Decay: 0.8175, Regularization: 0.0, Validation accuracy: 0.9798\n",
      "iteration 0 / 2000: loss 2.302606\n",
      "iteration 100 / 2000: loss 0.214734\n",
      "iteration 200 / 2000: loss 0.208963\n",
      "iteration 300 / 2000: loss 0.163231\n",
      "iteration 400 / 2000: loss 0.122417\n",
      "iteration 500 / 2000: loss 0.136464\n",
      "iteration 600 / 2000: loss 0.141205\n",
      "iteration 700 / 2000: loss 0.112204\n",
      "iteration 800 / 2000: loss 0.115018\n",
      "iteration 900 / 2000: loss 0.103715\n",
      "iteration 1000 / 2000: loss 0.120237\n",
      "iteration 1100 / 2000: loss 0.107758\n",
      "iteration 1200 / 2000: loss 0.103180\n",
      "iteration 1300 / 2000: loss 0.115391\n",
      "iteration 1400 / 2000: loss 0.107941\n",
      "iteration 1500 / 2000: loss 0.084510\n",
      "iteration 1600 / 2000: loss 0.099272\n",
      "iteration 1700 / 2000: loss 0.091168\n",
      "iteration 1800 / 2000: loss 0.087495\n",
      "iteration 1900 / 2000: loss 0.101897\n",
      "Hidden Size: 80, Learning Rate: 0.01, LR Decay: 0.8175, Regularization: 0.1, Validation accuracy: 0.9812\n",
      "iteration 0 / 2000: loss 2.302657\n",
      "iteration 100 / 2000: loss 0.254434\n",
      "iteration 200 / 2000: loss 0.249492\n",
      "iteration 300 / 2000: loss 0.256511\n",
      "iteration 400 / 2000: loss 0.163741\n",
      "iteration 500 / 2000: loss 0.173664\n",
      "iteration 600 / 2000: loss 0.109576\n",
      "iteration 700 / 2000: loss 0.154242\n",
      "iteration 800 / 2000: loss 0.130296\n",
      "iteration 900 / 2000: loss 0.116381\n",
      "iteration 1000 / 2000: loss 0.157038\n",
      "iteration 1100 / 2000: loss 0.120364\n",
      "iteration 1200 / 2000: loss 0.188458\n",
      "iteration 1300 / 2000: loss 0.153601\n",
      "iteration 1400 / 2000: loss 0.132900\n",
      "iteration 1500 / 2000: loss 0.141202\n",
      "iteration 1600 / 2000: loss 0.152468\n",
      "iteration 1700 / 2000: loss 0.153889\n",
      "iteration 1800 / 2000: loss 0.126318\n",
      "iteration 1900 / 2000: loss 0.117207\n",
      "Hidden Size: 80, Learning Rate: 0.01, LR Decay: 0.8175, Regularization: 0.2, Validation accuracy: 0.976\n",
      "iteration 0 / 2000: loss 2.302687\n",
      "iteration 100 / 2000: loss 0.285815\n",
      "iteration 200 / 2000: loss 0.325180\n",
      "iteration 300 / 2000: loss 0.204057\n",
      "iteration 400 / 2000: loss 0.184046\n",
      "iteration 500 / 2000: loss 0.239753\n",
      "iteration 600 / 2000: loss 0.157294\n",
      "iteration 700 / 2000: loss 0.183096\n",
      "iteration 800 / 2000: loss 0.166012\n",
      "iteration 900 / 2000: loss 0.207415\n",
      "iteration 1000 / 2000: loss 0.144473\n",
      "iteration 1100 / 2000: loss 0.258614\n",
      "iteration 1200 / 2000: loss 0.168634\n",
      "iteration 1300 / 2000: loss 0.159182\n",
      "iteration 1400 / 2000: loss 0.179048\n",
      "iteration 1500 / 2000: loss 0.190283\n",
      "iteration 1600 / 2000: loss 0.207043\n",
      "iteration 1700 / 2000: loss 0.177414\n",
      "iteration 1800 / 2000: loss 0.153725\n",
      "iteration 1900 / 2000: loss 0.157023\n",
      "Hidden Size: 80, Learning Rate: 0.01, LR Decay: 0.8175, Regularization: 0.30000000000000004, Validation accuracy: 0.9762\n",
      "iteration 0 / 2000: loss 2.302717\n",
      "iteration 100 / 2000: loss 0.280392\n",
      "iteration 200 / 2000: loss 0.234290\n",
      "iteration 300 / 2000: loss 0.250106\n",
      "iteration 400 / 2000: loss 0.212745\n",
      "iteration 500 / 2000: loss 0.245624\n",
      "iteration 600 / 2000: loss 0.196413\n",
      "iteration 700 / 2000: loss 0.244736\n",
      "iteration 800 / 2000: loss 0.255841\n",
      "iteration 900 / 2000: loss 0.193232\n",
      "iteration 1000 / 2000: loss 0.290103\n",
      "iteration 1100 / 2000: loss 0.234647\n",
      "iteration 1200 / 2000: loss 0.225682\n",
      "iteration 1300 / 2000: loss 0.218297\n",
      "iteration 1400 / 2000: loss 0.173782\n",
      "iteration 1500 / 2000: loss 0.199736\n",
      "iteration 1600 / 2000: loss 0.192490\n",
      "iteration 1700 / 2000: loss 0.171838\n",
      "iteration 1800 / 2000: loss 0.206619\n",
      "iteration 1900 / 2000: loss 0.165867\n",
      "Hidden Size: 80, Learning Rate: 0.01, LR Decay: 0.8175, Regularization: 0.4, Validation accuracy: 0.9756\n",
      "iteration 0 / 2000: loss 2.302582\n",
      "iteration 100 / 2000: loss 0.301925\n",
      "iteration 200 / 2000: loss 0.275870\n",
      "iteration 300 / 2000: loss 0.125025\n",
      "iteration 400 / 2000: loss 0.137750\n",
      "iteration 500 / 2000: loss 0.070922\n",
      "iteration 600 / 2000: loss 0.102697\n",
      "iteration 700 / 2000: loss 0.090919\n",
      "iteration 800 / 2000: loss 0.049355\n",
      "iteration 900 / 2000: loss 0.044766\n",
      "iteration 1000 / 2000: loss 0.065954\n",
      "iteration 1100 / 2000: loss 0.028438\n",
      "iteration 1200 / 2000: loss 0.091174\n",
      "iteration 1300 / 2000: loss 0.063975\n",
      "iteration 1400 / 2000: loss 0.048447\n",
      "iteration 1500 / 2000: loss 0.040252\n",
      "iteration 1600 / 2000: loss 0.029283\n",
      "iteration 1700 / 2000: loss 0.053453\n",
      "iteration 1800 / 2000: loss 0.030933\n",
      "iteration 1900 / 2000: loss 0.035561\n",
      "Hidden Size: 80, Learning Rate: 0.01, LR Decay: 0.645, Regularization: 0.0, Validation accuracy: 0.975\n",
      "iteration 0 / 2000: loss 2.302613\n",
      "iteration 100 / 2000: loss 0.217130\n",
      "iteration 200 / 2000: loss 0.276675\n",
      "iteration 300 / 2000: loss 0.139795\n",
      "iteration 400 / 2000: loss 0.171828\n",
      "iteration 500 / 2000: loss 0.172574\n",
      "iteration 600 / 2000: loss 0.120727\n",
      "iteration 700 / 2000: loss 0.172537\n",
      "iteration 800 / 2000: loss 0.110658\n",
      "iteration 900 / 2000: loss 0.110467\n",
      "iteration 1000 / 2000: loss 0.087131\n",
      "iteration 1100 / 2000: loss 0.107644\n",
      "iteration 1200 / 2000: loss 0.108389\n",
      "iteration 1300 / 2000: loss 0.095921\n",
      "iteration 1400 / 2000: loss 0.105289\n",
      "iteration 1500 / 2000: loss 0.098014\n",
      "iteration 1600 / 2000: loss 0.087241\n",
      "iteration 1700 / 2000: loss 0.148110\n",
      "iteration 1800 / 2000: loss 0.111954\n",
      "iteration 1900 / 2000: loss 0.090591\n",
      "Hidden Size: 80, Learning Rate: 0.01, LR Decay: 0.645, Regularization: 0.1, Validation accuracy: 0.9744\n",
      "iteration 0 / 2000: loss 2.302669\n",
      "iteration 100 / 2000: loss 0.326781\n",
      "iteration 200 / 2000: loss 0.227230\n",
      "iteration 300 / 2000: loss 0.210557\n",
      "iteration 400 / 2000: loss 0.273082\n",
      "iteration 500 / 2000: loss 0.135723\n",
      "iteration 600 / 2000: loss 0.142881\n",
      "iteration 700 / 2000: loss 0.142600\n",
      "iteration 800 / 2000: loss 0.147824\n",
      "iteration 900 / 2000: loss 0.133669\n",
      "iteration 1000 / 2000: loss 0.190140\n",
      "iteration 1100 / 2000: loss 0.210584\n",
      "iteration 1200 / 2000: loss 0.179255\n",
      "iteration 1300 / 2000: loss 0.146774\n",
      "iteration 1400 / 2000: loss 0.161031\n",
      "iteration 1500 / 2000: loss 0.136088\n",
      "iteration 1600 / 2000: loss 0.153866\n",
      "iteration 1700 / 2000: loss 0.125757\n",
      "iteration 1800 / 2000: loss 0.139578\n",
      "iteration 1900 / 2000: loss 0.157264\n",
      "Hidden Size: 80, Learning Rate: 0.01, LR Decay: 0.645, Regularization: 0.2, Validation accuracy: 0.9744\n",
      "iteration 0 / 2000: loss 2.302696\n",
      "iteration 100 / 2000: loss 0.337902\n",
      "iteration 200 / 2000: loss 0.242190\n",
      "iteration 300 / 2000: loss 0.265446\n",
      "iteration 400 / 2000: loss 0.189601\n",
      "iteration 500 / 2000: loss 0.185837\n",
      "iteration 600 / 2000: loss 0.199223\n",
      "iteration 700 / 2000: loss 0.194651\n",
      "iteration 800 / 2000: loss 0.215338\n",
      "iteration 900 / 2000: loss 0.195704\n",
      "iteration 1000 / 2000: loss 0.226328\n",
      "iteration 1100 / 2000: loss 0.174981\n",
      "iteration 1200 / 2000: loss 0.183302\n",
      "iteration 1300 / 2000: loss 0.184028\n",
      "iteration 1400 / 2000: loss 0.194495\n",
      "iteration 1500 / 2000: loss 0.185956\n",
      "iteration 1600 / 2000: loss 0.214580\n",
      "iteration 1700 / 2000: loss 0.206897\n",
      "iteration 1800 / 2000: loss 0.152229\n",
      "iteration 1900 / 2000: loss 0.160844\n",
      "Hidden Size: 80, Learning Rate: 0.01, LR Decay: 0.645, Regularization: 0.30000000000000004, Validation accuracy: 0.9732\n",
      "iteration 0 / 2000: loss 2.302722\n",
      "iteration 100 / 2000: loss 0.330249\n",
      "iteration 200 / 2000: loss 0.290487\n",
      "iteration 300 / 2000: loss 0.228536\n",
      "iteration 400 / 2000: loss 0.220256\n",
      "iteration 500 / 2000: loss 0.314578\n",
      "iteration 600 / 2000: loss 0.211807\n",
      "iteration 700 / 2000: loss 0.197891\n",
      "iteration 800 / 2000: loss 0.168041\n",
      "iteration 900 / 2000: loss 0.210644\n",
      "iteration 1000 / 2000: loss 0.179626\n",
      "iteration 1100 / 2000: loss 0.216517\n",
      "iteration 1200 / 2000: loss 0.190619\n",
      "iteration 1300 / 2000: loss 0.249101\n",
      "iteration 1400 / 2000: loss 0.169697\n",
      "iteration 1500 / 2000: loss 0.166452\n",
      "iteration 1600 / 2000: loss 0.215149\n",
      "iteration 1700 / 2000: loss 0.188134\n",
      "iteration 1800 / 2000: loss 0.214647\n",
      "iteration 1900 / 2000: loss 0.169840\n",
      "Hidden Size: 80, Learning Rate: 0.01, LR Decay: 0.645, Regularization: 0.4, Validation accuracy: 0.9708\n",
      "iteration 0 / 2000: loss 2.302583\n",
      "iteration 100 / 2000: loss 0.236006\n",
      "iteration 200 / 2000: loss 0.193734\n",
      "iteration 300 / 2000: loss 0.112567\n",
      "iteration 400 / 2000: loss 0.162039\n",
      "iteration 500 / 2000: loss 0.127378\n",
      "iteration 600 / 2000: loss 0.132202\n",
      "iteration 700 / 2000: loss 0.093116\n",
      "iteration 800 / 2000: loss 0.143696\n",
      "iteration 900 / 2000: loss 0.110522\n",
      "iteration 1000 / 2000: loss 0.109893\n",
      "iteration 1100 / 2000: loss 0.070297\n",
      "iteration 1200 / 2000: loss 0.125315\n",
      "iteration 1300 / 2000: loss 0.072313\n",
      "iteration 1400 / 2000: loss 0.123108\n",
      "iteration 1500 / 2000: loss 0.071494\n",
      "iteration 1600 / 2000: loss 0.087418\n",
      "iteration 1700 / 2000: loss 0.136237\n",
      "iteration 1800 / 2000: loss 0.060905\n",
      "iteration 1900 / 2000: loss 0.098641\n",
      "Hidden Size: 80, Learning Rate: 0.01, LR Decay: 0.47250000000000003, Regularization: 0.0, Validation accuracy: 0.969\n",
      "iteration 0 / 2000: loss 2.302627\n",
      "iteration 100 / 2000: loss 0.306793\n",
      "iteration 200 / 2000: loss 0.224712\n",
      "iteration 300 / 2000: loss 0.087856\n",
      "iteration 400 / 2000: loss 0.169746\n",
      "iteration 500 / 2000: loss 0.122893\n",
      "iteration 600 / 2000: loss 0.210903\n",
      "iteration 700 / 2000: loss 0.180179\n",
      "iteration 800 / 2000: loss 0.107687\n",
      "iteration 900 / 2000: loss 0.210988\n",
      "iteration 1000 / 2000: loss 0.128480\n",
      "iteration 1100 / 2000: loss 0.189047\n",
      "iteration 1200 / 2000: loss 0.161523\n",
      "iteration 1300 / 2000: loss 0.163188\n",
      "iteration 1400 / 2000: loss 0.140551\n",
      "iteration 1500 / 2000: loss 0.149373\n",
      "iteration 1600 / 2000: loss 0.151229\n",
      "iteration 1700 / 2000: loss 0.091106\n",
      "iteration 1800 / 2000: loss 0.154758\n",
      "iteration 1900 / 2000: loss 0.239267\n",
      "Hidden Size: 80, Learning Rate: 0.01, LR Decay: 0.47250000000000003, Regularization: 0.1, Validation accuracy: 0.9678\n",
      "iteration 0 / 2000: loss 2.302664\n",
      "iteration 100 / 2000: loss 0.313302\n",
      "iteration 200 / 2000: loss 0.184460\n",
      "iteration 300 / 2000: loss 0.303949\n",
      "iteration 400 / 2000: loss 0.251120\n",
      "iteration 500 / 2000: loss 0.165926\n",
      "iteration 600 / 2000: loss 0.251367\n",
      "iteration 700 / 2000: loss 0.182806\n",
      "iteration 800 / 2000: loss 0.249075\n",
      "iteration 900 / 2000: loss 0.177405\n",
      "iteration 1000 / 2000: loss 0.210307\n",
      "iteration 1100 / 2000: loss 0.171167\n",
      "iteration 1200 / 2000: loss 0.133853\n",
      "iteration 1300 / 2000: loss 0.167728\n",
      "iteration 1400 / 2000: loss 0.190571\n",
      "iteration 1500 / 2000: loss 0.178858\n",
      "iteration 1600 / 2000: loss 0.224161\n",
      "iteration 1700 / 2000: loss 0.152778\n",
      "iteration 1800 / 2000: loss 0.153213\n",
      "iteration 1900 / 2000: loss 0.207118\n",
      "Hidden Size: 80, Learning Rate: 0.01, LR Decay: 0.47250000000000003, Regularization: 0.2, Validation accuracy: 0.9662\n",
      "iteration 0 / 2000: loss 2.302657\n",
      "iteration 100 / 2000: loss 0.346172\n",
      "iteration 200 / 2000: loss 0.281366\n",
      "iteration 300 / 2000: loss 0.206253\n",
      "iteration 400 / 2000: loss 0.216483\n",
      "iteration 500 / 2000: loss 0.184663\n",
      "iteration 600 / 2000: loss 0.222522\n",
      "iteration 700 / 2000: loss 0.157505\n",
      "iteration 800 / 2000: loss 0.207814\n",
      "iteration 900 / 2000: loss 0.165516\n",
      "iteration 1000 / 2000: loss 0.190965\n",
      "iteration 1100 / 2000: loss 0.190943\n",
      "iteration 1200 / 2000: loss 0.191407\n",
      "iteration 1300 / 2000: loss 0.235175\n",
      "iteration 1400 / 2000: loss 0.197180\n",
      "iteration 1500 / 2000: loss 0.154161\n",
      "iteration 1600 / 2000: loss 0.188822\n",
      "iteration 1700 / 2000: loss 0.130935\n",
      "iteration 1800 / 2000: loss 0.184348\n",
      "iteration 1900 / 2000: loss 0.149441\n",
      "Hidden Size: 80, Learning Rate: 0.01, LR Decay: 0.47250000000000003, Regularization: 0.30000000000000004, Validation accuracy: 0.9658\n",
      "iteration 0 / 2000: loss 2.302687\n",
      "iteration 100 / 2000: loss 0.332704\n",
      "iteration 200 / 2000: loss 0.275703\n",
      "iteration 300 / 2000: loss 0.226401\n",
      "iteration 400 / 2000: loss 0.255669\n",
      "iteration 500 / 2000: loss 0.262661\n",
      "iteration 600 / 2000: loss 0.237441\n",
      "iteration 700 / 2000: loss 0.244142\n",
      "iteration 800 / 2000: loss 0.253842\n",
      "iteration 900 / 2000: loss 0.219111\n",
      "iteration 1000 / 2000: loss 0.223272\n",
      "iteration 1100 / 2000: loss 0.255355\n",
      "iteration 1200 / 2000: loss 0.225779\n",
      "iteration 1300 / 2000: loss 0.202889\n",
      "iteration 1400 / 2000: loss 0.252942\n",
      "iteration 1500 / 2000: loss 0.175436\n",
      "iteration 1600 / 2000: loss 0.222359\n",
      "iteration 1700 / 2000: loss 0.184153\n",
      "iteration 1800 / 2000: loss 0.202920\n",
      "iteration 1900 / 2000: loss 0.177296\n",
      "Hidden Size: 80, Learning Rate: 0.01, LR Decay: 0.47250000000000003, Regularization: 0.4, Validation accuracy: 0.967\n",
      "iteration 0 / 2000: loss 2.302576\n",
      "iteration 100 / 2000: loss 0.338680\n",
      "iteration 200 / 2000: loss 0.305337\n",
      "iteration 300 / 2000: loss 0.194804\n",
      "iteration 400 / 2000: loss 0.185272\n",
      "iteration 500 / 2000: loss 0.163912\n",
      "iteration 600 / 2000: loss 0.145035\n",
      "iteration 700 / 2000: loss 0.136369\n",
      "iteration 800 / 2000: loss 0.141385\n",
      "iteration 900 / 2000: loss 0.099586\n",
      "iteration 1000 / 2000: loss 0.134314\n",
      "iteration 1100 / 2000: loss 0.139219\n",
      "iteration 1200 / 2000: loss 0.193316\n",
      "iteration 1300 / 2000: loss 0.127235\n",
      "iteration 1400 / 2000: loss 0.137230\n",
      "iteration 1500 / 2000: loss 0.113545\n",
      "iteration 1600 / 2000: loss 0.177999\n",
      "iteration 1700 / 2000: loss 0.123379\n",
      "iteration 1800 / 2000: loss 0.161312\n",
      "iteration 1900 / 2000: loss 0.133182\n",
      "Hidden Size: 80, Learning Rate: 0.01, LR Decay: 0.3, Regularization: 0.0, Validation accuracy: 0.9548\n",
      "iteration 0 / 2000: loss 2.302608\n",
      "iteration 100 / 2000: loss 0.287038\n",
      "iteration 200 / 2000: loss 0.270940\n",
      "iteration 300 / 2000: loss 0.204838\n",
      "iteration 400 / 2000: loss 0.190879\n",
      "iteration 500 / 2000: loss 0.169270\n",
      "iteration 600 / 2000: loss 0.218351\n",
      "iteration 700 / 2000: loss 0.208665\n",
      "iteration 800 / 2000: loss 0.144420\n",
      "iteration 900 / 2000: loss 0.153512\n",
      "iteration 1000 / 2000: loss 0.175843\n",
      "iteration 1100 / 2000: loss 0.215370\n",
      "iteration 1200 / 2000: loss 0.223694\n",
      "iteration 1300 / 2000: loss 0.217569\n",
      "iteration 1400 / 2000: loss 0.180489\n",
      "iteration 1500 / 2000: loss 0.186299\n",
      "iteration 1600 / 2000: loss 0.144084\n",
      "iteration 1700 / 2000: loss 0.174015\n",
      "iteration 1800 / 2000: loss 0.305930\n",
      "iteration 1900 / 2000: loss 0.248761\n",
      "Hidden Size: 80, Learning Rate: 0.01, LR Decay: 0.3, Regularization: 0.1, Validation accuracy: 0.9558\n",
      "iteration 0 / 2000: loss 2.302657\n",
      "iteration 100 / 2000: loss 0.355716\n",
      "iteration 200 / 2000: loss 0.209647\n",
      "iteration 300 / 2000: loss 0.318322\n",
      "iteration 400 / 2000: loss 0.175206\n",
      "iteration 500 / 2000: loss 0.167673\n",
      "iteration 600 / 2000: loss 0.247214\n",
      "iteration 700 / 2000: loss 0.169133\n",
      "iteration 800 / 2000: loss 0.154222\n",
      "iteration 900 / 2000: loss 0.282174\n",
      "iteration 1000 / 2000: loss 0.207433\n",
      "iteration 1100 / 2000: loss 0.168121\n",
      "iteration 1200 / 2000: loss 0.274961\n",
      "iteration 1300 / 2000: loss 0.174913\n",
      "iteration 1400 / 2000: loss 0.226589\n",
      "iteration 1500 / 2000: loss 0.200480\n",
      "iteration 1600 / 2000: loss 0.249081\n",
      "iteration 1700 / 2000: loss 0.286803\n",
      "iteration 1800 / 2000: loss 0.187753\n",
      "iteration 1900 / 2000: loss 0.123766\n",
      "Hidden Size: 80, Learning Rate: 0.01, LR Decay: 0.3, Regularization: 0.2, Validation accuracy: 0.955\n",
      "iteration 0 / 2000: loss 2.302676\n",
      "iteration 100 / 2000: loss 0.358113\n",
      "iteration 200 / 2000: loss 0.234069\n",
      "iteration 300 / 2000: loss 0.226477\n",
      "iteration 400 / 2000: loss 0.301718\n",
      "iteration 500 / 2000: loss 0.207515\n",
      "iteration 600 / 2000: loss 0.296873\n",
      "iteration 700 / 2000: loss 0.223521\n",
      "iteration 800 / 2000: loss 0.199629\n",
      "iteration 900 / 2000: loss 0.252171\n",
      "iteration 1000 / 2000: loss 0.185397\n",
      "iteration 1100 / 2000: loss 0.222981\n",
      "iteration 1200 / 2000: loss 0.236065\n",
      "iteration 1300 / 2000: loss 0.214115\n",
      "iteration 1400 / 2000: loss 0.212210\n",
      "iteration 1500 / 2000: loss 0.176938\n",
      "iteration 1600 / 2000: loss 0.283273\n",
      "iteration 1700 / 2000: loss 0.202926\n",
      "iteration 1800 / 2000: loss 0.223267\n",
      "iteration 1900 / 2000: loss 0.216749\n",
      "Hidden Size: 80, Learning Rate: 0.01, LR Decay: 0.3, Regularization: 0.30000000000000004, Validation accuracy: 0.9546\n",
      "iteration 0 / 2000: loss 2.302711\n",
      "iteration 100 / 2000: loss 0.400770\n",
      "iteration 200 / 2000: loss 0.263784\n",
      "iteration 300 / 2000: loss 0.279495\n",
      "iteration 400 / 2000: loss 0.244472\n",
      "iteration 500 / 2000: loss 0.292633\n",
      "iteration 600 / 2000: loss 0.259390\n",
      "iteration 700 / 2000: loss 0.268951\n",
      "iteration 800 / 2000: loss 0.198339\n",
      "iteration 900 / 2000: loss 0.205670\n",
      "iteration 1000 / 2000: loss 0.277677\n",
      "iteration 1100 / 2000: loss 0.206846\n",
      "iteration 1200 / 2000: loss 0.197303\n",
      "iteration 1300 / 2000: loss 0.200265\n",
      "iteration 1400 / 2000: loss 0.326561\n",
      "iteration 1500 / 2000: loss 0.263610\n",
      "iteration 1600 / 2000: loss 0.277058\n",
      "iteration 1700 / 2000: loss 0.210452\n",
      "iteration 1800 / 2000: loss 0.202871\n",
      "iteration 1900 / 2000: loss 0.244446\n",
      "Hidden Size: 80, Learning Rate: 0.01, LR Decay: 0.3, Regularization: 0.4, Validation accuracy: 0.9544\n",
      "iteration 0 / 2000: loss 2.302586\n",
      "iteration 100 / 2000: loss 0.120537\n",
      "iteration 200 / 2000: loss 0.124207\n",
      "iteration 300 / 2000: loss 0.147462\n",
      "iteration 400 / 2000: loss 0.114478\n",
      "iteration 500 / 2000: loss 0.058254\n",
      "iteration 600 / 2000: loss 0.085839\n",
      "iteration 700 / 2000: loss 0.101959\n",
      "iteration 800 / 2000: loss 0.022438\n",
      "iteration 900 / 2000: loss 0.066810\n",
      "iteration 1000 / 2000: loss 0.098017\n",
      "iteration 1100 / 2000: loss 0.036946\n",
      "iteration 1200 / 2000: loss 0.132772\n",
      "iteration 1300 / 2000: loss 0.031669\n",
      "iteration 1400 / 2000: loss 0.053283\n",
      "iteration 1500 / 2000: loss 0.015468\n",
      "iteration 1600 / 2000: loss 0.018620\n",
      "iteration 1700 / 2000: loss 0.052813\n",
      "iteration 1800 / 2000: loss 0.019823\n",
      "iteration 1900 / 2000: loss 0.031267\n",
      "Hidden Size: 80, Learning Rate: 0.0075000250000000004, LR Decay: 0.99, Regularization: 0.0, Validation accuracy: 0.976\n",
      "iteration 0 / 2000: loss 2.302631\n",
      "iteration 100 / 2000: loss 0.297684\n",
      "iteration 200 / 2000: loss 0.156329\n",
      "iteration 300 / 2000: loss 0.261927\n",
      "iteration 400 / 2000: loss 0.124943\n",
      "iteration 500 / 2000: loss 0.198492\n",
      "iteration 600 / 2000: loss 0.145580\n",
      "iteration 700 / 2000: loss 0.114313\n",
      "iteration 800 / 2000: loss 0.182449\n",
      "iteration 900 / 2000: loss 0.136216\n",
      "iteration 1000 / 2000: loss 0.112189\n",
      "iteration 1100 / 2000: loss 0.142986\n",
      "iteration 1200 / 2000: loss 0.098093\n",
      "iteration 1300 / 2000: loss 0.113087\n",
      "iteration 1400 / 2000: loss 0.124374\n",
      "iteration 1500 / 2000: loss 0.118729\n",
      "iteration 1600 / 2000: loss 0.095413\n",
      "iteration 1700 / 2000: loss 0.100300\n",
      "iteration 1800 / 2000: loss 0.133325\n",
      "iteration 1900 / 2000: loss 0.146848\n",
      "Hidden Size: 80, Learning Rate: 0.0075000250000000004, LR Decay: 0.99, Regularization: 0.1, Validation accuracy: 0.9756\n",
      "iteration 0 / 2000: loss 2.302620\n",
      "iteration 100 / 2000: loss 0.230105\n",
      "iteration 200 / 2000: loss 0.303100\n",
      "iteration 300 / 2000: loss 0.221355\n",
      "iteration 400 / 2000: loss 0.186392\n",
      "iteration 500 / 2000: loss 0.159114\n",
      "iteration 600 / 2000: loss 0.182537\n",
      "iteration 700 / 2000: loss 0.163966\n",
      "iteration 800 / 2000: loss 0.171686\n",
      "iteration 900 / 2000: loss 0.152556\n",
      "iteration 1000 / 2000: loss 0.179359\n",
      "iteration 1100 / 2000: loss 0.136716\n",
      "iteration 1200 / 2000: loss 0.236816\n",
      "iteration 1300 / 2000: loss 0.150984\n",
      "iteration 1400 / 2000: loss 0.138813\n",
      "iteration 1500 / 2000: loss 0.162042\n",
      "iteration 1600 / 2000: loss 0.138775\n",
      "iteration 1700 / 2000: loss 0.156425\n",
      "iteration 1800 / 2000: loss 0.175623\n",
      "iteration 1900 / 2000: loss 0.149906\n",
      "Hidden Size: 80, Learning Rate: 0.0075000250000000004, LR Decay: 0.99, Regularization: 0.2, Validation accuracy: 0.9766\n",
      "iteration 0 / 2000: loss 2.302678\n",
      "iteration 100 / 2000: loss 0.345285\n",
      "iteration 200 / 2000: loss 0.231121\n",
      "iteration 300 / 2000: loss 0.193616\n",
      "iteration 400 / 2000: loss 0.229399\n",
      "iteration 500 / 2000: loss 0.201947\n",
      "iteration 600 / 2000: loss 0.183425\n",
      "iteration 700 / 2000: loss 0.219002\n",
      "iteration 800 / 2000: loss 0.209397\n",
      "iteration 900 / 2000: loss 0.213204\n",
      "iteration 1000 / 2000: loss 0.344683\n",
      "iteration 1100 / 2000: loss 0.223619\n",
      "iteration 1200 / 2000: loss 0.172497\n",
      "iteration 1300 / 2000: loss 0.196405\n",
      "iteration 1400 / 2000: loss 0.220169\n",
      "iteration 1500 / 2000: loss 0.161535\n",
      "iteration 1600 / 2000: loss 0.191810\n",
      "iteration 1700 / 2000: loss 0.232293\n",
      "iteration 1800 / 2000: loss 0.151101\n",
      "iteration 1900 / 2000: loss 0.177631\n",
      "Hidden Size: 80, Learning Rate: 0.0075000250000000004, LR Decay: 0.99, Regularization: 0.30000000000000004, Validation accuracy: 0.9734\n",
      "iteration 0 / 2000: loss 2.302723\n",
      "iteration 100 / 2000: loss 0.255776\n",
      "iteration 200 / 2000: loss 0.355260\n",
      "iteration 300 / 2000: loss 0.252263\n",
      "iteration 400 / 2000: loss 0.196718\n",
      "iteration 500 / 2000: loss 0.196420\n",
      "iteration 600 / 2000: loss 0.211781\n",
      "iteration 700 / 2000: loss 0.263887\n",
      "iteration 800 / 2000: loss 0.249642\n",
      "iteration 900 / 2000: loss 0.200580\n",
      "iteration 1000 / 2000: loss 0.234194\n",
      "iteration 1100 / 2000: loss 0.181939\n",
      "iteration 1200 / 2000: loss 0.171026\n",
      "iteration 1300 / 2000: loss 0.182476\n",
      "iteration 1400 / 2000: loss 0.265619\n",
      "iteration 1500 / 2000: loss 0.202519\n",
      "iteration 1600 / 2000: loss 0.223337\n",
      "iteration 1700 / 2000: loss 0.201135\n",
      "iteration 1800 / 2000: loss 0.271020\n",
      "iteration 1900 / 2000: loss 0.252253\n",
      "Hidden Size: 80, Learning Rate: 0.0075000250000000004, LR Decay: 0.99, Regularization: 0.4, Validation accuracy: 0.9682\n",
      "iteration 0 / 2000: loss 2.302604\n",
      "iteration 100 / 2000: loss 0.207864\n",
      "iteration 200 / 2000: loss 0.173983\n",
      "iteration 300 / 2000: loss 0.167626\n",
      "iteration 400 / 2000: loss 0.053192\n",
      "iteration 500 / 2000: loss 0.097935\n",
      "iteration 600 / 2000: loss 0.114472\n",
      "iteration 700 / 2000: loss 0.071615\n",
      "iteration 800 / 2000: loss 0.042372\n",
      "iteration 900 / 2000: loss 0.043549\n",
      "iteration 1000 / 2000: loss 0.072623\n",
      "iteration 1100 / 2000: loss 0.021953\n",
      "iteration 1200 / 2000: loss 0.025972\n",
      "iteration 1300 / 2000: loss 0.029668\n",
      "iteration 1400 / 2000: loss 0.029101\n",
      "iteration 1500 / 2000: loss 0.068107\n",
      "iteration 1600 / 2000: loss 0.055656\n",
      "iteration 1700 / 2000: loss 0.058745\n",
      "iteration 1800 / 2000: loss 0.032468\n",
      "iteration 1900 / 2000: loss 0.033856\n",
      "Hidden Size: 80, Learning Rate: 0.0075000250000000004, LR Decay: 0.8175, Regularization: 0.0, Validation accuracy: 0.976\n",
      "iteration 0 / 2000: loss 2.302622\n",
      "iteration 100 / 2000: loss 0.270027\n",
      "iteration 200 / 2000: loss 0.205994\n",
      "iteration 300 / 2000: loss 0.193561\n",
      "iteration 400 / 2000: loss 0.172996\n",
      "iteration 500 / 2000: loss 0.127800\n",
      "iteration 600 / 2000: loss 0.156568\n",
      "iteration 700 / 2000: loss 0.137044\n",
      "iteration 800 / 2000: loss 0.102586\n",
      "iteration 900 / 2000: loss 0.113772\n",
      "iteration 1000 / 2000: loss 0.144402\n",
      "iteration 1100 / 2000: loss 0.109959\n",
      "iteration 1200 / 2000: loss 0.128898\n",
      "iteration 1300 / 2000: loss 0.081375\n",
      "iteration 1400 / 2000: loss 0.105594\n",
      "iteration 1500 / 2000: loss 0.094485\n",
      "iteration 1600 / 2000: loss 0.084828\n",
      "iteration 1700 / 2000: loss 0.109028\n",
      "iteration 1800 / 2000: loss 0.112988\n",
      "iteration 1900 / 2000: loss 0.080157\n",
      "Hidden Size: 80, Learning Rate: 0.0075000250000000004, LR Decay: 0.8175, Regularization: 0.1, Validation accuracy: 0.9766\n",
      "iteration 0 / 2000: loss 2.302646\n",
      "iteration 100 / 2000: loss 0.262547\n",
      "iteration 200 / 2000: loss 0.230497\n",
      "iteration 300 / 2000: loss 0.189802\n",
      "iteration 400 / 2000: loss 0.222013\n",
      "iteration 500 / 2000: loss 0.157159\n",
      "iteration 600 / 2000: loss 0.215165\n",
      "iteration 700 / 2000: loss 0.162318\n",
      "iteration 800 / 2000: loss 0.124927\n",
      "iteration 900 / 2000: loss 0.142758\n",
      "iteration 1000 / 2000: loss 0.143828\n",
      "iteration 1100 / 2000: loss 0.142209\n",
      "iteration 1200 / 2000: loss 0.137343\n",
      "iteration 1300 / 2000: loss 0.180392\n",
      "iteration 1400 / 2000: loss 0.151597\n",
      "iteration 1500 / 2000: loss 0.113307\n",
      "iteration 1600 / 2000: loss 0.148415\n",
      "iteration 1700 / 2000: loss 0.135748\n",
      "iteration 1800 / 2000: loss 0.135899\n",
      "iteration 1900 / 2000: loss 0.163684\n",
      "Hidden Size: 80, Learning Rate: 0.0075000250000000004, LR Decay: 0.8175, Regularization: 0.2, Validation accuracy: 0.9754\n",
      "iteration 0 / 2000: loss 2.302683\n",
      "iteration 100 / 2000: loss 0.382994\n",
      "iteration 200 / 2000: loss 0.230446\n",
      "iteration 300 / 2000: loss 0.277830\n",
      "iteration 400 / 2000: loss 0.240959\n",
      "iteration 500 / 2000: loss 0.148113\n",
      "iteration 600 / 2000: loss 0.178441\n",
      "iteration 700 / 2000: loss 0.236254\n",
      "iteration 800 / 2000: loss 0.214705\n",
      "iteration 900 / 2000: loss 0.185686\n",
      "iteration 1000 / 2000: loss 0.144858\n",
      "iteration 1100 / 2000: loss 0.158420\n",
      "iteration 1200 / 2000: loss 0.183796\n",
      "iteration 1300 / 2000: loss 0.224058\n",
      "iteration 1400 / 2000: loss 0.171052\n",
      "iteration 1500 / 2000: loss 0.171362\n",
      "iteration 1600 / 2000: loss 0.162757\n",
      "iteration 1700 / 2000: loss 0.171072\n",
      "iteration 1800 / 2000: loss 0.173790\n",
      "iteration 1900 / 2000: loss 0.168356\n",
      "Hidden Size: 80, Learning Rate: 0.0075000250000000004, LR Decay: 0.8175, Regularization: 0.30000000000000004, Validation accuracy: 0.9726\n",
      "iteration 0 / 2000: loss 2.302709\n",
      "iteration 100 / 2000: loss 0.366415\n",
      "iteration 200 / 2000: loss 0.305791\n",
      "iteration 300 / 2000: loss 0.339413\n",
      "iteration 400 / 2000: loss 0.216484\n",
      "iteration 500 / 2000: loss 0.272534\n",
      "iteration 600 / 2000: loss 0.326993\n",
      "iteration 700 / 2000: loss 0.207248\n",
      "iteration 800 / 2000: loss 0.234973\n",
      "iteration 900 / 2000: loss 0.181039\n",
      "iteration 1000 / 2000: loss 0.223197\n",
      "iteration 1100 / 2000: loss 0.241527\n",
      "iteration 1200 / 2000: loss 0.221261\n",
      "iteration 1300 / 2000: loss 0.209684\n",
      "iteration 1400 / 2000: loss 0.221413\n",
      "iteration 1500 / 2000: loss 0.261699\n",
      "iteration 1600 / 2000: loss 0.216193\n",
      "iteration 1700 / 2000: loss 0.184258\n",
      "iteration 1800 / 2000: loss 0.194506\n",
      "iteration 1900 / 2000: loss 0.252027\n",
      "Hidden Size: 80, Learning Rate: 0.0075000250000000004, LR Decay: 0.8175, Regularization: 0.4, Validation accuracy: 0.9732\n",
      "iteration 0 / 2000: loss 2.302602\n",
      "iteration 100 / 2000: loss 0.221473\n",
      "iteration 200 / 2000: loss 0.145299\n",
      "iteration 300 / 2000: loss 0.146899\n",
      "iteration 400 / 2000: loss 0.171010\n",
      "iteration 500 / 2000: loss 0.128092\n",
      "iteration 600 / 2000: loss 0.094527\n",
      "iteration 700 / 2000: loss 0.092724\n",
      "iteration 800 / 2000: loss 0.122115\n",
      "iteration 900 / 2000: loss 0.085895\n",
      "iteration 1000 / 2000: loss 0.108297\n",
      "iteration 1100 / 2000: loss 0.056617\n",
      "iteration 1200 / 2000: loss 0.081313\n",
      "iteration 1300 / 2000: loss 0.057512\n",
      "iteration 1400 / 2000: loss 0.064448\n",
      "iteration 1500 / 2000: loss 0.062077\n",
      "iteration 1600 / 2000: loss 0.084539\n",
      "iteration 1700 / 2000: loss 0.090401\n",
      "iteration 1800 / 2000: loss 0.088663\n",
      "iteration 1900 / 2000: loss 0.056897\n",
      "Hidden Size: 80, Learning Rate: 0.0075000250000000004, LR Decay: 0.645, Regularization: 0.0, Validation accuracy: 0.974\n",
      "iteration 0 / 2000: loss 2.302607\n",
      "iteration 100 / 2000: loss 0.282685\n",
      "iteration 200 / 2000: loss 0.196141\n",
      "iteration 300 / 2000: loss 0.252348\n",
      "iteration 400 / 2000: loss 0.168471\n",
      "iteration 500 / 2000: loss 0.161339\n",
      "iteration 600 / 2000: loss 0.204938\n",
      "iteration 700 / 2000: loss 0.176932\n",
      "iteration 800 / 2000: loss 0.079840\n",
      "iteration 900 / 2000: loss 0.128453\n",
      "iteration 1000 / 2000: loss 0.143823\n",
      "iteration 1100 / 2000: loss 0.153526\n",
      "iteration 1200 / 2000: loss 0.139033\n",
      "iteration 1300 / 2000: loss 0.143573\n",
      "iteration 1400 / 2000: loss 0.139544\n",
      "iteration 1500 / 2000: loss 0.074580\n",
      "iteration 1600 / 2000: loss 0.092938\n",
      "iteration 1700 / 2000: loss 0.121532\n",
      "iteration 1800 / 2000: loss 0.108430\n",
      "iteration 1900 / 2000: loss 0.122662\n",
      "Hidden Size: 80, Learning Rate: 0.0075000250000000004, LR Decay: 0.645, Regularization: 0.1, Validation accuracy: 0.9724\n",
      "iteration 0 / 2000: loss 2.302682\n",
      "iteration 100 / 2000: loss 0.352446\n",
      "iteration 200 / 2000: loss 0.241471\n",
      "iteration 300 / 2000: loss 0.157523\n",
      "iteration 400 / 2000: loss 0.225261\n",
      "iteration 500 / 2000: loss 0.172103\n",
      "iteration 600 / 2000: loss 0.156803\n",
      "iteration 700 / 2000: loss 0.155121\n",
      "iteration 800 / 2000: loss 0.191283\n",
      "iteration 900 / 2000: loss 0.229414\n",
      "iteration 1000 / 2000: loss 0.136783\n",
      "iteration 1100 / 2000: loss 0.185681\n",
      "iteration 1200 / 2000: loss 0.125224\n",
      "iteration 1300 / 2000: loss 0.214094\n",
      "iteration 1400 / 2000: loss 0.142084\n",
      "iteration 1500 / 2000: loss 0.119264\n",
      "iteration 1600 / 2000: loss 0.151394\n",
      "iteration 1700 / 2000: loss 0.178165\n",
      "iteration 1800 / 2000: loss 0.180322\n",
      "iteration 1900 / 2000: loss 0.148431\n",
      "Hidden Size: 80, Learning Rate: 0.0075000250000000004, LR Decay: 0.645, Regularization: 0.2, Validation accuracy: 0.9684\n",
      "iteration 0 / 2000: loss 2.302690\n",
      "iteration 100 / 2000: loss 0.361835\n",
      "iteration 200 / 2000: loss 0.260138\n",
      "iteration 300 / 2000: loss 0.240984\n",
      "iteration 400 / 2000: loss 0.191078\n",
      "iteration 500 / 2000: loss 0.144791\n",
      "iteration 600 / 2000: loss 0.179616\n",
      "iteration 700 / 2000: loss 0.169017\n",
      "iteration 800 / 2000: loss 0.263258\n",
      "iteration 900 / 2000: loss 0.189954\n",
      "iteration 1000 / 2000: loss 0.212538\n",
      "iteration 1100 / 2000: loss 0.235711\n",
      "iteration 1200 / 2000: loss 0.137082\n",
      "iteration 1300 / 2000: loss 0.212224\n",
      "iteration 1400 / 2000: loss 0.148622\n",
      "iteration 1500 / 2000: loss 0.210175\n",
      "iteration 1600 / 2000: loss 0.191956\n",
      "iteration 1700 / 2000: loss 0.176270\n",
      "iteration 1800 / 2000: loss 0.199836\n",
      "iteration 1900 / 2000: loss 0.201282\n",
      "Hidden Size: 80, Learning Rate: 0.0075000250000000004, LR Decay: 0.645, Regularization: 0.30000000000000004, Validation accuracy: 0.9708\n",
      "iteration 0 / 2000: loss 2.302710\n",
      "iteration 100 / 2000: loss 0.258905\n",
      "iteration 200 / 2000: loss 0.196060\n",
      "iteration 300 / 2000: loss 0.261613\n",
      "iteration 400 / 2000: loss 0.249715\n",
      "iteration 500 / 2000: loss 0.267042\n",
      "iteration 600 / 2000: loss 0.262195\n",
      "iteration 700 / 2000: loss 0.281279\n",
      "iteration 800 / 2000: loss 0.236109\n",
      "iteration 900 / 2000: loss 0.257330\n",
      "iteration 1000 / 2000: loss 0.169441\n",
      "iteration 1100 / 2000: loss 0.251258\n",
      "iteration 1200 / 2000: loss 0.220693\n",
      "iteration 1300 / 2000: loss 0.285621\n",
      "iteration 1400 / 2000: loss 0.263316\n",
      "iteration 1500 / 2000: loss 0.220262\n",
      "iteration 1600 / 2000: loss 0.186560\n",
      "iteration 1700 / 2000: loss 0.200411\n",
      "iteration 1800 / 2000: loss 0.228156\n",
      "iteration 1900 / 2000: loss 0.278081\n",
      "Hidden Size: 80, Learning Rate: 0.0075000250000000004, LR Decay: 0.645, Regularization: 0.4, Validation accuracy: 0.9674\n",
      "iteration 0 / 2000: loss 2.302607\n",
      "iteration 100 / 2000: loss 0.329620\n",
      "iteration 200 / 2000: loss 0.264273\n",
      "iteration 300 / 2000: loss 0.184258\n",
      "iteration 400 / 2000: loss 0.173384\n",
      "iteration 500 / 2000: loss 0.165140\n",
      "iteration 600 / 2000: loss 0.091667\n",
      "iteration 700 / 2000: loss 0.100974\n",
      "iteration 800 / 2000: loss 0.181211\n",
      "iteration 900 / 2000: loss 0.146868\n",
      "iteration 1000 / 2000: loss 0.149257\n",
      "iteration 1100 / 2000: loss 0.102942\n",
      "iteration 1200 / 2000: loss 0.214813\n",
      "iteration 1300 / 2000: loss 0.068354\n",
      "iteration 1400 / 2000: loss 0.063200\n",
      "iteration 1500 / 2000: loss 0.105035\n",
      "iteration 1600 / 2000: loss 0.085117\n",
      "iteration 1700 / 2000: loss 0.152804\n",
      "iteration 1800 / 2000: loss 0.058786\n",
      "iteration 1900 / 2000: loss 0.129869\n",
      "Hidden Size: 80, Learning Rate: 0.0075000250000000004, LR Decay: 0.47250000000000003, Regularization: 0.0, Validation accuracy: 0.967\n",
      "iteration 0 / 2000: loss 2.302633\n",
      "iteration 100 / 2000: loss 0.320696\n",
      "iteration 200 / 2000: loss 0.203819\n",
      "iteration 300 / 2000: loss 0.258653\n",
      "iteration 400 / 2000: loss 0.200037\n",
      "iteration 500 / 2000: loss 0.160578\n",
      "iteration 600 / 2000: loss 0.121938\n",
      "iteration 700 / 2000: loss 0.154990\n",
      "iteration 800 / 2000: loss 0.124931\n",
      "iteration 900 / 2000: loss 0.152517\n",
      "iteration 1000 / 2000: loss 0.129464\n",
      "iteration 1100 / 2000: loss 0.171238\n",
      "iteration 1200 / 2000: loss 0.209302\n",
      "iteration 1300 / 2000: loss 0.141454\n",
      "iteration 1400 / 2000: loss 0.226059\n",
      "iteration 1500 / 2000: loss 0.103745\n",
      "iteration 1600 / 2000: loss 0.224705\n",
      "iteration 1700 / 2000: loss 0.208649\n",
      "iteration 1800 / 2000: loss 0.135080\n",
      "iteration 1900 / 2000: loss 0.194489\n",
      "Hidden Size: 80, Learning Rate: 0.0075000250000000004, LR Decay: 0.47250000000000003, Regularization: 0.1, Validation accuracy: 0.966\n",
      "iteration 0 / 2000: loss 2.302651\n",
      "iteration 100 / 2000: loss 0.354729\n",
      "iteration 200 / 2000: loss 0.253295\n",
      "iteration 300 / 2000: loss 0.234313\n",
      "iteration 400 / 2000: loss 0.202813\n",
      "iteration 500 / 2000: loss 0.203728\n",
      "iteration 600 / 2000: loss 0.214250\n",
      "iteration 700 / 2000: loss 0.225425\n",
      "iteration 800 / 2000: loss 0.220479\n",
      "iteration 900 / 2000: loss 0.166210\n",
      "iteration 1000 / 2000: loss 0.160324\n",
      "iteration 1100 / 2000: loss 0.174045\n",
      "iteration 1200 / 2000: loss 0.218772\n",
      "iteration 1300 / 2000: loss 0.186124\n",
      "iteration 1400 / 2000: loss 0.158056\n",
      "iteration 1500 / 2000: loss 0.154660\n",
      "iteration 1600 / 2000: loss 0.189410\n",
      "iteration 1700 / 2000: loss 0.158800\n",
      "iteration 1800 / 2000: loss 0.125305\n",
      "iteration 1900 / 2000: loss 0.236476\n",
      "Hidden Size: 80, Learning Rate: 0.0075000250000000004, LR Decay: 0.47250000000000003, Regularization: 0.2, Validation accuracy: 0.9648\n",
      "iteration 0 / 2000: loss 2.302705\n",
      "iteration 100 / 2000: loss 0.300834\n",
      "iteration 200 / 2000: loss 0.297184\n",
      "iteration 300 / 2000: loss 0.239209\n",
      "iteration 400 / 2000: loss 0.249401\n",
      "iteration 500 / 2000: loss 0.238070\n",
      "iteration 600 / 2000: loss 0.224671\n",
      "iteration 700 / 2000: loss 0.197861\n",
      "iteration 800 / 2000: loss 0.209070\n",
      "iteration 900 / 2000: loss 0.261650\n",
      "iteration 1000 / 2000: loss 0.196814\n",
      "iteration 1100 / 2000: loss 0.210831\n",
      "iteration 1200 / 2000: loss 0.237109\n",
      "iteration 1300 / 2000: loss 0.246911\n",
      "iteration 1400 / 2000: loss 0.207312\n",
      "iteration 1500 / 2000: loss 0.190680\n",
      "iteration 1600 / 2000: loss 0.254297\n",
      "iteration 1700 / 2000: loss 0.195908\n",
      "iteration 1800 / 2000: loss 0.227492\n",
      "iteration 1900 / 2000: loss 0.197629\n",
      "Hidden Size: 80, Learning Rate: 0.0075000250000000004, LR Decay: 0.47250000000000003, Regularization: 0.30000000000000004, Validation accuracy: 0.9608\n",
      "iteration 0 / 2000: loss 2.302703\n",
      "iteration 100 / 2000: loss 0.377631\n",
      "iteration 200 / 2000: loss 0.337129\n",
      "iteration 300 / 2000: loss 0.357512\n",
      "iteration 400 / 2000: loss 0.210054\n",
      "iteration 500 / 2000: loss 0.252887\n",
      "iteration 600 / 2000: loss 0.223240\n",
      "iteration 700 / 2000: loss 0.311977\n",
      "iteration 800 / 2000: loss 0.270264\n",
      "iteration 900 / 2000: loss 0.232353\n",
      "iteration 1000 / 2000: loss 0.308427\n",
      "iteration 1100 / 2000: loss 0.230805\n",
      "iteration 1200 / 2000: loss 0.184580\n",
      "iteration 1300 / 2000: loss 0.194866\n",
      "iteration 1400 / 2000: loss 0.191929\n",
      "iteration 1500 / 2000: loss 0.215365\n",
      "iteration 1600 / 2000: loss 0.209685\n",
      "iteration 1700 / 2000: loss 0.266264\n",
      "iteration 1800 / 2000: loss 0.172101\n",
      "iteration 1900 / 2000: loss 0.229590\n",
      "Hidden Size: 80, Learning Rate: 0.0075000250000000004, LR Decay: 0.47250000000000003, Regularization: 0.4, Validation accuracy: 0.9606\n",
      "iteration 0 / 2000: loss 2.302600\n",
      "iteration 100 / 2000: loss 0.551412\n",
      "iteration 200 / 2000: loss 0.224085\n",
      "iteration 300 / 2000: loss 0.177640\n",
      "iteration 400 / 2000: loss 0.169979\n",
      "iteration 500 / 2000: loss 0.177676\n",
      "iteration 600 / 2000: loss 0.231721\n",
      "iteration 700 / 2000: loss 0.207894\n",
      "iteration 800 / 2000: loss 0.138191\n",
      "iteration 900 / 2000: loss 0.192941\n",
      "iteration 1000 / 2000: loss 0.167941\n",
      "iteration 1100 / 2000: loss 0.157165\n",
      "iteration 1200 / 2000: loss 0.242427\n",
      "iteration 1300 / 2000: loss 0.144776\n",
      "iteration 1400 / 2000: loss 0.172604\n",
      "iteration 1500 / 2000: loss 0.207096\n",
      "iteration 1600 / 2000: loss 0.154670\n",
      "iteration 1700 / 2000: loss 0.228290\n",
      "iteration 1800 / 2000: loss 0.214522\n",
      "iteration 1900 / 2000: loss 0.200161\n",
      "Hidden Size: 80, Learning Rate: 0.0075000250000000004, LR Decay: 0.3, Regularization: 0.0, Validation accuracy: 0.9478\n",
      "iteration 0 / 2000: loss 2.302620\n",
      "iteration 100 / 2000: loss 0.435717\n",
      "iteration 200 / 2000: loss 0.225471\n",
      "iteration 300 / 2000: loss 0.180402\n",
      "iteration 400 / 2000: loss 0.216358\n",
      "iteration 500 / 2000: loss 0.137715\n",
      "iteration 600 / 2000: loss 0.231844\n",
      "iteration 700 / 2000: loss 0.242372\n",
      "iteration 800 / 2000: loss 0.212732\n",
      "iteration 900 / 2000: loss 0.210883\n",
      "iteration 1000 / 2000: loss 0.162043\n",
      "iteration 1100 / 2000: loss 0.267373\n",
      "iteration 1200 / 2000: loss 0.180246\n",
      "iteration 1300 / 2000: loss 0.221124\n",
      "iteration 1400 / 2000: loss 0.250681\n",
      "iteration 1500 / 2000: loss 0.271931\n",
      "iteration 1600 / 2000: loss 0.222643\n",
      "iteration 1700 / 2000: loss 0.264059\n",
      "iteration 1800 / 2000: loss 0.177413\n",
      "iteration 1900 / 2000: loss 0.285946\n",
      "Hidden Size: 80, Learning Rate: 0.0075000250000000004, LR Decay: 0.3, Regularization: 0.1, Validation accuracy: 0.947\n",
      "iteration 0 / 2000: loss 2.302644\n",
      "iteration 100 / 2000: loss 0.377708\n",
      "iteration 200 / 2000: loss 0.342379\n",
      "iteration 300 / 2000: loss 0.196541\n",
      "iteration 400 / 2000: loss 0.211183\n",
      "iteration 500 / 2000: loss 0.219024\n",
      "iteration 600 / 2000: loss 0.276962\n",
      "iteration 700 / 2000: loss 0.219431\n",
      "iteration 800 / 2000: loss 0.228194\n",
      "iteration 900 / 2000: loss 0.218144\n",
      "iteration 1000 / 2000: loss 0.215352\n",
      "iteration 1100 / 2000: loss 0.242904\n",
      "iteration 1200 / 2000: loss 0.296121\n",
      "iteration 1300 / 2000: loss 0.168376\n",
      "iteration 1400 / 2000: loss 0.250950\n",
      "iteration 1500 / 2000: loss 0.186044\n",
      "iteration 1600 / 2000: loss 0.205821\n",
      "iteration 1700 / 2000: loss 0.231598\n",
      "iteration 1800 / 2000: loss 0.188180\n",
      "iteration 1900 / 2000: loss 0.168480\n",
      "Hidden Size: 80, Learning Rate: 0.0075000250000000004, LR Decay: 0.3, Regularization: 0.2, Validation accuracy: 0.9496\n",
      "iteration 0 / 2000: loss 2.302659\n",
      "iteration 100 / 2000: loss 0.449020\n",
      "iteration 200 / 2000: loss 0.357270\n",
      "iteration 300 / 2000: loss 0.316565\n",
      "iteration 400 / 2000: loss 0.321700\n",
      "iteration 500 / 2000: loss 0.188796\n",
      "iteration 600 / 2000: loss 0.232253\n",
      "iteration 700 / 2000: loss 0.206483\n",
      "iteration 800 / 2000: loss 0.264804\n",
      "iteration 900 / 2000: loss 0.240388\n",
      "iteration 1000 / 2000: loss 0.226549\n",
      "iteration 1100 / 2000: loss 0.224959\n",
      "iteration 1200 / 2000: loss 0.242619\n",
      "iteration 1300 / 2000: loss 0.304213\n",
      "iteration 1400 / 2000: loss 0.212537\n",
      "iteration 1500 / 2000: loss 0.273839\n",
      "iteration 1600 / 2000: loss 0.175063\n",
      "iteration 1700 / 2000: loss 0.222341\n",
      "iteration 1800 / 2000: loss 0.202023\n",
      "iteration 1900 / 2000: loss 0.332071\n",
      "Hidden Size: 80, Learning Rate: 0.0075000250000000004, LR Decay: 0.3, Regularization: 0.30000000000000004, Validation accuracy: 0.943\n",
      "iteration 0 / 2000: loss 2.302706\n",
      "iteration 100 / 2000: loss 0.404355\n",
      "iteration 200 / 2000: loss 0.446430\n",
      "iteration 300 / 2000: loss 0.333540\n",
      "iteration 400 / 2000: loss 0.234250\n",
      "iteration 500 / 2000: loss 0.251901\n",
      "iteration 600 / 2000: loss 0.289500\n",
      "iteration 700 / 2000: loss 0.200971\n",
      "iteration 800 / 2000: loss 0.342456\n",
      "iteration 900 / 2000: loss 0.203858\n",
      "iteration 1000 / 2000: loss 0.246448\n",
      "iteration 1100 / 2000: loss 0.257067\n",
      "iteration 1200 / 2000: loss 0.245905\n",
      "iteration 1300 / 2000: loss 0.235775\n",
      "iteration 1400 / 2000: loss 0.293844\n",
      "iteration 1500 / 2000: loss 0.262830\n",
      "iteration 1600 / 2000: loss 0.261766\n",
      "iteration 1700 / 2000: loss 0.216300\n",
      "iteration 1800 / 2000: loss 0.282858\n",
      "iteration 1900 / 2000: loss 0.363355\n",
      "Hidden Size: 80, Learning Rate: 0.0075000250000000004, LR Decay: 0.3, Regularization: 0.4, Validation accuracy: 0.9434\n",
      "iteration 0 / 2000: loss 2.302598\n",
      "iteration 100 / 2000: loss 0.269370\n",
      "iteration 200 / 2000: loss 0.257986\n",
      "iteration 300 / 2000: loss 0.176218\n",
      "iteration 400 / 2000: loss 0.141850\n",
      "iteration 500 / 2000: loss 0.060868\n",
      "iteration 600 / 2000: loss 0.106441\n",
      "iteration 700 / 2000: loss 0.166930\n",
      "iteration 800 / 2000: loss 0.086241\n",
      "iteration 900 / 2000: loss 0.049278\n",
      "iteration 1000 / 2000: loss 0.036798\n",
      "iteration 1100 / 2000: loss 0.089468\n",
      "iteration 1200 / 2000: loss 0.042765\n",
      "iteration 1300 / 2000: loss 0.101907\n",
      "iteration 1400 / 2000: loss 0.076252\n",
      "iteration 1500 / 2000: loss 0.070238\n",
      "iteration 1600 / 2000: loss 0.015205\n",
      "iteration 1700 / 2000: loss 0.051782\n",
      "iteration 1800 / 2000: loss 0.046297\n",
      "iteration 1900 / 2000: loss 0.033258\n",
      "Hidden Size: 80, Learning Rate: 0.00500005, LR Decay: 0.99, Regularization: 0.0, Validation accuracy: 0.975\n",
      "iteration 0 / 2000: loss 2.302621\n",
      "iteration 100 / 2000: loss 0.317333\n",
      "iteration 200 / 2000: loss 0.211370\n",
      "iteration 300 / 2000: loss 0.152314\n",
      "iteration 400 / 2000: loss 0.229619\n",
      "iteration 500 / 2000: loss 0.097215\n",
      "iteration 600 / 2000: loss 0.143615\n",
      "iteration 700 / 2000: loss 0.096009\n",
      "iteration 800 / 2000: loss 0.115976\n",
      "iteration 900 / 2000: loss 0.145433\n",
      "iteration 1000 / 2000: loss 0.122488\n",
      "iteration 1100 / 2000: loss 0.109276\n",
      "iteration 1200 / 2000: loss 0.083222\n",
      "iteration 1300 / 2000: loss 0.101834\n",
      "iteration 1400 / 2000: loss 0.105734\n",
      "iteration 1500 / 2000: loss 0.085169\n",
      "iteration 1600 / 2000: loss 0.124459\n",
      "iteration 1700 / 2000: loss 0.105221\n",
      "iteration 1800 / 2000: loss 0.082832\n",
      "iteration 1900 / 2000: loss 0.099466\n",
      "Hidden Size: 80, Learning Rate: 0.00500005, LR Decay: 0.99, Regularization: 0.1, Validation accuracy: 0.9746\n",
      "iteration 0 / 2000: loss 2.302651\n",
      "iteration 100 / 2000: loss 0.401194\n",
      "iteration 200 / 2000: loss 0.213688\n",
      "iteration 300 / 2000: loss 0.242149\n",
      "iteration 400 / 2000: loss 0.208956\n",
      "iteration 500 / 2000: loss 0.237008\n",
      "iteration 600 / 2000: loss 0.165214\n",
      "iteration 700 / 2000: loss 0.165935\n",
      "iteration 800 / 2000: loss 0.206430\n",
      "iteration 900 / 2000: loss 0.160720\n",
      "iteration 1000 / 2000: loss 0.174233\n",
      "iteration 1100 / 2000: loss 0.169657\n",
      "iteration 1200 / 2000: loss 0.140767\n",
      "iteration 1300 / 2000: loss 0.157511\n",
      "iteration 1400 / 2000: loss 0.191287\n",
      "iteration 1500 / 2000: loss 0.140510\n",
      "iteration 1600 / 2000: loss 0.116938\n",
      "iteration 1700 / 2000: loss 0.162029\n",
      "iteration 1800 / 2000: loss 0.160005\n",
      "iteration 1900 / 2000: loss 0.121137\n",
      "Hidden Size: 80, Learning Rate: 0.00500005, LR Decay: 0.99, Regularization: 0.2, Validation accuracy: 0.9728\n",
      "iteration 0 / 2000: loss 2.302689\n",
      "iteration 100 / 2000: loss 0.274202\n",
      "iteration 200 / 2000: loss 0.309644\n",
      "iteration 300 / 2000: loss 0.226693\n",
      "iteration 400 / 2000: loss 0.200586\n",
      "iteration 500 / 2000: loss 0.194464\n",
      "iteration 600 / 2000: loss 0.194683\n",
      "iteration 700 / 2000: loss 0.170237\n",
      "iteration 800 / 2000: loss 0.164739\n",
      "iteration 900 / 2000: loss 0.194307\n",
      "iteration 1000 / 2000: loss 0.197768\n",
      "iteration 1100 / 2000: loss 0.209596\n",
      "iteration 1200 / 2000: loss 0.215768\n",
      "iteration 1300 / 2000: loss 0.161894\n",
      "iteration 1400 / 2000: loss 0.181776\n",
      "iteration 1500 / 2000: loss 0.179547\n",
      "iteration 1600 / 2000: loss 0.157187\n",
      "iteration 1700 / 2000: loss 0.150051\n",
      "iteration 1800 / 2000: loss 0.190787\n",
      "iteration 1900 / 2000: loss 0.192437\n",
      "Hidden Size: 80, Learning Rate: 0.00500005, LR Decay: 0.99, Regularization: 0.30000000000000004, Validation accuracy: 0.9732\n",
      "iteration 0 / 2000: loss 2.302708\n",
      "iteration 100 / 2000: loss 0.387735\n",
      "iteration 200 / 2000: loss 0.274609\n",
      "iteration 300 / 2000: loss 0.306981\n",
      "iteration 400 / 2000: loss 0.247381\n",
      "iteration 500 / 2000: loss 0.252985\n",
      "iteration 600 / 2000: loss 0.236133\n",
      "iteration 700 / 2000: loss 0.249214\n",
      "iteration 800 / 2000: loss 0.258670\n",
      "iteration 900 / 2000: loss 0.168216\n",
      "iteration 1000 / 2000: loss 0.211554\n",
      "iteration 1100 / 2000: loss 0.196289\n",
      "iteration 1200 / 2000: loss 0.183358\n",
      "iteration 1300 / 2000: loss 0.229910\n",
      "iteration 1400 / 2000: loss 0.216221\n",
      "iteration 1500 / 2000: loss 0.205890\n",
      "iteration 1600 / 2000: loss 0.215759\n",
      "iteration 1700 / 2000: loss 0.205058\n",
      "iteration 1800 / 2000: loss 0.218986\n",
      "iteration 1900 / 2000: loss 0.211399\n",
      "Hidden Size: 80, Learning Rate: 0.00500005, LR Decay: 0.99, Regularization: 0.4, Validation accuracy: 0.9734\n",
      "iteration 0 / 2000: loss 2.302591\n",
      "iteration 100 / 2000: loss 0.297330\n",
      "iteration 200 / 2000: loss 0.216527\n",
      "iteration 300 / 2000: loss 0.148870\n",
      "iteration 400 / 2000: loss 0.203771\n",
      "iteration 500 / 2000: loss 0.160247\n",
      "iteration 600 / 2000: loss 0.099710\n",
      "iteration 700 / 2000: loss 0.056543\n",
      "iteration 800 / 2000: loss 0.152847\n",
      "iteration 900 / 2000: loss 0.064029\n",
      "iteration 1000 / 2000: loss 0.092787\n",
      "iteration 1100 / 2000: loss 0.102082\n",
      "iteration 1200 / 2000: loss 0.071777\n",
      "iteration 1300 / 2000: loss 0.031435\n",
      "iteration 1400 / 2000: loss 0.085799\n",
      "iteration 1500 / 2000: loss 0.102252\n",
      "iteration 1600 / 2000: loss 0.073396\n",
      "iteration 1700 / 2000: loss 0.114534\n",
      "iteration 1800 / 2000: loss 0.044708\n",
      "iteration 1900 / 2000: loss 0.039812\n",
      "Hidden Size: 80, Learning Rate: 0.00500005, LR Decay: 0.8175, Regularization: 0.0, Validation accuracy: 0.976\n",
      "iteration 0 / 2000: loss 2.302618\n",
      "iteration 100 / 2000: loss 0.318249\n",
      "iteration 200 / 2000: loss 0.238886\n",
      "iteration 300 / 2000: loss 0.195191\n",
      "iteration 400 / 2000: loss 0.150221\n",
      "iteration 500 / 2000: loss 0.186019\n",
      "iteration 600 / 2000: loss 0.126420\n",
      "iteration 700 / 2000: loss 0.154124\n",
      "iteration 800 / 2000: loss 0.184967\n",
      "iteration 900 / 2000: loss 0.146462\n",
      "iteration 1000 / 2000: loss 0.123326\n",
      "iteration 1100 / 2000: loss 0.118707\n",
      "iteration 1200 / 2000: loss 0.107455\n",
      "iteration 1300 / 2000: loss 0.087081\n",
      "iteration 1400 / 2000: loss 0.135190\n",
      "iteration 1500 / 2000: loss 0.114934\n",
      "iteration 1600 / 2000: loss 0.196337\n",
      "iteration 1700 / 2000: loss 0.105866\n",
      "iteration 1800 / 2000: loss 0.083925\n",
      "iteration 1900 / 2000: loss 0.127306\n",
      "Hidden Size: 80, Learning Rate: 0.00500005, LR Decay: 0.8175, Regularization: 0.1, Validation accuracy: 0.9736\n",
      "iteration 0 / 2000: loss 2.302638\n",
      "iteration 100 / 2000: loss 0.302113\n",
      "iteration 200 / 2000: loss 0.284847\n",
      "iteration 300 / 2000: loss 0.223526\n",
      "iteration 400 / 2000: loss 0.244391\n",
      "iteration 500 / 2000: loss 0.182670\n",
      "iteration 600 / 2000: loss 0.217894\n",
      "iteration 700 / 2000: loss 0.220019\n",
      "iteration 800 / 2000: loss 0.176822\n",
      "iteration 900 / 2000: loss 0.212961\n",
      "iteration 1000 / 2000: loss 0.152297\n",
      "iteration 1100 / 2000: loss 0.181409\n",
      "iteration 1200 / 2000: loss 0.167112\n",
      "iteration 1300 / 2000: loss 0.111692\n",
      "iteration 1400 / 2000: loss 0.175605\n",
      "iteration 1500 / 2000: loss 0.127439\n",
      "iteration 1600 / 2000: loss 0.161286\n",
      "iteration 1700 / 2000: loss 0.153321\n",
      "iteration 1800 / 2000: loss 0.150053\n",
      "iteration 1900 / 2000: loss 0.123752\n",
      "Hidden Size: 80, Learning Rate: 0.00500005, LR Decay: 0.8175, Regularization: 0.2, Validation accuracy: 0.9714\n",
      "iteration 0 / 2000: loss 2.302683\n",
      "iteration 100 / 2000: loss 0.314894\n",
      "iteration 200 / 2000: loss 0.234286\n",
      "iteration 300 / 2000: loss 0.225276\n",
      "iteration 400 / 2000: loss 0.211066\n",
      "iteration 500 / 2000: loss 0.245974\n",
      "iteration 600 / 2000: loss 0.189501\n",
      "iteration 700 / 2000: loss 0.223706\n",
      "iteration 800 / 2000: loss 0.187943\n",
      "iteration 900 / 2000: loss 0.209784\n",
      "iteration 1000 / 2000: loss 0.196034\n",
      "iteration 1100 / 2000: loss 0.197870\n",
      "iteration 1200 / 2000: loss 0.209232\n",
      "iteration 1300 / 2000: loss 0.132786\n",
      "iteration 1400 / 2000: loss 0.174743\n",
      "iteration 1500 / 2000: loss 0.166418\n",
      "iteration 1600 / 2000: loss 0.158935\n",
      "iteration 1700 / 2000: loss 0.170436\n",
      "iteration 1800 / 2000: loss 0.168929\n",
      "iteration 1900 / 2000: loss 0.214496\n",
      "Hidden Size: 80, Learning Rate: 0.00500005, LR Decay: 0.8175, Regularization: 0.30000000000000004, Validation accuracy: 0.9712\n",
      "iteration 0 / 2000: loss 2.302697\n",
      "iteration 100 / 2000: loss 0.383932\n",
      "iteration 200 / 2000: loss 0.290985\n",
      "iteration 300 / 2000: loss 0.251690\n",
      "iteration 400 / 2000: loss 0.202205\n",
      "iteration 500 / 2000: loss 0.365302\n",
      "iteration 600 / 2000: loss 0.232791\n",
      "iteration 700 / 2000: loss 0.197598\n",
      "iteration 800 / 2000: loss 0.243206\n",
      "iteration 900 / 2000: loss 0.213595\n",
      "iteration 1000 / 2000: loss 0.206539\n",
      "iteration 1100 / 2000: loss 0.262389\n",
      "iteration 1200 / 2000: loss 0.182001\n",
      "iteration 1300 / 2000: loss 0.203872\n",
      "iteration 1400 / 2000: loss 0.183368\n",
      "iteration 1500 / 2000: loss 0.182690\n",
      "iteration 1600 / 2000: loss 0.177938\n",
      "iteration 1700 / 2000: loss 0.206784\n",
      "iteration 1800 / 2000: loss 0.248255\n",
      "iteration 1900 / 2000: loss 0.225185\n",
      "Hidden Size: 80, Learning Rate: 0.00500005, LR Decay: 0.8175, Regularization: 0.4, Validation accuracy: 0.9706\n",
      "iteration 0 / 2000: loss 2.302563\n",
      "iteration 100 / 2000: loss 0.320829\n",
      "iteration 200 / 2000: loss 0.246633\n",
      "iteration 300 / 2000: loss 0.210847\n",
      "iteration 400 / 2000: loss 0.173497\n",
      "iteration 500 / 2000: loss 0.168988\n",
      "iteration 600 / 2000: loss 0.103498\n",
      "iteration 700 / 2000: loss 0.135650\n",
      "iteration 800 / 2000: loss 0.113879\n",
      "iteration 900 / 2000: loss 0.103083\n",
      "iteration 1000 / 2000: loss 0.066382\n",
      "iteration 1100 / 2000: loss 0.175438\n",
      "iteration 1200 / 2000: loss 0.072970\n",
      "iteration 1300 / 2000: loss 0.100177\n",
      "iteration 1400 / 2000: loss 0.114210\n",
      "iteration 1500 / 2000: loss 0.095940\n",
      "iteration 1600 / 2000: loss 0.101143\n",
      "iteration 1700 / 2000: loss 0.116646\n",
      "iteration 1800 / 2000: loss 0.076451\n",
      "iteration 1900 / 2000: loss 0.091119\n",
      "Hidden Size: 80, Learning Rate: 0.00500005, LR Decay: 0.645, Regularization: 0.0, Validation accuracy: 0.9684\n",
      "iteration 0 / 2000: loss 2.302611\n",
      "iteration 100 / 2000: loss 0.342027\n",
      "iteration 200 / 2000: loss 0.175402\n",
      "iteration 300 / 2000: loss 0.252476\n",
      "iteration 400 / 2000: loss 0.246482\n",
      "iteration 500 / 2000: loss 0.111902\n",
      "iteration 600 / 2000: loss 0.156460\n",
      "iteration 700 / 2000: loss 0.172015\n",
      "iteration 800 / 2000: loss 0.170053\n",
      "iteration 900 / 2000: loss 0.210022\n",
      "iteration 1000 / 2000: loss 0.126732\n",
      "iteration 1100 / 2000: loss 0.145658\n",
      "iteration 1200 / 2000: loss 0.151605\n",
      "iteration 1300 / 2000: loss 0.122146\n",
      "iteration 1400 / 2000: loss 0.091385\n",
      "iteration 1500 / 2000: loss 0.125770\n",
      "iteration 1600 / 2000: loss 0.187101\n",
      "iteration 1700 / 2000: loss 0.118291\n",
      "iteration 1800 / 2000: loss 0.141109\n",
      "iteration 1900 / 2000: loss 0.104597\n",
      "Hidden Size: 80, Learning Rate: 0.00500005, LR Decay: 0.645, Regularization: 0.1, Validation accuracy: 0.9682\n",
      "iteration 0 / 2000: loss 2.302630\n",
      "iteration 100 / 2000: loss 0.238581\n",
      "iteration 200 / 2000: loss 0.277805\n",
      "iteration 300 / 2000: loss 0.347137\n",
      "iteration 400 / 2000: loss 0.215330\n",
      "iteration 500 / 2000: loss 0.245518\n",
      "iteration 600 / 2000: loss 0.202885\n",
      "iteration 700 / 2000: loss 0.171946\n",
      "iteration 800 / 2000: loss 0.193264\n",
      "iteration 900 / 2000: loss 0.155322\n",
      "iteration 1000 / 2000: loss 0.184833\n",
      "iteration 1100 / 2000: loss 0.179303\n",
      "iteration 1200 / 2000: loss 0.174301\n",
      "iteration 1300 / 2000: loss 0.141525\n",
      "iteration 1400 / 2000: loss 0.188095\n",
      "iteration 1500 / 2000: loss 0.204578\n",
      "iteration 1600 / 2000: loss 0.153199\n",
      "iteration 1700 / 2000: loss 0.187885\n",
      "iteration 1800 / 2000: loss 0.153509\n",
      "iteration 1900 / 2000: loss 0.169852\n",
      "Hidden Size: 80, Learning Rate: 0.00500005, LR Decay: 0.645, Regularization: 0.2, Validation accuracy: 0.9678\n",
      "iteration 0 / 2000: loss 2.302685\n",
      "iteration 100 / 2000: loss 0.358659\n",
      "iteration 200 / 2000: loss 0.232907\n",
      "iteration 300 / 2000: loss 0.186745\n",
      "iteration 400 / 2000: loss 0.204963\n",
      "iteration 500 / 2000: loss 0.258576\n",
      "iteration 600 / 2000: loss 0.214968\n",
      "iteration 700 / 2000: loss 0.173033\n",
      "iteration 800 / 2000: loss 0.262882\n",
      "iteration 900 / 2000: loss 0.247063\n",
      "iteration 1000 / 2000: loss 0.168207\n",
      "iteration 1100 / 2000: loss 0.179541\n",
      "iteration 1200 / 2000: loss 0.250727\n",
      "iteration 1300 / 2000: loss 0.177676\n",
      "iteration 1400 / 2000: loss 0.200801\n",
      "iteration 1500 / 2000: loss 0.208692\n",
      "iteration 1600 / 2000: loss 0.214436\n",
      "iteration 1700 / 2000: loss 0.178761\n",
      "iteration 1800 / 2000: loss 0.225291\n",
      "iteration 1900 / 2000: loss 0.211825\n",
      "Hidden Size: 80, Learning Rate: 0.00500005, LR Decay: 0.645, Regularization: 0.30000000000000004, Validation accuracy: 0.9652\n",
      "iteration 0 / 2000: loss 2.302717\n",
      "iteration 100 / 2000: loss 0.273641\n",
      "iteration 200 / 2000: loss 0.259358\n",
      "iteration 300 / 2000: loss 0.275629\n",
      "iteration 400 / 2000: loss 0.288332\n",
      "iteration 500 / 2000: loss 0.224108\n",
      "iteration 600 / 2000: loss 0.215127\n",
      "iteration 700 / 2000: loss 0.250461\n",
      "iteration 800 / 2000: loss 0.199044\n",
      "iteration 900 / 2000: loss 0.220564\n",
      "iteration 1000 / 2000: loss 0.169289\n",
      "iteration 1100 / 2000: loss 0.222960\n",
      "iteration 1200 / 2000: loss 0.229692\n",
      "iteration 1300 / 2000: loss 0.206012\n",
      "iteration 1400 / 2000: loss 0.221144\n",
      "iteration 1500 / 2000: loss 0.264477\n",
      "iteration 1600 / 2000: loss 0.247213\n",
      "iteration 1700 / 2000: loss 0.195997\n",
      "iteration 1800 / 2000: loss 0.182471\n",
      "iteration 1900 / 2000: loss 0.194875\n",
      "Hidden Size: 80, Learning Rate: 0.00500005, LR Decay: 0.645, Regularization: 0.4, Validation accuracy: 0.9624\n",
      "iteration 0 / 2000: loss 2.302561\n",
      "iteration 100 / 2000: loss 0.373877\n",
      "iteration 200 / 2000: loss 0.256477\n",
      "iteration 300 / 2000: loss 0.191636\n",
      "iteration 400 / 2000: loss 0.150865\n",
      "iteration 500 / 2000: loss 0.160350\n",
      "iteration 600 / 2000: loss 0.227844\n",
      "iteration 700 / 2000: loss 0.156053\n",
      "iteration 800 / 2000: loss 0.184349\n",
      "iteration 900 / 2000: loss 0.135195\n",
      "iteration 1000 / 2000: loss 0.185091\n",
      "iteration 1100 / 2000: loss 0.108591\n",
      "iteration 1200 / 2000: loss 0.231812\n",
      "iteration 1300 / 2000: loss 0.178739\n",
      "iteration 1400 / 2000: loss 0.123170\n",
      "iteration 1500 / 2000: loss 0.188297\n",
      "iteration 1600 / 2000: loss 0.166003\n",
      "iteration 1700 / 2000: loss 0.173844\n",
      "iteration 1800 / 2000: loss 0.252040\n",
      "iteration 1900 / 2000: loss 0.158900\n",
      "Hidden Size: 80, Learning Rate: 0.00500005, LR Decay: 0.47250000000000003, Regularization: 0.0, Validation accuracy: 0.9572\n",
      "iteration 0 / 2000: loss 2.302625\n",
      "iteration 100 / 2000: loss 0.419899\n",
      "iteration 200 / 2000: loss 0.237263\n",
      "iteration 300 / 2000: loss 0.212442\n",
      "iteration 400 / 2000: loss 0.245670\n",
      "iteration 500 / 2000: loss 0.230051\n",
      "iteration 600 / 2000: loss 0.163604\n",
      "iteration 700 / 2000: loss 0.182331\n",
      "iteration 800 / 2000: loss 0.225454\n",
      "iteration 900 / 2000: loss 0.167030\n",
      "iteration 1000 / 2000: loss 0.159467\n",
      "iteration 1100 / 2000: loss 0.122570\n",
      "iteration 1200 / 2000: loss 0.208056\n",
      "iteration 1300 / 2000: loss 0.173243\n",
      "iteration 1400 / 2000: loss 0.239949\n",
      "iteration 1500 / 2000: loss 0.196736\n",
      "iteration 1600 / 2000: loss 0.181395\n",
      "iteration 1700 / 2000: loss 0.268481\n",
      "iteration 1800 / 2000: loss 0.162372\n",
      "iteration 1900 / 2000: loss 0.168607\n",
      "Hidden Size: 80, Learning Rate: 0.00500005, LR Decay: 0.47250000000000003, Regularization: 0.1, Validation accuracy: 0.9556\n",
      "iteration 0 / 2000: loss 2.302645\n",
      "iteration 100 / 2000: loss 0.342413\n",
      "iteration 200 / 2000: loss 0.289278\n",
      "iteration 300 / 2000: loss 0.216325\n",
      "iteration 400 / 2000: loss 0.201421\n",
      "iteration 500 / 2000: loss 0.222677\n",
      "iteration 600 / 2000: loss 0.240382\n",
      "iteration 700 / 2000: loss 0.347768\n",
      "iteration 800 / 2000: loss 0.272743\n",
      "iteration 900 / 2000: loss 0.223673\n",
      "iteration 1000 / 2000: loss 0.264640\n",
      "iteration 1100 / 2000: loss 0.182832\n",
      "iteration 1200 / 2000: loss 0.223803\n",
      "iteration 1300 / 2000: loss 0.214715\n",
      "iteration 1400 / 2000: loss 0.242326\n",
      "iteration 1500 / 2000: loss 0.185248\n",
      "iteration 1600 / 2000: loss 0.198210\n",
      "iteration 1700 / 2000: loss 0.130283\n",
      "iteration 1800 / 2000: loss 0.208209\n",
      "iteration 1900 / 2000: loss 0.202126\n",
      "Hidden Size: 80, Learning Rate: 0.00500005, LR Decay: 0.47250000000000003, Regularization: 0.2, Validation accuracy: 0.9552\n",
      "iteration 0 / 2000: loss 2.302684\n",
      "iteration 100 / 2000: loss 0.351391\n",
      "iteration 200 / 2000: loss 0.245028\n",
      "iteration 300 / 2000: loss 0.340030\n",
      "iteration 400 / 2000: loss 0.310417\n",
      "iteration 500 / 2000: loss 0.245078\n",
      "iteration 600 / 2000: loss 0.222545\n",
      "iteration 700 / 2000: loss 0.256248\n",
      "iteration 800 / 2000: loss 0.233473\n",
      "iteration 900 / 2000: loss 0.326456\n",
      "iteration 1000 / 2000: loss 0.236612\n",
      "iteration 1100 / 2000: loss 0.262995\n",
      "iteration 1200 / 2000: loss 0.284094\n",
      "iteration 1300 / 2000: loss 0.244945\n",
      "iteration 1400 / 2000: loss 0.209399\n",
      "iteration 1500 / 2000: loss 0.200820\n",
      "iteration 1600 / 2000: loss 0.245044\n",
      "iteration 1700 / 2000: loss 0.245698\n",
      "iteration 1800 / 2000: loss 0.245557\n",
      "iteration 1900 / 2000: loss 0.177914\n",
      "Hidden Size: 80, Learning Rate: 0.00500005, LR Decay: 0.47250000000000003, Regularization: 0.30000000000000004, Validation accuracy: 0.9522\n",
      "iteration 0 / 2000: loss 2.302708\n",
      "iteration 100 / 2000: loss 0.383959\n",
      "iteration 200 / 2000: loss 0.385907\n",
      "iteration 300 / 2000: loss 0.339243\n",
      "iteration 400 / 2000: loss 0.272647\n",
      "iteration 500 / 2000: loss 0.209746\n",
      "iteration 600 / 2000: loss 0.213547\n",
      "iteration 700 / 2000: loss 0.227345\n",
      "iteration 800 / 2000: loss 0.302949\n",
      "iteration 900 / 2000: loss 0.263680\n",
      "iteration 1000 / 2000: loss 0.273647\n",
      "iteration 1100 / 2000: loss 0.295364\n",
      "iteration 1200 / 2000: loss 0.195996\n",
      "iteration 1300 / 2000: loss 0.198877\n",
      "iteration 1400 / 2000: loss 0.279800\n",
      "iteration 1500 / 2000: loss 0.206482\n",
      "iteration 1600 / 2000: loss 0.243305\n",
      "iteration 1700 / 2000: loss 0.189573\n",
      "iteration 1800 / 2000: loss 0.239014\n",
      "iteration 1900 / 2000: loss 0.250284\n",
      "Hidden Size: 80, Learning Rate: 0.00500005, LR Decay: 0.47250000000000003, Regularization: 0.4, Validation accuracy: 0.9526\n",
      "iteration 0 / 2000: loss 2.302573\n",
      "iteration 100 / 2000: loss 0.473085\n",
      "iteration 200 / 2000: loss 0.317698\n",
      "iteration 300 / 2000: loss 0.274220\n",
      "iteration 400 / 2000: loss 0.310213\n",
      "iteration 500 / 2000: loss 0.238981\n",
      "iteration 600 / 2000: loss 0.210586\n",
      "iteration 700 / 2000: loss 0.282707\n",
      "iteration 800 / 2000: loss 0.243032\n",
      "iteration 900 / 2000: loss 0.190078\n",
      "iteration 1000 / 2000: loss 0.136380\n",
      "iteration 1100 / 2000: loss 0.159818\n",
      "iteration 1200 / 2000: loss 0.272899\n",
      "iteration 1300 / 2000: loss 0.266243\n",
      "iteration 1400 / 2000: loss 0.222370\n",
      "iteration 1500 / 2000: loss 0.239096\n",
      "iteration 1600 / 2000: loss 0.251352\n",
      "iteration 1700 / 2000: loss 0.260808\n",
      "iteration 1800 / 2000: loss 0.291173\n",
      "iteration 1900 / 2000: loss 0.240390\n",
      "Hidden Size: 80, Learning Rate: 0.00500005, LR Decay: 0.3, Regularization: 0.0, Validation accuracy: 0.935\n",
      "iteration 0 / 2000: loss 2.302616\n",
      "iteration 100 / 2000: loss 0.510805\n",
      "iteration 200 / 2000: loss 0.337297\n",
      "iteration 300 / 2000: loss 0.382912\n",
      "iteration 400 / 2000: loss 0.166148\n",
      "iteration 500 / 2000: loss 0.274889\n",
      "iteration 600 / 2000: loss 0.209972\n",
      "iteration 700 / 2000: loss 0.243603\n",
      "iteration 800 / 2000: loss 0.232401\n",
      "iteration 900 / 2000: loss 0.280383\n",
      "iteration 1000 / 2000: loss 0.299106\n",
      "iteration 1100 / 2000: loss 0.287328\n",
      "iteration 1200 / 2000: loss 0.276885\n",
      "iteration 1300 / 2000: loss 0.258577\n",
      "iteration 1400 / 2000: loss 0.298001\n",
      "iteration 1500 / 2000: loss 0.242974\n",
      "iteration 1600 / 2000: loss 0.249629\n",
      "iteration 1700 / 2000: loss 0.246521\n",
      "iteration 1800 / 2000: loss 0.264210\n",
      "iteration 1900 / 2000: loss 0.343111\n",
      "Hidden Size: 80, Learning Rate: 0.00500005, LR Decay: 0.3, Regularization: 0.1, Validation accuracy: 0.9354\n",
      "iteration 0 / 2000: loss 2.302664\n",
      "iteration 100 / 2000: loss 0.538661\n",
      "iteration 200 / 2000: loss 0.260932\n",
      "iteration 300 / 2000: loss 0.301389\n",
      "iteration 400 / 2000: loss 0.292380\n",
      "iteration 500 / 2000: loss 0.282313\n",
      "iteration 600 / 2000: loss 0.260216\n",
      "iteration 700 / 2000: loss 0.179913\n",
      "iteration 800 / 2000: loss 0.274725\n",
      "iteration 900 / 2000: loss 0.325334\n",
      "iteration 1000 / 2000: loss 0.257896\n",
      "iteration 1100 / 2000: loss 0.226950\n",
      "iteration 1200 / 2000: loss 0.349138\n",
      "iteration 1300 / 2000: loss 0.279358\n",
      "iteration 1400 / 2000: loss 0.247678\n",
      "iteration 1500 / 2000: loss 0.167456\n",
      "iteration 1600 / 2000: loss 0.258560\n",
      "iteration 1700 / 2000: loss 0.223338\n",
      "iteration 1800 / 2000: loss 0.265141\n",
      "iteration 1900 / 2000: loss 0.255395\n",
      "Hidden Size: 80, Learning Rate: 0.00500005, LR Decay: 0.3, Regularization: 0.2, Validation accuracy: 0.9336\n",
      "iteration 0 / 2000: loss 2.302668\n",
      "iteration 100 / 2000: loss 0.506600\n",
      "iteration 200 / 2000: loss 0.336153\n",
      "iteration 300 / 2000: loss 0.271095\n",
      "iteration 400 / 2000: loss 0.367886\n",
      "iteration 500 / 2000: loss 0.251252\n",
      "iteration 600 / 2000: loss 0.325201\n",
      "iteration 700 / 2000: loss 0.267483\n",
      "iteration 800 / 2000: loss 0.316873\n",
      "iteration 900 / 2000: loss 0.365005\n",
      "iteration 1000 / 2000: loss 0.290942\n",
      "iteration 1100 / 2000: loss 0.229020\n",
      "iteration 1200 / 2000: loss 0.279131\n",
      "iteration 1300 / 2000: loss 0.315230\n",
      "iteration 1400 / 2000: loss 0.275693\n",
      "iteration 1500 / 2000: loss 0.283525\n",
      "iteration 1600 / 2000: loss 0.324809\n",
      "iteration 1700 / 2000: loss 0.297385\n",
      "iteration 1800 / 2000: loss 0.328038\n",
      "iteration 1900 / 2000: loss 0.278924\n",
      "Hidden Size: 80, Learning Rate: 0.00500005, LR Decay: 0.3, Regularization: 0.30000000000000004, Validation accuracy: 0.9334\n",
      "iteration 0 / 2000: loss 2.302735\n",
      "iteration 100 / 2000: loss 0.555709\n",
      "iteration 200 / 2000: loss 0.326659\n",
      "iteration 300 / 2000: loss 0.291383\n",
      "iteration 400 / 2000: loss 0.329401\n",
      "iteration 500 / 2000: loss 0.210111\n",
      "iteration 600 / 2000: loss 0.333241\n",
      "iteration 700 / 2000: loss 0.334228\n",
      "iteration 800 / 2000: loss 0.304671\n",
      "iteration 900 / 2000: loss 0.397899\n",
      "iteration 1000 / 2000: loss 0.210121\n",
      "iteration 1100 / 2000: loss 0.298850\n",
      "iteration 1200 / 2000: loss 0.246163\n",
      "iteration 1300 / 2000: loss 0.295948\n",
      "iteration 1400 / 2000: loss 0.247742\n",
      "iteration 1500 / 2000: loss 0.289051\n",
      "iteration 1600 / 2000: loss 0.328958\n",
      "iteration 1700 / 2000: loss 0.280918\n",
      "iteration 1800 / 2000: loss 0.251487\n",
      "iteration 1900 / 2000: loss 0.280874\n",
      "Hidden Size: 80, Learning Rate: 0.00500005, LR Decay: 0.3, Regularization: 0.4, Validation accuracy: 0.9322\n",
      "iteration 0 / 2000: loss 2.302600\n",
      "iteration 100 / 2000: loss 0.315244\n",
      "iteration 200 / 2000: loss 0.275686\n",
      "iteration 300 / 2000: loss 0.292264\n",
      "iteration 400 / 2000: loss 0.148209\n",
      "iteration 500 / 2000: loss 0.174402\n",
      "iteration 600 / 2000: loss 0.153248\n",
      "iteration 700 / 2000: loss 0.157532\n",
      "iteration 800 / 2000: loss 0.156330\n",
      "iteration 900 / 2000: loss 0.094593\n",
      "iteration 1000 / 2000: loss 0.154999\n",
      "iteration 1100 / 2000: loss 0.202364\n",
      "iteration 1200 / 2000: loss 0.128330\n",
      "iteration 1300 / 2000: loss 0.051184\n",
      "iteration 1400 / 2000: loss 0.064193\n",
      "iteration 1500 / 2000: loss 0.061586\n",
      "iteration 1600 / 2000: loss 0.037420\n",
      "iteration 1700 / 2000: loss 0.145422\n",
      "iteration 1800 / 2000: loss 0.053398\n",
      "iteration 1900 / 2000: loss 0.038169\n",
      "Hidden Size: 80, Learning Rate: 0.002500074999999999, LR Decay: 0.99, Regularization: 0.0, Validation accuracy: 0.9764\n",
      "iteration 0 / 2000: loss 2.302601\n",
      "iteration 100 / 2000: loss 0.333709\n",
      "iteration 200 / 2000: loss 0.228662\n",
      "iteration 300 / 2000: loss 0.260763\n",
      "iteration 400 / 2000: loss 0.189182\n",
      "iteration 500 / 2000: loss 0.188827\n",
      "iteration 600 / 2000: loss 0.128490\n",
      "iteration 700 / 2000: loss 0.165274\n",
      "iteration 800 / 2000: loss 0.101420\n",
      "iteration 900 / 2000: loss 0.165457\n",
      "iteration 1000 / 2000: loss 0.126322\n",
      "iteration 1100 / 2000: loss 0.110997\n",
      "iteration 1200 / 2000: loss 0.131164\n",
      "iteration 1300 / 2000: loss 0.140228\n",
      "iteration 1400 / 2000: loss 0.107421\n",
      "iteration 1500 / 2000: loss 0.108987\n",
      "iteration 1600 / 2000: loss 0.190113\n",
      "iteration 1700 / 2000: loss 0.131690\n",
      "iteration 1800 / 2000: loss 0.095956\n",
      "iteration 1900 / 2000: loss 0.105807\n",
      "Hidden Size: 80, Learning Rate: 0.002500074999999999, LR Decay: 0.99, Regularization: 0.1, Validation accuracy: 0.9726\n",
      "iteration 0 / 2000: loss 2.302649\n",
      "iteration 100 / 2000: loss 0.318145\n",
      "iteration 200 / 2000: loss 0.298459\n",
      "iteration 300 / 2000: loss 0.285082\n",
      "iteration 400 / 2000: loss 0.213671\n",
      "iteration 500 / 2000: loss 0.265338\n",
      "iteration 600 / 2000: loss 0.194169\n",
      "iteration 700 / 2000: loss 0.147924\n",
      "iteration 800 / 2000: loss 0.196815\n",
      "iteration 900 / 2000: loss 0.167149\n",
      "iteration 1000 / 2000: loss 0.206827\n",
      "iteration 1100 / 2000: loss 0.169177\n",
      "iteration 1200 / 2000: loss 0.201308\n",
      "iteration 1300 / 2000: loss 0.150221\n",
      "iteration 1400 / 2000: loss 0.153705\n",
      "iteration 1500 / 2000: loss 0.276804\n",
      "iteration 1600 / 2000: loss 0.149126\n",
      "iteration 1700 / 2000: loss 0.154573\n",
      "iteration 1800 / 2000: loss 0.129404\n",
      "iteration 1900 / 2000: loss 0.136874\n",
      "Hidden Size: 80, Learning Rate: 0.002500074999999999, LR Decay: 0.99, Regularization: 0.2, Validation accuracy: 0.9698\n",
      "iteration 0 / 2000: loss 2.302663\n",
      "iteration 100 / 2000: loss 0.315396\n",
      "iteration 200 / 2000: loss 0.359096\n",
      "iteration 300 / 2000: loss 0.285119\n",
      "iteration 400 / 2000: loss 0.223724\n",
      "iteration 500 / 2000: loss 0.277935\n",
      "iteration 600 / 2000: loss 0.238600\n",
      "iteration 700 / 2000: loss 0.223592\n",
      "iteration 800 / 2000: loss 0.239325\n",
      "iteration 900 / 2000: loss 0.264326\n",
      "iteration 1000 / 2000: loss 0.227448\n",
      "iteration 1100 / 2000: loss 0.159626\n",
      "iteration 1200 / 2000: loss 0.242408\n",
      "iteration 1300 / 2000: loss 0.188044\n",
      "iteration 1400 / 2000: loss 0.214389\n",
      "iteration 1500 / 2000: loss 0.160738\n",
      "iteration 1600 / 2000: loss 0.199999\n",
      "iteration 1700 / 2000: loss 0.160575\n",
      "iteration 1800 / 2000: loss 0.210138\n",
      "iteration 1900 / 2000: loss 0.239869\n",
      "Hidden Size: 80, Learning Rate: 0.002500074999999999, LR Decay: 0.99, Regularization: 0.30000000000000004, Validation accuracy: 0.9698\n",
      "iteration 0 / 2000: loss 2.302712\n",
      "iteration 100 / 2000: loss 0.309073\n",
      "iteration 200 / 2000: loss 0.418785\n",
      "iteration 300 / 2000: loss 0.347436\n",
      "iteration 400 / 2000: loss 0.264054\n",
      "iteration 500 / 2000: loss 0.225630\n",
      "iteration 600 / 2000: loss 0.260014\n",
      "iteration 700 / 2000: loss 0.202441\n",
      "iteration 800 / 2000: loss 0.232883\n",
      "iteration 900 / 2000: loss 0.226403\n",
      "iteration 1000 / 2000: loss 0.228619\n",
      "iteration 1100 / 2000: loss 0.179236\n",
      "iteration 1200 / 2000: loss 0.240530\n",
      "iteration 1300 / 2000: loss 0.181536\n",
      "iteration 1400 / 2000: loss 0.188911\n",
      "iteration 1500 / 2000: loss 0.207390\n",
      "iteration 1600 / 2000: loss 0.160026\n",
      "iteration 1700 / 2000: loss 0.193250\n",
      "iteration 1800 / 2000: loss 0.166546\n",
      "iteration 1900 / 2000: loss 0.196308\n",
      "Hidden Size: 80, Learning Rate: 0.002500074999999999, LR Decay: 0.99, Regularization: 0.4, Validation accuracy: 0.9706\n",
      "iteration 0 / 2000: loss 2.302600\n",
      "iteration 100 / 2000: loss 0.359659\n",
      "iteration 200 / 2000: loss 0.272288\n",
      "iteration 300 / 2000: loss 0.302608\n",
      "iteration 400 / 2000: loss 0.195151\n",
      "iteration 500 / 2000: loss 0.178522\n",
      "iteration 600 / 2000: loss 0.127859\n",
      "iteration 700 / 2000: loss 0.134203\n",
      "iteration 800 / 2000: loss 0.222308\n",
      "iteration 900 / 2000: loss 0.116890\n",
      "iteration 1000 / 2000: loss 0.109593\n",
      "iteration 1100 / 2000: loss 0.161980\n",
      "iteration 1200 / 2000: loss 0.112808\n",
      "iteration 1300 / 2000: loss 0.146320\n",
      "iteration 1400 / 2000: loss 0.095999\n",
      "iteration 1500 / 2000: loss 0.111348\n",
      "iteration 1600 / 2000: loss 0.138848\n",
      "iteration 1700 / 2000: loss 0.099086\n",
      "iteration 1800 / 2000: loss 0.116126\n",
      "iteration 1900 / 2000: loss 0.076712\n",
      "Hidden Size: 80, Learning Rate: 0.002500074999999999, LR Decay: 0.8175, Regularization: 0.0, Validation accuracy: 0.9666\n",
      "iteration 0 / 2000: loss 2.302617\n",
      "iteration 100 / 2000: loss 0.373780\n",
      "iteration 200 / 2000: loss 0.326129\n",
      "iteration 300 / 2000: loss 0.303345\n",
      "iteration 400 / 2000: loss 0.343778\n",
      "iteration 500 / 2000: loss 0.236779\n",
      "iteration 600 / 2000: loss 0.177549\n",
      "iteration 700 / 2000: loss 0.308837\n",
      "iteration 800 / 2000: loss 0.144403\n",
      "iteration 900 / 2000: loss 0.161464\n",
      "iteration 1000 / 2000: loss 0.188399\n",
      "iteration 1100 / 2000: loss 0.148264\n",
      "iteration 1200 / 2000: loss 0.184973\n",
      "iteration 1300 / 2000: loss 0.149183\n",
      "iteration 1400 / 2000: loss 0.132675\n",
      "iteration 1500 / 2000: loss 0.219450\n",
      "iteration 1600 / 2000: loss 0.122592\n",
      "iteration 1700 / 2000: loss 0.164235\n",
      "iteration 1800 / 2000: loss 0.203359\n",
      "iteration 1900 / 2000: loss 0.093930\n",
      "Hidden Size: 80, Learning Rate: 0.002500074999999999, LR Decay: 0.8175, Regularization: 0.1, Validation accuracy: 0.9684\n",
      "iteration 0 / 2000: loss 2.302641\n",
      "iteration 100 / 2000: loss 0.465952\n",
      "iteration 200 / 2000: loss 0.305558\n",
      "iteration 300 / 2000: loss 0.236558\n",
      "iteration 400 / 2000: loss 0.260886\n",
      "iteration 500 / 2000: loss 0.263757\n",
      "iteration 600 / 2000: loss 0.191732\n",
      "iteration 700 / 2000: loss 0.176484\n",
      "iteration 800 / 2000: loss 0.187391\n",
      "iteration 900 / 2000: loss 0.170246\n",
      "iteration 1000 / 2000: loss 0.192042\n",
      "iteration 1100 / 2000: loss 0.289877\n",
      "iteration 1200 / 2000: loss 0.212933\n",
      "iteration 1300 / 2000: loss 0.170451\n",
      "iteration 1400 / 2000: loss 0.157990\n",
      "iteration 1500 / 2000: loss 0.163106\n",
      "iteration 1600 / 2000: loss 0.151908\n",
      "iteration 1700 / 2000: loss 0.189682\n",
      "iteration 1800 / 2000: loss 0.232133\n",
      "iteration 1900 / 2000: loss 0.166675\n",
      "Hidden Size: 80, Learning Rate: 0.002500074999999999, LR Decay: 0.8175, Regularization: 0.2, Validation accuracy: 0.9658\n",
      "iteration 0 / 2000: loss 2.302668\n",
      "iteration 100 / 2000: loss 0.436891\n",
      "iteration 200 / 2000: loss 0.238551\n",
      "iteration 300 / 2000: loss 0.362132\n",
      "iteration 400 / 2000: loss 0.231172\n",
      "iteration 500 / 2000: loss 0.234176\n",
      "iteration 600 / 2000: loss 0.249813\n",
      "iteration 700 / 2000: loss 0.224090\n",
      "iteration 800 / 2000: loss 0.182133\n",
      "iteration 900 / 2000: loss 0.169327\n",
      "iteration 1000 / 2000: loss 0.175000\n",
      "iteration 1100 / 2000: loss 0.145572\n",
      "iteration 1200 / 2000: loss 0.230885\n",
      "iteration 1300 / 2000: loss 0.145661\n",
      "iteration 1400 / 2000: loss 0.153446\n",
      "iteration 1500 / 2000: loss 0.239214\n",
      "iteration 1600 / 2000: loss 0.207631\n",
      "iteration 1700 / 2000: loss 0.196455\n",
      "iteration 1800 / 2000: loss 0.202663\n",
      "iteration 1900 / 2000: loss 0.214375\n",
      "Hidden Size: 80, Learning Rate: 0.002500074999999999, LR Decay: 0.8175, Regularization: 0.30000000000000004, Validation accuracy: 0.9654\n",
      "iteration 0 / 2000: loss 2.302721\n",
      "iteration 100 / 2000: loss 0.404825\n",
      "iteration 200 / 2000: loss 0.283871\n",
      "iteration 300 / 2000: loss 0.371871\n",
      "iteration 400 / 2000: loss 0.273184\n",
      "iteration 500 / 2000: loss 0.351516\n",
      "iteration 600 / 2000: loss 0.240522\n",
      "iteration 700 / 2000: loss 0.258893\n",
      "iteration 800 / 2000: loss 0.211687\n",
      "iteration 900 / 2000: loss 0.249251\n",
      "iteration 1000 / 2000: loss 0.281653\n",
      "iteration 1100 / 2000: loss 0.229589\n",
      "iteration 1200 / 2000: loss 0.240198\n",
      "iteration 1300 / 2000: loss 0.207355\n",
      "iteration 1400 / 2000: loss 0.255126\n",
      "iteration 1500 / 2000: loss 0.213784\n",
      "iteration 1600 / 2000: loss 0.201267\n",
      "iteration 1700 / 2000: loss 0.259125\n",
      "iteration 1800 / 2000: loss 0.264007\n",
      "iteration 1900 / 2000: loss 0.204650\n",
      "Hidden Size: 80, Learning Rate: 0.002500074999999999, LR Decay: 0.8175, Regularization: 0.4, Validation accuracy: 0.9622\n",
      "iteration 0 / 2000: loss 2.302582\n",
      "iteration 100 / 2000: loss 0.430294\n",
      "iteration 200 / 2000: loss 0.356011\n",
      "iteration 300 / 2000: loss 0.230891\n",
      "iteration 400 / 2000: loss 0.232052\n",
      "iteration 500 / 2000: loss 0.211712\n",
      "iteration 600 / 2000: loss 0.195869\n",
      "iteration 700 / 2000: loss 0.153674\n",
      "iteration 800 / 2000: loss 0.148333\n",
      "iteration 900 / 2000: loss 0.170818\n",
      "iteration 1000 / 2000: loss 0.233907\n",
      "iteration 1100 / 2000: loss 0.185607\n",
      "iteration 1200 / 2000: loss 0.181349\n",
      "iteration 1300 / 2000: loss 0.222287\n",
      "iteration 1400 / 2000: loss 0.197175\n",
      "iteration 1500 / 2000: loss 0.196267\n",
      "iteration 1600 / 2000: loss 0.143543\n",
      "iteration 1700 / 2000: loss 0.130874\n",
      "iteration 1800 / 2000: loss 0.174691\n",
      "iteration 1900 / 2000: loss 0.137306\n",
      "Hidden Size: 80, Learning Rate: 0.002500074999999999, LR Decay: 0.645, Regularization: 0.0, Validation accuracy: 0.9586\n",
      "iteration 0 / 2000: loss 2.302628\n",
      "iteration 100 / 2000: loss 0.469346\n",
      "iteration 200 / 2000: loss 0.266193\n",
      "iteration 300 / 2000: loss 0.360837\n",
      "iteration 400 / 2000: loss 0.220043\n",
      "iteration 500 / 2000: loss 0.267738\n",
      "iteration 600 / 2000: loss 0.162812\n",
      "iteration 700 / 2000: loss 0.203738\n",
      "iteration 800 / 2000: loss 0.178899\n",
      "iteration 900 / 2000: loss 0.242747\n",
      "iteration 1000 / 2000: loss 0.185083\n",
      "iteration 1100 / 2000: loss 0.206925\n",
      "iteration 1200 / 2000: loss 0.184477\n",
      "iteration 1300 / 2000: loss 0.140434\n",
      "iteration 1400 / 2000: loss 0.157462\n",
      "iteration 1500 / 2000: loss 0.136623\n",
      "iteration 1600 / 2000: loss 0.131115\n",
      "iteration 1700 / 2000: loss 0.169933\n",
      "iteration 1800 / 2000: loss 0.177209\n",
      "iteration 1900 / 2000: loss 0.183863\n",
      "Hidden Size: 80, Learning Rate: 0.002500074999999999, LR Decay: 0.645, Regularization: 0.1, Validation accuracy: 0.9556\n",
      "iteration 0 / 2000: loss 2.302638\n",
      "iteration 100 / 2000: loss 0.453507\n",
      "iteration 200 / 2000: loss 0.276511\n",
      "iteration 300 / 2000: loss 0.191192\n",
      "iteration 400 / 2000: loss 0.321211\n",
      "iteration 500 / 2000: loss 0.220789\n",
      "iteration 600 / 2000: loss 0.196174\n",
      "iteration 700 / 2000: loss 0.260958\n",
      "iteration 800 / 2000: loss 0.221997\n",
      "iteration 900 / 2000: loss 0.210706\n",
      "iteration 1000 / 2000: loss 0.146493\n",
      "iteration 1100 / 2000: loss 0.248337\n",
      "iteration 1200 / 2000: loss 0.250634\n",
      "iteration 1300 / 2000: loss 0.181900\n",
      "iteration 1400 / 2000: loss 0.230312\n",
      "iteration 1500 / 2000: loss 0.143532\n",
      "iteration 1600 / 2000: loss 0.245746\n",
      "iteration 1700 / 2000: loss 0.198392\n",
      "iteration 1800 / 2000: loss 0.179132\n",
      "iteration 1900 / 2000: loss 0.217391\n",
      "Hidden Size: 80, Learning Rate: 0.002500074999999999, LR Decay: 0.645, Regularization: 0.2, Validation accuracy: 0.9566\n",
      "iteration 0 / 2000: loss 2.302679\n",
      "iteration 100 / 2000: loss 0.515622\n",
      "iteration 200 / 2000: loss 0.363604\n",
      "iteration 300 / 2000: loss 0.284185\n",
      "iteration 400 / 2000: loss 0.293712\n",
      "iteration 500 / 2000: loss 0.177298\n",
      "iteration 600 / 2000: loss 0.205976\n",
      "iteration 700 / 2000: loss 0.224445\n",
      "iteration 800 / 2000: loss 0.285607\n",
      "iteration 900 / 2000: loss 0.223154\n",
      "iteration 1000 / 2000: loss 0.233930\n",
      "iteration 1100 / 2000: loss 0.258459\n",
      "iteration 1200 / 2000: loss 0.267485\n",
      "iteration 1300 / 2000: loss 0.246700\n",
      "iteration 1400 / 2000: loss 0.183906\n",
      "iteration 1500 / 2000: loss 0.238046\n",
      "iteration 1600 / 2000: loss 0.234467\n",
      "iteration 1700 / 2000: loss 0.246793\n",
      "iteration 1800 / 2000: loss 0.244165\n",
      "iteration 1900 / 2000: loss 0.183191\n",
      "Hidden Size: 80, Learning Rate: 0.002500074999999999, LR Decay: 0.645, Regularization: 0.30000000000000004, Validation accuracy: 0.9542\n",
      "iteration 0 / 2000: loss 2.302686\n",
      "iteration 100 / 2000: loss 0.566168\n",
      "iteration 200 / 2000: loss 0.364480\n",
      "iteration 300 / 2000: loss 0.252156\n",
      "iteration 400 / 2000: loss 0.291368\n",
      "iteration 500 / 2000: loss 0.204577\n",
      "iteration 600 / 2000: loss 0.295693\n",
      "iteration 700 / 2000: loss 0.203040\n",
      "iteration 800 / 2000: loss 0.255755\n",
      "iteration 900 / 2000: loss 0.233934\n",
      "iteration 1000 / 2000: loss 0.204364\n",
      "iteration 1100 / 2000: loss 0.207040\n",
      "iteration 1200 / 2000: loss 0.216516\n",
      "iteration 1300 / 2000: loss 0.304352\n",
      "iteration 1400 / 2000: loss 0.232763\n",
      "iteration 1500 / 2000: loss 0.223460\n",
      "iteration 1600 / 2000: loss 0.257500\n",
      "iteration 1700 / 2000: loss 0.250930\n",
      "iteration 1800 / 2000: loss 0.238642\n",
      "iteration 1900 / 2000: loss 0.211506\n",
      "Hidden Size: 80, Learning Rate: 0.002500074999999999, LR Decay: 0.645, Regularization: 0.4, Validation accuracy: 0.9518\n",
      "iteration 0 / 2000: loss 2.302603\n",
      "iteration 100 / 2000: loss 0.644756\n",
      "iteration 200 / 2000: loss 0.385957\n",
      "iteration 300 / 2000: loss 0.337158\n",
      "iteration 400 / 2000: loss 0.217539\n",
      "iteration 500 / 2000: loss 0.231495\n",
      "iteration 600 / 2000: loss 0.224892\n",
      "iteration 700 / 2000: loss 0.214177\n",
      "iteration 800 / 2000: loss 0.261393\n",
      "iteration 900 / 2000: loss 0.263341\n",
      "iteration 1000 / 2000: loss 0.166685\n",
      "iteration 1100 / 2000: loss 0.230455\n",
      "iteration 1200 / 2000: loss 0.216963\n",
      "iteration 1300 / 2000: loss 0.176162\n",
      "iteration 1400 / 2000: loss 0.308666\n",
      "iteration 1500 / 2000: loss 0.262379\n",
      "iteration 1600 / 2000: loss 0.370056\n",
      "iteration 1700 / 2000: loss 0.294029\n",
      "iteration 1800 / 2000: loss 0.235970\n",
      "iteration 1900 / 2000: loss 0.228166\n",
      "Hidden Size: 80, Learning Rate: 0.002500074999999999, LR Decay: 0.47250000000000003, Regularization: 0.0, Validation accuracy: 0.9368\n",
      "iteration 0 / 2000: loss 2.302622\n",
      "iteration 100 / 2000: loss 0.668298\n",
      "iteration 200 / 2000: loss 0.325672\n",
      "iteration 300 / 2000: loss 0.293192\n",
      "iteration 400 / 2000: loss 0.357030\n",
      "iteration 500 / 2000: loss 0.259316\n",
      "iteration 600 / 2000: loss 0.238565\n",
      "iteration 700 / 2000: loss 0.204156\n",
      "iteration 800 / 2000: loss 0.296322\n",
      "iteration 900 / 2000: loss 0.326897\n",
      "iteration 1000 / 2000: loss 0.152894\n",
      "iteration 1100 / 2000: loss 0.286084\n",
      "iteration 1200 / 2000: loss 0.258613\n",
      "iteration 1300 / 2000: loss 0.247735\n",
      "iteration 1400 / 2000: loss 0.182751\n",
      "iteration 1500 / 2000: loss 0.219309\n",
      "iteration 1600 / 2000: loss 0.188280\n",
      "iteration 1700 / 2000: loss 0.254465\n",
      "iteration 1800 / 2000: loss 0.245235\n",
      "iteration 1900 / 2000: loss 0.263375\n",
      "Hidden Size: 80, Learning Rate: 0.002500074999999999, LR Decay: 0.47250000000000003, Regularization: 0.1, Validation accuracy: 0.9374\n",
      "iteration 0 / 2000: loss 2.302627\n",
      "iteration 100 / 2000: loss 0.674927\n",
      "iteration 200 / 2000: loss 0.390344\n",
      "iteration 300 / 2000: loss 0.355415\n",
      "iteration 400 / 2000: loss 0.254608\n",
      "iteration 500 / 2000: loss 0.216326\n",
      "iteration 600 / 2000: loss 0.327126\n",
      "iteration 700 / 2000: loss 0.332574\n",
      "iteration 800 / 2000: loss 0.298742\n",
      "iteration 900 / 2000: loss 0.235408\n",
      "iteration 1000 / 2000: loss 0.334320\n",
      "iteration 1100 / 2000: loss 0.207025\n",
      "iteration 1200 / 2000: loss 0.282848\n",
      "iteration 1300 / 2000: loss 0.275710\n",
      "iteration 1400 / 2000: loss 0.367274\n",
      "iteration 1500 / 2000: loss 0.393315\n",
      "iteration 1600 / 2000: loss 0.325767\n",
      "iteration 1700 / 2000: loss 0.326193\n",
      "iteration 1800 / 2000: loss 0.292172\n",
      "iteration 1900 / 2000: loss 0.223742\n",
      "Hidden Size: 80, Learning Rate: 0.002500074999999999, LR Decay: 0.47250000000000003, Regularization: 0.2, Validation accuracy: 0.935\n",
      "iteration 0 / 2000: loss 2.302699\n",
      "iteration 100 / 2000: loss 0.703337\n",
      "iteration 200 / 2000: loss 0.376620\n",
      "iteration 300 / 2000: loss 0.365980\n",
      "iteration 400 / 2000: loss 0.302124\n",
      "iteration 500 / 2000: loss 0.332919\n",
      "iteration 600 / 2000: loss 0.392773\n",
      "iteration 700 / 2000: loss 0.300417\n",
      "iteration 800 / 2000: loss 0.234232\n",
      "iteration 900 / 2000: loss 0.285239\n",
      "iteration 1000 / 2000: loss 0.336154\n",
      "iteration 1100 / 2000: loss 0.230337\n",
      "iteration 1200 / 2000: loss 0.275288\n",
      "iteration 1300 / 2000: loss 0.202486\n",
      "iteration 1400 / 2000: loss 0.307920\n",
      "iteration 1500 / 2000: loss 0.355696\n",
      "iteration 1600 / 2000: loss 0.326344\n",
      "iteration 1700 / 2000: loss 0.208761\n",
      "iteration 1800 / 2000: loss 0.325923\n",
      "iteration 1900 / 2000: loss 0.285538\n",
      "Hidden Size: 80, Learning Rate: 0.002500074999999999, LR Decay: 0.47250000000000003, Regularization: 0.30000000000000004, Validation accuracy: 0.9332\n",
      "iteration 0 / 2000: loss 2.302678\n",
      "iteration 100 / 2000: loss 0.707130\n",
      "iteration 200 / 2000: loss 0.362726\n",
      "iteration 300 / 2000: loss 0.352010\n",
      "iteration 400 / 2000: loss 0.318871\n",
      "iteration 500 / 2000: loss 0.348351\n",
      "iteration 600 / 2000: loss 0.344068\n",
      "iteration 700 / 2000: loss 0.293218\n",
      "iteration 800 / 2000: loss 0.263499\n",
      "iteration 900 / 2000: loss 0.270036\n",
      "iteration 1000 / 2000: loss 0.383130\n",
      "iteration 1100 / 2000: loss 0.296268\n",
      "iteration 1200 / 2000: loss 0.276385\n",
      "iteration 1300 / 2000: loss 0.315996\n",
      "iteration 1400 / 2000: loss 0.350001\n",
      "iteration 1500 / 2000: loss 0.410754\n",
      "iteration 1600 / 2000: loss 0.277738\n",
      "iteration 1700 / 2000: loss 0.231797\n",
      "iteration 1800 / 2000: loss 0.274085\n",
      "iteration 1900 / 2000: loss 0.283399\n",
      "Hidden Size: 80, Learning Rate: 0.002500074999999999, LR Decay: 0.47250000000000003, Regularization: 0.4, Validation accuracy: 0.9326\n",
      "iteration 0 / 2000: loss 2.302577\n",
      "iteration 100 / 2000: loss 1.249016\n",
      "iteration 200 / 2000: loss 0.415755\n",
      "iteration 300 / 2000: loss 0.293930\n",
      "iteration 400 / 2000: loss 0.330436\n",
      "iteration 500 / 2000: loss 0.362898\n",
      "iteration 600 / 2000: loss 0.343443\n",
      "iteration 700 / 2000: loss 0.272683\n",
      "iteration 800 / 2000: loss 0.269774\n",
      "iteration 900 / 2000: loss 0.360393\n",
      "iteration 1000 / 2000: loss 0.293516\n",
      "iteration 1100 / 2000: loss 0.270252\n",
      "iteration 1200 / 2000: loss 0.290527\n",
      "iteration 1300 / 2000: loss 0.295949\n",
      "iteration 1400 / 2000: loss 0.321537\n",
      "iteration 1500 / 2000: loss 0.449748\n",
      "iteration 1600 / 2000: loss 0.355588\n",
      "iteration 1700 / 2000: loss 0.288740\n",
      "iteration 1800 / 2000: loss 0.351521\n",
      "iteration 1900 / 2000: loss 0.207523\n",
      "Hidden Size: 80, Learning Rate: 0.002500074999999999, LR Decay: 0.3, Regularization: 0.0, Validation accuracy: 0.9152\n",
      "iteration 0 / 2000: loss 2.302630\n",
      "iteration 100 / 2000: loss 1.185970\n",
      "iteration 200 / 2000: loss 0.483390\n",
      "iteration 300 / 2000: loss 0.354540\n",
      "iteration 400 / 2000: loss 0.306898\n",
      "iteration 500 / 2000: loss 0.328440\n",
      "iteration 600 / 2000: loss 0.228526\n",
      "iteration 700 / 2000: loss 0.278547\n",
      "iteration 800 / 2000: loss 0.388597\n",
      "iteration 900 / 2000: loss 0.354477\n",
      "iteration 1000 / 2000: loss 0.353623\n",
      "iteration 1100 / 2000: loss 0.422941\n",
      "iteration 1200 / 2000: loss 0.323818\n",
      "iteration 1300 / 2000: loss 0.300689\n",
      "iteration 1400 / 2000: loss 0.244989\n",
      "iteration 1500 / 2000: loss 0.320964\n",
      "iteration 1600 / 2000: loss 0.261960\n",
      "iteration 1700 / 2000: loss 0.253806\n",
      "iteration 1800 / 2000: loss 0.347435\n",
      "iteration 1900 / 2000: loss 0.280424\n",
      "Hidden Size: 80, Learning Rate: 0.002500074999999999, LR Decay: 0.3, Regularization: 0.1, Validation accuracy: 0.916\n",
      "iteration 0 / 2000: loss 2.302650\n",
      "iteration 100 / 2000: loss 1.171075\n",
      "iteration 200 / 2000: loss 0.398773\n",
      "iteration 300 / 2000: loss 0.341864\n",
      "iteration 400 / 2000: loss 0.419416\n",
      "iteration 500 / 2000: loss 0.301269\n",
      "iteration 600 / 2000: loss 0.304269\n",
      "iteration 700 / 2000: loss 0.430295\n",
      "iteration 800 / 2000: loss 0.245352\n",
      "iteration 900 / 2000: loss 0.300109\n",
      "iteration 1000 / 2000: loss 0.317808\n",
      "iteration 1100 / 2000: loss 0.298315\n",
      "iteration 1200 / 2000: loss 0.358806\n",
      "iteration 1300 / 2000: loss 0.327866\n",
      "iteration 1400 / 2000: loss 0.292821\n",
      "iteration 1500 / 2000: loss 0.344681\n",
      "iteration 1600 / 2000: loss 0.380100\n",
      "iteration 1700 / 2000: loss 0.352433\n",
      "iteration 1800 / 2000: loss 0.317582\n",
      "iteration 1900 / 2000: loss 0.266084\n",
      "Hidden Size: 80, Learning Rate: 0.002500074999999999, LR Decay: 0.3, Regularization: 0.2, Validation accuracy: 0.9162\n",
      "iteration 0 / 2000: loss 2.302679\n",
      "iteration 100 / 2000: loss 1.206531\n",
      "iteration 200 / 2000: loss 0.397247\n",
      "iteration 300 / 2000: loss 0.307299\n",
      "iteration 400 / 2000: loss 0.555496\n",
      "iteration 500 / 2000: loss 0.416275\n",
      "iteration 600 / 2000: loss 0.362574\n",
      "iteration 700 / 2000: loss 0.342586\n",
      "iteration 800 / 2000: loss 0.343119\n",
      "iteration 900 / 2000: loss 0.294041\n",
      "iteration 1000 / 2000: loss 0.238722\n",
      "iteration 1100 / 2000: loss 0.344057\n",
      "iteration 1200 / 2000: loss 0.250447\n",
      "iteration 1300 / 2000: loss 0.390072\n",
      "iteration 1400 / 2000: loss 0.387114\n",
      "iteration 1500 / 2000: loss 0.374239\n",
      "iteration 1600 / 2000: loss 0.276993\n",
      "iteration 1700 / 2000: loss 0.389972\n",
      "iteration 1800 / 2000: loss 0.292436\n",
      "iteration 1900 / 2000: loss 0.369965\n",
      "Hidden Size: 80, Learning Rate: 0.002500074999999999, LR Decay: 0.3, Regularization: 0.30000000000000004, Validation accuracy: 0.915\n",
      "iteration 0 / 2000: loss 2.302738\n",
      "iteration 100 / 2000: loss 1.097398\n",
      "iteration 200 / 2000: loss 0.643364\n",
      "iteration 300 / 2000: loss 0.417437\n",
      "iteration 400 / 2000: loss 0.423617\n",
      "iteration 500 / 2000: loss 0.283026\n",
      "iteration 600 / 2000: loss 0.387127\n",
      "iteration 700 / 2000: loss 0.395547\n",
      "iteration 800 / 2000: loss 0.464609\n",
      "iteration 900 / 2000: loss 0.426233\n",
      "iteration 1000 / 2000: loss 0.353704\n",
      "iteration 1100 / 2000: loss 0.386980\n",
      "iteration 1200 / 2000: loss 0.364891\n",
      "iteration 1300 / 2000: loss 0.370082\n",
      "iteration 1400 / 2000: loss 0.312628\n",
      "iteration 1500 / 2000: loss 0.357677\n",
      "iteration 1600 / 2000: loss 0.499515\n",
      "iteration 1700 / 2000: loss 0.497213\n",
      "iteration 1800 / 2000: loss 0.344062\n",
      "iteration 1900 / 2000: loss 0.433136\n",
      "Hidden Size: 80, Learning Rate: 0.002500074999999999, LR Decay: 0.3, Regularization: 0.4, Validation accuracy: 0.9158\n",
      "iteration 0 / 2000: loss 2.302578\n",
      "iteration 100 / 2000: loss 2.302586\n",
      "iteration 200 / 2000: loss 2.302577\n",
      "iteration 300 / 2000: loss 2.302580\n",
      "iteration 400 / 2000: loss 2.302582\n",
      "iteration 500 / 2000: loss 2.302571\n",
      "iteration 600 / 2000: loss 2.302575\n",
      "iteration 700 / 2000: loss 2.302582\n",
      "iteration 800 / 2000: loss 2.302575\n",
      "iteration 900 / 2000: loss 2.302582\n",
      "iteration 1000 / 2000: loss 2.302579\n",
      "iteration 1100 / 2000: loss 2.302574\n",
      "iteration 1200 / 2000: loss 2.302576\n",
      "iteration 1300 / 2000: loss 2.302572\n",
      "iteration 1400 / 2000: loss 2.302582\n",
      "iteration 1500 / 2000: loss 2.302573\n",
      "iteration 1600 / 2000: loss 2.302563\n",
      "iteration 1700 / 2000: loss 2.302571\n",
      "iteration 1800 / 2000: loss 2.302579\n",
      "iteration 1900 / 2000: loss 2.302564\n",
      "Hidden Size: 80, Learning Rate: 1e-07, LR Decay: 0.99, Regularization: 0.0, Validation accuracy: 0.1028\n",
      "iteration 0 / 2000: loss 2.302606\n",
      "iteration 100 / 2000: loss 2.302613\n",
      "iteration 200 / 2000: loss 2.302612\n",
      "iteration 300 / 2000: loss 2.302606\n",
      "iteration 400 / 2000: loss 2.302603\n",
      "iteration 500 / 2000: loss 2.302610\n",
      "iteration 600 / 2000: loss 2.302591\n",
      "iteration 700 / 2000: loss 2.302600\n",
      "iteration 800 / 2000: loss 2.302609\n",
      "iteration 900 / 2000: loss 2.302607\n",
      "iteration 1000 / 2000: loss 2.302602\n",
      "iteration 1100 / 2000: loss 2.302615\n",
      "iteration 1200 / 2000: loss 2.302599\n",
      "iteration 1300 / 2000: loss 2.302606\n",
      "iteration 1400 / 2000: loss 2.302587\n",
      "iteration 1500 / 2000: loss 2.302603\n",
      "iteration 1600 / 2000: loss 2.302606\n",
      "iteration 1700 / 2000: loss 2.302593\n",
      "iteration 1800 / 2000: loss 2.302612\n",
      "iteration 1900 / 2000: loss 2.302594\n",
      "Hidden Size: 80, Learning Rate: 1e-07, LR Decay: 0.99, Regularization: 0.1, Validation accuracy: 0.0982\n",
      "iteration 0 / 2000: loss 2.302665\n",
      "iteration 100 / 2000: loss 2.302662\n",
      "iteration 200 / 2000: loss 2.302655\n",
      "iteration 300 / 2000: loss 2.302662\n",
      "iteration 400 / 2000: loss 2.302658\n",
      "iteration 500 / 2000: loss 2.302658\n",
      "iteration 600 / 2000: loss 2.302652\n",
      "iteration 700 / 2000: loss 2.302650\n",
      "iteration 800 / 2000: loss 2.302666\n",
      "iteration 900 / 2000: loss 2.302645\n",
      "iteration 1000 / 2000: loss 2.302653\n",
      "iteration 1100 / 2000: loss 2.302670\n",
      "iteration 1200 / 2000: loss 2.302659\n",
      "iteration 1300 / 2000: loss 2.302657\n",
      "iteration 1400 / 2000: loss 2.302655\n",
      "iteration 1500 / 2000: loss 2.302650\n",
      "iteration 1600 / 2000: loss 2.302660\n",
      "iteration 1700 / 2000: loss 2.302652\n",
      "iteration 1800 / 2000: loss 2.302665\n",
      "iteration 1900 / 2000: loss 2.302657\n",
      "Hidden Size: 80, Learning Rate: 1e-07, LR Decay: 0.99, Regularization: 0.2, Validation accuracy: 0.1032\n",
      "iteration 0 / 2000: loss 2.302676\n",
      "iteration 100 / 2000: loss 2.302671\n",
      "iteration 200 / 2000: loss 2.302686\n",
      "iteration 300 / 2000: loss 2.302673\n",
      "iteration 400 / 2000: loss 2.302673\n",
      "iteration 500 / 2000: loss 2.302655\n",
      "iteration 600 / 2000: loss 2.302668\n",
      "iteration 700 / 2000: loss 2.302690\n",
      "iteration 800 / 2000: loss 2.302669\n",
      "iteration 900 / 2000: loss 2.302672\n",
      "iteration 1000 / 2000: loss 2.302673\n",
      "iteration 1100 / 2000: loss 2.302670\n",
      "iteration 1200 / 2000: loss 2.302662\n",
      "iteration 1300 / 2000: loss 2.302690\n",
      "iteration 1400 / 2000: loss 2.302679\n",
      "iteration 1500 / 2000: loss 2.302658\n",
      "iteration 1600 / 2000: loss 2.302678\n",
      "iteration 1700 / 2000: loss 2.302669\n",
      "iteration 1800 / 2000: loss 2.302668\n",
      "iteration 1900 / 2000: loss 2.302684\n",
      "Hidden Size: 80, Learning Rate: 1e-07, LR Decay: 0.99, Regularization: 0.30000000000000004, Validation accuracy: 0.1316\n",
      "iteration 0 / 2000: loss 2.302682\n",
      "iteration 100 / 2000: loss 2.302677\n",
      "iteration 200 / 2000: loss 2.302695\n",
      "iteration 300 / 2000: loss 2.302694\n",
      "iteration 400 / 2000: loss 2.302689\n",
      "iteration 500 / 2000: loss 2.302683\n",
      "iteration 600 / 2000: loss 2.302683\n",
      "iteration 700 / 2000: loss 2.302705\n",
      "iteration 800 / 2000: loss 2.302683\n",
      "iteration 900 / 2000: loss 2.302689\n",
      "iteration 1000 / 2000: loss 2.302684\n",
      "iteration 1100 / 2000: loss 2.302693\n",
      "iteration 1200 / 2000: loss 2.302685\n",
      "iteration 1300 / 2000: loss 2.302693\n",
      "iteration 1400 / 2000: loss 2.302685\n",
      "iteration 1500 / 2000: loss 2.302691\n",
      "iteration 1600 / 2000: loss 2.302696\n",
      "iteration 1700 / 2000: loss 2.302684\n",
      "iteration 1800 / 2000: loss 2.302691\n",
      "iteration 1900 / 2000: loss 2.302691\n",
      "Hidden Size: 80, Learning Rate: 1e-07, LR Decay: 0.99, Regularization: 0.4, Validation accuracy: 0.1236\n",
      "iteration 0 / 2000: loss 2.302584\n",
      "iteration 100 / 2000: loss 2.302586\n",
      "iteration 200 / 2000: loss 2.302573\n",
      "iteration 300 / 2000: loss 2.302565\n",
      "iteration 400 / 2000: loss 2.302574\n",
      "iteration 500 / 2000: loss 2.302577\n",
      "iteration 600 / 2000: loss 2.302569\n",
      "iteration 700 / 2000: loss 2.302577\n",
      "iteration 800 / 2000: loss 2.302570\n",
      "iteration 900 / 2000: loss 2.302570\n",
      "iteration 1000 / 2000: loss 2.302583\n",
      "iteration 1100 / 2000: loss 2.302582\n",
      "iteration 1200 / 2000: loss 2.302577\n",
      "iteration 1300 / 2000: loss 2.302566\n",
      "iteration 1400 / 2000: loss 2.302566\n",
      "iteration 1500 / 2000: loss 2.302578\n",
      "iteration 1600 / 2000: loss 2.302578\n",
      "iteration 1700 / 2000: loss 2.302585\n",
      "iteration 1800 / 2000: loss 2.302576\n",
      "iteration 1900 / 2000: loss 2.302571\n",
      "Hidden Size: 80, Learning Rate: 1e-07, LR Decay: 0.8175, Regularization: 0.0, Validation accuracy: 0.104\n",
      "iteration 0 / 2000: loss 2.302621\n",
      "iteration 100 / 2000: loss 2.302629\n",
      "iteration 200 / 2000: loss 2.302605\n",
      "iteration 300 / 2000: loss 2.302615\n",
      "iteration 400 / 2000: loss 2.302609\n",
      "iteration 500 / 2000: loss 2.302624\n",
      "iteration 600 / 2000: loss 2.302610\n",
      "iteration 700 / 2000: loss 2.302609\n",
      "iteration 800 / 2000: loss 2.302612\n",
      "iteration 900 / 2000: loss 2.302617\n",
      "iteration 1000 / 2000: loss 2.302613\n",
      "iteration 1100 / 2000: loss 2.302609\n",
      "iteration 1200 / 2000: loss 2.302617\n",
      "iteration 1300 / 2000: loss 2.302614\n",
      "iteration 1400 / 2000: loss 2.302598\n",
      "iteration 1500 / 2000: loss 2.302614\n",
      "iteration 1600 / 2000: loss 2.302625\n",
      "iteration 1700 / 2000: loss 2.302623\n",
      "iteration 1800 / 2000: loss 2.302606\n",
      "iteration 1900 / 2000: loss 2.302612\n",
      "Hidden Size: 80, Learning Rate: 1e-07, LR Decay: 0.8175, Regularization: 0.1, Validation accuracy: 0.1016\n",
      "iteration 0 / 2000: loss 2.302648\n",
      "iteration 100 / 2000: loss 2.302657\n",
      "iteration 200 / 2000: loss 2.302649\n",
      "iteration 300 / 2000: loss 2.302652\n",
      "iteration 400 / 2000: loss 2.302643\n",
      "iteration 500 / 2000: loss 2.302637\n",
      "iteration 600 / 2000: loss 2.302646\n",
      "iteration 700 / 2000: loss 2.302657\n",
      "iteration 800 / 2000: loss 2.302644\n",
      "iteration 900 / 2000: loss 2.302641\n",
      "iteration 1000 / 2000: loss 2.302641\n",
      "iteration 1100 / 2000: loss 2.302634\n",
      "iteration 1200 / 2000: loss 2.302642\n",
      "iteration 1300 / 2000: loss 2.302644\n",
      "iteration 1400 / 2000: loss 2.302642\n",
      "iteration 1500 / 2000: loss 2.302641\n",
      "iteration 1600 / 2000: loss 2.302640\n",
      "iteration 1700 / 2000: loss 2.302639\n",
      "iteration 1800 / 2000: loss 2.302658\n",
      "iteration 1900 / 2000: loss 2.302641\n",
      "Hidden Size: 80, Learning Rate: 1e-07, LR Decay: 0.8175, Regularization: 0.2, Validation accuracy: 0.1156\n",
      "iteration 0 / 2000: loss 2.302674\n",
      "iteration 100 / 2000: loss 2.302686\n",
      "iteration 200 / 2000: loss 2.302668\n",
      "iteration 300 / 2000: loss 2.302684\n",
      "iteration 400 / 2000: loss 2.302677\n",
      "iteration 500 / 2000: loss 2.302676\n",
      "iteration 600 / 2000: loss 2.302669\n",
      "iteration 700 / 2000: loss 2.302677\n",
      "iteration 800 / 2000: loss 2.302667\n",
      "iteration 900 / 2000: loss 2.302687\n",
      "iteration 1000 / 2000: loss 2.302680\n",
      "iteration 1100 / 2000: loss 2.302672\n",
      "iteration 1200 / 2000: loss 2.302664\n",
      "iteration 1300 / 2000: loss 2.302665\n",
      "iteration 1400 / 2000: loss 2.302667\n",
      "iteration 1500 / 2000: loss 2.302683\n",
      "iteration 1600 / 2000: loss 2.302670\n",
      "iteration 1700 / 2000: loss 2.302669\n",
      "iteration 1800 / 2000: loss 2.302678\n",
      "iteration 1900 / 2000: loss 2.302672\n",
      "Hidden Size: 80, Learning Rate: 1e-07, LR Decay: 0.8175, Regularization: 0.30000000000000004, Validation accuracy: 0.1132\n",
      "iteration 0 / 2000: loss 2.302716\n",
      "iteration 100 / 2000: loss 2.302718\n",
      "iteration 200 / 2000: loss 2.302710\n",
      "iteration 300 / 2000: loss 2.302712\n",
      "iteration 400 / 2000: loss 2.302721\n",
      "iteration 500 / 2000: loss 2.302722\n",
      "iteration 600 / 2000: loss 2.302724\n",
      "iteration 700 / 2000: loss 2.302706\n",
      "iteration 800 / 2000: loss 2.302714\n",
      "iteration 900 / 2000: loss 2.302710\n",
      "iteration 1000 / 2000: loss 2.302701\n",
      "iteration 1100 / 2000: loss 2.302720\n",
      "iteration 1200 / 2000: loss 2.302721\n",
      "iteration 1300 / 2000: loss 2.302706\n",
      "iteration 1400 / 2000: loss 2.302703\n",
      "iteration 1500 / 2000: loss 2.302707\n",
      "iteration 1600 / 2000: loss 2.302712\n",
      "iteration 1700 / 2000: loss 2.302713\n",
      "iteration 1800 / 2000: loss 2.302703\n",
      "iteration 1900 / 2000: loss 2.302717\n",
      "Hidden Size: 80, Learning Rate: 1e-07, LR Decay: 0.8175, Regularization: 0.4, Validation accuracy: 0.0998\n",
      "iteration 0 / 2000: loss 2.302587\n",
      "iteration 100 / 2000: loss 2.302582\n",
      "iteration 200 / 2000: loss 2.302584\n",
      "iteration 300 / 2000: loss 2.302584\n",
      "iteration 400 / 2000: loss 2.302578\n",
      "iteration 500 / 2000: loss 2.302586\n",
      "iteration 600 / 2000: loss 2.302589\n",
      "iteration 700 / 2000: loss 2.302558\n",
      "iteration 800 / 2000: loss 2.302585\n",
      "iteration 900 / 2000: loss 2.302583\n",
      "iteration 1000 / 2000: loss 2.302573\n",
      "iteration 1100 / 2000: loss 2.302587\n",
      "iteration 1200 / 2000: loss 2.302602\n",
      "iteration 1300 / 2000: loss 2.302585\n",
      "iteration 1400 / 2000: loss 2.302584\n",
      "iteration 1500 / 2000: loss 2.302577\n",
      "iteration 1600 / 2000: loss 2.302586\n",
      "iteration 1700 / 2000: loss 2.302592\n",
      "iteration 1800 / 2000: loss 2.302577\n",
      "iteration 1900 / 2000: loss 2.302575\n",
      "Hidden Size: 80, Learning Rate: 1e-07, LR Decay: 0.645, Regularization: 0.0, Validation accuracy: 0.1054\n",
      "iteration 0 / 2000: loss 2.302629\n",
      "iteration 100 / 2000: loss 2.302620\n",
      "iteration 200 / 2000: loss 2.302623\n",
      "iteration 300 / 2000: loss 2.302597\n",
      "iteration 400 / 2000: loss 2.302615\n",
      "iteration 500 / 2000: loss 2.302612\n",
      "iteration 600 / 2000: loss 2.302618\n",
      "iteration 700 / 2000: loss 2.302623\n",
      "iteration 800 / 2000: loss 2.302649\n",
      "iteration 900 / 2000: loss 2.302627\n",
      "iteration 1000 / 2000: loss 2.302630\n",
      "iteration 1100 / 2000: loss 2.302620\n",
      "iteration 1200 / 2000: loss 2.302626\n",
      "iteration 1300 / 2000: loss 2.302636\n",
      "iteration 1400 / 2000: loss 2.302617\n",
      "iteration 1500 / 2000: loss 2.302631\n",
      "iteration 1600 / 2000: loss 2.302624\n",
      "iteration 1700 / 2000: loss 2.302642\n",
      "iteration 1800 / 2000: loss 2.302636\n",
      "iteration 1900 / 2000: loss 2.302623\n",
      "Hidden Size: 80, Learning Rate: 1e-07, LR Decay: 0.645, Regularization: 0.1, Validation accuracy: 0.0794\n",
      "iteration 0 / 2000: loss 2.302651\n",
      "iteration 100 / 2000: loss 2.302640\n",
      "iteration 200 / 2000: loss 2.302640\n",
      "iteration 300 / 2000: loss 2.302646\n",
      "iteration 400 / 2000: loss 2.302644\n",
      "iteration 500 / 2000: loss 2.302639\n",
      "iteration 600 / 2000: loss 2.302660\n",
      "iteration 700 / 2000: loss 2.302640\n",
      "iteration 800 / 2000: loss 2.302650\n",
      "iteration 900 / 2000: loss 2.302644\n",
      "iteration 1000 / 2000: loss 2.302637\n",
      "iteration 1100 / 2000: loss 2.302637\n",
      "iteration 1200 / 2000: loss 2.302643\n",
      "iteration 1300 / 2000: loss 2.302656\n",
      "iteration 1400 / 2000: loss 2.302635\n",
      "iteration 1500 / 2000: loss 2.302645\n",
      "iteration 1600 / 2000: loss 2.302654\n",
      "iteration 1700 / 2000: loss 2.302654\n",
      "iteration 1800 / 2000: loss 2.302643\n",
      "iteration 1900 / 2000: loss 2.302650\n",
      "Hidden Size: 80, Learning Rate: 1e-07, LR Decay: 0.645, Regularization: 0.2, Validation accuracy: 0.1056\n",
      "iteration 0 / 2000: loss 2.302701\n",
      "iteration 100 / 2000: loss 2.302708\n",
      "iteration 200 / 2000: loss 2.302705\n",
      "iteration 300 / 2000: loss 2.302711\n",
      "iteration 400 / 2000: loss 2.302700\n",
      "iteration 500 / 2000: loss 2.302697\n",
      "iteration 600 / 2000: loss 2.302699\n",
      "iteration 700 / 2000: loss 2.302705\n",
      "iteration 800 / 2000: loss 2.302710\n",
      "iteration 900 / 2000: loss 2.302705\n",
      "iteration 1000 / 2000: loss 2.302697\n",
      "iteration 1100 / 2000: loss 2.302693\n",
      "iteration 1200 / 2000: loss 2.302716\n",
      "iteration 1300 / 2000: loss 2.302707\n",
      "iteration 1400 / 2000: loss 2.302706\n",
      "iteration 1500 / 2000: loss 2.302703\n",
      "iteration 1600 / 2000: loss 2.302705\n",
      "iteration 1700 / 2000: loss 2.302704\n",
      "iteration 1800 / 2000: loss 2.302694\n",
      "iteration 1900 / 2000: loss 2.302700\n",
      "Hidden Size: 80, Learning Rate: 1e-07, LR Decay: 0.645, Regularization: 0.30000000000000004, Validation accuracy: 0.061\n",
      "iteration 0 / 2000: loss 2.302733\n",
      "iteration 100 / 2000: loss 2.302730\n",
      "iteration 200 / 2000: loss 2.302712\n",
      "iteration 300 / 2000: loss 2.302722\n",
      "iteration 400 / 2000: loss 2.302720\n",
      "iteration 500 / 2000: loss 2.302733\n",
      "iteration 600 / 2000: loss 2.302723\n",
      "iteration 700 / 2000: loss 2.302723\n",
      "iteration 800 / 2000: loss 2.302713\n",
      "iteration 900 / 2000: loss 2.302733\n",
      "iteration 1000 / 2000: loss 2.302730\n",
      "iteration 1100 / 2000: loss 2.302738\n",
      "iteration 1200 / 2000: loss 2.302723\n",
      "iteration 1300 / 2000: loss 2.302726\n",
      "iteration 1400 / 2000: loss 2.302726\n",
      "iteration 1500 / 2000: loss 2.302718\n",
      "iteration 1600 / 2000: loss 2.302725\n",
      "iteration 1700 / 2000: loss 2.302730\n",
      "iteration 1800 / 2000: loss 2.302734\n",
      "iteration 1900 / 2000: loss 2.302740\n",
      "Hidden Size: 80, Learning Rate: 1e-07, LR Decay: 0.645, Regularization: 0.4, Validation accuracy: 0.0794\n",
      "iteration 0 / 2000: loss 2.302579\n",
      "iteration 100 / 2000: loss 2.302577\n",
      "iteration 200 / 2000: loss 2.302571\n",
      "iteration 300 / 2000: loss 2.302589\n",
      "iteration 400 / 2000: loss 2.302582\n",
      "iteration 500 / 2000: loss 2.302596\n",
      "iteration 600 / 2000: loss 2.302586\n",
      "iteration 700 / 2000: loss 2.302583\n",
      "iteration 800 / 2000: loss 2.302602\n",
      "iteration 900 / 2000: loss 2.302576\n",
      "iteration 1000 / 2000: loss 2.302595\n",
      "iteration 1100 / 2000: loss 2.302580\n",
      "iteration 1200 / 2000: loss 2.302589\n",
      "iteration 1300 / 2000: loss 2.302596\n",
      "iteration 1400 / 2000: loss 2.302587\n",
      "iteration 1500 / 2000: loss 2.302589\n",
      "iteration 1600 / 2000: loss 2.302582\n",
      "iteration 1700 / 2000: loss 2.302604\n",
      "iteration 1800 / 2000: loss 2.302593\n",
      "iteration 1900 / 2000: loss 2.302583\n",
      "Hidden Size: 80, Learning Rate: 1e-07, LR Decay: 0.47250000000000003, Regularization: 0.0, Validation accuracy: 0.0848\n",
      "iteration 0 / 2000: loss 2.302634\n",
      "iteration 100 / 2000: loss 2.302630\n",
      "iteration 200 / 2000: loss 2.302629\n",
      "iteration 300 / 2000: loss 2.302638\n",
      "iteration 400 / 2000: loss 2.302630\n",
      "iteration 500 / 2000: loss 2.302631\n",
      "iteration 600 / 2000: loss 2.302628\n",
      "iteration 700 / 2000: loss 2.302628\n",
      "iteration 800 / 2000: loss 2.302626\n",
      "iteration 900 / 2000: loss 2.302626\n",
      "iteration 1000 / 2000: loss 2.302626\n",
      "iteration 1100 / 2000: loss 2.302637\n",
      "iteration 1200 / 2000: loss 2.302639\n",
      "iteration 1300 / 2000: loss 2.302637\n",
      "iteration 1400 / 2000: loss 2.302622\n",
      "iteration 1500 / 2000: loss 2.302633\n",
      "iteration 1600 / 2000: loss 2.302643\n",
      "iteration 1700 / 2000: loss 2.302626\n",
      "iteration 1800 / 2000: loss 2.302635\n",
      "iteration 1900 / 2000: loss 2.302616\n",
      "Hidden Size: 80, Learning Rate: 1e-07, LR Decay: 0.47250000000000003, Regularization: 0.1, Validation accuracy: 0.0802\n",
      "iteration 0 / 2000: loss 2.302658\n",
      "iteration 100 / 2000: loss 2.302643\n",
      "iteration 200 / 2000: loss 2.302646\n",
      "iteration 300 / 2000: loss 2.302652\n",
      "iteration 400 / 2000: loss 2.302635\n",
      "iteration 500 / 2000: loss 2.302648\n",
      "iteration 600 / 2000: loss 2.302646\n",
      "iteration 700 / 2000: loss 2.302643\n",
      "iteration 800 / 2000: loss 2.302641\n",
      "iteration 900 / 2000: loss 2.302655\n",
      "iteration 1000 / 2000: loss 2.302640\n",
      "iteration 1100 / 2000: loss 2.302638\n",
      "iteration 1200 / 2000: loss 2.302648\n",
      "iteration 1300 / 2000: loss 2.302645\n",
      "iteration 1400 / 2000: loss 2.302642\n",
      "iteration 1500 / 2000: loss 2.302651\n",
      "iteration 1600 / 2000: loss 2.302648\n",
      "iteration 1700 / 2000: loss 2.302639\n",
      "iteration 1800 / 2000: loss 2.302650\n",
      "iteration 1900 / 2000: loss 2.302634\n",
      "Hidden Size: 80, Learning Rate: 1e-07, LR Decay: 0.47250000000000003, Regularization: 0.2, Validation accuracy: 0.1024\n",
      "iteration 0 / 2000: loss 2.302666\n",
      "iteration 100 / 2000: loss 2.302674\n",
      "iteration 200 / 2000: loss 2.302671\n",
      "iteration 300 / 2000: loss 2.302670\n",
      "iteration 400 / 2000: loss 2.302680\n",
      "iteration 500 / 2000: loss 2.302660\n",
      "iteration 600 / 2000: loss 2.302686\n",
      "iteration 700 / 2000: loss 2.302670\n",
      "iteration 800 / 2000: loss 2.302670\n",
      "iteration 900 / 2000: loss 2.302678\n",
      "iteration 1000 / 2000: loss 2.302665\n",
      "iteration 1100 / 2000: loss 2.302667\n",
      "iteration 1200 / 2000: loss 2.302687\n",
      "iteration 1300 / 2000: loss 2.302667\n",
      "iteration 1400 / 2000: loss 2.302689\n",
      "iteration 1500 / 2000: loss 2.302678\n",
      "iteration 1600 / 2000: loss 2.302665\n",
      "iteration 1700 / 2000: loss 2.302683\n",
      "iteration 1800 / 2000: loss 2.302682\n",
      "iteration 1900 / 2000: loss 2.302682\n",
      "Hidden Size: 80, Learning Rate: 1e-07, LR Decay: 0.47250000000000003, Regularization: 0.30000000000000004, Validation accuracy: 0.0894\n",
      "iteration 0 / 2000: loss 2.302737\n",
      "iteration 100 / 2000: loss 2.302725\n",
      "iteration 200 / 2000: loss 2.302730\n",
      "iteration 300 / 2000: loss 2.302729\n",
      "iteration 400 / 2000: loss 2.302734\n",
      "iteration 500 / 2000: loss 2.302721\n",
      "iteration 600 / 2000: loss 2.302725\n",
      "iteration 700 / 2000: loss 2.302721\n",
      "iteration 800 / 2000: loss 2.302725\n",
      "iteration 900 / 2000: loss 2.302729\n",
      "iteration 1000 / 2000: loss 2.302733\n",
      "iteration 1100 / 2000: loss 2.302723\n",
      "iteration 1200 / 2000: loss 2.302719\n",
      "iteration 1300 / 2000: loss 2.302738\n",
      "iteration 1400 / 2000: loss 2.302725\n",
      "iteration 1500 / 2000: loss 2.302722\n",
      "iteration 1600 / 2000: loss 2.302717\n",
      "iteration 1700 / 2000: loss 2.302720\n",
      "iteration 1800 / 2000: loss 2.302735\n",
      "iteration 1900 / 2000: loss 2.302730\n",
      "Hidden Size: 80, Learning Rate: 1e-07, LR Decay: 0.47250000000000003, Regularization: 0.4, Validation accuracy: 0.0516\n",
      "iteration 0 / 2000: loss 2.302597\n",
      "iteration 100 / 2000: loss 2.302591\n",
      "iteration 200 / 2000: loss 2.302609\n",
      "iteration 300 / 2000: loss 2.302601\n",
      "iteration 400 / 2000: loss 2.302603\n",
      "iteration 500 / 2000: loss 2.302622\n",
      "iteration 600 / 2000: loss 2.302593\n",
      "iteration 700 / 2000: loss 2.302576\n",
      "iteration 800 / 2000: loss 2.302599\n",
      "iteration 900 / 2000: loss 2.302603\n",
      "iteration 1000 / 2000: loss 2.302602\n",
      "iteration 1100 / 2000: loss 2.302577\n",
      "iteration 1200 / 2000: loss 2.302573\n",
      "iteration 1300 / 2000: loss 2.302595\n",
      "iteration 1400 / 2000: loss 2.302586\n",
      "iteration 1500 / 2000: loss 2.302596\n",
      "iteration 1600 / 2000: loss 2.302599\n",
      "iteration 1700 / 2000: loss 2.302606\n",
      "iteration 1800 / 2000: loss 2.302584\n",
      "iteration 1900 / 2000: loss 2.302584\n",
      "Hidden Size: 80, Learning Rate: 1e-07, LR Decay: 0.3, Regularization: 0.0, Validation accuracy: 0.0994\n",
      "iteration 0 / 2000: loss 2.302634\n",
      "iteration 100 / 2000: loss 2.302644\n",
      "iteration 200 / 2000: loss 2.302656\n",
      "iteration 300 / 2000: loss 2.302636\n",
      "iteration 400 / 2000: loss 2.302645\n",
      "iteration 500 / 2000: loss 2.302651\n",
      "iteration 600 / 2000: loss 2.302629\n",
      "iteration 700 / 2000: loss 2.302638\n",
      "iteration 800 / 2000: loss 2.302633\n",
      "iteration 900 / 2000: loss 2.302640\n",
      "iteration 1000 / 2000: loss 2.302650\n",
      "iteration 1100 / 2000: loss 2.302641\n",
      "iteration 1200 / 2000: loss 2.302649\n",
      "iteration 1300 / 2000: loss 2.302638\n",
      "iteration 1400 / 2000: loss 2.302645\n",
      "iteration 1500 / 2000: loss 2.302631\n",
      "iteration 1600 / 2000: loss 2.302649\n",
      "iteration 1700 / 2000: loss 2.302643\n",
      "iteration 1800 / 2000: loss 2.302642\n",
      "iteration 1900 / 2000: loss 2.302648\n",
      "Hidden Size: 80, Learning Rate: 1e-07, LR Decay: 0.3, Regularization: 0.1, Validation accuracy: 0.064\n",
      "iteration 0 / 2000: loss 2.302648\n",
      "iteration 100 / 2000: loss 2.302647\n",
      "iteration 200 / 2000: loss 2.302641\n",
      "iteration 300 / 2000: loss 2.302637\n",
      "iteration 400 / 2000: loss 2.302654\n",
      "iteration 500 / 2000: loss 2.302660\n",
      "iteration 600 / 2000: loss 2.302641\n",
      "iteration 700 / 2000: loss 2.302658\n",
      "iteration 800 / 2000: loss 2.302644\n",
      "iteration 900 / 2000: loss 2.302651\n",
      "iteration 1000 / 2000: loss 2.302660\n",
      "iteration 1100 / 2000: loss 2.302647\n",
      "iteration 1200 / 2000: loss 2.302671\n",
      "iteration 1300 / 2000: loss 2.302647\n",
      "iteration 1400 / 2000: loss 2.302637\n",
      "iteration 1500 / 2000: loss 2.302639\n",
      "iteration 1600 / 2000: loss 2.302650\n",
      "iteration 1700 / 2000: loss 2.302638\n",
      "iteration 1800 / 2000: loss 2.302634\n",
      "iteration 1900 / 2000: loss 2.302643\n",
      "Hidden Size: 80, Learning Rate: 1e-07, LR Decay: 0.3, Regularization: 0.2, Validation accuracy: 0.125\n",
      "iteration 0 / 2000: loss 2.302669\n",
      "iteration 100 / 2000: loss 2.302693\n",
      "iteration 200 / 2000: loss 2.302680\n",
      "iteration 300 / 2000: loss 2.302670\n",
      "iteration 400 / 2000: loss 2.302672\n",
      "iteration 500 / 2000: loss 2.302669\n",
      "iteration 600 / 2000: loss 2.302673\n",
      "iteration 700 / 2000: loss 2.302680\n",
      "iteration 800 / 2000: loss 2.302675\n",
      "iteration 900 / 2000: loss 2.302666\n",
      "iteration 1000 / 2000: loss 2.302674\n",
      "iteration 1100 / 2000: loss 2.302681\n",
      "iteration 1200 / 2000: loss 2.302670\n",
      "iteration 1300 / 2000: loss 2.302687\n",
      "iteration 1400 / 2000: loss 2.302675\n",
      "iteration 1500 / 2000: loss 2.302680\n",
      "iteration 1600 / 2000: loss 2.302675\n",
      "iteration 1700 / 2000: loss 2.302657\n",
      "iteration 1800 / 2000: loss 2.302664\n",
      "iteration 1900 / 2000: loss 2.302671\n",
      "Hidden Size: 80, Learning Rate: 1e-07, LR Decay: 0.3, Regularization: 0.30000000000000004, Validation accuracy: 0.0886\n",
      "iteration 0 / 2000: loss 2.302717\n",
      "iteration 100 / 2000: loss 2.302710\n",
      "iteration 200 / 2000: loss 2.302722\n",
      "iteration 300 / 2000: loss 2.302721\n",
      "iteration 400 / 2000: loss 2.302714\n",
      "iteration 500 / 2000: loss 2.302718\n",
      "iteration 600 / 2000: loss 2.302713\n",
      "iteration 700 / 2000: loss 2.302733\n",
      "iteration 800 / 2000: loss 2.302720\n",
      "iteration 900 / 2000: loss 2.302709\n",
      "iteration 1000 / 2000: loss 2.302720\n",
      "iteration 1100 / 2000: loss 2.302721\n",
      "iteration 1200 / 2000: loss 2.302701\n",
      "iteration 1300 / 2000: loss 2.302718\n",
      "iteration 1400 / 2000: loss 2.302710\n",
      "iteration 1500 / 2000: loss 2.302713\n",
      "iteration 1600 / 2000: loss 2.302711\n",
      "iteration 1700 / 2000: loss 2.302703\n",
      "iteration 1800 / 2000: loss 2.302715\n",
      "iteration 1900 / 2000: loss 2.302720\n",
      "Hidden Size: 80, Learning Rate: 1e-07, LR Decay: 0.3, Regularization: 0.4, Validation accuracy: 0.077\n",
      "iteration 0 / 2000: loss 2.302573\n",
      "iteration 100 / 2000: loss 0.290586\n",
      "iteration 200 / 2000: loss 0.145651\n",
      "iteration 300 / 2000: loss 0.098770\n",
      "iteration 400 / 2000: loss 0.104103\n",
      "iteration 500 / 2000: loss 0.176880\n",
      "iteration 600 / 2000: loss 0.134441\n",
      "iteration 700 / 2000: loss 0.043557\n",
      "iteration 800 / 2000: loss 0.095907\n",
      "iteration 900 / 2000: loss 0.049896\n",
      "iteration 1000 / 2000: loss 0.052409\n",
      "iteration 1100 / 2000: loss 0.061011\n",
      "iteration 1200 / 2000: loss 0.078331\n",
      "iteration 1300 / 2000: loss 0.054404\n",
      "iteration 1400 / 2000: loss 0.027089\n",
      "iteration 1500 / 2000: loss 0.045985\n",
      "iteration 1600 / 2000: loss 0.004611\n",
      "iteration 1700 / 2000: loss 0.079510\n",
      "iteration 1800 / 2000: loss 0.010034\n",
      "iteration 1900 / 2000: loss 0.047660\n",
      "Hidden Size: 90, Learning Rate: 0.01, LR Decay: 0.99, Regularization: 0.0, Validation accuracy: 0.9726\n",
      "iteration 0 / 2000: loss 2.302622\n",
      "iteration 100 / 2000: loss 0.219559\n",
      "iteration 200 / 2000: loss 0.221636\n",
      "iteration 300 / 2000: loss 0.155116\n",
      "iteration 400 / 2000: loss 0.140426\n",
      "iteration 500 / 2000: loss 0.145606\n",
      "iteration 600 / 2000: loss 0.110772\n",
      "iteration 700 / 2000: loss 0.123618\n",
      "iteration 800 / 2000: loss 0.128511\n",
      "iteration 900 / 2000: loss 0.148495\n",
      "iteration 1000 / 2000: loss 0.089537\n",
      "iteration 1100 / 2000: loss 0.096125\n",
      "iteration 1200 / 2000: loss 0.134075\n",
      "iteration 1300 / 2000: loss 0.111687\n",
      "iteration 1400 / 2000: loss 0.114889\n",
      "iteration 1500 / 2000: loss 0.123055\n",
      "iteration 1600 / 2000: loss 0.107428\n",
      "iteration 1700 / 2000: loss 0.094043\n",
      "iteration 1800 / 2000: loss 0.139607\n",
      "iteration 1900 / 2000: loss 0.084285\n",
      "Hidden Size: 90, Learning Rate: 0.01, LR Decay: 0.99, Regularization: 0.1, Validation accuracy: 0.9754\n",
      "iteration 0 / 2000: loss 2.302674\n",
      "iteration 100 / 2000: loss 0.323799\n",
      "iteration 200 / 2000: loss 0.218046\n",
      "iteration 300 / 2000: loss 0.195831\n",
      "iteration 400 / 2000: loss 0.231505\n",
      "iteration 500 / 2000: loss 0.163320\n",
      "iteration 600 / 2000: loss 0.206849\n",
      "iteration 700 / 2000: loss 0.155735\n",
      "iteration 800 / 2000: loss 0.184068\n",
      "iteration 900 / 2000: loss 0.245407\n",
      "iteration 1000 / 2000: loss 0.152640\n",
      "iteration 1100 / 2000: loss 0.183409\n",
      "iteration 1200 / 2000: loss 0.159895\n",
      "iteration 1300 / 2000: loss 0.145151\n",
      "iteration 1400 / 2000: loss 0.178808\n",
      "iteration 1500 / 2000: loss 0.157457\n",
      "iteration 1600 / 2000: loss 0.131212\n",
      "iteration 1700 / 2000: loss 0.194013\n",
      "iteration 1800 / 2000: loss 0.291828\n",
      "iteration 1900 / 2000: loss 0.155769\n",
      "Hidden Size: 90, Learning Rate: 0.01, LR Decay: 0.99, Regularization: 0.2, Validation accuracy: 0.9704\n",
      "iteration 0 / 2000: loss 2.302710\n",
      "iteration 100 / 2000: loss 0.303375\n",
      "iteration 200 / 2000: loss 0.194827\n",
      "iteration 300 / 2000: loss 0.218105\n",
      "iteration 400 / 2000: loss 0.240390\n",
      "iteration 500 / 2000: loss 0.259040\n",
      "iteration 600 / 2000: loss 0.193665\n",
      "iteration 700 / 2000: loss 0.233943\n",
      "iteration 800 / 2000: loss 0.200552\n",
      "iteration 900 / 2000: loss 0.216106\n",
      "iteration 1000 / 2000: loss 0.182594\n",
      "iteration 1100 / 2000: loss 0.240661\n",
      "iteration 1200 / 2000: loss 0.200802\n",
      "iteration 1300 / 2000: loss 0.179769\n",
      "iteration 1400 / 2000: loss 0.238980\n",
      "iteration 1500 / 2000: loss 0.226165\n",
      "iteration 1600 / 2000: loss 0.175471\n",
      "iteration 1700 / 2000: loss 0.208328\n",
      "iteration 1800 / 2000: loss 0.196117\n",
      "iteration 1900 / 2000: loss 0.252325\n",
      "Hidden Size: 90, Learning Rate: 0.01, LR Decay: 0.99, Regularization: 0.30000000000000004, Validation accuracy: 0.9724\n",
      "iteration 0 / 2000: loss 2.302767\n",
      "iteration 100 / 2000: loss 0.312228\n",
      "iteration 200 / 2000: loss 0.300977\n",
      "iteration 300 / 2000: loss 0.279457\n",
      "iteration 400 / 2000: loss 0.263608\n",
      "iteration 500 / 2000: loss 0.210928\n",
      "iteration 600 / 2000: loss 0.222267\n",
      "iteration 700 / 2000: loss 0.309717\n",
      "iteration 800 / 2000: loss 0.291703\n",
      "iteration 900 / 2000: loss 0.281399\n",
      "iteration 1000 / 2000: loss 0.246238\n",
      "iteration 1100 / 2000: loss 0.246110\n",
      "iteration 1200 / 2000: loss 0.213881\n",
      "iteration 1300 / 2000: loss 0.263496\n",
      "iteration 1400 / 2000: loss 0.245846\n",
      "iteration 1500 / 2000: loss 0.246965\n",
      "iteration 1600 / 2000: loss 0.215350\n",
      "iteration 1700 / 2000: loss 0.228139\n",
      "iteration 1800 / 2000: loss 0.217733\n",
      "iteration 1900 / 2000: loss 0.200395\n",
      "Hidden Size: 90, Learning Rate: 0.01, LR Decay: 0.99, Regularization: 0.4, Validation accuracy: 0.964\n",
      "iteration 0 / 2000: loss 2.302605\n",
      "iteration 100 / 2000: loss 0.363676\n",
      "iteration 200 / 2000: loss 0.193272\n",
      "iteration 300 / 2000: loss 0.170569\n",
      "iteration 400 / 2000: loss 0.122895\n",
      "iteration 500 / 2000: loss 0.044818\n",
      "iteration 600 / 2000: loss 0.074482\n",
      "iteration 700 / 2000: loss 0.074888\n",
      "iteration 800 / 2000: loss 0.046161\n",
      "iteration 900 / 2000: loss 0.065069\n",
      "iteration 1000 / 2000: loss 0.028035\n",
      "iteration 1100 / 2000: loss 0.035134\n",
      "iteration 1200 / 2000: loss 0.026407\n",
      "iteration 1300 / 2000: loss 0.015781\n",
      "iteration 1400 / 2000: loss 0.022340\n",
      "iteration 1500 / 2000: loss 0.016039\n",
      "iteration 1600 / 2000: loss 0.013309\n",
      "iteration 1700 / 2000: loss 0.018858\n",
      "iteration 1800 / 2000: loss 0.018266\n",
      "iteration 1900 / 2000: loss 0.024514\n",
      "Hidden Size: 90, Learning Rate: 0.01, LR Decay: 0.8175, Regularization: 0.0, Validation accuracy: 0.9788\n",
      "iteration 0 / 2000: loss 2.302610\n",
      "iteration 100 / 2000: loss 0.343863\n",
      "iteration 200 / 2000: loss 0.236242\n",
      "iteration 300 / 2000: loss 0.215962\n",
      "iteration 400 / 2000: loss 0.145712\n",
      "iteration 500 / 2000: loss 0.135830\n",
      "iteration 600 / 2000: loss 0.188531\n",
      "iteration 700 / 2000: loss 0.113067\n",
      "iteration 800 / 2000: loss 0.098545\n",
      "iteration 900 / 2000: loss 0.095287\n",
      "iteration 1000 / 2000: loss 0.074854\n",
      "iteration 1100 / 2000: loss 0.074545\n",
      "iteration 1200 / 2000: loss 0.082030\n",
      "iteration 1300 / 2000: loss 0.105409\n",
      "iteration 1400 / 2000: loss 0.127919\n",
      "iteration 1500 / 2000: loss 0.074181\n",
      "iteration 1600 / 2000: loss 0.115499\n",
      "iteration 1700 / 2000: loss 0.087574\n",
      "iteration 1800 / 2000: loss 0.113644\n",
      "iteration 1900 / 2000: loss 0.099154\n",
      "Hidden Size: 90, Learning Rate: 0.01, LR Decay: 0.8175, Regularization: 0.1, Validation accuracy: 0.977\n",
      "iteration 0 / 2000: loss 2.302648\n",
      "iteration 100 / 2000: loss 0.309245\n",
      "iteration 200 / 2000: loss 0.206208\n",
      "iteration 300 / 2000: loss 0.173428\n",
      "iteration 400 / 2000: loss 0.184727\n",
      "iteration 500 / 2000: loss 0.184903\n",
      "iteration 600 / 2000: loss 0.156267\n",
      "iteration 700 / 2000: loss 0.136080\n",
      "iteration 800 / 2000: loss 0.181199\n",
      "iteration 900 / 2000: loss 0.164495\n",
      "iteration 1000 / 2000: loss 0.155389\n",
      "iteration 1100 / 2000: loss 0.172139\n",
      "iteration 1200 / 2000: loss 0.119039\n",
      "iteration 1300 / 2000: loss 0.144791\n",
      "iteration 1400 / 2000: loss 0.130296\n",
      "iteration 1500 / 2000: loss 0.145406\n",
      "iteration 1600 / 2000: loss 0.229735\n",
      "iteration 1700 / 2000: loss 0.169892\n",
      "iteration 1800 / 2000: loss 0.134088\n",
      "iteration 1900 / 2000: loss 0.150407\n",
      "Hidden Size: 90, Learning Rate: 0.01, LR Decay: 0.8175, Regularization: 0.2, Validation accuracy: 0.9778\n",
      "iteration 0 / 2000: loss 2.302700\n",
      "iteration 100 / 2000: loss 0.292189\n",
      "iteration 200 / 2000: loss 0.264362\n",
      "iteration 300 / 2000: loss 0.214567\n",
      "iteration 400 / 2000: loss 0.230051\n",
      "iteration 500 / 2000: loss 0.261516\n",
      "iteration 600 / 2000: loss 0.181583\n",
      "iteration 700 / 2000: loss 0.279431\n",
      "iteration 800 / 2000: loss 0.218331\n",
      "iteration 900 / 2000: loss 0.172867\n",
      "iteration 1000 / 2000: loss 0.142944\n",
      "iteration 1100 / 2000: loss 0.152604\n",
      "iteration 1200 / 2000: loss 0.154418\n",
      "iteration 1300 / 2000: loss 0.190562\n",
      "iteration 1400 / 2000: loss 0.164274\n",
      "iteration 1500 / 2000: loss 0.188657\n",
      "iteration 1600 / 2000: loss 0.181092\n",
      "iteration 1700 / 2000: loss 0.189111\n",
      "iteration 1800 / 2000: loss 0.181527\n",
      "iteration 1900 / 2000: loss 0.170698\n",
      "Hidden Size: 90, Learning Rate: 0.01, LR Decay: 0.8175, Regularization: 0.30000000000000004, Validation accuracy: 0.9758\n",
      "iteration 0 / 2000: loss 2.302738\n",
      "iteration 100 / 2000: loss 0.238222\n",
      "iteration 200 / 2000: loss 0.324530\n",
      "iteration 300 / 2000: loss 0.252177\n",
      "iteration 400 / 2000: loss 0.284105\n",
      "iteration 500 / 2000: loss 0.207740\n",
      "iteration 600 / 2000: loss 0.207719\n",
      "iteration 700 / 2000: loss 0.232514\n",
      "iteration 800 / 2000: loss 0.199244\n",
      "iteration 900 / 2000: loss 0.187150\n",
      "iteration 1000 / 2000: loss 0.196353\n",
      "iteration 1100 / 2000: loss 0.256451\n",
      "iteration 1200 / 2000: loss 0.158549\n",
      "iteration 1300 / 2000: loss 0.213778\n",
      "iteration 1400 / 2000: loss 0.163772\n",
      "iteration 1500 / 2000: loss 0.174550\n",
      "iteration 1600 / 2000: loss 0.200994\n",
      "iteration 1700 / 2000: loss 0.213480\n",
      "iteration 1800 / 2000: loss 0.204942\n",
      "iteration 1900 / 2000: loss 0.211892\n",
      "Hidden Size: 90, Learning Rate: 0.01, LR Decay: 0.8175, Regularization: 0.4, Validation accuracy: 0.976\n",
      "iteration 0 / 2000: loss 2.302623\n",
      "iteration 100 / 2000: loss 0.291416\n",
      "iteration 200 / 2000: loss 0.191055\n",
      "iteration 300 / 2000: loss 0.195411\n",
      "iteration 400 / 2000: loss 0.119280\n",
      "iteration 500 / 2000: loss 0.164918\n",
      "iteration 600 / 2000: loss 0.073435\n",
      "iteration 700 / 2000: loss 0.038731\n",
      "iteration 800 / 2000: loss 0.058234\n",
      "iteration 900 / 2000: loss 0.076997\n",
      "iteration 1000 / 2000: loss 0.053093\n",
      "iteration 1100 / 2000: loss 0.056366\n",
      "iteration 1200 / 2000: loss 0.083472\n",
      "iteration 1300 / 2000: loss 0.058381\n",
      "iteration 1400 / 2000: loss 0.044947\n",
      "iteration 1500 / 2000: loss 0.055524\n",
      "iteration 1600 / 2000: loss 0.068897\n",
      "iteration 1700 / 2000: loss 0.032503\n",
      "iteration 1800 / 2000: loss 0.018925\n",
      "iteration 1900 / 2000: loss 0.061109\n",
      "Hidden Size: 90, Learning Rate: 0.01, LR Decay: 0.645, Regularization: 0.0, Validation accuracy: 0.9738\n",
      "iteration 0 / 2000: loss 2.302599\n",
      "iteration 100 / 2000: loss 0.360362\n",
      "iteration 200 / 2000: loss 0.170937\n",
      "iteration 300 / 2000: loss 0.162273\n",
      "iteration 400 / 2000: loss 0.225122\n",
      "iteration 500 / 2000: loss 0.118113\n",
      "iteration 600 / 2000: loss 0.141233\n",
      "iteration 700 / 2000: loss 0.162937\n",
      "iteration 800 / 2000: loss 0.108815\n",
      "iteration 900 / 2000: loss 0.120641\n",
      "iteration 1000 / 2000: loss 0.088431\n",
      "iteration 1100 / 2000: loss 0.153913\n",
      "iteration 1200 / 2000: loss 0.090846\n",
      "iteration 1300 / 2000: loss 0.097877\n",
      "iteration 1400 / 2000: loss 0.106000\n",
      "iteration 1500 / 2000: loss 0.093221\n",
      "iteration 1600 / 2000: loss 0.125694\n",
      "iteration 1700 / 2000: loss 0.091831\n",
      "iteration 1800 / 2000: loss 0.095616\n",
      "iteration 1900 / 2000: loss 0.108313\n",
      "Hidden Size: 90, Learning Rate: 0.01, LR Decay: 0.645, Regularization: 0.1, Validation accuracy: 0.9754\n",
      "iteration 0 / 2000: loss 2.302663\n",
      "iteration 100 / 2000: loss 0.334761\n",
      "iteration 200 / 2000: loss 0.205548\n",
      "iteration 300 / 2000: loss 0.223870\n",
      "iteration 400 / 2000: loss 0.184564\n",
      "iteration 500 / 2000: loss 0.143532\n",
      "iteration 600 / 2000: loss 0.162506\n",
      "iteration 700 / 2000: loss 0.173736\n",
      "iteration 800 / 2000: loss 0.132487\n",
      "iteration 900 / 2000: loss 0.154380\n",
      "iteration 1000 / 2000: loss 0.122382\n",
      "iteration 1100 / 2000: loss 0.138762\n",
      "iteration 1200 / 2000: loss 0.132848\n",
      "iteration 1300 / 2000: loss 0.187112\n",
      "iteration 1400 / 2000: loss 0.161821\n",
      "iteration 1500 / 2000: loss 0.165100\n",
      "iteration 1600 / 2000: loss 0.175050\n",
      "iteration 1700 / 2000: loss 0.122953\n",
      "iteration 1800 / 2000: loss 0.128946\n",
      "iteration 1900 / 2000: loss 0.173028\n",
      "Hidden Size: 90, Learning Rate: 0.01, LR Decay: 0.645, Regularization: 0.2, Validation accuracy: 0.975\n",
      "iteration 0 / 2000: loss 2.302711\n",
      "iteration 100 / 2000: loss 0.264801\n",
      "iteration 200 / 2000: loss 0.261331\n",
      "iteration 300 / 2000: loss 0.160465\n",
      "iteration 400 / 2000: loss 0.150634\n",
      "iteration 500 / 2000: loss 0.189404\n",
      "iteration 600 / 2000: loss 0.190293\n",
      "iteration 700 / 2000: loss 0.198667\n",
      "iteration 800 / 2000: loss 0.219114\n",
      "iteration 900 / 2000: loss 0.147006\n",
      "iteration 1000 / 2000: loss 0.179096\n",
      "iteration 1100 / 2000: loss 0.166854\n",
      "iteration 1200 / 2000: loss 0.225859\n",
      "iteration 1300 / 2000: loss 0.163421\n",
      "iteration 1400 / 2000: loss 0.204614\n",
      "iteration 1500 / 2000: loss 0.190952\n",
      "iteration 1600 / 2000: loss 0.152167\n",
      "iteration 1700 / 2000: loss 0.196636\n",
      "iteration 1800 / 2000: loss 0.153215\n",
      "iteration 1900 / 2000: loss 0.152838\n",
      "Hidden Size: 90, Learning Rate: 0.01, LR Decay: 0.645, Regularization: 0.30000000000000004, Validation accuracy: 0.9718\n",
      "iteration 0 / 2000: loss 2.302731\n",
      "iteration 100 / 2000: loss 0.276166\n",
      "iteration 200 / 2000: loss 0.317255\n",
      "iteration 300 / 2000: loss 0.212169\n",
      "iteration 400 / 2000: loss 0.270272\n",
      "iteration 500 / 2000: loss 0.241906\n",
      "iteration 600 / 2000: loss 0.241835\n",
      "iteration 700 / 2000: loss 0.178466\n",
      "iteration 800 / 2000: loss 0.193546\n",
      "iteration 900 / 2000: loss 0.201781\n",
      "iteration 1000 / 2000: loss 0.217489\n",
      "iteration 1100 / 2000: loss 0.191637\n",
      "iteration 1200 / 2000: loss 0.223681\n",
      "iteration 1300 / 2000: loss 0.219050\n",
      "iteration 1400 / 2000: loss 0.221244\n",
      "iteration 1500 / 2000: loss 0.222974\n",
      "iteration 1600 / 2000: loss 0.209336\n",
      "iteration 1700 / 2000: loss 0.272638\n",
      "iteration 1800 / 2000: loss 0.172595\n",
      "iteration 1900 / 2000: loss 0.221518\n",
      "Hidden Size: 90, Learning Rate: 0.01, LR Decay: 0.645, Regularization: 0.4, Validation accuracy: 0.9722\n",
      "iteration 0 / 2000: loss 2.302601\n",
      "iteration 100 / 2000: loss 0.260045\n",
      "iteration 200 / 2000: loss 0.206625\n",
      "iteration 300 / 2000: loss 0.164695\n",
      "iteration 400 / 2000: loss 0.123955\n",
      "iteration 500 / 2000: loss 0.157093\n",
      "iteration 600 / 2000: loss 0.094239\n",
      "iteration 700 / 2000: loss 0.124083\n",
      "iteration 800 / 2000: loss 0.117343\n",
      "iteration 900 / 2000: loss 0.079489\n",
      "iteration 1000 / 2000: loss 0.174623\n",
      "iteration 1100 / 2000: loss 0.154654\n",
      "iteration 1200 / 2000: loss 0.038066\n",
      "iteration 1300 / 2000: loss 0.162840\n",
      "iteration 1400 / 2000: loss 0.078003\n",
      "iteration 1500 / 2000: loss 0.084378\n",
      "iteration 1600 / 2000: loss 0.089992\n",
      "iteration 1700 / 2000: loss 0.147292\n",
      "iteration 1800 / 2000: loss 0.102321\n",
      "iteration 1900 / 2000: loss 0.050414\n",
      "Hidden Size: 90, Learning Rate: 0.01, LR Decay: 0.47250000000000003, Regularization: 0.0, Validation accuracy: 0.9686\n",
      "iteration 0 / 2000: loss 2.302607\n",
      "iteration 100 / 2000: loss 0.327960\n",
      "iteration 200 / 2000: loss 0.292042\n",
      "iteration 300 / 2000: loss 0.163364\n",
      "iteration 400 / 2000: loss 0.178718\n",
      "iteration 500 / 2000: loss 0.132692\n",
      "iteration 600 / 2000: loss 0.148909\n",
      "iteration 700 / 2000: loss 0.133748\n",
      "iteration 800 / 2000: loss 0.200046\n",
      "iteration 900 / 2000: loss 0.136026\n",
      "iteration 1000 / 2000: loss 0.094313\n",
      "iteration 1100 / 2000: loss 0.124969\n",
      "iteration 1200 / 2000: loss 0.102051\n",
      "iteration 1300 / 2000: loss 0.082972\n",
      "iteration 1400 / 2000: loss 0.071447\n",
      "iteration 1500 / 2000: loss 0.144149\n",
      "iteration 1600 / 2000: loss 0.103448\n",
      "iteration 1700 / 2000: loss 0.110609\n",
      "iteration 1800 / 2000: loss 0.105034\n",
      "iteration 1900 / 2000: loss 0.182557\n",
      "Hidden Size: 90, Learning Rate: 0.01, LR Decay: 0.47250000000000003, Regularization: 0.1, Validation accuracy: 0.9688\n",
      "iteration 0 / 2000: loss 2.302661\n",
      "iteration 100 / 2000: loss 0.279736\n",
      "iteration 200 / 2000: loss 0.260591\n",
      "iteration 300 / 2000: loss 0.201894\n",
      "iteration 400 / 2000: loss 0.258865\n",
      "iteration 500 / 2000: loss 0.136555\n",
      "iteration 600 / 2000: loss 0.201974\n",
      "iteration 700 / 2000: loss 0.166063\n",
      "iteration 800 / 2000: loss 0.147940\n",
      "iteration 900 / 2000: loss 0.182769\n",
      "iteration 1000 / 2000: loss 0.117010\n",
      "iteration 1100 / 2000: loss 0.196692\n",
      "iteration 1200 / 2000: loss 0.146544\n",
      "iteration 1300 / 2000: loss 0.124667\n",
      "iteration 1400 / 2000: loss 0.154809\n",
      "iteration 1500 / 2000: loss 0.127863\n",
      "iteration 1600 / 2000: loss 0.140852\n",
      "iteration 1700 / 2000: loss 0.154287\n",
      "iteration 1800 / 2000: loss 0.130598\n",
      "iteration 1900 / 2000: loss 0.149459\n",
      "Hidden Size: 90, Learning Rate: 0.01, LR Decay: 0.47250000000000003, Regularization: 0.2, Validation accuracy: 0.9682\n",
      "iteration 0 / 2000: loss 2.302686\n",
      "iteration 100 / 2000: loss 0.418211\n",
      "iteration 200 / 2000: loss 0.296513\n",
      "iteration 300 / 2000: loss 0.242989\n",
      "iteration 400 / 2000: loss 0.166069\n",
      "iteration 500 / 2000: loss 0.214122\n",
      "iteration 600 / 2000: loss 0.198800\n",
      "iteration 700 / 2000: loss 0.247294\n",
      "iteration 800 / 2000: loss 0.176572\n",
      "iteration 900 / 2000: loss 0.193031\n",
      "iteration 1000 / 2000: loss 0.180803\n",
      "iteration 1100 / 2000: loss 0.178796\n",
      "iteration 1200 / 2000: loss 0.182724\n",
      "iteration 1300 / 2000: loss 0.250236\n",
      "iteration 1400 / 2000: loss 0.209959\n",
      "iteration 1500 / 2000: loss 0.246241\n",
      "iteration 1600 / 2000: loss 0.195477\n",
      "iteration 1700 / 2000: loss 0.164915\n",
      "iteration 1800 / 2000: loss 0.238081\n",
      "iteration 1900 / 2000: loss 0.235296\n",
      "Hidden Size: 90, Learning Rate: 0.01, LR Decay: 0.47250000000000003, Regularization: 0.30000000000000004, Validation accuracy: 0.9678\n",
      "iteration 0 / 2000: loss 2.302725\n",
      "iteration 100 / 2000: loss 0.347663\n",
      "iteration 200 / 2000: loss 0.228533\n",
      "iteration 300 / 2000: loss 0.365515\n",
      "iteration 400 / 2000: loss 0.276164\n",
      "iteration 500 / 2000: loss 0.217891\n",
      "iteration 600 / 2000: loss 0.214265\n",
      "iteration 700 / 2000: loss 0.176037\n",
      "iteration 800 / 2000: loss 0.170454\n",
      "iteration 900 / 2000: loss 0.211177\n",
      "iteration 1000 / 2000: loss 0.326194\n",
      "iteration 1100 / 2000: loss 0.205037\n",
      "iteration 1200 / 2000: loss 0.156637\n",
      "iteration 1300 / 2000: loss 0.224802\n",
      "iteration 1400 / 2000: loss 0.166502\n",
      "iteration 1500 / 2000: loss 0.193524\n",
      "iteration 1600 / 2000: loss 0.235469\n",
      "iteration 1700 / 2000: loss 0.261815\n",
      "iteration 1800 / 2000: loss 0.230391\n",
      "iteration 1900 / 2000: loss 0.195259\n",
      "Hidden Size: 90, Learning Rate: 0.01, LR Decay: 0.47250000000000003, Regularization: 0.4, Validation accuracy: 0.9644\n",
      "iteration 0 / 2000: loss 2.302565\n",
      "iteration 100 / 2000: loss 0.329774\n",
      "iteration 200 / 2000: loss 0.198271\n",
      "iteration 300 / 2000: loss 0.271033\n",
      "iteration 400 / 2000: loss 0.194786\n",
      "iteration 500 / 2000: loss 0.216196\n",
      "iteration 600 / 2000: loss 0.154624\n",
      "iteration 700 / 2000: loss 0.168138\n",
      "iteration 800 / 2000: loss 0.173455\n",
      "iteration 900 / 2000: loss 0.115517\n",
      "iteration 1000 / 2000: loss 0.254680\n",
      "iteration 1100 / 2000: loss 0.101021\n",
      "iteration 1200 / 2000: loss 0.146995\n",
      "iteration 1300 / 2000: loss 0.099811\n",
      "iteration 1400 / 2000: loss 0.169924\n",
      "iteration 1500 / 2000: loss 0.185225\n",
      "iteration 1600 / 2000: loss 0.218228\n",
      "iteration 1700 / 2000: loss 0.112288\n",
      "iteration 1800 / 2000: loss 0.120273\n",
      "iteration 1900 / 2000: loss 0.128866\n",
      "Hidden Size: 90, Learning Rate: 0.01, LR Decay: 0.3, Regularization: 0.0, Validation accuracy: 0.9542\n",
      "iteration 0 / 2000: loss 2.302622\n",
      "iteration 100 / 2000: loss 0.268967\n",
      "iteration 200 / 2000: loss 0.210626\n",
      "iteration 300 / 2000: loss 0.206236\n",
      "iteration 400 / 2000: loss 0.149629\n",
      "iteration 500 / 2000: loss 0.196255\n",
      "iteration 600 / 2000: loss 0.168833\n",
      "iteration 700 / 2000: loss 0.248526\n",
      "iteration 800 / 2000: loss 0.141472\n",
      "iteration 900 / 2000: loss 0.234266\n",
      "iteration 1000 / 2000: loss 0.180598\n",
      "iteration 1100 / 2000: loss 0.146346\n",
      "iteration 1200 / 2000: loss 0.188481\n",
      "iteration 1300 / 2000: loss 0.124619\n",
      "iteration 1400 / 2000: loss 0.196795\n",
      "iteration 1500 / 2000: loss 0.210448\n",
      "iteration 1600 / 2000: loss 0.147829\n",
      "iteration 1700 / 2000: loss 0.236989\n",
      "iteration 1800 / 2000: loss 0.247995\n",
      "iteration 1900 / 2000: loss 0.167981\n",
      "Hidden Size: 90, Learning Rate: 0.01, LR Decay: 0.3, Regularization: 0.1, Validation accuracy: 0.9556\n",
      "iteration 0 / 2000: loss 2.302636\n",
      "iteration 100 / 2000: loss 0.324204\n",
      "iteration 200 / 2000: loss 0.335945\n",
      "iteration 300 / 2000: loss 0.226146\n",
      "iteration 400 / 2000: loss 0.262327\n",
      "iteration 500 / 2000: loss 0.261330\n",
      "iteration 600 / 2000: loss 0.210739\n",
      "iteration 700 / 2000: loss 0.226474\n",
      "iteration 800 / 2000: loss 0.209249\n",
      "iteration 900 / 2000: loss 0.182114\n",
      "iteration 1000 / 2000: loss 0.125584\n",
      "iteration 1100 / 2000: loss 0.143963\n",
      "iteration 1200 / 2000: loss 0.169208\n",
      "iteration 1300 / 2000: loss 0.283069\n",
      "iteration 1400 / 2000: loss 0.166354\n",
      "iteration 1500 / 2000: loss 0.190677\n",
      "iteration 1600 / 2000: loss 0.210617\n",
      "iteration 1700 / 2000: loss 0.159470\n",
      "iteration 1800 / 2000: loss 0.127936\n",
      "iteration 1900 / 2000: loss 0.187813\n",
      "Hidden Size: 90, Learning Rate: 0.01, LR Decay: 0.3, Regularization: 0.2, Validation accuracy: 0.9524\n",
      "iteration 0 / 2000: loss 2.302662\n",
      "iteration 100 / 2000: loss 0.381560\n",
      "iteration 200 / 2000: loss 0.282770\n",
      "iteration 300 / 2000: loss 0.247969\n",
      "iteration 400 / 2000: loss 0.195036\n",
      "iteration 500 / 2000: loss 0.188488\n",
      "iteration 600 / 2000: loss 0.201176\n",
      "iteration 700 / 2000: loss 0.206676\n",
      "iteration 800 / 2000: loss 0.204045\n",
      "iteration 900 / 2000: loss 0.222064\n",
      "iteration 1000 / 2000: loss 0.180894\n",
      "iteration 1100 / 2000: loss 0.192994\n",
      "iteration 1200 / 2000: loss 0.294385\n",
      "iteration 1300 / 2000: loss 0.304224\n",
      "iteration 1400 / 2000: loss 0.276280\n",
      "iteration 1500 / 2000: loss 0.177523\n",
      "iteration 1600 / 2000: loss 0.359182\n",
      "iteration 1700 / 2000: loss 0.263666\n",
      "iteration 1800 / 2000: loss 0.229339\n",
      "iteration 1900 / 2000: loss 0.234195\n",
      "Hidden Size: 90, Learning Rate: 0.01, LR Decay: 0.3, Regularization: 0.30000000000000004, Validation accuracy: 0.9528\n",
      "iteration 0 / 2000: loss 2.302724\n",
      "iteration 100 / 2000: loss 0.513959\n",
      "iteration 200 / 2000: loss 0.242258\n",
      "iteration 300 / 2000: loss 0.275599\n",
      "iteration 400 / 2000: loss 0.301146\n",
      "iteration 500 / 2000: loss 0.220946\n",
      "iteration 600 / 2000: loss 0.250890\n",
      "iteration 700 / 2000: loss 0.168302\n",
      "iteration 800 / 2000: loss 0.261629\n",
      "iteration 900 / 2000: loss 0.265207\n",
      "iteration 1000 / 2000: loss 0.332738\n",
      "iteration 1100 / 2000: loss 0.198177\n",
      "iteration 1200 / 2000: loss 0.229849\n",
      "iteration 1300 / 2000: loss 0.252975\n",
      "iteration 1400 / 2000: loss 0.246843\n",
      "iteration 1500 / 2000: loss 0.229413\n",
      "iteration 1600 / 2000: loss 0.197985\n",
      "iteration 1700 / 2000: loss 0.212047\n",
      "iteration 1800 / 2000: loss 0.227681\n",
      "iteration 1900 / 2000: loss 0.375596\n",
      "Hidden Size: 90, Learning Rate: 0.01, LR Decay: 0.3, Regularization: 0.4, Validation accuracy: 0.9562\n",
      "iteration 0 / 2000: loss 2.302573\n",
      "iteration 100 / 2000: loss 0.183737\n",
      "iteration 200 / 2000: loss 0.199569\n",
      "iteration 300 / 2000: loss 0.136814\n",
      "iteration 400 / 2000: loss 0.157103\n",
      "iteration 500 / 2000: loss 0.077813\n",
      "iteration 600 / 2000: loss 0.109676\n",
      "iteration 700 / 2000: loss 0.111449\n",
      "iteration 800 / 2000: loss 0.038013\n",
      "iteration 900 / 2000: loss 0.071574\n",
      "iteration 1000 / 2000: loss 0.057901\n",
      "iteration 1100 / 2000: loss 0.024882\n",
      "iteration 1200 / 2000: loss 0.043027\n",
      "iteration 1300 / 2000: loss 0.017746\n",
      "iteration 1400 / 2000: loss 0.049377\n",
      "iteration 1500 / 2000: loss 0.019350\n",
      "iteration 1600 / 2000: loss 0.061362\n",
      "iteration 1700 / 2000: loss 0.049809\n",
      "iteration 1800 / 2000: loss 0.019086\n",
      "iteration 1900 / 2000: loss 0.045905\n",
      "Hidden Size: 90, Learning Rate: 0.0075000250000000004, LR Decay: 0.99, Regularization: 0.0, Validation accuracy: 0.9766\n",
      "iteration 0 / 2000: loss 2.302601\n",
      "iteration 100 / 2000: loss 0.304141\n",
      "iteration 200 / 2000: loss 0.244588\n",
      "iteration 300 / 2000: loss 0.213360\n",
      "iteration 400 / 2000: loss 0.114554\n",
      "iteration 500 / 2000: loss 0.130830\n",
      "iteration 600 / 2000: loss 0.178087\n",
      "iteration 700 / 2000: loss 0.139204\n",
      "iteration 800 / 2000: loss 0.108451\n",
      "iteration 900 / 2000: loss 0.123158\n",
      "iteration 1000 / 2000: loss 0.187824\n",
      "iteration 1100 / 2000: loss 0.094998\n",
      "iteration 1200 / 2000: loss 0.097952\n",
      "iteration 1300 / 2000: loss 0.140929\n",
      "iteration 1400 / 2000: loss 0.141247\n",
      "iteration 1500 / 2000: loss 0.079179\n",
      "iteration 1600 / 2000: loss 0.094055\n",
      "iteration 1700 / 2000: loss 0.108087\n",
      "iteration 1800 / 2000: loss 0.090677\n",
      "iteration 1900 / 2000: loss 0.083214\n",
      "Hidden Size: 90, Learning Rate: 0.0075000250000000004, LR Decay: 0.99, Regularization: 0.1, Validation accuracy: 0.9756\n",
      "iteration 0 / 2000: loss 2.302685\n",
      "iteration 100 / 2000: loss 0.334814\n",
      "iteration 200 / 2000: loss 0.288957\n",
      "iteration 300 / 2000: loss 0.249739\n",
      "iteration 400 / 2000: loss 0.208283\n",
      "iteration 500 / 2000: loss 0.165860\n",
      "iteration 600 / 2000: loss 0.233844\n",
      "iteration 700 / 2000: loss 0.143236\n",
      "iteration 800 / 2000: loss 0.177984\n",
      "iteration 900 / 2000: loss 0.184965\n",
      "iteration 1000 / 2000: loss 0.185461\n",
      "iteration 1100 / 2000: loss 0.183968\n",
      "iteration 1200 / 2000: loss 0.191032\n",
      "iteration 1300 / 2000: loss 0.230398\n",
      "iteration 1400 / 2000: loss 0.165813\n",
      "iteration 1500 / 2000: loss 0.150104\n",
      "iteration 1600 / 2000: loss 0.154722\n",
      "iteration 1700 / 2000: loss 0.168352\n",
      "iteration 1800 / 2000: loss 0.201963\n",
      "iteration 1900 / 2000: loss 0.116686\n",
      "Hidden Size: 90, Learning Rate: 0.0075000250000000004, LR Decay: 0.99, Regularization: 0.2, Validation accuracy: 0.972\n",
      "iteration 0 / 2000: loss 2.302693\n",
      "iteration 100 / 2000: loss 0.286429\n",
      "iteration 200 / 2000: loss 0.262082\n",
      "iteration 300 / 2000: loss 0.245914\n",
      "iteration 400 / 2000: loss 0.196023\n",
      "iteration 500 / 2000: loss 0.253372\n",
      "iteration 600 / 2000: loss 0.183844\n",
      "iteration 700 / 2000: loss 0.235940\n",
      "iteration 800 / 2000: loss 0.174657\n",
      "iteration 900 / 2000: loss 0.224211\n",
      "iteration 1000 / 2000: loss 0.158611\n",
      "iteration 1100 / 2000: loss 0.156154\n",
      "iteration 1200 / 2000: loss 0.218517\n",
      "iteration 1300 / 2000: loss 0.195637\n",
      "iteration 1400 / 2000: loss 0.181462\n",
      "iteration 1500 / 2000: loss 0.231652\n",
      "iteration 1600 / 2000: loss 0.183127\n",
      "iteration 1700 / 2000: loss 0.181863\n",
      "iteration 1800 / 2000: loss 0.175059\n",
      "iteration 1900 / 2000: loss 0.163584\n",
      "Hidden Size: 90, Learning Rate: 0.0075000250000000004, LR Decay: 0.99, Regularization: 0.30000000000000004, Validation accuracy: 0.9734\n",
      "iteration 0 / 2000: loss 2.302727\n",
      "iteration 100 / 2000: loss 0.389405\n",
      "iteration 200 / 2000: loss 0.275650\n",
      "iteration 300 / 2000: loss 0.233065\n",
      "iteration 400 / 2000: loss 0.208151\n",
      "iteration 500 / 2000: loss 0.307431\n",
      "iteration 600 / 2000: loss 0.201421\n",
      "iteration 700 / 2000: loss 0.215050\n",
      "iteration 800 / 2000: loss 0.257248\n",
      "iteration 900 / 2000: loss 0.268133\n",
      "iteration 1000 / 2000: loss 0.272727\n",
      "iteration 1100 / 2000: loss 0.273366\n",
      "iteration 1200 / 2000: loss 0.237096\n",
      "iteration 1300 / 2000: loss 0.208946\n",
      "iteration 1400 / 2000: loss 0.232051\n",
      "iteration 1500 / 2000: loss 0.212329\n",
      "iteration 1600 / 2000: loss 0.195965\n",
      "iteration 1700 / 2000: loss 0.256838\n",
      "iteration 1800 / 2000: loss 0.233209\n",
      "iteration 1900 / 2000: loss 0.217273\n",
      "Hidden Size: 90, Learning Rate: 0.0075000250000000004, LR Decay: 0.99, Regularization: 0.4, Validation accuracy: 0.9682\n",
      "iteration 0 / 2000: loss 2.302598\n",
      "iteration 100 / 2000: loss 0.339892\n",
      "iteration 200 / 2000: loss 0.175749\n",
      "iteration 300 / 2000: loss 0.137249\n",
      "iteration 400 / 2000: loss 0.080696\n",
      "iteration 500 / 2000: loss 0.131064\n",
      "iteration 600 / 2000: loss 0.152392\n",
      "iteration 700 / 2000: loss 0.048750\n",
      "iteration 800 / 2000: loss 0.158467\n",
      "iteration 900 / 2000: loss 0.123805\n",
      "iteration 1000 / 2000: loss 0.093941\n",
      "iteration 1100 / 2000: loss 0.073916\n",
      "iteration 1200 / 2000: loss 0.043001\n",
      "iteration 1300 / 2000: loss 0.027730\n",
      "iteration 1400 / 2000: loss 0.023183\n",
      "iteration 1500 / 2000: loss 0.051792\n",
      "iteration 1600 / 2000: loss 0.053263\n",
      "iteration 1700 / 2000: loss 0.035465\n",
      "iteration 1800 / 2000: loss 0.024484\n",
      "iteration 1900 / 2000: loss 0.023535\n",
      "Hidden Size: 90, Learning Rate: 0.0075000250000000004, LR Decay: 0.8175, Regularization: 0.0, Validation accuracy: 0.9738\n",
      "iteration 0 / 2000: loss 2.302632\n",
      "iteration 100 / 2000: loss 0.183436\n",
      "iteration 200 / 2000: loss 0.317728\n",
      "iteration 300 / 2000: loss 0.137947\n",
      "iteration 400 / 2000: loss 0.256306\n",
      "iteration 500 / 2000: loss 0.149059\n",
      "iteration 600 / 2000: loss 0.201000\n",
      "iteration 700 / 2000: loss 0.128282\n",
      "iteration 800 / 2000: loss 0.118564\n",
      "iteration 900 / 2000: loss 0.081345\n",
      "iteration 1000 / 2000: loss 0.107014\n",
      "iteration 1100 / 2000: loss 0.139409\n",
      "iteration 1200 / 2000: loss 0.098533\n",
      "iteration 1300 / 2000: loss 0.118114\n",
      "iteration 1400 / 2000: loss 0.110258\n",
      "iteration 1500 / 2000: loss 0.087017\n",
      "iteration 1600 / 2000: loss 0.097728\n",
      "iteration 1700 / 2000: loss 0.132945\n",
      "iteration 1800 / 2000: loss 0.120335\n",
      "iteration 1900 / 2000: loss 0.148844\n",
      "Hidden Size: 90, Learning Rate: 0.0075000250000000004, LR Decay: 0.8175, Regularization: 0.1, Validation accuracy: 0.977\n",
      "iteration 0 / 2000: loss 2.302682\n",
      "iteration 100 / 2000: loss 0.217929\n",
      "iteration 200 / 2000: loss 0.290762\n",
      "iteration 300 / 2000: loss 0.208118\n",
      "iteration 400 / 2000: loss 0.163503\n",
      "iteration 500 / 2000: loss 0.229191\n",
      "iteration 600 / 2000: loss 0.118018\n",
      "iteration 700 / 2000: loss 0.198998\n",
      "iteration 800 / 2000: loss 0.130695\n",
      "iteration 900 / 2000: loss 0.148586\n",
      "iteration 1000 / 2000: loss 0.206986\n",
      "iteration 1100 / 2000: loss 0.186850\n",
      "iteration 1200 / 2000: loss 0.115975\n",
      "iteration 1300 / 2000: loss 0.150141\n",
      "iteration 1400 / 2000: loss 0.156664\n",
      "iteration 1500 / 2000: loss 0.162113\n",
      "iteration 1600 / 2000: loss 0.167705\n",
      "iteration 1700 / 2000: loss 0.130842\n",
      "iteration 1800 / 2000: loss 0.178464\n",
      "iteration 1900 / 2000: loss 0.136737\n",
      "Hidden Size: 90, Learning Rate: 0.0075000250000000004, LR Decay: 0.8175, Regularization: 0.2, Validation accuracy: 0.9782\n",
      "iteration 0 / 2000: loss 2.302712\n",
      "iteration 100 / 2000: loss 0.316696\n",
      "iteration 200 / 2000: loss 0.207143\n",
      "iteration 300 / 2000: loss 0.321712\n",
      "iteration 400 / 2000: loss 0.232969\n",
      "iteration 500 / 2000: loss 0.265517\n",
      "iteration 600 / 2000: loss 0.165800\n",
      "iteration 700 / 2000: loss 0.228585\n",
      "iteration 800 / 2000: loss 0.197102\n",
      "iteration 900 / 2000: loss 0.201242\n",
      "iteration 1000 / 2000: loss 0.188866\n",
      "iteration 1100 / 2000: loss 0.204257\n",
      "iteration 1200 / 2000: loss 0.180332\n",
      "iteration 1300 / 2000: loss 0.166637\n",
      "iteration 1400 / 2000: loss 0.201803\n",
      "iteration 1500 / 2000: loss 0.169108\n",
      "iteration 1600 / 2000: loss 0.192292\n",
      "iteration 1700 / 2000: loss 0.159357\n",
      "iteration 1800 / 2000: loss 0.178199\n",
      "iteration 1900 / 2000: loss 0.155581\n",
      "Hidden Size: 90, Learning Rate: 0.0075000250000000004, LR Decay: 0.8175, Regularization: 0.30000000000000004, Validation accuracy: 0.9742\n",
      "iteration 0 / 2000: loss 2.302720\n",
      "iteration 100 / 2000: loss 0.325736\n",
      "iteration 200 / 2000: loss 0.263263\n",
      "iteration 300 / 2000: loss 0.246442\n",
      "iteration 400 / 2000: loss 0.255174\n",
      "iteration 500 / 2000: loss 0.254841\n",
      "iteration 600 / 2000: loss 0.183710\n",
      "iteration 700 / 2000: loss 0.208578\n",
      "iteration 800 / 2000: loss 0.232777\n",
      "iteration 900 / 2000: loss 0.174804\n",
      "iteration 1000 / 2000: loss 0.196224\n",
      "iteration 1100 / 2000: loss 0.207861\n",
      "iteration 1200 / 2000: loss 0.224383\n",
      "iteration 1300 / 2000: loss 0.178830\n",
      "iteration 1400 / 2000: loss 0.178140\n",
      "iteration 1500 / 2000: loss 0.232915\n",
      "iteration 1600 / 2000: loss 0.178545\n",
      "iteration 1700 / 2000: loss 0.174156\n",
      "iteration 1800 / 2000: loss 0.193120\n",
      "iteration 1900 / 2000: loss 0.169191\n",
      "Hidden Size: 90, Learning Rate: 0.0075000250000000004, LR Decay: 0.8175, Regularization: 0.4, Validation accuracy: 0.9716\n",
      "iteration 0 / 2000: loss 2.302560\n",
      "iteration 100 / 2000: loss 0.430401\n",
      "iteration 200 / 2000: loss 0.154495\n",
      "iteration 300 / 2000: loss 0.091522\n",
      "iteration 400 / 2000: loss 0.117342\n",
      "iteration 500 / 2000: loss 0.099009\n",
      "iteration 600 / 2000: loss 0.052729\n",
      "iteration 700 / 2000: loss 0.096702\n",
      "iteration 800 / 2000: loss 0.085543\n",
      "iteration 900 / 2000: loss 0.102957\n",
      "iteration 1000 / 2000: loss 0.103985\n",
      "iteration 1100 / 2000: loss 0.063685\n",
      "iteration 1200 / 2000: loss 0.035140\n",
      "iteration 1300 / 2000: loss 0.066323\n",
      "iteration 1400 / 2000: loss 0.061777\n",
      "iteration 1500 / 2000: loss 0.082915\n",
      "iteration 1600 / 2000: loss 0.045640\n",
      "iteration 1700 / 2000: loss 0.086144\n",
      "iteration 1800 / 2000: loss 0.055182\n",
      "iteration 1900 / 2000: loss 0.078184\n",
      "Hidden Size: 90, Learning Rate: 0.0075000250000000004, LR Decay: 0.645, Regularization: 0.0, Validation accuracy: 0.9732\n",
      "iteration 0 / 2000: loss 2.302634\n",
      "iteration 100 / 2000: loss 0.311482\n",
      "iteration 200 / 2000: loss 0.261105\n",
      "iteration 300 / 2000: loss 0.270118\n",
      "iteration 400 / 2000: loss 0.127419\n",
      "iteration 500 / 2000: loss 0.171425\n",
      "iteration 600 / 2000: loss 0.154590\n",
      "iteration 700 / 2000: loss 0.146278\n",
      "iteration 800 / 2000: loss 0.116232\n",
      "iteration 900 / 2000: loss 0.086367\n",
      "iteration 1000 / 2000: loss 0.081047\n",
      "iteration 1100 / 2000: loss 0.112403\n",
      "iteration 1200 / 2000: loss 0.108235\n",
      "iteration 1300 / 2000: loss 0.118196\n",
      "iteration 1400 / 2000: loss 0.122957\n",
      "iteration 1500 / 2000: loss 0.135808\n",
      "iteration 1600 / 2000: loss 0.093843\n",
      "iteration 1700 / 2000: loss 0.100088\n",
      "iteration 1800 / 2000: loss 0.088912\n",
      "iteration 1900 / 2000: loss 0.157036\n",
      "Hidden Size: 90, Learning Rate: 0.0075000250000000004, LR Decay: 0.645, Regularization: 0.1, Validation accuracy: 0.9742\n",
      "iteration 0 / 2000: loss 2.302668\n",
      "iteration 100 / 2000: loss 0.366456\n",
      "iteration 200 / 2000: loss 0.267845\n",
      "iteration 300 / 2000: loss 0.200772\n",
      "iteration 400 / 2000: loss 0.161561\n",
      "iteration 500 / 2000: loss 0.243571\n",
      "iteration 600 / 2000: loss 0.107432\n",
      "iteration 700 / 2000: loss 0.179705\n",
      "iteration 800 / 2000: loss 0.225245\n",
      "iteration 900 / 2000: loss 0.167772\n",
      "iteration 1000 / 2000: loss 0.174707\n",
      "iteration 1100 / 2000: loss 0.165350\n",
      "iteration 1200 / 2000: loss 0.150213\n",
      "iteration 1300 / 2000: loss 0.165679\n",
      "iteration 1400 / 2000: loss 0.191073\n",
      "iteration 1500 / 2000: loss 0.149636\n",
      "iteration 1600 / 2000: loss 0.133528\n",
      "iteration 1700 / 2000: loss 0.186131\n",
      "iteration 1800 / 2000: loss 0.155240\n",
      "iteration 1900 / 2000: loss 0.119270\n",
      "Hidden Size: 90, Learning Rate: 0.0075000250000000004, LR Decay: 0.645, Regularization: 0.2, Validation accuracy: 0.9736\n",
      "iteration 0 / 2000: loss 2.302685\n",
      "iteration 100 / 2000: loss 0.328679\n",
      "iteration 200 / 2000: loss 0.232081\n",
      "iteration 300 / 2000: loss 0.201786\n",
      "iteration 400 / 2000: loss 0.178570\n",
      "iteration 500 / 2000: loss 0.268607\n",
      "iteration 600 / 2000: loss 0.159896\n",
      "iteration 700 / 2000: loss 0.207143\n",
      "iteration 800 / 2000: loss 0.197164\n",
      "iteration 900 / 2000: loss 0.202341\n",
      "iteration 1000 / 2000: loss 0.220966\n",
      "iteration 1100 / 2000: loss 0.173852\n",
      "iteration 1200 / 2000: loss 0.169524\n",
      "iteration 1300 / 2000: loss 0.183484\n",
      "iteration 1400 / 2000: loss 0.189046\n",
      "iteration 1500 / 2000: loss 0.153939\n",
      "iteration 1600 / 2000: loss 0.212605\n",
      "iteration 1700 / 2000: loss 0.179796\n",
      "iteration 1800 / 2000: loss 0.164885\n",
      "iteration 1900 / 2000: loss 0.187690\n",
      "Hidden Size: 90, Learning Rate: 0.0075000250000000004, LR Decay: 0.645, Regularization: 0.30000000000000004, Validation accuracy: 0.9718\n",
      "iteration 0 / 2000: loss 2.302723\n",
      "iteration 100 / 2000: loss 0.310500\n",
      "iteration 200 / 2000: loss 0.244000\n",
      "iteration 300 / 2000: loss 0.271054\n",
      "iteration 400 / 2000: loss 0.273164\n",
      "iteration 500 / 2000: loss 0.225726\n",
      "iteration 600 / 2000: loss 0.238889\n",
      "iteration 700 / 2000: loss 0.254164\n",
      "iteration 800 / 2000: loss 0.217690\n",
      "iteration 900 / 2000: loss 0.236498\n",
      "iteration 1000 / 2000: loss 0.230311\n",
      "iteration 1100 / 2000: loss 0.276467\n",
      "iteration 1200 / 2000: loss 0.211327\n",
      "iteration 1300 / 2000: loss 0.231897\n",
      "iteration 1400 / 2000: loss 0.198516\n",
      "iteration 1500 / 2000: loss 0.210740\n",
      "iteration 1600 / 2000: loss 0.265043\n",
      "iteration 1700 / 2000: loss 0.187292\n",
      "iteration 1800 / 2000: loss 0.234354\n",
      "iteration 1900 / 2000: loss 0.263269\n",
      "Hidden Size: 90, Learning Rate: 0.0075000250000000004, LR Decay: 0.645, Regularization: 0.4, Validation accuracy: 0.9696\n",
      "iteration 0 / 2000: loss 2.302572\n",
      "iteration 100 / 2000: loss 0.290033\n",
      "iteration 200 / 2000: loss 0.234811\n",
      "iteration 300 / 2000: loss 0.199838\n",
      "iteration 400 / 2000: loss 0.108427\n",
      "iteration 500 / 2000: loss 0.210265\n",
      "iteration 600 / 2000: loss 0.150021\n",
      "iteration 700 / 2000: loss 0.104107\n",
      "iteration 800 / 2000: loss 0.072194\n",
      "iteration 900 / 2000: loss 0.064765\n",
      "iteration 1000 / 2000: loss 0.119029\n",
      "iteration 1100 / 2000: loss 0.080788\n",
      "iteration 1200 / 2000: loss 0.096342\n",
      "iteration 1300 / 2000: loss 0.104191\n",
      "iteration 1400 / 2000: loss 0.114499\n",
      "iteration 1500 / 2000: loss 0.126264\n",
      "iteration 1600 / 2000: loss 0.101967\n",
      "iteration 1700 / 2000: loss 0.123578\n",
      "iteration 1800 / 2000: loss 0.069000\n",
      "iteration 1900 / 2000: loss 0.110110\n",
      "Hidden Size: 90, Learning Rate: 0.0075000250000000004, LR Decay: 0.47250000000000003, Regularization: 0.0, Validation accuracy: 0.9656\n",
      "iteration 0 / 2000: loss 2.302627\n",
      "iteration 100 / 2000: loss 0.317430\n",
      "iteration 200 / 2000: loss 0.206372\n",
      "iteration 300 / 2000: loss 0.189054\n",
      "iteration 400 / 2000: loss 0.201092\n",
      "iteration 500 / 2000: loss 0.196134\n",
      "iteration 600 / 2000: loss 0.108137\n",
      "iteration 700 / 2000: loss 0.122900\n",
      "iteration 800 / 2000: loss 0.152971\n",
      "iteration 900 / 2000: loss 0.102954\n",
      "iteration 1000 / 2000: loss 0.126124\n",
      "iteration 1100 / 2000: loss 0.214175\n",
      "iteration 1200 / 2000: loss 0.200938\n",
      "iteration 1300 / 2000: loss 0.159431\n",
      "iteration 1400 / 2000: loss 0.239661\n",
      "iteration 1500 / 2000: loss 0.145995\n",
      "iteration 1600 / 2000: loss 0.133241\n",
      "iteration 1700 / 2000: loss 0.223158\n",
      "iteration 1800 / 2000: loss 0.124945\n",
      "iteration 1900 / 2000: loss 0.177391\n",
      "Hidden Size: 90, Learning Rate: 0.0075000250000000004, LR Decay: 0.47250000000000003, Regularization: 0.1, Validation accuracy: 0.9644\n",
      "iteration 0 / 2000: loss 2.302664\n",
      "iteration 100 / 2000: loss 0.398928\n",
      "iteration 200 / 2000: loss 0.293415\n",
      "iteration 300 / 2000: loss 0.228572\n",
      "iteration 400 / 2000: loss 0.213469\n",
      "iteration 500 / 2000: loss 0.210770\n",
      "iteration 600 / 2000: loss 0.180373\n",
      "iteration 700 / 2000: loss 0.184484\n",
      "iteration 800 / 2000: loss 0.233160\n",
      "iteration 900 / 2000: loss 0.192465\n",
      "iteration 1000 / 2000: loss 0.258006\n",
      "iteration 1100 / 2000: loss 0.118159\n",
      "iteration 1200 / 2000: loss 0.136282\n",
      "iteration 1300 / 2000: loss 0.175576\n",
      "iteration 1400 / 2000: loss 0.187277\n",
      "iteration 1500 / 2000: loss 0.215187\n",
      "iteration 1600 / 2000: loss 0.151357\n",
      "iteration 1700 / 2000: loss 0.150938\n",
      "iteration 1800 / 2000: loss 0.138894\n",
      "iteration 1900 / 2000: loss 0.172004\n",
      "Hidden Size: 90, Learning Rate: 0.0075000250000000004, LR Decay: 0.47250000000000003, Regularization: 0.2, Validation accuracy: 0.964\n",
      "iteration 0 / 2000: loss 2.302702\n",
      "iteration 100 / 2000: loss 0.443654\n",
      "iteration 200 / 2000: loss 0.277096\n",
      "iteration 300 / 2000: loss 0.228363\n",
      "iteration 400 / 2000: loss 0.217179\n",
      "iteration 500 / 2000: loss 0.193214\n",
      "iteration 600 / 2000: loss 0.235356\n",
      "iteration 700 / 2000: loss 0.158390\n",
      "iteration 800 / 2000: loss 0.207004\n",
      "iteration 900 / 2000: loss 0.208040\n",
      "iteration 1000 / 2000: loss 0.179593\n",
      "iteration 1100 / 2000: loss 0.261804\n",
      "iteration 1200 / 2000: loss 0.161074\n",
      "iteration 1300 / 2000: loss 0.145379\n",
      "iteration 1400 / 2000: loss 0.260176\n",
      "iteration 1500 / 2000: loss 0.257809\n",
      "iteration 1600 / 2000: loss 0.221898\n",
      "iteration 1700 / 2000: loss 0.194420\n",
      "iteration 1800 / 2000: loss 0.249841\n",
      "iteration 1900 / 2000: loss 0.142664\n",
      "Hidden Size: 90, Learning Rate: 0.0075000250000000004, LR Decay: 0.47250000000000003, Regularization: 0.30000000000000004, Validation accuracy: 0.9624\n",
      "iteration 0 / 2000: loss 2.302686\n",
      "iteration 100 / 2000: loss 0.379247\n",
      "iteration 200 / 2000: loss 0.227225\n",
      "iteration 300 / 2000: loss 0.335913\n",
      "iteration 400 / 2000: loss 0.289400\n",
      "iteration 500 / 2000: loss 0.220199\n",
      "iteration 600 / 2000: loss 0.229420\n",
      "iteration 700 / 2000: loss 0.203184\n",
      "iteration 800 / 2000: loss 0.278146\n",
      "iteration 900 / 2000: loss 0.232798\n",
      "iteration 1000 / 2000: loss 0.246340\n",
      "iteration 1100 / 2000: loss 0.288952\n",
      "iteration 1200 / 2000: loss 0.246832\n",
      "iteration 1300 / 2000: loss 0.191771\n",
      "iteration 1400 / 2000: loss 0.261765\n",
      "iteration 1500 / 2000: loss 0.235275\n",
      "iteration 1600 / 2000: loss 0.233958\n",
      "iteration 1700 / 2000: loss 0.268965\n",
      "iteration 1800 / 2000: loss 0.197939\n",
      "iteration 1900 / 2000: loss 0.216787\n",
      "Hidden Size: 90, Learning Rate: 0.0075000250000000004, LR Decay: 0.47250000000000003, Regularization: 0.4, Validation accuracy: 0.9604\n",
      "iteration 0 / 2000: loss 2.302569\n",
      "iteration 100 / 2000: loss 0.380373\n",
      "iteration 200 / 2000: loss 0.285907\n",
      "iteration 300 / 2000: loss 0.260148\n",
      "iteration 400 / 2000: loss 0.235895\n",
      "iteration 500 / 2000: loss 0.218389\n",
      "iteration 600 / 2000: loss 0.196092\n",
      "iteration 700 / 2000: loss 0.211604\n",
      "iteration 800 / 2000: loss 0.193361\n",
      "iteration 900 / 2000: loss 0.158228\n",
      "iteration 1000 / 2000: loss 0.220836\n",
      "iteration 1100 / 2000: loss 0.199043\n",
      "iteration 1200 / 2000: loss 0.142978\n",
      "iteration 1300 / 2000: loss 0.173917\n",
      "iteration 1400 / 2000: loss 0.145481\n",
      "iteration 1500 / 2000: loss 0.181129\n",
      "iteration 1600 / 2000: loss 0.177307\n",
      "iteration 1700 / 2000: loss 0.218729\n",
      "iteration 1800 / 2000: loss 0.232910\n",
      "iteration 1900 / 2000: loss 0.179608\n",
      "Hidden Size: 90, Learning Rate: 0.0075000250000000004, LR Decay: 0.3, Regularization: 0.0, Validation accuracy: 0.9488\n",
      "iteration 0 / 2000: loss 2.302595\n",
      "iteration 100 / 2000: loss 0.308759\n",
      "iteration 200 / 2000: loss 0.218399\n",
      "iteration 300 / 2000: loss 0.351765\n",
      "iteration 400 / 2000: loss 0.358238\n",
      "iteration 500 / 2000: loss 0.212254\n",
      "iteration 600 / 2000: loss 0.209685\n",
      "iteration 700 / 2000: loss 0.235357\n",
      "iteration 800 / 2000: loss 0.143717\n",
      "iteration 900 / 2000: loss 0.215885\n",
      "iteration 1000 / 2000: loss 0.228204\n",
      "iteration 1100 / 2000: loss 0.203679\n",
      "iteration 1200 / 2000: loss 0.299182\n",
      "iteration 1300 / 2000: loss 0.237576\n",
      "iteration 1400 / 2000: loss 0.207853\n",
      "iteration 1500 / 2000: loss 0.236555\n",
      "iteration 1600 / 2000: loss 0.228666\n",
      "iteration 1700 / 2000: loss 0.212863\n",
      "iteration 1800 / 2000: loss 0.181109\n",
      "iteration 1900 / 2000: loss 0.187933\n",
      "Hidden Size: 90, Learning Rate: 0.0075000250000000004, LR Decay: 0.3, Regularization: 0.1, Validation accuracy: 0.9472\n",
      "iteration 0 / 2000: loss 2.302644\n",
      "iteration 100 / 2000: loss 0.308766\n",
      "iteration 200 / 2000: loss 0.344822\n",
      "iteration 300 / 2000: loss 0.271108\n",
      "iteration 400 / 2000: loss 0.234245\n",
      "iteration 500 / 2000: loss 0.269844\n",
      "iteration 600 / 2000: loss 0.221758\n",
      "iteration 700 / 2000: loss 0.165781\n",
      "iteration 800 / 2000: loss 0.196871\n",
      "iteration 900 / 2000: loss 0.165467\n",
      "iteration 1000 / 2000: loss 0.201958\n",
      "iteration 1100 / 2000: loss 0.261916\n",
      "iteration 1200 / 2000: loss 0.239306\n",
      "iteration 1300 / 2000: loss 0.248085\n",
      "iteration 1400 / 2000: loss 0.238668\n",
      "iteration 1500 / 2000: loss 0.278434\n",
      "iteration 1600 / 2000: loss 0.193686\n",
      "iteration 1700 / 2000: loss 0.254823\n",
      "iteration 1800 / 2000: loss 0.166364\n",
      "iteration 1900 / 2000: loss 0.238509\n",
      "Hidden Size: 90, Learning Rate: 0.0075000250000000004, LR Decay: 0.3, Regularization: 0.2, Validation accuracy: 0.9442\n",
      "iteration 0 / 2000: loss 2.302670\n",
      "iteration 100 / 2000: loss 0.476599\n",
      "iteration 200 / 2000: loss 0.314159\n",
      "iteration 300 / 2000: loss 0.341214\n",
      "iteration 400 / 2000: loss 0.179902\n",
      "iteration 500 / 2000: loss 0.260847\n",
      "iteration 600 / 2000: loss 0.224570\n",
      "iteration 700 / 2000: loss 0.283474\n",
      "iteration 800 / 2000: loss 0.364728\n",
      "iteration 900 / 2000: loss 0.303301\n",
      "iteration 1000 / 2000: loss 0.271006\n",
      "iteration 1100 / 2000: loss 0.299892\n",
      "iteration 1200 / 2000: loss 0.253351\n",
      "iteration 1300 / 2000: loss 0.183637\n",
      "iteration 1400 / 2000: loss 0.221920\n",
      "iteration 1500 / 2000: loss 0.229283\n",
      "iteration 1600 / 2000: loss 0.273383\n",
      "iteration 1700 / 2000: loss 0.336082\n",
      "iteration 1800 / 2000: loss 0.378662\n",
      "iteration 1900 / 2000: loss 0.345274\n",
      "Hidden Size: 90, Learning Rate: 0.0075000250000000004, LR Decay: 0.3, Regularization: 0.30000000000000004, Validation accuracy: 0.9468\n",
      "iteration 0 / 2000: loss 2.302732\n",
      "iteration 100 / 2000: loss 0.450261\n",
      "iteration 200 / 2000: loss 0.365352\n",
      "iteration 300 / 2000: loss 0.367499\n",
      "iteration 400 / 2000: loss 0.289438\n",
      "iteration 500 / 2000: loss 0.381478\n",
      "iteration 600 / 2000: loss 0.354600\n",
      "iteration 700 / 2000: loss 0.210257\n",
      "iteration 800 / 2000: loss 0.268436\n",
      "iteration 900 / 2000: loss 0.296398\n",
      "iteration 1000 / 2000: loss 0.240817\n",
      "iteration 1100 / 2000: loss 0.214903\n",
      "iteration 1200 / 2000: loss 0.258940\n",
      "iteration 1300 / 2000: loss 0.273680\n",
      "iteration 1400 / 2000: loss 0.237451\n",
      "iteration 1500 / 2000: loss 0.229553\n",
      "iteration 1600 / 2000: loss 0.290112\n",
      "iteration 1700 / 2000: loss 0.284908\n",
      "iteration 1800 / 2000: loss 0.199061\n",
      "iteration 1900 / 2000: loss 0.258393\n",
      "Hidden Size: 90, Learning Rate: 0.0075000250000000004, LR Decay: 0.3, Regularization: 0.4, Validation accuracy: 0.9476\n",
      "iteration 0 / 2000: loss 2.302585\n",
      "iteration 100 / 2000: loss 0.318424\n",
      "iteration 200 / 2000: loss 0.170033\n",
      "iteration 300 / 2000: loss 0.072054\n",
      "iteration 400 / 2000: loss 0.088892\n",
      "iteration 500 / 2000: loss 0.107412\n",
      "iteration 600 / 2000: loss 0.076197\n",
      "iteration 700 / 2000: loss 0.084383\n",
      "iteration 800 / 2000: loss 0.099969\n",
      "iteration 900 / 2000: loss 0.047787\n",
      "iteration 1000 / 2000: loss 0.088456\n",
      "iteration 1100 / 2000: loss 0.056408\n",
      "iteration 1200 / 2000: loss 0.037067\n",
      "iteration 1300 / 2000: loss 0.032857\n",
      "iteration 1400 / 2000: loss 0.046342\n",
      "iteration 1500 / 2000: loss 0.012411\n",
      "iteration 1600 / 2000: loss 0.060679\n",
      "iteration 1700 / 2000: loss 0.010943\n",
      "iteration 1800 / 2000: loss 0.024142\n",
      "iteration 1900 / 2000: loss 0.039870\n",
      "Hidden Size: 90, Learning Rate: 0.00500005, LR Decay: 0.99, Regularization: 0.0, Validation accuracy: 0.9764\n",
      "iteration 0 / 2000: loss 2.302642\n",
      "iteration 100 / 2000: loss 0.320856\n",
      "iteration 200 / 2000: loss 0.178494\n",
      "iteration 300 / 2000: loss 0.129879\n",
      "iteration 400 / 2000: loss 0.202465\n",
      "iteration 500 / 2000: loss 0.186385\n",
      "iteration 600 / 2000: loss 0.077415\n",
      "iteration 700 / 2000: loss 0.126223\n",
      "iteration 800 / 2000: loss 0.105568\n",
      "iteration 900 / 2000: loss 0.114435\n",
      "iteration 1000 / 2000: loss 0.112599\n",
      "iteration 1100 / 2000: loss 0.146179\n",
      "iteration 1200 / 2000: loss 0.100277\n",
      "iteration 1300 / 2000: loss 0.111953\n",
      "iteration 1400 / 2000: loss 0.113177\n",
      "iteration 1500 / 2000: loss 0.086527\n",
      "iteration 1600 / 2000: loss 0.082130\n",
      "iteration 1700 / 2000: loss 0.100314\n",
      "iteration 1800 / 2000: loss 0.139303\n",
      "iteration 1900 / 2000: loss 0.084654\n",
      "Hidden Size: 90, Learning Rate: 0.00500005, LR Decay: 0.99, Regularization: 0.1, Validation accuracy: 0.9772\n",
      "iteration 0 / 2000: loss 2.302658\n",
      "iteration 100 / 2000: loss 0.299368\n",
      "iteration 200 / 2000: loss 0.162080\n",
      "iteration 300 / 2000: loss 0.198594\n",
      "iteration 400 / 2000: loss 0.185307\n",
      "iteration 500 / 2000: loss 0.156750\n",
      "iteration 600 / 2000: loss 0.143270\n",
      "iteration 700 / 2000: loss 0.176681\n",
      "iteration 800 / 2000: loss 0.161538\n",
      "iteration 900 / 2000: loss 0.165925\n",
      "iteration 1000 / 2000: loss 0.178619\n",
      "iteration 1100 / 2000: loss 0.160809\n",
      "iteration 1200 / 2000: loss 0.160407\n",
      "iteration 1300 / 2000: loss 0.129805\n",
      "iteration 1400 / 2000: loss 0.131158\n",
      "iteration 1500 / 2000: loss 0.172098\n",
      "iteration 1600 / 2000: loss 0.165250\n",
      "iteration 1700 / 2000: loss 0.119094\n",
      "iteration 1800 / 2000: loss 0.132228\n",
      "iteration 1900 / 2000: loss 0.140829\n",
      "Hidden Size: 90, Learning Rate: 0.00500005, LR Decay: 0.99, Regularization: 0.2, Validation accuracy: 0.977\n",
      "iteration 0 / 2000: loss 2.302667\n",
      "iteration 100 / 2000: loss 0.302620\n",
      "iteration 200 / 2000: loss 0.195138\n",
      "iteration 300 / 2000: loss 0.244184\n",
      "iteration 400 / 2000: loss 0.212716\n",
      "iteration 500 / 2000: loss 0.235409\n",
      "iteration 600 / 2000: loss 0.175480\n",
      "iteration 700 / 2000: loss 0.199208\n",
      "iteration 800 / 2000: loss 0.184626\n",
      "iteration 900 / 2000: loss 0.208636\n",
      "iteration 1000 / 2000: loss 0.174801\n",
      "iteration 1100 / 2000: loss 0.211169\n",
      "iteration 1200 / 2000: loss 0.202509\n",
      "iteration 1300 / 2000: loss 0.238200\n",
      "iteration 1400 / 2000: loss 0.171459\n",
      "iteration 1500 / 2000: loss 0.167765\n",
      "iteration 1600 / 2000: loss 0.212693\n",
      "iteration 1700 / 2000: loss 0.172392\n",
      "iteration 1800 / 2000: loss 0.149323\n",
      "iteration 1900 / 2000: loss 0.200804\n",
      "Hidden Size: 90, Learning Rate: 0.00500005, LR Decay: 0.99, Regularization: 0.30000000000000004, Validation accuracy: 0.9732\n",
      "iteration 0 / 2000: loss 2.302739\n",
      "iteration 100 / 2000: loss 0.259361\n",
      "iteration 200 / 2000: loss 0.252478\n",
      "iteration 300 / 2000: loss 0.286519\n",
      "iteration 400 / 2000: loss 0.209430\n",
      "iteration 500 / 2000: loss 0.236640\n",
      "iteration 600 / 2000: loss 0.288890\n",
      "iteration 700 / 2000: loss 0.228250\n",
      "iteration 800 / 2000: loss 0.204048\n",
      "iteration 900 / 2000: loss 0.229728\n",
      "iteration 1000 / 2000: loss 0.265806\n",
      "iteration 1100 / 2000: loss 0.247226\n",
      "iteration 1200 / 2000: loss 0.240333\n",
      "iteration 1300 / 2000: loss 0.198276\n",
      "iteration 1400 / 2000: loss 0.204058\n",
      "iteration 1500 / 2000: loss 0.198487\n",
      "iteration 1600 / 2000: loss 0.202851\n",
      "iteration 1700 / 2000: loss 0.268031\n",
      "iteration 1800 / 2000: loss 0.214284\n",
      "iteration 1900 / 2000: loss 0.214444\n",
      "Hidden Size: 90, Learning Rate: 0.00500005, LR Decay: 0.99, Regularization: 0.4, Validation accuracy: 0.9692\n",
      "iteration 0 / 2000: loss 2.302557\n",
      "iteration 100 / 2000: loss 0.334283\n",
      "iteration 200 / 2000: loss 0.287551\n",
      "iteration 300 / 2000: loss 0.193485\n",
      "iteration 400 / 2000: loss 0.233512\n",
      "iteration 500 / 2000: loss 0.159767\n",
      "iteration 600 / 2000: loss 0.086090\n",
      "iteration 700 / 2000: loss 0.118256\n",
      "iteration 800 / 2000: loss 0.141904\n",
      "iteration 900 / 2000: loss 0.115457\n",
      "iteration 1000 / 2000: loss 0.044132\n",
      "iteration 1100 / 2000: loss 0.085927\n",
      "iteration 1200 / 2000: loss 0.044044\n",
      "iteration 1300 / 2000: loss 0.045756\n",
      "iteration 1400 / 2000: loss 0.140264\n",
      "iteration 1500 / 2000: loss 0.056414\n",
      "iteration 1600 / 2000: loss 0.113283\n",
      "iteration 1700 / 2000: loss 0.048705\n",
      "iteration 1800 / 2000: loss 0.049682\n",
      "iteration 1900 / 2000: loss 0.062455\n",
      "Hidden Size: 90, Learning Rate: 0.00500005, LR Decay: 0.8175, Regularization: 0.0, Validation accuracy: 0.9752\n",
      "iteration 0 / 2000: loss 2.302615\n",
      "iteration 100 / 2000: loss 0.240431\n",
      "iteration 200 / 2000: loss 0.282583\n",
      "iteration 300 / 2000: loss 0.193022\n",
      "iteration 400 / 2000: loss 0.175225\n",
      "iteration 500 / 2000: loss 0.125631\n",
      "iteration 600 / 2000: loss 0.135218\n",
      "iteration 700 / 2000: loss 0.163998\n",
      "iteration 800 / 2000: loss 0.148401\n",
      "iteration 900 / 2000: loss 0.142976\n",
      "iteration 1000 / 2000: loss 0.194107\n",
      "iteration 1100 / 2000: loss 0.150784\n",
      "iteration 1200 / 2000: loss 0.169930\n",
      "iteration 1300 / 2000: loss 0.105070\n",
      "iteration 1400 / 2000: loss 0.092746\n",
      "iteration 1500 / 2000: loss 0.099441\n",
      "iteration 1600 / 2000: loss 0.086134\n",
      "iteration 1700 / 2000: loss 0.141550\n",
      "iteration 1800 / 2000: loss 0.103731\n",
      "iteration 1900 / 2000: loss 0.101155\n",
      "Hidden Size: 90, Learning Rate: 0.00500005, LR Decay: 0.8175, Regularization: 0.1, Validation accuracy: 0.975\n",
      "iteration 0 / 2000: loss 2.302654\n",
      "iteration 100 / 2000: loss 0.321890\n",
      "iteration 200 / 2000: loss 0.267091\n",
      "iteration 300 / 2000: loss 0.171330\n",
      "iteration 400 / 2000: loss 0.216495\n",
      "iteration 500 / 2000: loss 0.178183\n",
      "iteration 600 / 2000: loss 0.182644\n",
      "iteration 700 / 2000: loss 0.210178\n",
      "iteration 800 / 2000: loss 0.178873\n",
      "iteration 900 / 2000: loss 0.134163\n",
      "iteration 1000 / 2000: loss 0.189810\n",
      "iteration 1100 / 2000: loss 0.144821\n",
      "iteration 1200 / 2000: loss 0.151666\n",
      "iteration 1300 / 2000: loss 0.138193\n",
      "iteration 1400 / 2000: loss 0.169678\n",
      "iteration 1500 / 2000: loss 0.175357\n",
      "iteration 1600 / 2000: loss 0.143739\n",
      "iteration 1700 / 2000: loss 0.123913\n",
      "iteration 1800 / 2000: loss 0.158792\n",
      "iteration 1900 / 2000: loss 0.124756\n",
      "Hidden Size: 90, Learning Rate: 0.00500005, LR Decay: 0.8175, Regularization: 0.2, Validation accuracy: 0.9734\n",
      "iteration 0 / 2000: loss 2.302686\n",
      "iteration 100 / 2000: loss 0.275720\n",
      "iteration 200 / 2000: loss 0.337137\n",
      "iteration 300 / 2000: loss 0.167845\n",
      "iteration 400 / 2000: loss 0.259285\n",
      "iteration 500 / 2000: loss 0.286363\n",
      "iteration 600 / 2000: loss 0.261129\n",
      "iteration 700 / 2000: loss 0.179947\n",
      "iteration 800 / 2000: loss 0.221981\n",
      "iteration 900 / 2000: loss 0.161345\n",
      "iteration 1000 / 2000: loss 0.225937\n",
      "iteration 1100 / 2000: loss 0.185647\n",
      "iteration 1200 / 2000: loss 0.292307\n",
      "iteration 1300 / 2000: loss 0.172766\n",
      "iteration 1400 / 2000: loss 0.186591\n",
      "iteration 1500 / 2000: loss 0.186010\n",
      "iteration 1600 / 2000: loss 0.166492\n",
      "iteration 1700 / 2000: loss 0.196049\n",
      "iteration 1800 / 2000: loss 0.183265\n",
      "iteration 1900 / 2000: loss 0.187041\n",
      "Hidden Size: 90, Learning Rate: 0.00500005, LR Decay: 0.8175, Regularization: 0.30000000000000004, Validation accuracy: 0.9718\n",
      "iteration 0 / 2000: loss 2.302715\n",
      "iteration 100 / 2000: loss 0.379146\n",
      "iteration 200 / 2000: loss 0.277660\n",
      "iteration 300 / 2000: loss 0.272344\n",
      "iteration 400 / 2000: loss 0.212876\n",
      "iteration 500 / 2000: loss 0.245947\n",
      "iteration 600 / 2000: loss 0.249614\n",
      "iteration 700 / 2000: loss 0.201429\n",
      "iteration 800 / 2000: loss 0.190589\n",
      "iteration 900 / 2000: loss 0.200772\n",
      "iteration 1000 / 2000: loss 0.251213\n",
      "iteration 1100 / 2000: loss 0.210073\n",
      "iteration 1200 / 2000: loss 0.252849\n",
      "iteration 1300 / 2000: loss 0.288128\n",
      "iteration 1400 / 2000: loss 0.198497\n",
      "iteration 1500 / 2000: loss 0.185465\n",
      "iteration 1600 / 2000: loss 0.182625\n",
      "iteration 1700 / 2000: loss 0.197786\n",
      "iteration 1800 / 2000: loss 0.189416\n",
      "iteration 1900 / 2000: loss 0.180575\n",
      "Hidden Size: 90, Learning Rate: 0.00500005, LR Decay: 0.8175, Regularization: 0.4, Validation accuracy: 0.9696\n",
      "iteration 0 / 2000: loss 2.302577\n",
      "iteration 100 / 2000: loss 0.331322\n",
      "iteration 200 / 2000: loss 0.235613\n",
      "iteration 300 / 2000: loss 0.238110\n",
      "iteration 400 / 2000: loss 0.170999\n",
      "iteration 500 / 2000: loss 0.151945\n",
      "iteration 600 / 2000: loss 0.166823\n",
      "iteration 700 / 2000: loss 0.152472\n",
      "iteration 800 / 2000: loss 0.163792\n",
      "iteration 900 / 2000: loss 0.080321\n",
      "iteration 1000 / 2000: loss 0.106530\n",
      "iteration 1100 / 2000: loss 0.170150\n",
      "iteration 1200 / 2000: loss 0.082046\n",
      "iteration 1300 / 2000: loss 0.086144\n",
      "iteration 1400 / 2000: loss 0.075463\n",
      "iteration 1500 / 2000: loss 0.107156\n",
      "iteration 1600 / 2000: loss 0.094238\n",
      "iteration 1700 / 2000: loss 0.075218\n",
      "iteration 1800 / 2000: loss 0.080702\n",
      "iteration 1900 / 2000: loss 0.141038\n",
      "Hidden Size: 90, Learning Rate: 0.00500005, LR Decay: 0.645, Regularization: 0.0, Validation accuracy: 0.9686\n",
      "iteration 0 / 2000: loss 2.302636\n",
      "iteration 100 / 2000: loss 0.303682\n",
      "iteration 200 / 2000: loss 0.288730\n",
      "iteration 300 / 2000: loss 0.251281\n",
      "iteration 400 / 2000: loss 0.227709\n",
      "iteration 500 / 2000: loss 0.119140\n",
      "iteration 600 / 2000: loss 0.118885\n",
      "iteration 700 / 2000: loss 0.125305\n",
      "iteration 800 / 2000: loss 0.118947\n",
      "iteration 900 / 2000: loss 0.102316\n",
      "iteration 1000 / 2000: loss 0.204712\n",
      "iteration 1100 / 2000: loss 0.085461\n",
      "iteration 1200 / 2000: loss 0.142700\n",
      "iteration 1300 / 2000: loss 0.084303\n",
      "iteration 1400 / 2000: loss 0.108816\n",
      "iteration 1500 / 2000: loss 0.156395\n",
      "iteration 1600 / 2000: loss 0.171893\n",
      "iteration 1700 / 2000: loss 0.211947\n",
      "iteration 1800 / 2000: loss 0.209444\n",
      "iteration 1900 / 2000: loss 0.132926\n",
      "Hidden Size: 90, Learning Rate: 0.00500005, LR Decay: 0.645, Regularization: 0.1, Validation accuracy: 0.9682\n",
      "iteration 0 / 2000: loss 2.302650\n",
      "iteration 100 / 2000: loss 0.323323\n",
      "iteration 200 / 2000: loss 0.223432\n",
      "iteration 300 / 2000: loss 0.217241\n",
      "iteration 400 / 2000: loss 0.215798\n",
      "iteration 500 / 2000: loss 0.247380\n",
      "iteration 600 / 2000: loss 0.214142\n",
      "iteration 700 / 2000: loss 0.150343\n",
      "iteration 800 / 2000: loss 0.145885\n",
      "iteration 900 / 2000: loss 0.238196\n",
      "iteration 1000 / 2000: loss 0.219002\n",
      "iteration 1100 / 2000: loss 0.221190\n",
      "iteration 1200 / 2000: loss 0.139300\n",
      "iteration 1300 / 2000: loss 0.157631\n",
      "iteration 1400 / 2000: loss 0.220973\n",
      "iteration 1500 / 2000: loss 0.168268\n",
      "iteration 1600 / 2000: loss 0.139106\n",
      "iteration 1700 / 2000: loss 0.145499\n",
      "iteration 1800 / 2000: loss 0.147579\n",
      "iteration 1900 / 2000: loss 0.260239\n",
      "Hidden Size: 90, Learning Rate: 0.00500005, LR Decay: 0.645, Regularization: 0.2, Validation accuracy: 0.9666\n",
      "iteration 0 / 2000: loss 2.302700\n",
      "iteration 100 / 2000: loss 0.354476\n",
      "iteration 200 / 2000: loss 0.240942\n",
      "iteration 300 / 2000: loss 0.238102\n",
      "iteration 400 / 2000: loss 0.232899\n",
      "iteration 500 / 2000: loss 0.199501\n",
      "iteration 600 / 2000: loss 0.225231\n",
      "iteration 700 / 2000: loss 0.223788\n",
      "iteration 800 / 2000: loss 0.179870\n",
      "iteration 900 / 2000: loss 0.264046\n",
      "iteration 1000 / 2000: loss 0.189595\n",
      "iteration 1100 / 2000: loss 0.155184\n",
      "iteration 1200 / 2000: loss 0.177746\n",
      "iteration 1300 / 2000: loss 0.194861\n",
      "iteration 1400 / 2000: loss 0.234339\n",
      "iteration 1500 / 2000: loss 0.177319\n",
      "iteration 1600 / 2000: loss 0.202478\n",
      "iteration 1700 / 2000: loss 0.143049\n",
      "iteration 1800 / 2000: loss 0.180573\n",
      "iteration 1900 / 2000: loss 0.157587\n",
      "Hidden Size: 90, Learning Rate: 0.00500005, LR Decay: 0.645, Regularization: 0.30000000000000004, Validation accuracy: 0.9654\n",
      "iteration 0 / 2000: loss 2.302729\n",
      "iteration 100 / 2000: loss 0.400334\n",
      "iteration 200 / 2000: loss 0.301216\n",
      "iteration 300 / 2000: loss 0.217057\n",
      "iteration 400 / 2000: loss 0.215202\n",
      "iteration 500 / 2000: loss 0.313170\n",
      "iteration 600 / 2000: loss 0.299213\n",
      "iteration 700 / 2000: loss 0.209540\n",
      "iteration 800 / 2000: loss 0.232720\n",
      "iteration 900 / 2000: loss 0.234882\n",
      "iteration 1000 / 2000: loss 0.248431\n",
      "iteration 1100 / 2000: loss 0.213684\n",
      "iteration 1200 / 2000: loss 0.181161\n",
      "iteration 1300 / 2000: loss 0.207688\n",
      "iteration 1400 / 2000: loss 0.220728\n",
      "iteration 1500 / 2000: loss 0.263920\n",
      "iteration 1600 / 2000: loss 0.205083\n",
      "iteration 1700 / 2000: loss 0.196536\n",
      "iteration 1800 / 2000: loss 0.274963\n",
      "iteration 1900 / 2000: loss 0.188870\n",
      "Hidden Size: 90, Learning Rate: 0.00500005, LR Decay: 0.645, Regularization: 0.4, Validation accuracy: 0.9654\n",
      "iteration 0 / 2000: loss 2.302588\n",
      "iteration 100 / 2000: loss 0.324882\n",
      "iteration 200 / 2000: loss 0.279689\n",
      "iteration 300 / 2000: loss 0.147783\n",
      "iteration 400 / 2000: loss 0.181828\n",
      "iteration 500 / 2000: loss 0.220861\n",
      "iteration 600 / 2000: loss 0.171619\n",
      "iteration 700 / 2000: loss 0.248646\n",
      "iteration 800 / 2000: loss 0.147669\n",
      "iteration 900 / 2000: loss 0.228435\n",
      "iteration 1000 / 2000: loss 0.154562\n",
      "iteration 1100 / 2000: loss 0.173976\n",
      "iteration 1200 / 2000: loss 0.219413\n",
      "iteration 1300 / 2000: loss 0.182779\n",
      "iteration 1400 / 2000: loss 0.087849\n",
      "iteration 1500 / 2000: loss 0.125338\n",
      "iteration 1600 / 2000: loss 0.171074\n",
      "iteration 1700 / 2000: loss 0.202645\n",
      "iteration 1800 / 2000: loss 0.101943\n",
      "iteration 1900 / 2000: loss 0.173883\n",
      "Hidden Size: 90, Learning Rate: 0.00500005, LR Decay: 0.47250000000000003, Regularization: 0.0, Validation accuracy: 0.954\n",
      "iteration 0 / 2000: loss 2.302612\n",
      "iteration 100 / 2000: loss 0.429439\n",
      "iteration 200 / 2000: loss 0.295759\n",
      "iteration 300 / 2000: loss 0.231639\n",
      "iteration 400 / 2000: loss 0.305635\n",
      "iteration 500 / 2000: loss 0.213405\n",
      "iteration 600 / 2000: loss 0.254832\n",
      "iteration 700 / 2000: loss 0.179214\n",
      "iteration 800 / 2000: loss 0.135917\n",
      "iteration 900 / 2000: loss 0.150748\n",
      "iteration 1000 / 2000: loss 0.219279\n",
      "iteration 1100 / 2000: loss 0.185425\n",
      "iteration 1200 / 2000: loss 0.186201\n",
      "iteration 1300 / 2000: loss 0.172408\n",
      "iteration 1400 / 2000: loss 0.165653\n",
      "iteration 1500 / 2000: loss 0.159010\n",
      "iteration 1600 / 2000: loss 0.230819\n",
      "iteration 1700 / 2000: loss 0.214500\n",
      "iteration 1800 / 2000: loss 0.199287\n",
      "iteration 1900 / 2000: loss 0.236314\n",
      "Hidden Size: 90, Learning Rate: 0.00500005, LR Decay: 0.47250000000000003, Regularization: 0.1, Validation accuracy: 0.9566\n",
      "iteration 0 / 2000: loss 2.302666\n",
      "iteration 100 / 2000: loss 0.248970\n",
      "iteration 200 / 2000: loss 0.278808\n",
      "iteration 300 / 2000: loss 0.283292\n",
      "iteration 400 / 2000: loss 0.271931\n",
      "iteration 500 / 2000: loss 0.206036\n",
      "iteration 600 / 2000: loss 0.174184\n",
      "iteration 700 / 2000: loss 0.201307\n",
      "iteration 800 / 2000: loss 0.180816\n",
      "iteration 900 / 2000: loss 0.166707\n",
      "iteration 1000 / 2000: loss 0.223422\n",
      "iteration 1100 / 2000: loss 0.214690\n",
      "iteration 1200 / 2000: loss 0.175006\n",
      "iteration 1300 / 2000: loss 0.229640\n",
      "iteration 1400 / 2000: loss 0.177406\n",
      "iteration 1500 / 2000: loss 0.180571\n",
      "iteration 1600 / 2000: loss 0.143805\n",
      "iteration 1700 / 2000: loss 0.216563\n",
      "iteration 1800 / 2000: loss 0.179087\n",
      "iteration 1900 / 2000: loss 0.264180\n",
      "Hidden Size: 90, Learning Rate: 0.00500005, LR Decay: 0.47250000000000003, Regularization: 0.2, Validation accuracy: 0.9566\n",
      "iteration 0 / 2000: loss 2.302697\n",
      "iteration 100 / 2000: loss 0.364902\n",
      "iteration 200 / 2000: loss 0.335413\n",
      "iteration 300 / 2000: loss 0.360436\n",
      "iteration 400 / 2000: loss 0.223224\n",
      "iteration 500 / 2000: loss 0.264004\n",
      "iteration 600 / 2000: loss 0.198346\n",
      "iteration 700 / 2000: loss 0.273906\n",
      "iteration 800 / 2000: loss 0.251761\n",
      "iteration 900 / 2000: loss 0.281320\n",
      "iteration 1000 / 2000: loss 0.256138\n",
      "iteration 1100 / 2000: loss 0.203452\n",
      "iteration 1200 / 2000: loss 0.250371\n",
      "iteration 1300 / 2000: loss 0.162937\n",
      "iteration 1400 / 2000: loss 0.204732\n",
      "iteration 1500 / 2000: loss 0.181876\n",
      "iteration 1600 / 2000: loss 0.231768\n",
      "iteration 1700 / 2000: loss 0.191312\n",
      "iteration 1800 / 2000: loss 0.244891\n",
      "iteration 1900 / 2000: loss 0.238017\n",
      "Hidden Size: 90, Learning Rate: 0.00500005, LR Decay: 0.47250000000000003, Regularization: 0.30000000000000004, Validation accuracy: 0.958\n",
      "iteration 0 / 2000: loss 2.302740\n",
      "iteration 100 / 2000: loss 0.416536\n",
      "iteration 200 / 2000: loss 0.406517\n",
      "iteration 300 / 2000: loss 0.324116\n",
      "iteration 400 / 2000: loss 0.294440\n",
      "iteration 500 / 2000: loss 0.257126\n",
      "iteration 600 / 2000: loss 0.273031\n",
      "iteration 700 / 2000: loss 0.273474\n",
      "iteration 800 / 2000: loss 0.234083\n",
      "iteration 900 / 2000: loss 0.228044\n",
      "iteration 1000 / 2000: loss 0.265378\n",
      "iteration 1100 / 2000: loss 0.291006\n",
      "iteration 1200 / 2000: loss 0.273555\n",
      "iteration 1300 / 2000: loss 0.257510\n",
      "iteration 1400 / 2000: loss 0.234659\n",
      "iteration 1500 / 2000: loss 0.241054\n",
      "iteration 1600 / 2000: loss 0.262240\n",
      "iteration 1700 / 2000: loss 0.270651\n",
      "iteration 1800 / 2000: loss 0.270420\n",
      "iteration 1900 / 2000: loss 0.290579\n",
      "Hidden Size: 90, Learning Rate: 0.00500005, LR Decay: 0.47250000000000003, Regularization: 0.4, Validation accuracy: 0.9544\n",
      "iteration 0 / 2000: loss 2.302594\n",
      "iteration 100 / 2000: loss 0.379961\n",
      "iteration 200 / 2000: loss 0.263578\n",
      "iteration 300 / 2000: loss 0.257566\n",
      "iteration 400 / 2000: loss 0.224765\n",
      "iteration 500 / 2000: loss 0.166722\n",
      "iteration 600 / 2000: loss 0.262645\n",
      "iteration 700 / 2000: loss 0.235481\n",
      "iteration 800 / 2000: loss 0.229470\n",
      "iteration 900 / 2000: loss 0.173921\n",
      "iteration 1000 / 2000: loss 0.349399\n",
      "iteration 1100 / 2000: loss 0.317672\n",
      "iteration 1200 / 2000: loss 0.299445\n",
      "iteration 1300 / 2000: loss 0.255594\n",
      "iteration 1400 / 2000: loss 0.294587\n",
      "iteration 1500 / 2000: loss 0.175718\n",
      "iteration 1600 / 2000: loss 0.270487\n",
      "iteration 1700 / 2000: loss 0.281602\n",
      "iteration 1800 / 2000: loss 0.240882\n",
      "iteration 1900 / 2000: loss 0.206307\n",
      "Hidden Size: 90, Learning Rate: 0.00500005, LR Decay: 0.3, Regularization: 0.0, Validation accuracy: 0.9364\n",
      "iteration 0 / 2000: loss 2.302616\n",
      "iteration 100 / 2000: loss 0.439318\n",
      "iteration 200 / 2000: loss 0.352973\n",
      "iteration 300 / 2000: loss 0.255287\n",
      "iteration 400 / 2000: loss 0.254109\n",
      "iteration 500 / 2000: loss 0.267214\n",
      "iteration 600 / 2000: loss 0.200740\n",
      "iteration 700 / 2000: loss 0.250297\n",
      "iteration 800 / 2000: loss 0.248763\n",
      "iteration 900 / 2000: loss 0.274639\n",
      "iteration 1000 / 2000: loss 0.182816\n",
      "iteration 1100 / 2000: loss 0.215122\n",
      "iteration 1200 / 2000: loss 0.200150\n",
      "iteration 1300 / 2000: loss 0.300939\n",
      "iteration 1400 / 2000: loss 0.327292\n",
      "iteration 1500 / 2000: loss 0.278699\n",
      "iteration 1600 / 2000: loss 0.264236\n",
      "iteration 1700 / 2000: loss 0.178952\n",
      "iteration 1800 / 2000: loss 0.182796\n",
      "iteration 1900 / 2000: loss 0.244839\n",
      "Hidden Size: 90, Learning Rate: 0.00500005, LR Decay: 0.3, Regularization: 0.1, Validation accuracy: 0.9342\n",
      "iteration 0 / 2000: loss 2.302637\n",
      "iteration 100 / 2000: loss 0.598809\n",
      "iteration 200 / 2000: loss 0.290229\n",
      "iteration 300 / 2000: loss 0.405112\n",
      "iteration 400 / 2000: loss 0.254861\n",
      "iteration 500 / 2000: loss 0.328270\n",
      "iteration 600 / 2000: loss 0.314602\n",
      "iteration 700 / 2000: loss 0.259693\n",
      "iteration 800 / 2000: loss 0.283293\n",
      "iteration 900 / 2000: loss 0.262150\n",
      "iteration 1000 / 2000: loss 0.330844\n",
      "iteration 1100 / 2000: loss 0.266018\n",
      "iteration 1200 / 2000: loss 0.312945\n",
      "iteration 1300 / 2000: loss 0.339412\n",
      "iteration 1400 / 2000: loss 0.257188\n",
      "iteration 1500 / 2000: loss 0.331866\n",
      "iteration 1600 / 2000: loss 0.324945\n",
      "iteration 1700 / 2000: loss 0.243882\n",
      "iteration 1800 / 2000: loss 0.236908\n",
      "iteration 1900 / 2000: loss 0.354502\n",
      "Hidden Size: 90, Learning Rate: 0.00500005, LR Decay: 0.3, Regularization: 0.2, Validation accuracy: 0.937\n",
      "iteration 0 / 2000: loss 2.302686\n",
      "iteration 100 / 2000: loss 0.534566\n",
      "iteration 200 / 2000: loss 0.330994\n",
      "iteration 300 / 2000: loss 0.368679\n",
      "iteration 400 / 2000: loss 0.331849\n",
      "iteration 500 / 2000: loss 0.314154\n",
      "iteration 600 / 2000: loss 0.231271\n",
      "iteration 700 / 2000: loss 0.405352\n",
      "iteration 800 / 2000: loss 0.274635\n",
      "iteration 900 / 2000: loss 0.204500\n",
      "iteration 1000 / 2000: loss 0.263184\n",
      "iteration 1100 / 2000: loss 0.281394\n",
      "iteration 1200 / 2000: loss 0.317169\n",
      "iteration 1300 / 2000: loss 0.297358\n",
      "iteration 1400 / 2000: loss 0.264263\n",
      "iteration 1500 / 2000: loss 0.259397\n",
      "iteration 1600 / 2000: loss 0.366410\n",
      "iteration 1700 / 2000: loss 0.247444\n",
      "iteration 1800 / 2000: loss 0.312552\n",
      "iteration 1900 / 2000: loss 0.263720\n",
      "Hidden Size: 90, Learning Rate: 0.00500005, LR Decay: 0.3, Regularization: 0.30000000000000004, Validation accuracy: 0.9332\n",
      "iteration 0 / 2000: loss 2.302741\n",
      "iteration 100 / 2000: loss 0.459278\n",
      "iteration 200 / 2000: loss 0.317959\n",
      "iteration 300 / 2000: loss 0.343619\n",
      "iteration 400 / 2000: loss 0.335800\n",
      "iteration 500 / 2000: loss 0.320099\n",
      "iteration 600 / 2000: loss 0.332488\n",
      "iteration 700 / 2000: loss 0.299366\n",
      "iteration 800 / 2000: loss 0.321375\n",
      "iteration 900 / 2000: loss 0.335546\n",
      "iteration 1000 / 2000: loss 0.235986\n",
      "iteration 1100 / 2000: loss 0.371511\n",
      "iteration 1200 / 2000: loss 0.401484\n",
      "iteration 1300 / 2000: loss 0.287666\n",
      "iteration 1400 / 2000: loss 0.317019\n",
      "iteration 1500 / 2000: loss 0.307731\n",
      "iteration 1600 / 2000: loss 0.302297\n",
      "iteration 1700 / 2000: loss 0.347867\n",
      "iteration 1800 / 2000: loss 0.259799\n",
      "iteration 1900 / 2000: loss 0.290354\n",
      "Hidden Size: 90, Learning Rate: 0.00500005, LR Decay: 0.3, Regularization: 0.4, Validation accuracy: 0.9356\n",
      "iteration 0 / 2000: loss 2.302581\n",
      "iteration 100 / 2000: loss 0.279364\n",
      "iteration 200 / 2000: loss 0.309331\n",
      "iteration 300 / 2000: loss 0.188755\n",
      "iteration 400 / 2000: loss 0.272327\n",
      "iteration 500 / 2000: loss 0.214832\n",
      "iteration 600 / 2000: loss 0.111107\n",
      "iteration 700 / 2000: loss 0.158997\n",
      "iteration 800 / 2000: loss 0.129808\n",
      "iteration 900 / 2000: loss 0.138418\n",
      "iteration 1000 / 2000: loss 0.066013\n",
      "iteration 1100 / 2000: loss 0.089933\n",
      "iteration 1200 / 2000: loss 0.117802\n",
      "iteration 1300 / 2000: loss 0.068531\n",
      "iteration 1400 / 2000: loss 0.068676\n",
      "iteration 1500 / 2000: loss 0.041374\n",
      "iteration 1600 / 2000: loss 0.055747\n",
      "iteration 1700 / 2000: loss 0.085705\n",
      "iteration 1800 / 2000: loss 0.061250\n",
      "iteration 1900 / 2000: loss 0.045731\n",
      "Hidden Size: 90, Learning Rate: 0.002500074999999999, LR Decay: 0.99, Regularization: 0.0, Validation accuracy: 0.9758\n",
      "iteration 0 / 2000: loss 2.302627\n",
      "iteration 100 / 2000: loss 0.353952\n",
      "iteration 200 / 2000: loss 0.304557\n",
      "iteration 300 / 2000: loss 0.256793\n",
      "iteration 400 / 2000: loss 0.205433\n",
      "iteration 500 / 2000: loss 0.196029\n",
      "iteration 600 / 2000: loss 0.170159\n",
      "iteration 700 / 2000: loss 0.158531\n",
      "iteration 800 / 2000: loss 0.134848\n",
      "iteration 900 / 2000: loss 0.102813\n",
      "iteration 1000 / 2000: loss 0.234179\n",
      "iteration 1100 / 2000: loss 0.156641\n",
      "iteration 1200 / 2000: loss 0.093389\n",
      "iteration 1300 / 2000: loss 0.132608\n",
      "iteration 1400 / 2000: loss 0.101975\n",
      "iteration 1500 / 2000: loss 0.085595\n",
      "iteration 1600 / 2000: loss 0.086043\n",
      "iteration 1700 / 2000: loss 0.085703\n",
      "iteration 1800 / 2000: loss 0.130975\n",
      "iteration 1900 / 2000: loss 0.100495\n",
      "Hidden Size: 90, Learning Rate: 0.002500074999999999, LR Decay: 0.99, Regularization: 0.1, Validation accuracy: 0.9744\n",
      "iteration 0 / 2000: loss 2.302677\n",
      "iteration 100 / 2000: loss 0.405592\n",
      "iteration 200 / 2000: loss 0.206804\n",
      "iteration 300 / 2000: loss 0.234528\n",
      "iteration 400 / 2000: loss 0.172283\n",
      "iteration 500 / 2000: loss 0.180010\n",
      "iteration 600 / 2000: loss 0.208335\n",
      "iteration 700 / 2000: loss 0.167723\n",
      "iteration 800 / 2000: loss 0.177996\n",
      "iteration 900 / 2000: loss 0.167347\n",
      "iteration 1000 / 2000: loss 0.193343\n",
      "iteration 1100 / 2000: loss 0.186735\n",
      "iteration 1200 / 2000: loss 0.233296\n",
      "iteration 1300 / 2000: loss 0.133244\n",
      "iteration 1400 / 2000: loss 0.134005\n",
      "iteration 1500 / 2000: loss 0.199727\n",
      "iteration 1600 / 2000: loss 0.137047\n",
      "iteration 1700 / 2000: loss 0.186740\n",
      "iteration 1800 / 2000: loss 0.138906\n",
      "iteration 1900 / 2000: loss 0.132480\n",
      "Hidden Size: 90, Learning Rate: 0.002500074999999999, LR Decay: 0.99, Regularization: 0.2, Validation accuracy: 0.9706\n",
      "iteration 0 / 2000: loss 2.302691\n",
      "iteration 100 / 2000: loss 0.372815\n",
      "iteration 200 / 2000: loss 0.228341\n",
      "iteration 300 / 2000: loss 0.227144\n",
      "iteration 400 / 2000: loss 0.326124\n",
      "iteration 500 / 2000: loss 0.227976\n",
      "iteration 600 / 2000: loss 0.193365\n",
      "iteration 700 / 2000: loss 0.183413\n",
      "iteration 800 / 2000: loss 0.174286\n",
      "iteration 900 / 2000: loss 0.170476\n",
      "iteration 1000 / 2000: loss 0.175459\n",
      "iteration 1100 / 2000: loss 0.163159\n",
      "iteration 1200 / 2000: loss 0.174936\n",
      "iteration 1300 / 2000: loss 0.191009\n",
      "iteration 1400 / 2000: loss 0.145028\n",
      "iteration 1500 / 2000: loss 0.158090\n",
      "iteration 1600 / 2000: loss 0.164313\n",
      "iteration 1700 / 2000: loss 0.182228\n",
      "iteration 1800 / 2000: loss 0.168382\n",
      "iteration 1900 / 2000: loss 0.174785\n",
      "Hidden Size: 90, Learning Rate: 0.002500074999999999, LR Decay: 0.99, Regularization: 0.30000000000000004, Validation accuracy: 0.9704\n",
      "iteration 0 / 2000: loss 2.302709\n",
      "iteration 100 / 2000: loss 0.537803\n",
      "iteration 200 / 2000: loss 0.333602\n",
      "iteration 300 / 2000: loss 0.334544\n",
      "iteration 400 / 2000: loss 0.210642\n",
      "iteration 500 / 2000: loss 0.231780\n",
      "iteration 600 / 2000: loss 0.233464\n",
      "iteration 700 / 2000: loss 0.180714\n",
      "iteration 800 / 2000: loss 0.245214\n",
      "iteration 900 / 2000: loss 0.221490\n",
      "iteration 1000 / 2000: loss 0.186638\n",
      "iteration 1100 / 2000: loss 0.250664\n",
      "iteration 1200 / 2000: loss 0.218236\n",
      "iteration 1300 / 2000: loss 0.255997\n",
      "iteration 1400 / 2000: loss 0.208209\n",
      "iteration 1500 / 2000: loss 0.196152\n",
      "iteration 1600 / 2000: loss 0.189923\n",
      "iteration 1700 / 2000: loss 0.201305\n",
      "iteration 1800 / 2000: loss 0.209841\n",
      "iteration 1900 / 2000: loss 0.214899\n",
      "Hidden Size: 90, Learning Rate: 0.002500074999999999, LR Decay: 0.99, Regularization: 0.4, Validation accuracy: 0.9716\n",
      "iteration 0 / 2000: loss 2.302590\n",
      "iteration 100 / 2000: loss 0.365832\n",
      "iteration 200 / 2000: loss 0.239264\n",
      "iteration 300 / 2000: loss 0.196270\n",
      "iteration 400 / 2000: loss 0.262001\n",
      "iteration 500 / 2000: loss 0.190725\n",
      "iteration 600 / 2000: loss 0.140031\n",
      "iteration 700 / 2000: loss 0.148441\n",
      "iteration 800 / 2000: loss 0.164534\n",
      "iteration 900 / 2000: loss 0.138983\n",
      "iteration 1000 / 2000: loss 0.129218\n",
      "iteration 1100 / 2000: loss 0.131298\n",
      "iteration 1200 / 2000: loss 0.183145\n",
      "iteration 1300 / 2000: loss 0.124598\n",
      "iteration 1400 / 2000: loss 0.117796\n",
      "iteration 1500 / 2000: loss 0.087518\n",
      "iteration 1600 / 2000: loss 0.118874\n",
      "iteration 1700 / 2000: loss 0.087223\n",
      "iteration 1800 / 2000: loss 0.143338\n",
      "iteration 1900 / 2000: loss 0.084535\n",
      "Hidden Size: 90, Learning Rate: 0.002500074999999999, LR Decay: 0.8175, Regularization: 0.0, Validation accuracy: 0.9682\n",
      "iteration 0 / 2000: loss 2.302611\n",
      "iteration 100 / 2000: loss 0.333946\n",
      "iteration 200 / 2000: loss 0.277452\n",
      "iteration 300 / 2000: loss 0.255552\n",
      "iteration 400 / 2000: loss 0.221678\n",
      "iteration 500 / 2000: loss 0.226044\n",
      "iteration 600 / 2000: loss 0.273274\n",
      "iteration 700 / 2000: loss 0.170055\n",
      "iteration 800 / 2000: loss 0.116624\n",
      "iteration 900 / 2000: loss 0.198053\n",
      "iteration 1000 / 2000: loss 0.176508\n",
      "iteration 1100 / 2000: loss 0.147298\n",
      "iteration 1200 / 2000: loss 0.109131\n",
      "iteration 1300 / 2000: loss 0.133818\n",
      "iteration 1400 / 2000: loss 0.119379\n",
      "iteration 1500 / 2000: loss 0.129449\n",
      "iteration 1600 / 2000: loss 0.105470\n",
      "iteration 1700 / 2000: loss 0.143643\n",
      "iteration 1800 / 2000: loss 0.187603\n",
      "iteration 1900 / 2000: loss 0.128409\n",
      "Hidden Size: 90, Learning Rate: 0.002500074999999999, LR Decay: 0.8175, Regularization: 0.1, Validation accuracy: 0.9674\n",
      "iteration 0 / 2000: loss 2.302641\n",
      "iteration 100 / 2000: loss 0.339337\n",
      "iteration 200 / 2000: loss 0.278996\n",
      "iteration 300 / 2000: loss 0.265273\n",
      "iteration 400 / 2000: loss 0.192237\n",
      "iteration 500 / 2000: loss 0.304726\n",
      "iteration 600 / 2000: loss 0.199615\n",
      "iteration 700 / 2000: loss 0.269972\n",
      "iteration 800 / 2000: loss 0.216780\n",
      "iteration 900 / 2000: loss 0.174215\n",
      "iteration 1000 / 2000: loss 0.228384\n",
      "iteration 1100 / 2000: loss 0.180932\n",
      "iteration 1200 / 2000: loss 0.218617\n",
      "iteration 1300 / 2000: loss 0.191204\n",
      "iteration 1400 / 2000: loss 0.196387\n",
      "iteration 1500 / 2000: loss 0.166412\n",
      "iteration 1600 / 2000: loss 0.153783\n",
      "iteration 1700 / 2000: loss 0.115648\n",
      "iteration 1800 / 2000: loss 0.194776\n",
      "iteration 1900 / 2000: loss 0.150454\n",
      "Hidden Size: 90, Learning Rate: 0.002500074999999999, LR Decay: 0.8175, Regularization: 0.2, Validation accuracy: 0.968\n",
      "iteration 0 / 2000: loss 2.302708\n",
      "iteration 100 / 2000: loss 0.331108\n",
      "iteration 200 / 2000: loss 0.297181\n",
      "iteration 300 / 2000: loss 0.351891\n",
      "iteration 400 / 2000: loss 0.362092\n",
      "iteration 500 / 2000: loss 0.338439\n",
      "iteration 600 / 2000: loss 0.220794\n",
      "iteration 700 / 2000: loss 0.306594\n",
      "iteration 800 / 2000: loss 0.262872\n",
      "iteration 900 / 2000: loss 0.192557\n",
      "iteration 1000 / 2000: loss 0.223155\n",
      "iteration 1100 / 2000: loss 0.220242\n",
      "iteration 1200 / 2000: loss 0.208536\n",
      "iteration 1300 / 2000: loss 0.176142\n",
      "iteration 1400 / 2000: loss 0.277379\n",
      "iteration 1500 / 2000: loss 0.149684\n",
      "iteration 1600 / 2000: loss 0.170298\n",
      "iteration 1700 / 2000: loss 0.205055\n",
      "iteration 1800 / 2000: loss 0.199279\n",
      "iteration 1900 / 2000: loss 0.190533\n",
      "Hidden Size: 90, Learning Rate: 0.002500074999999999, LR Decay: 0.8175, Regularization: 0.30000000000000004, Validation accuracy: 0.9666\n",
      "iteration 0 / 2000: loss 2.302700\n",
      "iteration 100 / 2000: loss 0.402599\n",
      "iteration 200 / 2000: loss 0.307064\n",
      "iteration 300 / 2000: loss 0.262199\n",
      "iteration 400 / 2000: loss 0.351481\n",
      "iteration 500 / 2000: loss 0.236766\n",
      "iteration 600 / 2000: loss 0.313734\n",
      "iteration 700 / 2000: loss 0.222280\n",
      "iteration 800 / 2000: loss 0.260352\n",
      "iteration 900 / 2000: loss 0.235391\n",
      "iteration 1000 / 2000: loss 0.200576\n",
      "iteration 1100 / 2000: loss 0.259337\n",
      "iteration 1200 / 2000: loss 0.242170\n",
      "iteration 1300 / 2000: loss 0.283350\n",
      "iteration 1400 / 2000: loss 0.210638\n",
      "iteration 1500 / 2000: loss 0.279334\n",
      "iteration 1600 / 2000: loss 0.229817\n",
      "iteration 1700 / 2000: loss 0.219816\n",
      "iteration 1800 / 2000: loss 0.215910\n",
      "iteration 1900 / 2000: loss 0.260740\n",
      "Hidden Size: 90, Learning Rate: 0.002500074999999999, LR Decay: 0.8175, Regularization: 0.4, Validation accuracy: 0.965\n",
      "iteration 0 / 2000: loss 2.302635\n",
      "iteration 100 / 2000: loss 0.478512\n",
      "iteration 200 / 2000: loss 0.351958\n",
      "iteration 300 / 2000: loss 0.150110\n",
      "iteration 400 / 2000: loss 0.256358\n",
      "iteration 500 / 2000: loss 0.214437\n",
      "iteration 600 / 2000: loss 0.157394\n",
      "iteration 700 / 2000: loss 0.232009\n",
      "iteration 800 / 2000: loss 0.186330\n",
      "iteration 900 / 2000: loss 0.177536\n",
      "iteration 1000 / 2000: loss 0.156556\n",
      "iteration 1100 / 2000: loss 0.135305\n",
      "iteration 1200 / 2000: loss 0.142091\n",
      "iteration 1300 / 2000: loss 0.206950\n",
      "iteration 1400 / 2000: loss 0.195063\n",
      "iteration 1500 / 2000: loss 0.109936\n",
      "iteration 1600 / 2000: loss 0.160434\n",
      "iteration 1700 / 2000: loss 0.251830\n",
      "iteration 1800 / 2000: loss 0.165755\n",
      "iteration 1900 / 2000: loss 0.116435\n",
      "Hidden Size: 90, Learning Rate: 0.002500074999999999, LR Decay: 0.645, Regularization: 0.0, Validation accuracy: 0.9564\n",
      "iteration 0 / 2000: loss 2.302611\n",
      "iteration 100 / 2000: loss 0.481007\n",
      "iteration 200 / 2000: loss 0.253491\n",
      "iteration 300 / 2000: loss 0.250465\n",
      "iteration 400 / 2000: loss 0.176020\n",
      "iteration 500 / 2000: loss 0.191468\n",
      "iteration 600 / 2000: loss 0.244766\n",
      "iteration 700 / 2000: loss 0.185626\n",
      "iteration 800 / 2000: loss 0.256262\n",
      "iteration 900 / 2000: loss 0.184882\n",
      "iteration 1000 / 2000: loss 0.126509\n",
      "iteration 1100 / 2000: loss 0.179754\n",
      "iteration 1200 / 2000: loss 0.163797\n",
      "iteration 1300 / 2000: loss 0.186320\n",
      "iteration 1400 / 2000: loss 0.191141\n",
      "iteration 1500 / 2000: loss 0.178187\n",
      "iteration 1600 / 2000: loss 0.363496\n",
      "iteration 1700 / 2000: loss 0.221561\n",
      "iteration 1800 / 2000: loss 0.195004\n",
      "iteration 1900 / 2000: loss 0.170523\n",
      "Hidden Size: 90, Learning Rate: 0.002500074999999999, LR Decay: 0.645, Regularization: 0.1, Validation accuracy: 0.9534\n",
      "iteration 0 / 2000: loss 2.302651\n",
      "iteration 100 / 2000: loss 0.397694\n",
      "iteration 200 / 2000: loss 0.371432\n",
      "iteration 300 / 2000: loss 0.341112\n",
      "iteration 400 / 2000: loss 0.315457\n",
      "iteration 500 / 2000: loss 0.194199\n",
      "iteration 600 / 2000: loss 0.204487\n",
      "iteration 700 / 2000: loss 0.224193\n",
      "iteration 800 / 2000: loss 0.271883\n",
      "iteration 900 / 2000: loss 0.299982\n",
      "iteration 1000 / 2000: loss 0.256561\n",
      "iteration 1100 / 2000: loss 0.208344\n",
      "iteration 1200 / 2000: loss 0.233249\n",
      "iteration 1300 / 2000: loss 0.199894\n",
      "iteration 1400 / 2000: loss 0.194150\n",
      "iteration 1500 / 2000: loss 0.159566\n",
      "iteration 1600 / 2000: loss 0.348747\n",
      "iteration 1700 / 2000: loss 0.257761\n",
      "iteration 1800 / 2000: loss 0.239852\n",
      "iteration 1900 / 2000: loss 0.188891\n",
      "Hidden Size: 90, Learning Rate: 0.002500074999999999, LR Decay: 0.645, Regularization: 0.2, Validation accuracy: 0.9552\n",
      "iteration 0 / 2000: loss 2.302696\n",
      "iteration 100 / 2000: loss 0.491446\n",
      "iteration 200 / 2000: loss 0.442855\n",
      "iteration 300 / 2000: loss 0.333310\n",
      "iteration 400 / 2000: loss 0.258202\n",
      "iteration 500 / 2000: loss 0.247454\n",
      "iteration 600 / 2000: loss 0.327379\n",
      "iteration 700 / 2000: loss 0.236890\n",
      "iteration 800 / 2000: loss 0.225121\n",
      "iteration 900 / 2000: loss 0.305618\n",
      "iteration 1000 / 2000: loss 0.307027\n",
      "iteration 1100 / 2000: loss 0.223431\n",
      "iteration 1200 / 2000: loss 0.195479\n",
      "iteration 1300 / 2000: loss 0.208623\n",
      "iteration 1400 / 2000: loss 0.219874\n",
      "iteration 1500 / 2000: loss 0.295447\n",
      "iteration 1600 / 2000: loss 0.193017\n",
      "iteration 1700 / 2000: loss 0.322632\n",
      "iteration 1800 / 2000: loss 0.248797\n",
      "iteration 1900 / 2000: loss 0.180863\n",
      "Hidden Size: 90, Learning Rate: 0.002500074999999999, LR Decay: 0.645, Regularization: 0.30000000000000004, Validation accuracy: 0.953\n",
      "iteration 0 / 2000: loss 2.302713\n",
      "iteration 100 / 2000: loss 0.497701\n",
      "iteration 200 / 2000: loss 0.290192\n",
      "iteration 300 / 2000: loss 0.340215\n",
      "iteration 400 / 2000: loss 0.260888\n",
      "iteration 500 / 2000: loss 0.329072\n",
      "iteration 600 / 2000: loss 0.240924\n",
      "iteration 700 / 2000: loss 0.244004\n",
      "iteration 800 / 2000: loss 0.306598\n",
      "iteration 900 / 2000: loss 0.287955\n",
      "iteration 1000 / 2000: loss 0.233492\n",
      "iteration 1100 / 2000: loss 0.289067\n",
      "iteration 1200 / 2000: loss 0.212388\n",
      "iteration 1300 / 2000: loss 0.266635\n",
      "iteration 1400 / 2000: loss 0.205109\n",
      "iteration 1500 / 2000: loss 0.245324\n",
      "iteration 1600 / 2000: loss 0.239936\n",
      "iteration 1700 / 2000: loss 0.241627\n",
      "iteration 1800 / 2000: loss 0.192988\n",
      "iteration 1900 / 2000: loss 0.225203\n",
      "Hidden Size: 90, Learning Rate: 0.002500074999999999, LR Decay: 0.645, Regularization: 0.4, Validation accuracy: 0.9528\n",
      "iteration 0 / 2000: loss 2.302577\n",
      "iteration 100 / 2000: loss 0.513573\n",
      "iteration 200 / 2000: loss 0.347434\n",
      "iteration 300 / 2000: loss 0.214628\n",
      "iteration 400 / 2000: loss 0.226210\n",
      "iteration 500 / 2000: loss 0.194536\n",
      "iteration 600 / 2000: loss 0.248382\n",
      "iteration 700 / 2000: loss 0.208026\n",
      "iteration 800 / 2000: loss 0.174734\n",
      "iteration 900 / 2000: loss 0.257686\n",
      "iteration 1000 / 2000: loss 0.172489\n",
      "iteration 1100 / 2000: loss 0.262050\n",
      "iteration 1200 / 2000: loss 0.282685\n",
      "iteration 1300 / 2000: loss 0.177571\n",
      "iteration 1400 / 2000: loss 0.162568\n",
      "iteration 1500 / 2000: loss 0.208339\n",
      "iteration 1600 / 2000: loss 0.279750\n",
      "iteration 1700 / 2000: loss 0.266917\n",
      "iteration 1800 / 2000: loss 0.140698\n",
      "iteration 1900 / 2000: loss 0.152452\n",
      "Hidden Size: 90, Learning Rate: 0.002500074999999999, LR Decay: 0.47250000000000003, Regularization: 0.0, Validation accuracy: 0.9396\n",
      "iteration 0 / 2000: loss 2.302616\n",
      "iteration 100 / 2000: loss 0.608451\n",
      "iteration 200 / 2000: loss 0.311138\n",
      "iteration 300 / 2000: loss 0.450031\n",
      "iteration 400 / 2000: loss 0.317865\n",
      "iteration 500 / 2000: loss 0.194453\n",
      "iteration 600 / 2000: loss 0.263766\n",
      "iteration 700 / 2000: loss 0.314971\n",
      "iteration 800 / 2000: loss 0.257376\n",
      "iteration 900 / 2000: loss 0.246769\n",
      "iteration 1000 / 2000: loss 0.215012\n",
      "iteration 1100 / 2000: loss 0.239759\n",
      "iteration 1200 / 2000: loss 0.207366\n",
      "iteration 1300 / 2000: loss 0.238222\n",
      "iteration 1400 / 2000: loss 0.304690\n",
      "iteration 1500 / 2000: loss 0.168055\n",
      "iteration 1600 / 2000: loss 0.296714\n",
      "iteration 1700 / 2000: loss 0.201366\n",
      "iteration 1800 / 2000: loss 0.264358\n",
      "iteration 1900 / 2000: loss 0.255020\n",
      "Hidden Size: 90, Learning Rate: 0.002500074999999999, LR Decay: 0.47250000000000003, Regularization: 0.1, Validation accuracy: 0.936\n",
      "iteration 0 / 2000: loss 2.302684\n",
      "iteration 100 / 2000: loss 0.639131\n",
      "iteration 200 / 2000: loss 0.315835\n",
      "iteration 300 / 2000: loss 0.483097\n",
      "iteration 400 / 2000: loss 0.377939\n",
      "iteration 500 / 2000: loss 0.279554\n",
      "iteration 600 / 2000: loss 0.357373\n",
      "iteration 700 / 2000: loss 0.255176\n",
      "iteration 800 / 2000: loss 0.282236\n",
      "iteration 900 / 2000: loss 0.276378\n",
      "iteration 1000 / 2000: loss 0.382907\n",
      "iteration 1100 / 2000: loss 0.267895\n",
      "iteration 1200 / 2000: loss 0.259143\n",
      "iteration 1300 / 2000: loss 0.251588\n",
      "iteration 1400 / 2000: loss 0.209214\n",
      "iteration 1500 / 2000: loss 0.279290\n",
      "iteration 1600 / 2000: loss 0.273865\n",
      "iteration 1700 / 2000: loss 0.306413\n",
      "iteration 1800 / 2000: loss 0.196014\n",
      "iteration 1900 / 2000: loss 0.198582\n",
      "Hidden Size: 90, Learning Rate: 0.002500074999999999, LR Decay: 0.47250000000000003, Regularization: 0.2, Validation accuracy: 0.9348\n",
      "iteration 0 / 2000: loss 2.302707\n",
      "iteration 100 / 2000: loss 0.615301\n",
      "iteration 200 / 2000: loss 0.433431\n",
      "iteration 300 / 2000: loss 0.328939\n",
      "iteration 400 / 2000: loss 0.419560\n",
      "iteration 500 / 2000: loss 0.341935\n",
      "iteration 600 / 2000: loss 0.325056\n",
      "iteration 700 / 2000: loss 0.342374\n",
      "iteration 800 / 2000: loss 0.233533\n",
      "iteration 900 / 2000: loss 0.373133\n",
      "iteration 1000 / 2000: loss 0.324105\n",
      "iteration 1100 / 2000: loss 0.274117\n",
      "iteration 1200 / 2000: loss 0.264552\n",
      "iteration 1300 / 2000: loss 0.269996\n",
      "iteration 1400 / 2000: loss 0.342601\n",
      "iteration 1500 / 2000: loss 0.272469\n",
      "iteration 1600 / 2000: loss 0.215434\n",
      "iteration 1700 / 2000: loss 0.209089\n",
      "iteration 1800 / 2000: loss 0.241572\n",
      "iteration 1900 / 2000: loss 0.421373\n",
      "Hidden Size: 90, Learning Rate: 0.002500074999999999, LR Decay: 0.47250000000000003, Regularization: 0.30000000000000004, Validation accuracy: 0.9344\n",
      "iteration 0 / 2000: loss 2.302751\n",
      "iteration 100 / 2000: loss 0.697842\n",
      "iteration 200 / 2000: loss 0.440060\n",
      "iteration 300 / 2000: loss 0.399757\n",
      "iteration 400 / 2000: loss 0.253834\n",
      "iteration 500 / 2000: loss 0.315496\n",
      "iteration 600 / 2000: loss 0.311745\n",
      "iteration 700 / 2000: loss 0.421939\n",
      "iteration 800 / 2000: loss 0.297736\n",
      "iteration 900 / 2000: loss 0.365827\n",
      "iteration 1000 / 2000: loss 0.349076\n",
      "iteration 1100 / 2000: loss 0.336195\n",
      "iteration 1200 / 2000: loss 0.294424\n",
      "iteration 1300 / 2000: loss 0.290977\n",
      "iteration 1400 / 2000: loss 0.308741\n",
      "iteration 1500 / 2000: loss 0.391929\n",
      "iteration 1600 / 2000: loss 0.258228\n",
      "iteration 1700 / 2000: loss 0.287312\n",
      "iteration 1800 / 2000: loss 0.232181\n",
      "iteration 1900 / 2000: loss 0.291064\n",
      "Hidden Size: 90, Learning Rate: 0.002500074999999999, LR Decay: 0.47250000000000003, Regularization: 0.4, Validation accuracy: 0.9316\n",
      "iteration 0 / 2000: loss 2.302571\n",
      "iteration 100 / 2000: loss 1.206269\n",
      "iteration 200 / 2000: loss 0.400083\n",
      "iteration 300 / 2000: loss 0.350531\n",
      "iteration 400 / 2000: loss 0.367951\n",
      "iteration 500 / 2000: loss 0.332243\n",
      "iteration 600 / 2000: loss 0.335836\n",
      "iteration 700 / 2000: loss 0.359059\n",
      "iteration 800 / 2000: loss 0.330576\n",
      "iteration 900 / 2000: loss 0.412917\n",
      "iteration 1000 / 2000: loss 0.383139\n",
      "iteration 1100 / 2000: loss 0.354110\n",
      "iteration 1200 / 2000: loss 0.314836\n",
      "iteration 1300 / 2000: loss 0.267225\n",
      "iteration 1400 / 2000: loss 0.293583\n",
      "iteration 1500 / 2000: loss 0.373459\n",
      "iteration 1600 / 2000: loss 0.290572\n",
      "iteration 1700 / 2000: loss 0.289794\n",
      "iteration 1800 / 2000: loss 0.351583\n",
      "iteration 1900 / 2000: loss 0.280787\n",
      "Hidden Size: 90, Learning Rate: 0.002500074999999999, LR Decay: 0.3, Regularization: 0.0, Validation accuracy: 0.9158\n",
      "iteration 0 / 2000: loss 2.302611\n",
      "iteration 100 / 2000: loss 1.116363\n",
      "iteration 200 / 2000: loss 0.465510\n",
      "iteration 300 / 2000: loss 0.402047\n",
      "iteration 400 / 2000: loss 0.357897\n",
      "iteration 500 / 2000: loss 0.242805\n",
      "iteration 600 / 2000: loss 0.271735\n",
      "iteration 700 / 2000: loss 0.339476\n",
      "iteration 800 / 2000: loss 0.283701\n",
      "iteration 900 / 2000: loss 0.281090\n",
      "iteration 1000 / 2000: loss 0.356145\n",
      "iteration 1100 / 2000: loss 0.353578\n",
      "iteration 1200 / 2000: loss 0.365422\n",
      "iteration 1300 / 2000: loss 0.345305\n",
      "iteration 1400 / 2000: loss 0.315366\n",
      "iteration 1500 / 2000: loss 0.380772\n",
      "iteration 1600 / 2000: loss 0.359540\n",
      "iteration 1700 / 2000: loss 0.270177\n",
      "iteration 1800 / 2000: loss 0.290008\n",
      "iteration 1900 / 2000: loss 0.301092\n",
      "Hidden Size: 90, Learning Rate: 0.002500074999999999, LR Decay: 0.3, Regularization: 0.1, Validation accuracy: 0.9154\n",
      "iteration 0 / 2000: loss 2.302647\n",
      "iteration 100 / 2000: loss 1.076481\n",
      "iteration 200 / 2000: loss 0.618722\n",
      "iteration 300 / 2000: loss 0.423886\n",
      "iteration 400 / 2000: loss 0.285021\n",
      "iteration 500 / 2000: loss 0.337506\n",
      "iteration 600 / 2000: loss 0.304250\n",
      "iteration 700 / 2000: loss 0.273456\n",
      "iteration 800 / 2000: loss 0.356970\n",
      "iteration 900 / 2000: loss 0.223690\n",
      "iteration 1000 / 2000: loss 0.341002\n",
      "iteration 1100 / 2000: loss 0.279728\n",
      "iteration 1200 / 2000: loss 0.371680\n",
      "iteration 1300 / 2000: loss 0.348252\n",
      "iteration 1400 / 2000: loss 0.322107\n",
      "iteration 1500 / 2000: loss 0.352037\n",
      "iteration 1600 / 2000: loss 0.319347\n",
      "iteration 1700 / 2000: loss 0.340616\n",
      "iteration 1800 / 2000: loss 0.369419\n",
      "iteration 1900 / 2000: loss 0.300071\n",
      "Hidden Size: 90, Learning Rate: 0.002500074999999999, LR Decay: 0.3, Regularization: 0.2, Validation accuracy: 0.9152\n",
      "iteration 0 / 2000: loss 2.302654\n",
      "iteration 100 / 2000: loss 1.133919\n",
      "iteration 200 / 2000: loss 0.438887\n",
      "iteration 300 / 2000: loss 0.341387\n",
      "iteration 400 / 2000: loss 0.284037\n",
      "iteration 500 / 2000: loss 0.365432\n",
      "iteration 600 / 2000: loss 0.285007\n",
      "iteration 700 / 2000: loss 0.304511\n",
      "iteration 800 / 2000: loss 0.330037\n",
      "iteration 900 / 2000: loss 0.339683\n",
      "iteration 1000 / 2000: loss 0.255958\n",
      "iteration 1100 / 2000: loss 0.443118\n",
      "iteration 1200 / 2000: loss 0.367909\n",
      "iteration 1300 / 2000: loss 0.230572\n",
      "iteration 1400 / 2000: loss 0.268354\n",
      "iteration 1500 / 2000: loss 0.386896\n",
      "iteration 1600 / 2000: loss 0.482596\n",
      "iteration 1700 / 2000: loss 0.350898\n",
      "iteration 1800 / 2000: loss 0.342885\n",
      "iteration 1900 / 2000: loss 0.414978\n",
      "Hidden Size: 90, Learning Rate: 0.002500074999999999, LR Decay: 0.3, Regularization: 0.30000000000000004, Validation accuracy: 0.9164\n",
      "iteration 0 / 2000: loss 2.302724\n",
      "iteration 100 / 2000: loss 1.152612\n",
      "iteration 200 / 2000: loss 0.463970\n",
      "iteration 300 / 2000: loss 0.371711\n",
      "iteration 400 / 2000: loss 0.384473\n",
      "iteration 500 / 2000: loss 0.441018\n",
      "iteration 600 / 2000: loss 0.382968\n",
      "iteration 700 / 2000: loss 0.301479\n",
      "iteration 800 / 2000: loss 0.330505\n",
      "iteration 900 / 2000: loss 0.366309\n",
      "iteration 1000 / 2000: loss 0.402801\n",
      "iteration 1100 / 2000: loss 0.292584\n",
      "iteration 1200 / 2000: loss 0.313467\n",
      "iteration 1300 / 2000: loss 0.356209\n",
      "iteration 1400 / 2000: loss 0.266919\n",
      "iteration 1500 / 2000: loss 0.384518\n",
      "iteration 1600 / 2000: loss 0.398947\n",
      "iteration 1700 / 2000: loss 0.361420\n",
      "iteration 1800 / 2000: loss 0.396606\n",
      "iteration 1900 / 2000: loss 0.436320\n",
      "Hidden Size: 90, Learning Rate: 0.002500074999999999, LR Decay: 0.3, Regularization: 0.4, Validation accuracy: 0.9166\n",
      "iteration 0 / 2000: loss 2.302574\n",
      "iteration 100 / 2000: loss 2.302589\n",
      "iteration 200 / 2000: loss 2.302597\n",
      "iteration 300 / 2000: loss 2.302586\n",
      "iteration 400 / 2000: loss 2.302587\n",
      "iteration 500 / 2000: loss 2.302582\n",
      "iteration 600 / 2000: loss 2.302583\n",
      "iteration 700 / 2000: loss 2.302584\n",
      "iteration 800 / 2000: loss 2.302568\n",
      "iteration 900 / 2000: loss 2.302582\n",
      "iteration 1000 / 2000: loss 2.302592\n",
      "iteration 1100 / 2000: loss 2.302590\n",
      "iteration 1200 / 2000: loss 2.302597\n",
      "iteration 1300 / 2000: loss 2.302586\n",
      "iteration 1400 / 2000: loss 2.302576\n",
      "iteration 1500 / 2000: loss 2.302587\n",
      "iteration 1600 / 2000: loss 2.302566\n",
      "iteration 1700 / 2000: loss 2.302582\n",
      "iteration 1800 / 2000: loss 2.302586\n",
      "iteration 1900 / 2000: loss 2.302574\n",
      "Hidden Size: 90, Learning Rate: 1e-07, LR Decay: 0.99, Regularization: 0.0, Validation accuracy: 0.1518\n",
      "iteration 0 / 2000: loss 2.302624\n",
      "iteration 100 / 2000: loss 2.302617\n",
      "iteration 200 / 2000: loss 2.302619\n",
      "iteration 300 / 2000: loss 2.302608\n",
      "iteration 400 / 2000: loss 2.302622\n",
      "iteration 500 / 2000: loss 2.302647\n",
      "iteration 600 / 2000: loss 2.302616\n",
      "iteration 700 / 2000: loss 2.302615\n",
      "iteration 800 / 2000: loss 2.302620\n",
      "iteration 900 / 2000: loss 2.302609\n",
      "iteration 1000 / 2000: loss 2.302608\n",
      "iteration 1100 / 2000: loss 2.302612\n",
      "iteration 1200 / 2000: loss 2.302617\n",
      "iteration 1300 / 2000: loss 2.302625\n",
      "iteration 1400 / 2000: loss 2.302615\n",
      "iteration 1500 / 2000: loss 2.302627\n",
      "iteration 1600 / 2000: loss 2.302604\n",
      "iteration 1700 / 2000: loss 2.302612\n",
      "iteration 1800 / 2000: loss 2.302613\n",
      "iteration 1900 / 2000: loss 2.302612\n",
      "Hidden Size: 90, Learning Rate: 1e-07, LR Decay: 0.99, Regularization: 0.1, Validation accuracy: 0.0812\n",
      "iteration 0 / 2000: loss 2.302705\n",
      "iteration 100 / 2000: loss 2.302695\n",
      "iteration 200 / 2000: loss 2.302711\n",
      "iteration 300 / 2000: loss 2.302707\n",
      "iteration 400 / 2000: loss 2.302701\n",
      "iteration 500 / 2000: loss 2.302700\n",
      "iteration 600 / 2000: loss 2.302696\n",
      "iteration 700 / 2000: loss 2.302688\n",
      "iteration 800 / 2000: loss 2.302702\n",
      "iteration 900 / 2000: loss 2.302695\n",
      "iteration 1000 / 2000: loss 2.302690\n",
      "iteration 1100 / 2000: loss 2.302691\n",
      "iteration 1200 / 2000: loss 2.302696\n",
      "iteration 1300 / 2000: loss 2.302696\n",
      "iteration 1400 / 2000: loss 2.302690\n",
      "iteration 1500 / 2000: loss 2.302685\n",
      "iteration 1600 / 2000: loss 2.302698\n",
      "iteration 1700 / 2000: loss 2.302700\n",
      "iteration 1800 / 2000: loss 2.302701\n",
      "iteration 1900 / 2000: loss 2.302696\n",
      "Hidden Size: 90, Learning Rate: 1e-07, LR Decay: 0.99, Regularization: 0.2, Validation accuracy: 0.0494\n",
      "iteration 0 / 2000: loss 2.302685\n",
      "iteration 100 / 2000: loss 2.302677\n",
      "iteration 200 / 2000: loss 2.302685\n",
      "iteration 300 / 2000: loss 2.302687\n",
      "iteration 400 / 2000: loss 2.302672\n",
      "iteration 500 / 2000: loss 2.302690\n",
      "iteration 600 / 2000: loss 2.302687\n",
      "iteration 700 / 2000: loss 2.302672\n",
      "iteration 800 / 2000: loss 2.302677\n",
      "iteration 900 / 2000: loss 2.302664\n",
      "iteration 1000 / 2000: loss 2.302687\n",
      "iteration 1100 / 2000: loss 2.302681\n",
      "iteration 1200 / 2000: loss 2.302676\n",
      "iteration 1300 / 2000: loss 2.302688\n",
      "iteration 1400 / 2000: loss 2.302677\n",
      "iteration 1500 / 2000: loss 2.302690\n",
      "iteration 1600 / 2000: loss 2.302677\n",
      "iteration 1700 / 2000: loss 2.302675\n",
      "iteration 1800 / 2000: loss 2.302661\n",
      "iteration 1900 / 2000: loss 2.302673\n",
      "Hidden Size: 90, Learning Rate: 1e-07, LR Decay: 0.99, Regularization: 0.30000000000000004, Validation accuracy: 0.1178\n",
      "iteration 0 / 2000: loss 2.302720\n",
      "iteration 100 / 2000: loss 2.302727\n",
      "iteration 200 / 2000: loss 2.302723\n",
      "iteration 300 / 2000: loss 2.302732\n",
      "iteration 400 / 2000: loss 2.302744\n",
      "iteration 500 / 2000: loss 2.302742\n",
      "iteration 600 / 2000: loss 2.302730\n",
      "iteration 700 / 2000: loss 2.302731\n",
      "iteration 800 / 2000: loss 2.302728\n",
      "iteration 900 / 2000: loss 2.302742\n",
      "iteration 1000 / 2000: loss 2.302725\n",
      "iteration 1100 / 2000: loss 2.302725\n",
      "iteration 1200 / 2000: loss 2.302744\n",
      "iteration 1300 / 2000: loss 2.302736\n",
      "iteration 1400 / 2000: loss 2.302739\n",
      "iteration 1500 / 2000: loss 2.302737\n",
      "iteration 1600 / 2000: loss 2.302733\n",
      "iteration 1700 / 2000: loss 2.302702\n",
      "iteration 1800 / 2000: loss 2.302735\n",
      "iteration 1900 / 2000: loss 2.302728\n",
      "Hidden Size: 90, Learning Rate: 1e-07, LR Decay: 0.99, Regularization: 0.4, Validation accuracy: 0.085\n",
      "iteration 0 / 2000: loss 2.302577\n",
      "iteration 100 / 2000: loss 2.302578\n",
      "iteration 200 / 2000: loss 2.302582\n",
      "iteration 300 / 2000: loss 2.302564\n",
      "iteration 400 / 2000: loss 2.302573\n",
      "iteration 500 / 2000: loss 2.302563\n",
      "iteration 600 / 2000: loss 2.302578\n",
      "iteration 700 / 2000: loss 2.302571\n",
      "iteration 800 / 2000: loss 2.302563\n",
      "iteration 900 / 2000: loss 2.302574\n",
      "iteration 1000 / 2000: loss 2.302562\n",
      "iteration 1100 / 2000: loss 2.302561\n",
      "iteration 1200 / 2000: loss 2.302574\n",
      "iteration 1300 / 2000: loss 2.302568\n",
      "iteration 1400 / 2000: loss 2.302572\n",
      "iteration 1500 / 2000: loss 2.302571\n",
      "iteration 1600 / 2000: loss 2.302563\n",
      "iteration 1700 / 2000: loss 2.302567\n",
      "iteration 1800 / 2000: loss 2.302568\n",
      "iteration 1900 / 2000: loss 2.302569\n",
      "Hidden Size: 90, Learning Rate: 1e-07, LR Decay: 0.8175, Regularization: 0.0, Validation accuracy: 0.1438\n",
      "iteration 0 / 2000: loss 2.302650\n",
      "iteration 100 / 2000: loss 2.302624\n",
      "iteration 200 / 2000: loss 2.302635\n",
      "iteration 300 / 2000: loss 2.302635\n",
      "iteration 400 / 2000: loss 2.302632\n",
      "iteration 500 / 2000: loss 2.302623\n",
      "iteration 600 / 2000: loss 2.302639\n",
      "iteration 700 / 2000: loss 2.302636\n",
      "iteration 800 / 2000: loss 2.302649\n",
      "iteration 900 / 2000: loss 2.302641\n",
      "iteration 1000 / 2000: loss 2.302657\n",
      "iteration 1100 / 2000: loss 2.302633\n",
      "iteration 1200 / 2000: loss 2.302623\n",
      "iteration 1300 / 2000: loss 2.302626\n",
      "iteration 1400 / 2000: loss 2.302628\n",
      "iteration 1500 / 2000: loss 2.302643\n",
      "iteration 1600 / 2000: loss 2.302650\n",
      "iteration 1700 / 2000: loss 2.302641\n",
      "iteration 1800 / 2000: loss 2.302631\n",
      "iteration 1900 / 2000: loss 2.302635\n",
      "Hidden Size: 90, Learning Rate: 1e-07, LR Decay: 0.8175, Regularization: 0.1, Validation accuracy: 0.0872\n",
      "iteration 0 / 2000: loss 2.302663\n",
      "iteration 100 / 2000: loss 2.302670\n",
      "iteration 200 / 2000: loss 2.302660\n",
      "iteration 300 / 2000: loss 2.302653\n",
      "iteration 400 / 2000: loss 2.302659\n",
      "iteration 500 / 2000: loss 2.302662\n",
      "iteration 600 / 2000: loss 2.302659\n",
      "iteration 700 / 2000: loss 2.302657\n",
      "iteration 800 / 2000: loss 2.302655\n",
      "iteration 900 / 2000: loss 2.302662\n",
      "iteration 1000 / 2000: loss 2.302649\n",
      "iteration 1100 / 2000: loss 2.302653\n",
      "iteration 1200 / 2000: loss 2.302663\n",
      "iteration 1300 / 2000: loss 2.302659\n",
      "iteration 1400 / 2000: loss 2.302672\n",
      "iteration 1500 / 2000: loss 2.302658\n",
      "iteration 1600 / 2000: loss 2.302649\n",
      "iteration 1700 / 2000: loss 2.302665\n",
      "iteration 1800 / 2000: loss 2.302667\n",
      "iteration 1900 / 2000: loss 2.302653\n",
      "Hidden Size: 90, Learning Rate: 1e-07, LR Decay: 0.8175, Regularization: 0.2, Validation accuracy: 0.0872\n",
      "iteration 0 / 2000: loss 2.302692\n",
      "iteration 100 / 2000: loss 2.302703\n",
      "iteration 200 / 2000: loss 2.302700\n",
      "iteration 300 / 2000: loss 2.302701\n",
      "iteration 400 / 2000: loss 2.302691\n",
      "iteration 500 / 2000: loss 2.302702\n",
      "iteration 600 / 2000: loss 2.302690\n",
      "iteration 700 / 2000: loss 2.302698\n",
      "iteration 800 / 2000: loss 2.302693\n",
      "iteration 900 / 2000: loss 2.302686\n",
      "iteration 1000 / 2000: loss 2.302677\n",
      "iteration 1100 / 2000: loss 2.302689\n",
      "iteration 1200 / 2000: loss 2.302687\n",
      "iteration 1300 / 2000: loss 2.302700\n",
      "iteration 1400 / 2000: loss 2.302692\n",
      "iteration 1500 / 2000: loss 2.302679\n",
      "iteration 1600 / 2000: loss 2.302694\n",
      "iteration 1700 / 2000: loss 2.302680\n",
      "iteration 1800 / 2000: loss 2.302696\n",
      "iteration 1900 / 2000: loss 2.302696\n",
      "Hidden Size: 90, Learning Rate: 1e-07, LR Decay: 0.8175, Regularization: 0.30000000000000004, Validation accuracy: 0.125\n",
      "iteration 0 / 2000: loss 2.302726\n",
      "iteration 100 / 2000: loss 2.302726\n",
      "iteration 200 / 2000: loss 2.302713\n",
      "iteration 300 / 2000: loss 2.302721\n",
      "iteration 400 / 2000: loss 2.302725\n",
      "iteration 500 / 2000: loss 2.302729\n",
      "iteration 600 / 2000: loss 2.302725\n",
      "iteration 700 / 2000: loss 2.302732\n",
      "iteration 800 / 2000: loss 2.302734\n",
      "iteration 900 / 2000: loss 2.302722\n",
      "iteration 1000 / 2000: loss 2.302724\n",
      "iteration 1100 / 2000: loss 2.302727\n",
      "iteration 1200 / 2000: loss 2.302726\n",
      "iteration 1300 / 2000: loss 2.302732\n",
      "iteration 1400 / 2000: loss 2.302726\n",
      "iteration 1500 / 2000: loss 2.302721\n",
      "iteration 1600 / 2000: loss 2.302736\n",
      "iteration 1700 / 2000: loss 2.302727\n",
      "iteration 1800 / 2000: loss 2.302712\n",
      "iteration 1900 / 2000: loss 2.302734\n",
      "Hidden Size: 90, Learning Rate: 1e-07, LR Decay: 0.8175, Regularization: 0.4, Validation accuracy: 0.1098\n",
      "iteration 0 / 2000: loss 2.302606\n",
      "iteration 100 / 2000: loss 2.302602\n",
      "iteration 200 / 2000: loss 2.302600\n",
      "iteration 300 / 2000: loss 2.302596\n",
      "iteration 400 / 2000: loss 2.302583\n",
      "iteration 500 / 2000: loss 2.302594\n",
      "iteration 600 / 2000: loss 2.302578\n",
      "iteration 700 / 2000: loss 2.302585\n",
      "iteration 800 / 2000: loss 2.302588\n",
      "iteration 900 / 2000: loss 2.302591\n",
      "iteration 1000 / 2000: loss 2.302593\n",
      "iteration 1100 / 2000: loss 2.302599\n",
      "iteration 1200 / 2000: loss 2.302610\n",
      "iteration 1300 / 2000: loss 2.302596\n",
      "iteration 1400 / 2000: loss 2.302579\n",
      "iteration 1500 / 2000: loss 2.302581\n",
      "iteration 1600 / 2000: loss 2.302582\n",
      "iteration 1700 / 2000: loss 2.302595\n",
      "iteration 1800 / 2000: loss 2.302599\n",
      "iteration 1900 / 2000: loss 2.302583\n",
      "Hidden Size: 90, Learning Rate: 1e-07, LR Decay: 0.645, Regularization: 0.0, Validation accuracy: 0.052\n",
      "iteration 0 / 2000: loss 2.302607\n",
      "iteration 100 / 2000: loss 2.302608\n",
      "iteration 200 / 2000: loss 2.302606\n",
      "iteration 300 / 2000: loss 2.302606\n",
      "iteration 400 / 2000: loss 2.302600\n",
      "iteration 500 / 2000: loss 2.302600\n",
      "iteration 600 / 2000: loss 2.302598\n",
      "iteration 700 / 2000: loss 2.302588\n",
      "iteration 800 / 2000: loss 2.302602\n",
      "iteration 900 / 2000: loss 2.302612\n",
      "iteration 1000 / 2000: loss 2.302595\n",
      "iteration 1100 / 2000: loss 2.302610\n",
      "iteration 1200 / 2000: loss 2.302594\n",
      "iteration 1300 / 2000: loss 2.302615\n",
      "iteration 1400 / 2000: loss 2.302604\n",
      "iteration 1500 / 2000: loss 2.302591\n",
      "iteration 1600 / 2000: loss 2.302601\n",
      "iteration 1700 / 2000: loss 2.302604\n",
      "iteration 1800 / 2000: loss 2.302602\n",
      "iteration 1900 / 2000: loss 2.302617\n",
      "Hidden Size: 90, Learning Rate: 1e-07, LR Decay: 0.645, Regularization: 0.1, Validation accuracy: 0.1134\n",
      "iteration 0 / 2000: loss 2.302651\n",
      "iteration 100 / 2000: loss 2.302646\n",
      "iteration 200 / 2000: loss 2.302645\n",
      "iteration 300 / 2000: loss 2.302642\n",
      "iteration 400 / 2000: loss 2.302647\n",
      "iteration 500 / 2000: loss 2.302648\n",
      "iteration 600 / 2000: loss 2.302648\n",
      "iteration 700 / 2000: loss 2.302644\n",
      "iteration 800 / 2000: loss 2.302648\n",
      "iteration 900 / 2000: loss 2.302639\n",
      "iteration 1000 / 2000: loss 2.302630\n",
      "iteration 1100 / 2000: loss 2.302650\n",
      "iteration 1200 / 2000: loss 2.302646\n",
      "iteration 1300 / 2000: loss 2.302644\n",
      "iteration 1400 / 2000: loss 2.302644\n",
      "iteration 1500 / 2000: loss 2.302648\n",
      "iteration 1600 / 2000: loss 2.302638\n",
      "iteration 1700 / 2000: loss 2.302645\n",
      "iteration 1800 / 2000: loss 2.302647\n",
      "iteration 1900 / 2000: loss 2.302629\n",
      "Hidden Size: 90, Learning Rate: 1e-07, LR Decay: 0.645, Regularization: 0.2, Validation accuracy: 0.1456\n",
      "iteration 0 / 2000: loss 2.302699\n",
      "iteration 100 / 2000: loss 2.302711\n",
      "iteration 200 / 2000: loss 2.302704\n",
      "iteration 300 / 2000: loss 2.302707\n",
      "iteration 400 / 2000: loss 2.302703\n",
      "iteration 500 / 2000: loss 2.302713\n",
      "iteration 600 / 2000: loss 2.302707\n",
      "iteration 700 / 2000: loss 2.302705\n",
      "iteration 800 / 2000: loss 2.302697\n",
      "iteration 900 / 2000: loss 2.302710\n",
      "iteration 1000 / 2000: loss 2.302698\n",
      "iteration 1100 / 2000: loss 2.302723\n",
      "iteration 1200 / 2000: loss 2.302698\n",
      "iteration 1300 / 2000: loss 2.302699\n",
      "iteration 1400 / 2000: loss 2.302711\n",
      "iteration 1500 / 2000: loss 2.302715\n",
      "iteration 1600 / 2000: loss 2.302697\n",
      "iteration 1700 / 2000: loss 2.302709\n",
      "iteration 1800 / 2000: loss 2.302700\n",
      "iteration 1900 / 2000: loss 2.302697\n",
      "Hidden Size: 90, Learning Rate: 1e-07, LR Decay: 0.645, Regularization: 0.30000000000000004, Validation accuracy: 0.111\n",
      "iteration 0 / 2000: loss 2.302734\n",
      "iteration 100 / 2000: loss 2.302712\n",
      "iteration 200 / 2000: loss 2.302725\n",
      "iteration 300 / 2000: loss 2.302729\n",
      "iteration 400 / 2000: loss 2.302725\n",
      "iteration 500 / 2000: loss 2.302736\n",
      "iteration 600 / 2000: loss 2.302727\n",
      "iteration 700 / 2000: loss 2.302718\n",
      "iteration 800 / 2000: loss 2.302726\n",
      "iteration 900 / 2000: loss 2.302737\n",
      "iteration 1000 / 2000: loss 2.302726\n",
      "iteration 1100 / 2000: loss 2.302722\n",
      "iteration 1200 / 2000: loss 2.302733\n",
      "iteration 1300 / 2000: loss 2.302721\n",
      "iteration 1400 / 2000: loss 2.302722\n",
      "iteration 1500 / 2000: loss 2.302722\n",
      "iteration 1600 / 2000: loss 2.302716\n",
      "iteration 1700 / 2000: loss 2.302722\n",
      "iteration 1800 / 2000: loss 2.302736\n",
      "iteration 1900 / 2000: loss 2.302725\n",
      "Hidden Size: 90, Learning Rate: 1e-07, LR Decay: 0.645, Regularization: 0.4, Validation accuracy: 0.1064\n",
      "iteration 0 / 2000: loss 2.302570\n",
      "iteration 100 / 2000: loss 2.302580\n",
      "iteration 200 / 2000: loss 2.302569\n",
      "iteration 300 / 2000: loss 2.302588\n",
      "iteration 400 / 2000: loss 2.302580\n",
      "iteration 500 / 2000: loss 2.302589\n",
      "iteration 600 / 2000: loss 2.302590\n",
      "iteration 700 / 2000: loss 2.302590\n",
      "iteration 800 / 2000: loss 2.302579\n",
      "iteration 900 / 2000: loss 2.302579\n",
      "iteration 1000 / 2000: loss 2.302592\n",
      "iteration 1100 / 2000: loss 2.302578\n",
      "iteration 1200 / 2000: loss 2.302570\n",
      "iteration 1300 / 2000: loss 2.302567\n",
      "iteration 1400 / 2000: loss 2.302572\n",
      "iteration 1500 / 2000: loss 2.302575\n",
      "iteration 1600 / 2000: loss 2.302593\n",
      "iteration 1700 / 2000: loss 2.302581\n",
      "iteration 1800 / 2000: loss 2.302571\n",
      "iteration 1900 / 2000: loss 2.302569\n",
      "Hidden Size: 90, Learning Rate: 1e-07, LR Decay: 0.47250000000000003, Regularization: 0.0, Validation accuracy: 0.1084\n",
      "iteration 0 / 2000: loss 2.302595\n",
      "iteration 100 / 2000: loss 2.302615\n",
      "iteration 200 / 2000: loss 2.302612\n",
      "iteration 300 / 2000: loss 2.302600\n",
      "iteration 400 / 2000: loss 2.302598\n",
      "iteration 500 / 2000: loss 2.302607\n",
      "iteration 600 / 2000: loss 2.302601\n",
      "iteration 700 / 2000: loss 2.302610\n",
      "iteration 800 / 2000: loss 2.302609\n",
      "iteration 900 / 2000: loss 2.302615\n",
      "iteration 1000 / 2000: loss 2.302597\n",
      "iteration 1100 / 2000: loss 2.302594\n",
      "iteration 1200 / 2000: loss 2.302604\n",
      "iteration 1300 / 2000: loss 2.302598\n",
      "iteration 1400 / 2000: loss 2.302614\n",
      "iteration 1500 / 2000: loss 2.302614\n",
      "iteration 1600 / 2000: loss 2.302604\n",
      "iteration 1700 / 2000: loss 2.302595\n",
      "iteration 1800 / 2000: loss 2.302605\n",
      "iteration 1900 / 2000: loss 2.302604\n",
      "Hidden Size: 90, Learning Rate: 1e-07, LR Decay: 0.47250000000000003, Regularization: 0.1, Validation accuracy: 0.1614\n",
      "iteration 0 / 2000: loss 2.302670\n",
      "iteration 100 / 2000: loss 2.302672\n",
      "iteration 200 / 2000: loss 2.302649\n",
      "iteration 300 / 2000: loss 2.302655\n",
      "iteration 400 / 2000: loss 2.302660\n",
      "iteration 500 / 2000: loss 2.302671\n",
      "iteration 600 / 2000: loss 2.302659\n",
      "iteration 700 / 2000: loss 2.302674\n",
      "iteration 800 / 2000: loss 2.302666\n",
      "iteration 900 / 2000: loss 2.302659\n",
      "iteration 1000 / 2000: loss 2.302675\n",
      "iteration 1100 / 2000: loss 2.302657\n",
      "iteration 1200 / 2000: loss 2.302670\n",
      "iteration 1300 / 2000: loss 2.302673\n",
      "iteration 1400 / 2000: loss 2.302648\n",
      "iteration 1500 / 2000: loss 2.302673\n",
      "iteration 1600 / 2000: loss 2.302674\n",
      "iteration 1700 / 2000: loss 2.302665\n",
      "iteration 1800 / 2000: loss 2.302671\n",
      "iteration 1900 / 2000: loss 2.302663\n",
      "Hidden Size: 90, Learning Rate: 1e-07, LR Decay: 0.47250000000000003, Regularization: 0.2, Validation accuracy: 0.0972\n",
      "iteration 0 / 2000: loss 2.302716\n",
      "iteration 100 / 2000: loss 2.302700\n",
      "iteration 200 / 2000: loss 2.302711\n",
      "iteration 300 / 2000: loss 2.302704\n",
      "iteration 400 / 2000: loss 2.302695\n",
      "iteration 500 / 2000: loss 2.302697\n",
      "iteration 600 / 2000: loss 2.302699\n",
      "iteration 700 / 2000: loss 2.302707\n",
      "iteration 800 / 2000: loss 2.302700\n",
      "iteration 900 / 2000: loss 2.302707\n",
      "iteration 1000 / 2000: loss 2.302702\n",
      "iteration 1100 / 2000: loss 2.302684\n",
      "iteration 1200 / 2000: loss 2.302704\n",
      "iteration 1300 / 2000: loss 2.302706\n",
      "iteration 1400 / 2000: loss 2.302691\n",
      "iteration 1500 / 2000: loss 2.302714\n",
      "iteration 1600 / 2000: loss 2.302693\n",
      "iteration 1700 / 2000: loss 2.302689\n",
      "iteration 1800 / 2000: loss 2.302704\n",
      "iteration 1900 / 2000: loss 2.302711\n",
      "Hidden Size: 90, Learning Rate: 1e-07, LR Decay: 0.47250000000000003, Regularization: 0.30000000000000004, Validation accuracy: 0.088\n",
      "iteration 0 / 2000: loss 2.302713\n",
      "iteration 100 / 2000: loss 2.302720\n",
      "iteration 200 / 2000: loss 2.302713\n",
      "iteration 300 / 2000: loss 2.302725\n",
      "iteration 400 / 2000: loss 2.302704\n",
      "iteration 500 / 2000: loss 2.302712\n",
      "iteration 600 / 2000: loss 2.302692\n",
      "iteration 700 / 2000: loss 2.302708\n",
      "iteration 800 / 2000: loss 2.302720\n",
      "iteration 900 / 2000: loss 2.302704\n",
      "iteration 1000 / 2000: loss 2.302698\n",
      "iteration 1100 / 2000: loss 2.302708\n",
      "iteration 1200 / 2000: loss 2.302707\n",
      "iteration 1300 / 2000: loss 2.302711\n",
      "iteration 1400 / 2000: loss 2.302706\n",
      "iteration 1500 / 2000: loss 2.302704\n",
      "iteration 1600 / 2000: loss 2.302719\n",
      "iteration 1700 / 2000: loss 2.302713\n",
      "iteration 1800 / 2000: loss 2.302717\n",
      "iteration 1900 / 2000: loss 2.302707\n",
      "Hidden Size: 90, Learning Rate: 1e-07, LR Decay: 0.47250000000000003, Regularization: 0.4, Validation accuracy: 0.1436\n",
      "iteration 0 / 2000: loss 2.302591\n",
      "iteration 100 / 2000: loss 2.302583\n",
      "iteration 200 / 2000: loss 2.302587\n",
      "iteration 300 / 2000: loss 2.302583\n",
      "iteration 400 / 2000: loss 2.302587\n",
      "iteration 500 / 2000: loss 2.302580\n",
      "iteration 600 / 2000: loss 2.302593\n",
      "iteration 700 / 2000: loss 2.302577\n",
      "iteration 800 / 2000: loss 2.302577\n",
      "iteration 900 / 2000: loss 2.302576\n",
      "iteration 1000 / 2000: loss 2.302600\n",
      "iteration 1100 / 2000: loss 2.302593\n",
      "iteration 1200 / 2000: loss 2.302585\n",
      "iteration 1300 / 2000: loss 2.302572\n",
      "iteration 1400 / 2000: loss 2.302582\n",
      "iteration 1500 / 2000: loss 2.302585\n",
      "iteration 1600 / 2000: loss 2.302584\n",
      "iteration 1700 / 2000: loss 2.302598\n",
      "iteration 1800 / 2000: loss 2.302583\n",
      "iteration 1900 / 2000: loss 2.302591\n",
      "Hidden Size: 90, Learning Rate: 1e-07, LR Decay: 0.3, Regularization: 0.0, Validation accuracy: 0.108\n",
      "iteration 0 / 2000: loss 2.302619\n",
      "iteration 100 / 2000: loss 2.302617\n",
      "iteration 200 / 2000: loss 2.302619\n",
      "iteration 300 / 2000: loss 2.302609\n",
      "iteration 400 / 2000: loss 2.302607\n",
      "iteration 500 / 2000: loss 2.302627\n",
      "iteration 600 / 2000: loss 2.302629\n",
      "iteration 700 / 2000: loss 2.302610\n",
      "iteration 800 / 2000: loss 2.302613\n",
      "iteration 900 / 2000: loss 2.302606\n",
      "iteration 1000 / 2000: loss 2.302615\n",
      "iteration 1100 / 2000: loss 2.302609\n",
      "iteration 1200 / 2000: loss 2.302622\n",
      "iteration 1300 / 2000: loss 2.302624\n",
      "iteration 1400 / 2000: loss 2.302627\n",
      "iteration 1500 / 2000: loss 2.302628\n",
      "iteration 1600 / 2000: loss 2.302620\n",
      "iteration 1700 / 2000: loss 2.302628\n",
      "iteration 1800 / 2000: loss 2.302626\n",
      "iteration 1900 / 2000: loss 2.302615\n",
      "Hidden Size: 90, Learning Rate: 1e-07, LR Decay: 0.3, Regularization: 0.1, Validation accuracy: 0.1414\n",
      "iteration 0 / 2000: loss 2.302645\n",
      "iteration 100 / 2000: loss 2.302630\n",
      "iteration 200 / 2000: loss 2.302626\n",
      "iteration 300 / 2000: loss 2.302623\n",
      "iteration 400 / 2000: loss 2.302631\n",
      "iteration 500 / 2000: loss 2.302656\n",
      "iteration 600 / 2000: loss 2.302632\n",
      "iteration 700 / 2000: loss 2.302640\n",
      "iteration 800 / 2000: loss 2.302636\n",
      "iteration 900 / 2000: loss 2.302640\n",
      "iteration 1000 / 2000: loss 2.302637\n",
      "iteration 1100 / 2000: loss 2.302638\n",
      "iteration 1200 / 2000: loss 2.302651\n",
      "iteration 1300 / 2000: loss 2.302633\n",
      "iteration 1400 / 2000: loss 2.302637\n",
      "iteration 1500 / 2000: loss 2.302613\n",
      "iteration 1600 / 2000: loss 2.302653\n",
      "iteration 1700 / 2000: loss 2.302636\n",
      "iteration 1800 / 2000: loss 2.302656\n",
      "iteration 1900 / 2000: loss 2.302654\n",
      "Hidden Size: 90, Learning Rate: 1e-07, LR Decay: 0.3, Regularization: 0.2, Validation accuracy: 0.1444\n",
      "iteration 0 / 2000: loss 2.302684\n",
      "iteration 100 / 2000: loss 2.302668\n",
      "iteration 200 / 2000: loss 2.302670\n",
      "iteration 300 / 2000: loss 2.302666\n",
      "iteration 400 / 2000: loss 2.302679\n",
      "iteration 500 / 2000: loss 2.302676\n",
      "iteration 600 / 2000: loss 2.302674\n",
      "iteration 700 / 2000: loss 2.302681\n",
      "iteration 800 / 2000: loss 2.302691\n",
      "iteration 900 / 2000: loss 2.302677\n",
      "iteration 1000 / 2000: loss 2.302686\n",
      "iteration 1100 / 2000: loss 2.302686\n",
      "iteration 1200 / 2000: loss 2.302667\n",
      "iteration 1300 / 2000: loss 2.302676\n",
      "iteration 1400 / 2000: loss 2.302665\n",
      "iteration 1500 / 2000: loss 2.302657\n",
      "iteration 1600 / 2000: loss 2.302654\n",
      "iteration 1700 / 2000: loss 2.302665\n",
      "iteration 1800 / 2000: loss 2.302694\n",
      "iteration 1900 / 2000: loss 2.302674\n",
      "Hidden Size: 90, Learning Rate: 1e-07, LR Decay: 0.3, Regularization: 0.30000000000000004, Validation accuracy: 0.1294\n",
      "iteration 0 / 2000: loss 2.302723\n",
      "iteration 100 / 2000: loss 2.302732\n",
      "iteration 200 / 2000: loss 2.302735\n",
      "iteration 300 / 2000: loss 2.302724\n",
      "iteration 400 / 2000: loss 2.302738\n",
      "iteration 500 / 2000: loss 2.302732\n",
      "iteration 600 / 2000: loss 2.302731\n",
      "iteration 700 / 2000: loss 2.302735\n",
      "iteration 800 / 2000: loss 2.302729\n",
      "iteration 900 / 2000: loss 2.302728\n",
      "iteration 1000 / 2000: loss 2.302727\n",
      "iteration 1100 / 2000: loss 2.302739\n",
      "iteration 1200 / 2000: loss 2.302732\n",
      "iteration 1300 / 2000: loss 2.302743\n",
      "iteration 1400 / 2000: loss 2.302726\n",
      "iteration 1500 / 2000: loss 2.302739\n",
      "iteration 1600 / 2000: loss 2.302742\n",
      "iteration 1700 / 2000: loss 2.302730\n",
      "iteration 1800 / 2000: loss 2.302722\n",
      "iteration 1900 / 2000: loss 2.302734\n",
      "Hidden Size: 90, Learning Rate: 1e-07, LR Decay: 0.3, Regularization: 0.4, Validation accuracy: 0.1026\n"
     ]
    }
   ],
   "source": [
    "best_net = None # store the best model into this \n",
    "\n",
    "#################################################################################\n",
    "# TODO#8: Tune hyperparameters using the validation set. Store your best trained#\n",
    "# model in best_net.                                                            #\n",
    "#                                                                               #\n",
    "# To help debug your network, it may help to use visualizations similar to the  #\n",
    "# ones we used above; these visualizations will have significant qualitative    #\n",
    "# differences from the ones we saw above for the poorly tuned network.          #\n",
    "#                                                                               #\n",
    "# Tweaking hyperparameters by hand can be fun, but you might find it useful to  #\n",
    "# write code to sweep through possible combinations of hyperparameters          #\n",
    "# automatically like we did on the previous exercises.                          #\n",
    "#################################################################################\n",
    "\n",
    "#method name \"The Brute Force\"\n",
    "best_net = None\n",
    "best_val_acc = -1\n",
    "\n",
    "# Hyperparameters ranges\n",
    "hidden_sizes = range(10, 100, 10)\n",
    "learning_rates = np.linspace(1e-2, 1e-7, num=5)\n",
    "learning_rate_decays = np.linspace(0.99, 0.3, num=5)\n",
    "regs = np.linspace(0.0, 0.4, num=5)\n",
    "\n",
    "for hidden_size in hidden_sizes:\n",
    "    for learning_rate in learning_rates:\n",
    "        for learning_rate_decay in learning_rate_decays:\n",
    "            for reg in regs:\n",
    "                # Create and train the network\n",
    "                net = TwoLayerNet(input_size, hidden_size, num_classes)\n",
    "                stats = net.train(X_train, y_train, X_val, y_val,\n",
    "                                  num_iters=2000, batch_size=200,\n",
    "                                  learning_rate=learning_rate,\n",
    "                                  learning_rate_decay=learning_rate_decay,\n",
    "                                  reg=reg, verbose=True)\n",
    "                \n",
    "                # Predict on the validation set\n",
    "                val_acc = (net.predict(X_val) == y_val).mean()\n",
    "                print(f'Hidden Size: {hidden_size}, Learning Rate: {learning_rate}, '\n",
    "                      f'LR Decay: {learning_rate_decay}, Regularization: {reg}, '\n",
    "                      f'Validation accuracy: {val_acc}')\n",
    "\n",
    "                # Update best model if validation accuracy improves\n",
    "                if val_acc > best_val_acc:\n",
    "                    best_val_acc = val_acc\n",
    "                    best_net = net\n",
    "\n",
    "#################################################################################\n",
    "#                               END OF TODO#8                                   #\n",
    "#################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x800 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best_net hyperparams are : \n",
      "Hidden Size: 80, \n",
      "Learning Rate: 0.01, \n",
      "LR Decay: 0.8175, \n",
      "Regularization: 0.1\n",
      "Validation set accuracy:  0.9812\n"
     ]
    }
   ],
   "source": [
    "# visualize the weights of the best network\n",
    "show_net_weights(best_net)\n",
    "\n",
    "val_acc = (best_net.predict(X_val) == y_val).mean()\n",
    "print(\"best_net hyperparams are : \\nHidden Size: 80, \\nLearning Rate: 0.01, \\nLR Decay: 0.8175, \\nRegularization: 0.1\")\n",
    "\n",
    "print('Validation set accuracy: ', val_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# try the best hyperparameters with more epoch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 0 / 100000: loss 2.302635\n",
      "iteration 100 / 100000: loss 0.274605\n",
      "iteration 200 / 100000: loss 0.188203\n",
      "iteration 300 / 100000: loss 0.204182\n",
      "iteration 400 / 100000: loss 0.195978\n",
      "iteration 500 / 100000: loss 0.120012\n",
      "iteration 600 / 100000: loss 0.153781\n",
      "iteration 700 / 100000: loss 0.094288\n",
      "iteration 800 / 100000: loss 0.140840\n",
      "iteration 900 / 100000: loss 0.146651\n",
      "iteration 1000 / 100000: loss 0.130434\n",
      "iteration 1100 / 100000: loss 0.090662\n",
      "iteration 1200 / 100000: loss 0.088293\n",
      "iteration 1300 / 100000: loss 0.090594\n",
      "iteration 1400 / 100000: loss 0.101759\n",
      "iteration 1500 / 100000: loss 0.093270\n",
      "iteration 1600 / 100000: loss 0.093100\n",
      "iteration 1700 / 100000: loss 0.121070\n",
      "iteration 1800 / 100000: loss 0.077839\n",
      "iteration 1900 / 100000: loss 0.070440\n",
      "iteration 2000 / 100000: loss 0.102412\n",
      "iteration 2100 / 100000: loss 0.107814\n",
      "iteration 2200 / 100000: loss 0.086132\n",
      "iteration 2300 / 100000: loss 0.084793\n",
      "iteration 2400 / 100000: loss 0.084037\n",
      "iteration 2500 / 100000: loss 0.117763\n",
      "iteration 2600 / 100000: loss 0.100596\n",
      "iteration 2700 / 100000: loss 0.091165\n",
      "iteration 2800 / 100000: loss 0.073319\n",
      "iteration 2900 / 100000: loss 0.084606\n",
      "iteration 3000 / 100000: loss 0.075322\n",
      "iteration 3100 / 100000: loss 0.106922\n",
      "iteration 3200 / 100000: loss 0.104732\n",
      "iteration 3300 / 100000: loss 0.088545\n",
      "iteration 3400 / 100000: loss 0.077194\n",
      "iteration 3500 / 100000: loss 0.090126\n",
      "iteration 3600 / 100000: loss 0.088217\n",
      "iteration 3700 / 100000: loss 0.086554\n",
      "iteration 3800 / 100000: loss 0.125347\n",
      "iteration 3900 / 100000: loss 0.083502\n",
      "iteration 4000 / 100000: loss 0.075122\n",
      "iteration 4100 / 100000: loss 0.090403\n",
      "iteration 4200 / 100000: loss 0.081825\n",
      "iteration 4300 / 100000: loss 0.088919\n",
      "iteration 4400 / 100000: loss 0.085436\n",
      "iteration 4500 / 100000: loss 0.089520\n",
      "iteration 4600 / 100000: loss 0.091767\n",
      "iteration 4700 / 100000: loss 0.093446\n",
      "iteration 4800 / 100000: loss 0.088613\n",
      "iteration 4900 / 100000: loss 0.084465\n",
      "iteration 5000 / 100000: loss 0.095136\n",
      "iteration 5100 / 100000: loss 0.096581\n",
      "iteration 5200 / 100000: loss 0.101619\n",
      "iteration 5300 / 100000: loss 0.081468\n",
      "iteration 5400 / 100000: loss 0.093181\n",
      "iteration 5500 / 100000: loss 0.105452\n",
      "iteration 5600 / 100000: loss 0.102393\n",
      "iteration 5700 / 100000: loss 0.091845\n",
      "iteration 5800 / 100000: loss 0.074033\n",
      "iteration 5900 / 100000: loss 0.085838\n",
      "iteration 6000 / 100000: loss 0.094208\n",
      "iteration 6100 / 100000: loss 0.074676\n",
      "iteration 6200 / 100000: loss 0.076158\n",
      "iteration 6300 / 100000: loss 0.094182\n",
      "iteration 6400 / 100000: loss 0.102247\n",
      "iteration 6500 / 100000: loss 0.097605\n",
      "iteration 6600 / 100000: loss 0.113988\n",
      "iteration 6700 / 100000: loss 0.077239\n",
      "iteration 6800 / 100000: loss 0.093244\n",
      "iteration 6900 / 100000: loss 0.083985\n",
      "iteration 7000 / 100000: loss 0.083163\n",
      "iteration 7100 / 100000: loss 0.111235\n",
      "iteration 7200 / 100000: loss 0.090829\n",
      "iteration 7300 / 100000: loss 0.081429\n",
      "iteration 7400 / 100000: loss 0.075671\n",
      "iteration 7500 / 100000: loss 0.084482\n",
      "iteration 7600 / 100000: loss 0.080616\n",
      "iteration 7700 / 100000: loss 0.089982\n",
      "iteration 7800 / 100000: loss 0.090126\n",
      "iteration 7900 / 100000: loss 0.075324\n",
      "iteration 8000 / 100000: loss 0.083609\n",
      "iteration 8100 / 100000: loss 0.082177\n",
      "iteration 8200 / 100000: loss 0.081121\n",
      "iteration 8300 / 100000: loss 0.075555\n",
      "iteration 8400 / 100000: loss 0.067901\n",
      "iteration 8500 / 100000: loss 0.077108\n",
      "iteration 8600 / 100000: loss 0.089101\n",
      "iteration 8700 / 100000: loss 0.076094\n",
      "iteration 8800 / 100000: loss 0.079103\n",
      "iteration 8900 / 100000: loss 0.086043\n",
      "iteration 9000 / 100000: loss 0.092770\n",
      "iteration 9100 / 100000: loss 0.083438\n",
      "iteration 9200 / 100000: loss 0.090148\n",
      "iteration 9300 / 100000: loss 0.091821\n",
      "iteration 9400 / 100000: loss 0.096315\n",
      "iteration 9500 / 100000: loss 0.111758\n",
      "iteration 9600 / 100000: loss 0.081764\n",
      "iteration 9700 / 100000: loss 0.076326\n",
      "iteration 9800 / 100000: loss 0.095866\n",
      "iteration 9900 / 100000: loss 0.091968\n",
      "iteration 10000 / 100000: loss 0.093246\n",
      "iteration 10100 / 100000: loss 0.095500\n",
      "iteration 10200 / 100000: loss 0.076463\n",
      "iteration 10300 / 100000: loss 0.084813\n",
      "iteration 10400 / 100000: loss 0.103853\n",
      "iteration 10500 / 100000: loss 0.085467\n",
      "iteration 10600 / 100000: loss 0.068228\n",
      "iteration 10700 / 100000: loss 0.081093\n",
      "iteration 10800 / 100000: loss 0.080336\n",
      "iteration 10900 / 100000: loss 0.086214\n",
      "iteration 11000 / 100000: loss 0.087041\n",
      "iteration 11100 / 100000: loss 0.103869\n",
      "iteration 11200 / 100000: loss 0.090031\n",
      "iteration 11300 / 100000: loss 0.084130\n",
      "iteration 11400 / 100000: loss 0.099852\n",
      "iteration 11500 / 100000: loss 0.082841\n",
      "iteration 11600 / 100000: loss 0.081053\n",
      "iteration 11700 / 100000: loss 0.107573\n",
      "iteration 11800 / 100000: loss 0.078823\n",
      "iteration 11900 / 100000: loss 0.079396\n",
      "iteration 12000 / 100000: loss 0.082511\n",
      "iteration 12100 / 100000: loss 0.091345\n",
      "iteration 12200 / 100000: loss 0.110800\n",
      "iteration 12300 / 100000: loss 0.083923\n",
      "iteration 12400 / 100000: loss 0.085487\n",
      "iteration 12500 / 100000: loss 0.093847\n",
      "iteration 12600 / 100000: loss 0.097751\n",
      "iteration 12700 / 100000: loss 0.076375\n",
      "iteration 12800 / 100000: loss 0.070026\n",
      "iteration 12900 / 100000: loss 0.092831\n",
      "iteration 13000 / 100000: loss 0.085723\n",
      "iteration 13100 / 100000: loss 0.101650\n",
      "iteration 13200 / 100000: loss 0.086871\n",
      "iteration 13300 / 100000: loss 0.083037\n",
      "iteration 13400 / 100000: loss 0.074161\n",
      "iteration 13500 / 100000: loss 0.086228\n",
      "iteration 13600 / 100000: loss 0.071336\n",
      "iteration 13700 / 100000: loss 0.076165\n",
      "iteration 13800 / 100000: loss 0.072251\n",
      "iteration 13900 / 100000: loss 0.102673\n",
      "iteration 14000 / 100000: loss 0.084906\n",
      "iteration 14100 / 100000: loss 0.113193\n",
      "iteration 14200 / 100000: loss 0.079442\n",
      "iteration 14300 / 100000: loss 0.072739\n",
      "iteration 14400 / 100000: loss 0.075154\n",
      "iteration 14500 / 100000: loss 0.088342\n",
      "iteration 14600 / 100000: loss 0.086036\n",
      "iteration 14700 / 100000: loss 0.094425\n",
      "iteration 14800 / 100000: loss 0.082209\n",
      "iteration 14900 / 100000: loss 0.100601\n",
      "iteration 15000 / 100000: loss 0.085995\n",
      "iteration 15100 / 100000: loss 0.079540\n",
      "iteration 15200 / 100000: loss 0.088346\n",
      "iteration 15300 / 100000: loss 0.087119\n",
      "iteration 15400 / 100000: loss 0.078950\n",
      "iteration 15500 / 100000: loss 0.102214\n",
      "iteration 15600 / 100000: loss 0.085851\n",
      "iteration 15700 / 100000: loss 0.095082\n",
      "iteration 15800 / 100000: loss 0.091507\n",
      "iteration 15900 / 100000: loss 0.082445\n",
      "iteration 16000 / 100000: loss 0.069024\n",
      "iteration 16100 / 100000: loss 0.083289\n",
      "iteration 16200 / 100000: loss 0.072201\n",
      "iteration 16300 / 100000: loss 0.091987\n",
      "iteration 16400 / 100000: loss 0.104336\n",
      "iteration 16500 / 100000: loss 0.073670\n",
      "iteration 16600 / 100000: loss 0.089329\n",
      "iteration 16700 / 100000: loss 0.103081\n",
      "iteration 16800 / 100000: loss 0.081194\n",
      "iteration 16900 / 100000: loss 0.077000\n",
      "iteration 17000 / 100000: loss 0.067338\n",
      "iteration 17100 / 100000: loss 0.084532\n",
      "iteration 17200 / 100000: loss 0.074108\n",
      "iteration 17300 / 100000: loss 0.078703\n",
      "iteration 17400 / 100000: loss 0.077610\n",
      "iteration 17500 / 100000: loss 0.072991\n",
      "iteration 17600 / 100000: loss 0.070449\n",
      "iteration 17700 / 100000: loss 0.084398\n",
      "iteration 17800 / 100000: loss 0.087185\n",
      "iteration 17900 / 100000: loss 0.105868\n",
      "iteration 18000 / 100000: loss 0.080922\n",
      "iteration 18100 / 100000: loss 0.106938\n",
      "iteration 18200 / 100000: loss 0.086991\n",
      "iteration 18300 / 100000: loss 0.087370\n",
      "iteration 18400 / 100000: loss 0.092098\n",
      "iteration 18500 / 100000: loss 0.097292\n",
      "iteration 18600 / 100000: loss 0.093784\n",
      "iteration 18700 / 100000: loss 0.091202\n",
      "iteration 18800 / 100000: loss 0.079670\n",
      "iteration 18900 / 100000: loss 0.097532\n",
      "iteration 19000 / 100000: loss 0.073086\n",
      "iteration 19100 / 100000: loss 0.086018\n",
      "iteration 19200 / 100000: loss 0.081860\n",
      "iteration 19300 / 100000: loss 0.078702\n",
      "iteration 19400 / 100000: loss 0.081993\n",
      "iteration 19500 / 100000: loss 0.086694\n",
      "iteration 19600 / 100000: loss 0.116054\n",
      "iteration 19700 / 100000: loss 0.085311\n",
      "iteration 19800 / 100000: loss 0.081041\n",
      "iteration 19900 / 100000: loss 0.093765\n",
      "iteration 20000 / 100000: loss 0.099468\n",
      "iteration 20100 / 100000: loss 0.080188\n",
      "iteration 20200 / 100000: loss 0.078096\n",
      "iteration 20300 / 100000: loss 0.095998\n",
      "iteration 20400 / 100000: loss 0.090395\n",
      "iteration 20500 / 100000: loss 0.075644\n",
      "iteration 20600 / 100000: loss 0.075651\n",
      "iteration 20700 / 100000: loss 0.079622\n",
      "iteration 20800 / 100000: loss 0.085437\n",
      "iteration 20900 / 100000: loss 0.071552\n",
      "iteration 21000 / 100000: loss 0.107348\n",
      "iteration 21100 / 100000: loss 0.072699\n",
      "iteration 21200 / 100000: loss 0.081361\n",
      "iteration 21300 / 100000: loss 0.098620\n",
      "iteration 21400 / 100000: loss 0.084514\n",
      "iteration 21500 / 100000: loss 0.090631\n",
      "iteration 21600 / 100000: loss 0.073841\n",
      "iteration 21700 / 100000: loss 0.089428\n",
      "iteration 21800 / 100000: loss 0.073935\n",
      "iteration 21900 / 100000: loss 0.100025\n",
      "iteration 22000 / 100000: loss 0.080109\n",
      "iteration 22100 / 100000: loss 0.096826\n",
      "iteration 22200 / 100000: loss 0.091898\n",
      "iteration 22300 / 100000: loss 0.096746\n",
      "iteration 22400 / 100000: loss 0.077876\n",
      "iteration 22500 / 100000: loss 0.075978\n",
      "iteration 22600 / 100000: loss 0.074448\n",
      "iteration 22700 / 100000: loss 0.082694\n",
      "iteration 22800 / 100000: loss 0.090481\n",
      "iteration 22900 / 100000: loss 0.088391\n",
      "iteration 23000 / 100000: loss 0.073185\n",
      "iteration 23100 / 100000: loss 0.084430\n",
      "iteration 23200 / 100000: loss 0.082321\n",
      "iteration 23300 / 100000: loss 0.082613\n",
      "iteration 23400 / 100000: loss 0.088253\n",
      "iteration 23500 / 100000: loss 0.077508\n",
      "iteration 23600 / 100000: loss 0.080813\n",
      "iteration 23700 / 100000: loss 0.111025\n",
      "iteration 23800 / 100000: loss 0.075914\n",
      "iteration 23900 / 100000: loss 0.109058\n",
      "iteration 24000 / 100000: loss 0.085322\n",
      "iteration 24100 / 100000: loss 0.083906\n",
      "iteration 24200 / 100000: loss 0.077659\n",
      "iteration 24300 / 100000: loss 0.082079\n",
      "iteration 24400 / 100000: loss 0.088022\n",
      "iteration 24500 / 100000: loss 0.082069\n",
      "iteration 24600 / 100000: loss 0.078502\n",
      "iteration 24700 / 100000: loss 0.091507\n",
      "iteration 24800 / 100000: loss 0.074411\n",
      "iteration 24900 / 100000: loss 0.094362\n",
      "iteration 25000 / 100000: loss 0.086536\n",
      "iteration 25100 / 100000: loss 0.080549\n",
      "iteration 25200 / 100000: loss 0.089250\n",
      "iteration 25300 / 100000: loss 0.095392\n",
      "iteration 25400 / 100000: loss 0.069071\n",
      "iteration 25500 / 100000: loss 0.112593\n",
      "iteration 25600 / 100000: loss 0.079611\n",
      "iteration 25700 / 100000: loss 0.123050\n",
      "iteration 25800 / 100000: loss 0.085699\n",
      "iteration 25900 / 100000: loss 0.095011\n",
      "iteration 26000 / 100000: loss 0.090387\n",
      "iteration 26100 / 100000: loss 0.080155\n",
      "iteration 26200 / 100000: loss 0.085745\n",
      "iteration 26300 / 100000: loss 0.080594\n",
      "iteration 26400 / 100000: loss 0.078348\n",
      "iteration 26500 / 100000: loss 0.089133\n",
      "iteration 26600 / 100000: loss 0.080015\n",
      "iteration 26700 / 100000: loss 0.078984\n",
      "iteration 26800 / 100000: loss 0.077073\n",
      "iteration 26900 / 100000: loss 0.082749\n",
      "iteration 27000 / 100000: loss 0.113679\n",
      "iteration 27100 / 100000: loss 0.093543\n",
      "iteration 27200 / 100000: loss 0.070157\n",
      "iteration 27300 / 100000: loss 0.074444\n",
      "iteration 27400 / 100000: loss 0.092321\n",
      "iteration 27500 / 100000: loss 0.081306\n",
      "iteration 27600 / 100000: loss 0.114659\n",
      "iteration 27700 / 100000: loss 0.091864\n",
      "iteration 27800 / 100000: loss 0.093768\n",
      "iteration 27900 / 100000: loss 0.080695\n",
      "iteration 28000 / 100000: loss 0.106574\n",
      "iteration 28100 / 100000: loss 0.130946\n",
      "iteration 28200 / 100000: loss 0.092540\n",
      "iteration 28300 / 100000: loss 0.099795\n",
      "iteration 28400 / 100000: loss 0.079355\n",
      "iteration 28500 / 100000: loss 0.094295\n",
      "iteration 28600 / 100000: loss 0.091385\n",
      "iteration 28700 / 100000: loss 0.086595\n",
      "iteration 28800 / 100000: loss 0.086925\n",
      "iteration 28900 / 100000: loss 0.076408\n",
      "iteration 29000 / 100000: loss 0.080225\n",
      "iteration 29100 / 100000: loss 0.099417\n",
      "iteration 29200 / 100000: loss 0.098984\n",
      "iteration 29300 / 100000: loss 0.088168\n",
      "iteration 29400 / 100000: loss 0.071248\n",
      "iteration 29500 / 100000: loss 0.085964\n",
      "iteration 29600 / 100000: loss 0.075999\n",
      "iteration 29700 / 100000: loss 0.080518\n",
      "iteration 29800 / 100000: loss 0.091875\n",
      "iteration 29900 / 100000: loss 0.092368\n",
      "iteration 30000 / 100000: loss 0.094758\n",
      "iteration 30100 / 100000: loss 0.095991\n",
      "iteration 30200 / 100000: loss 0.096892\n",
      "iteration 30300 / 100000: loss 0.081321\n",
      "iteration 30400 / 100000: loss 0.098998\n",
      "iteration 30500 / 100000: loss 0.079586\n",
      "iteration 30600 / 100000: loss 0.084058\n",
      "iteration 30700 / 100000: loss 0.083494\n",
      "iteration 30800 / 100000: loss 0.109264\n",
      "iteration 30900 / 100000: loss 0.077772\n",
      "iteration 31000 / 100000: loss 0.088567\n",
      "iteration 31100 / 100000: loss 0.074716\n",
      "iteration 31200 / 100000: loss 0.074161\n",
      "iteration 31300 / 100000: loss 0.074253\n",
      "iteration 31400 / 100000: loss 0.096255\n",
      "iteration 31500 / 100000: loss 0.094630\n",
      "iteration 31600 / 100000: loss 0.078381\n",
      "iteration 31700 / 100000: loss 0.096632\n",
      "iteration 31800 / 100000: loss 0.085394\n",
      "iteration 31900 / 100000: loss 0.083304\n",
      "iteration 32000 / 100000: loss 0.084025\n",
      "iteration 32100 / 100000: loss 0.080269\n",
      "iteration 32200 / 100000: loss 0.107781\n",
      "iteration 32300 / 100000: loss 0.089696\n",
      "iteration 32400 / 100000: loss 0.094506\n",
      "iteration 32500 / 100000: loss 0.095516\n",
      "iteration 32600 / 100000: loss 0.077633\n",
      "iteration 32700 / 100000: loss 0.081564\n",
      "iteration 32800 / 100000: loss 0.106731\n",
      "iteration 32900 / 100000: loss 0.097435\n",
      "iteration 33000 / 100000: loss 0.096067\n",
      "iteration 33100 / 100000: loss 0.077691\n",
      "iteration 33200 / 100000: loss 0.078514\n",
      "iteration 33300 / 100000: loss 0.074702\n",
      "iteration 33400 / 100000: loss 0.077898\n",
      "iteration 33500 / 100000: loss 0.089902\n",
      "iteration 33600 / 100000: loss 0.082850\n",
      "iteration 33700 / 100000: loss 0.070552\n",
      "iteration 33800 / 100000: loss 0.097205\n",
      "iteration 33900 / 100000: loss 0.084782\n",
      "iteration 34000 / 100000: loss 0.074709\n",
      "iteration 34100 / 100000: loss 0.070214\n",
      "iteration 34200 / 100000: loss 0.087905\n",
      "iteration 34300 / 100000: loss 0.110632\n",
      "iteration 34400 / 100000: loss 0.101235\n",
      "iteration 34500 / 100000: loss 0.075630\n",
      "iteration 34600 / 100000: loss 0.073465\n",
      "iteration 34700 / 100000: loss 0.075207\n",
      "iteration 34800 / 100000: loss 0.072726\n",
      "iteration 34900 / 100000: loss 0.083881\n",
      "iteration 35000 / 100000: loss 0.076561\n",
      "iteration 35100 / 100000: loss 0.072016\n",
      "iteration 35200 / 100000: loss 0.094772\n",
      "iteration 35300 / 100000: loss 0.095460\n",
      "iteration 35400 / 100000: loss 0.072019\n",
      "iteration 35500 / 100000: loss 0.083338\n",
      "iteration 35600 / 100000: loss 0.091246\n",
      "iteration 35700 / 100000: loss 0.090298\n",
      "iteration 35800 / 100000: loss 0.113247\n",
      "iteration 35900 / 100000: loss 0.088267\n",
      "iteration 36000 / 100000: loss 0.079008\n",
      "iteration 36100 / 100000: loss 0.089092\n",
      "iteration 36200 / 100000: loss 0.086725\n",
      "iteration 36300 / 100000: loss 0.083277\n",
      "iteration 36400 / 100000: loss 0.069801\n",
      "iteration 36500 / 100000: loss 0.079663\n",
      "iteration 36600 / 100000: loss 0.113616\n",
      "iteration 36700 / 100000: loss 0.075109\n",
      "iteration 36800 / 100000: loss 0.092325\n",
      "iteration 36900 / 100000: loss 0.082685\n",
      "iteration 37000 / 100000: loss 0.086279\n",
      "iteration 37100 / 100000: loss 0.088007\n",
      "iteration 37200 / 100000: loss 0.101122\n",
      "iteration 37300 / 100000: loss 0.125101\n",
      "iteration 37400 / 100000: loss 0.122901\n",
      "iteration 37500 / 100000: loss 0.096406\n",
      "iteration 37600 / 100000: loss 0.079062\n",
      "iteration 37700 / 100000: loss 0.070220\n",
      "iteration 37800 / 100000: loss 0.093483\n",
      "iteration 37900 / 100000: loss 0.071883\n",
      "iteration 38000 / 100000: loss 0.082553\n",
      "iteration 38100 / 100000: loss 0.096255\n",
      "iteration 38200 / 100000: loss 0.074240\n",
      "iteration 38300 / 100000: loss 0.077806\n",
      "iteration 38400 / 100000: loss 0.089059\n",
      "iteration 38500 / 100000: loss 0.089743\n",
      "iteration 38600 / 100000: loss 0.098312\n",
      "iteration 38700 / 100000: loss 0.095524\n",
      "iteration 38800 / 100000: loss 0.103183\n",
      "iteration 38900 / 100000: loss 0.081510\n",
      "iteration 39000 / 100000: loss 0.080115\n",
      "iteration 39100 / 100000: loss 0.099486\n",
      "iteration 39200 / 100000: loss 0.104314\n",
      "iteration 39300 / 100000: loss 0.081329\n",
      "iteration 39400 / 100000: loss 0.073904\n",
      "iteration 39500 / 100000: loss 0.101092\n",
      "iteration 39600 / 100000: loss 0.087894\n",
      "iteration 39700 / 100000: loss 0.087667\n",
      "iteration 39800 / 100000: loss 0.102539\n",
      "iteration 39900 / 100000: loss 0.077619\n",
      "iteration 40000 / 100000: loss 0.077893\n",
      "iteration 40100 / 100000: loss 0.110934\n",
      "iteration 40200 / 100000: loss 0.086042\n",
      "iteration 40300 / 100000: loss 0.108491\n",
      "iteration 40400 / 100000: loss 0.118805\n",
      "iteration 40500 / 100000: loss 0.094549\n",
      "iteration 40600 / 100000: loss 0.091067\n",
      "iteration 40700 / 100000: loss 0.085684\n",
      "iteration 40800 / 100000: loss 0.083700\n",
      "iteration 40900 / 100000: loss 0.077656\n",
      "iteration 41000 / 100000: loss 0.087657\n",
      "iteration 41100 / 100000: loss 0.086584\n",
      "iteration 41200 / 100000: loss 0.078725\n",
      "iteration 41300 / 100000: loss 0.075307\n",
      "iteration 41400 / 100000: loss 0.103066\n",
      "iteration 41500 / 100000: loss 0.079600\n",
      "iteration 41600 / 100000: loss 0.090904\n",
      "iteration 41700 / 100000: loss 0.078881\n",
      "iteration 41800 / 100000: loss 0.081719\n",
      "iteration 41900 / 100000: loss 0.084616\n",
      "iteration 42000 / 100000: loss 0.092607\n",
      "iteration 42100 / 100000: loss 0.077819\n",
      "iteration 42200 / 100000: loss 0.077705\n",
      "iteration 42300 / 100000: loss 0.086091\n",
      "iteration 42400 / 100000: loss 0.104824\n",
      "iteration 42500 / 100000: loss 0.104837\n",
      "iteration 42600 / 100000: loss 0.111211\n",
      "iteration 42700 / 100000: loss 0.090214\n",
      "iteration 42800 / 100000: loss 0.099800\n",
      "iteration 42900 / 100000: loss 0.088701\n",
      "iteration 43000 / 100000: loss 0.119134\n",
      "iteration 43100 / 100000: loss 0.105088\n",
      "iteration 43200 / 100000: loss 0.095744\n",
      "iteration 43300 / 100000: loss 0.078640\n",
      "iteration 43400 / 100000: loss 0.076148\n",
      "iteration 43500 / 100000: loss 0.092534\n",
      "iteration 43600 / 100000: loss 0.091333\n",
      "iteration 43700 / 100000: loss 0.078977\n",
      "iteration 43800 / 100000: loss 0.088872\n",
      "iteration 43900 / 100000: loss 0.083027\n",
      "iteration 44000 / 100000: loss 0.071490\n",
      "iteration 44100 / 100000: loss 0.074310\n",
      "iteration 44200 / 100000: loss 0.072089\n",
      "iteration 44300 / 100000: loss 0.079933\n",
      "iteration 44400 / 100000: loss 0.085246\n",
      "iteration 44500 / 100000: loss 0.074008\n",
      "iteration 44600 / 100000: loss 0.085342\n",
      "iteration 44700 / 100000: loss 0.084138\n",
      "iteration 44800 / 100000: loss 0.074105\n",
      "iteration 44900 / 100000: loss 0.085545\n",
      "iteration 45000 / 100000: loss 0.101561\n",
      "iteration 45100 / 100000: loss 0.092750\n",
      "iteration 45200 / 100000: loss 0.124263\n",
      "iteration 45300 / 100000: loss 0.111229\n",
      "iteration 45400 / 100000: loss 0.094029\n",
      "iteration 45500 / 100000: loss 0.076435\n",
      "iteration 45600 / 100000: loss 0.079424\n",
      "iteration 45700 / 100000: loss 0.094567\n",
      "iteration 45800 / 100000: loss 0.086233\n",
      "iteration 45900 / 100000: loss 0.090578\n",
      "iteration 46000 / 100000: loss 0.074585\n",
      "iteration 46100 / 100000: loss 0.101741\n",
      "iteration 46200 / 100000: loss 0.080234\n",
      "iteration 46300 / 100000: loss 0.078334\n",
      "iteration 46400 / 100000: loss 0.100153\n",
      "iteration 46500 / 100000: loss 0.084359\n",
      "iteration 46600 / 100000: loss 0.078633\n",
      "iteration 46700 / 100000: loss 0.131721\n",
      "iteration 46800 / 100000: loss 0.081157\n",
      "iteration 46900 / 100000: loss 0.092547\n",
      "iteration 47000 / 100000: loss 0.084507\n",
      "iteration 47100 / 100000: loss 0.079237\n",
      "iteration 47200 / 100000: loss 0.093103\n",
      "iteration 47300 / 100000: loss 0.077684\n",
      "iteration 47400 / 100000: loss 0.077328\n",
      "iteration 47500 / 100000: loss 0.096331\n",
      "iteration 47600 / 100000: loss 0.115741\n",
      "iteration 47700 / 100000: loss 0.075550\n",
      "iteration 47800 / 100000: loss 0.081318\n",
      "iteration 47900 / 100000: loss 0.101801\n",
      "iteration 48000 / 100000: loss 0.100953\n",
      "iteration 48100 / 100000: loss 0.089489\n",
      "iteration 48200 / 100000: loss 0.091994\n",
      "iteration 48300 / 100000: loss 0.081073\n",
      "iteration 48400 / 100000: loss 0.083216\n",
      "iteration 48500 / 100000: loss 0.101185\n",
      "iteration 48600 / 100000: loss 0.081582\n",
      "iteration 48700 / 100000: loss 0.102971\n",
      "iteration 48800 / 100000: loss 0.114785\n",
      "iteration 48900 / 100000: loss 0.080071\n",
      "iteration 49000 / 100000: loss 0.091499\n",
      "iteration 49100 / 100000: loss 0.075015\n",
      "iteration 49200 / 100000: loss 0.108893\n",
      "iteration 49300 / 100000: loss 0.094544\n",
      "iteration 49400 / 100000: loss 0.075918\n",
      "iteration 49500 / 100000: loss 0.079766\n",
      "iteration 49600 / 100000: loss 0.137670\n",
      "iteration 49700 / 100000: loss 0.079444\n",
      "iteration 49800 / 100000: loss 0.111363\n",
      "iteration 49900 / 100000: loss 0.075773\n",
      "iteration 50000 / 100000: loss 0.077135\n",
      "iteration 50100 / 100000: loss 0.081078\n",
      "iteration 50200 / 100000: loss 0.078873\n",
      "iteration 50300 / 100000: loss 0.098501\n",
      "iteration 50400 / 100000: loss 0.082538\n",
      "iteration 50500 / 100000: loss 0.098394\n",
      "iteration 50600 / 100000: loss 0.093450\n",
      "iteration 50700 / 100000: loss 0.092897\n",
      "iteration 50800 / 100000: loss 0.069874\n",
      "iteration 50900 / 100000: loss 0.089290\n",
      "iteration 51000 / 100000: loss 0.086486\n",
      "iteration 51100 / 100000: loss 0.081737\n",
      "iteration 51200 / 100000: loss 0.085485\n",
      "iteration 51300 / 100000: loss 0.085076\n",
      "iteration 51400 / 100000: loss 0.095551\n",
      "iteration 51500 / 100000: loss 0.084985\n",
      "iteration 51600 / 100000: loss 0.089793\n",
      "iteration 51700 / 100000: loss 0.096736\n",
      "iteration 51800 / 100000: loss 0.089361\n",
      "iteration 51900 / 100000: loss 0.096174\n",
      "iteration 52000 / 100000: loss 0.079757\n",
      "iteration 52100 / 100000: loss 0.082822\n",
      "iteration 52200 / 100000: loss 0.090062\n",
      "iteration 52300 / 100000: loss 0.084170\n",
      "iteration 52400 / 100000: loss 0.117201\n",
      "iteration 52500 / 100000: loss 0.101191\n",
      "iteration 52600 / 100000: loss 0.082956\n",
      "iteration 52700 / 100000: loss 0.085126\n",
      "iteration 52800 / 100000: loss 0.080881\n",
      "iteration 52900 / 100000: loss 0.073922\n",
      "iteration 53000 / 100000: loss 0.086987\n",
      "iteration 53100 / 100000: loss 0.071891\n",
      "iteration 53200 / 100000: loss 0.080974\n",
      "iteration 53300 / 100000: loss 0.088491\n",
      "iteration 53400 / 100000: loss 0.101952\n",
      "iteration 53500 / 100000: loss 0.086328\n",
      "iteration 53600 / 100000: loss 0.083016\n",
      "iteration 53700 / 100000: loss 0.095173\n",
      "iteration 53800 / 100000: loss 0.074663\n",
      "iteration 53900 / 100000: loss 0.090969\n",
      "iteration 54000 / 100000: loss 0.094456\n",
      "iteration 54100 / 100000: loss 0.100515\n",
      "iteration 54200 / 100000: loss 0.083882\n",
      "iteration 54300 / 100000: loss 0.088182\n",
      "iteration 54400 / 100000: loss 0.126121\n",
      "iteration 54500 / 100000: loss 0.099523\n",
      "iteration 54600 / 100000: loss 0.094334\n",
      "iteration 54700 / 100000: loss 0.089576\n",
      "iteration 54800 / 100000: loss 0.070732\n",
      "iteration 54900 / 100000: loss 0.099052\n",
      "iteration 55000 / 100000: loss 0.080815\n",
      "iteration 55100 / 100000: loss 0.106447\n",
      "iteration 55200 / 100000: loss 0.074282\n",
      "iteration 55300 / 100000: loss 0.079000\n",
      "iteration 55400 / 100000: loss 0.078754\n",
      "iteration 55500 / 100000: loss 0.089266\n",
      "iteration 55600 / 100000: loss 0.087811\n",
      "iteration 55700 / 100000: loss 0.114277\n",
      "iteration 55800 / 100000: loss 0.107656\n",
      "iteration 55900 / 100000: loss 0.082525\n",
      "iteration 56000 / 100000: loss 0.080543\n",
      "iteration 56100 / 100000: loss 0.082586\n",
      "iteration 56200 / 100000: loss 0.089144\n",
      "iteration 56300 / 100000: loss 0.078132\n",
      "iteration 56400 / 100000: loss 0.081459\n",
      "iteration 56500 / 100000: loss 0.065404\n",
      "iteration 56600 / 100000: loss 0.108407\n",
      "iteration 56700 / 100000: loss 0.081076\n",
      "iteration 56800 / 100000: loss 0.079233\n",
      "iteration 56900 / 100000: loss 0.103004\n",
      "iteration 57000 / 100000: loss 0.076014\n",
      "iteration 57100 / 100000: loss 0.069353\n",
      "iteration 57200 / 100000: loss 0.088786\n",
      "iteration 57300 / 100000: loss 0.079272\n",
      "iteration 57400 / 100000: loss 0.095459\n",
      "iteration 57500 / 100000: loss 0.087501\n",
      "iteration 57600 / 100000: loss 0.093462\n",
      "iteration 57700 / 100000: loss 0.083363\n",
      "iteration 57800 / 100000: loss 0.073162\n",
      "iteration 57900 / 100000: loss 0.076844\n",
      "iteration 58000 / 100000: loss 0.092506\n",
      "iteration 58100 / 100000: loss 0.091446\n",
      "iteration 58200 / 100000: loss 0.089123\n",
      "iteration 58300 / 100000: loss 0.075866\n",
      "iteration 58400 / 100000: loss 0.111188\n",
      "iteration 58500 / 100000: loss 0.108580\n",
      "iteration 58600 / 100000: loss 0.094759\n",
      "iteration 58700 / 100000: loss 0.085351\n",
      "iteration 58800 / 100000: loss 0.092780\n",
      "iteration 58900 / 100000: loss 0.081997\n",
      "iteration 59000 / 100000: loss 0.108702\n",
      "iteration 59100 / 100000: loss 0.090754\n",
      "iteration 59200 / 100000: loss 0.083954\n",
      "iteration 59300 / 100000: loss 0.085407\n",
      "iteration 59400 / 100000: loss 0.117224\n",
      "iteration 59500 / 100000: loss 0.080187\n",
      "iteration 59600 / 100000: loss 0.074470\n",
      "iteration 59700 / 100000: loss 0.096655\n",
      "iteration 59800 / 100000: loss 0.088429\n",
      "iteration 59900 / 100000: loss 0.118270\n",
      "iteration 60000 / 100000: loss 0.074841\n",
      "iteration 60100 / 100000: loss 0.087940\n",
      "iteration 60200 / 100000: loss 0.085010\n",
      "iteration 60300 / 100000: loss 0.089951\n",
      "iteration 60400 / 100000: loss 0.102288\n",
      "iteration 60500 / 100000: loss 0.083175\n",
      "iteration 60600 / 100000: loss 0.096196\n",
      "iteration 60700 / 100000: loss 0.077273\n",
      "iteration 60800 / 100000: loss 0.068597\n",
      "iteration 60900 / 100000: loss 0.084913\n",
      "iteration 61000 / 100000: loss 0.081987\n",
      "iteration 61100 / 100000: loss 0.081089\n",
      "iteration 61200 / 100000: loss 0.066702\n",
      "iteration 61300 / 100000: loss 0.086484\n",
      "iteration 61400 / 100000: loss 0.096067\n",
      "iteration 61500 / 100000: loss 0.094643\n",
      "iteration 61600 / 100000: loss 0.097919\n",
      "iteration 61700 / 100000: loss 0.114501\n",
      "iteration 61800 / 100000: loss 0.076014\n",
      "iteration 61900 / 100000: loss 0.072371\n",
      "iteration 62000 / 100000: loss 0.083673\n",
      "iteration 62100 / 100000: loss 0.112766\n",
      "iteration 62200 / 100000: loss 0.083109\n",
      "iteration 62300 / 100000: loss 0.089260\n",
      "iteration 62400 / 100000: loss 0.101023\n",
      "iteration 62500 / 100000: loss 0.082222\n",
      "iteration 62600 / 100000: loss 0.074115\n",
      "iteration 62700 / 100000: loss 0.079988\n",
      "iteration 62800 / 100000: loss 0.068575\n",
      "iteration 62900 / 100000: loss 0.079997\n",
      "iteration 63000 / 100000: loss 0.074627\n",
      "iteration 63100 / 100000: loss 0.098565\n",
      "iteration 63200 / 100000: loss 0.098752\n",
      "iteration 63300 / 100000: loss 0.090873\n",
      "iteration 63400 / 100000: loss 0.074513\n",
      "iteration 63500 / 100000: loss 0.099894\n",
      "iteration 63600 / 100000: loss 0.094773\n",
      "iteration 63700 / 100000: loss 0.078688\n",
      "iteration 63800 / 100000: loss 0.081140\n",
      "iteration 63900 / 100000: loss 0.089663\n",
      "iteration 64000 / 100000: loss 0.097073\n",
      "iteration 64100 / 100000: loss 0.081382\n",
      "iteration 64200 / 100000: loss 0.090548\n",
      "iteration 64300 / 100000: loss 0.083643\n",
      "iteration 64400 / 100000: loss 0.093797\n",
      "iteration 64500 / 100000: loss 0.096420\n",
      "iteration 64600 / 100000: loss 0.088084\n",
      "iteration 64700 / 100000: loss 0.105793\n",
      "iteration 64800 / 100000: loss 0.090879\n",
      "iteration 64900 / 100000: loss 0.086877\n",
      "iteration 65000 / 100000: loss 0.074459\n",
      "iteration 65100 / 100000: loss 0.082420\n",
      "iteration 65200 / 100000: loss 0.089795\n",
      "iteration 65300 / 100000: loss 0.077873\n",
      "iteration 65400 / 100000: loss 0.084906\n",
      "iteration 65500 / 100000: loss 0.096172\n",
      "iteration 65600 / 100000: loss 0.083519\n",
      "iteration 65700 / 100000: loss 0.078890\n",
      "iteration 65800 / 100000: loss 0.089712\n",
      "iteration 65900 / 100000: loss 0.089919\n",
      "iteration 66000 / 100000: loss 0.080478\n",
      "iteration 66100 / 100000: loss 0.068131\n",
      "iteration 66200 / 100000: loss 0.071961\n",
      "iteration 66300 / 100000: loss 0.094538\n",
      "iteration 66400 / 100000: loss 0.077335\n",
      "iteration 66500 / 100000: loss 0.091853\n",
      "iteration 66600 / 100000: loss 0.098527\n",
      "iteration 66700 / 100000: loss 0.109849\n",
      "iteration 66800 / 100000: loss 0.107174\n",
      "iteration 66900 / 100000: loss 0.074542\n",
      "iteration 67000 / 100000: loss 0.083179\n",
      "iteration 67100 / 100000: loss 0.082010\n",
      "iteration 67200 / 100000: loss 0.082645\n",
      "iteration 67300 / 100000: loss 0.094305\n",
      "iteration 67400 / 100000: loss 0.091181\n",
      "iteration 67500 / 100000: loss 0.106175\n",
      "iteration 67600 / 100000: loss 0.081412\n",
      "iteration 67700 / 100000: loss 0.081620\n",
      "iteration 67800 / 100000: loss 0.094813\n",
      "iteration 67900 / 100000: loss 0.085296\n",
      "iteration 68000 / 100000: loss 0.117431\n",
      "iteration 68100 / 100000: loss 0.077509\n",
      "iteration 68200 / 100000: loss 0.101702\n",
      "iteration 68300 / 100000: loss 0.090018\n",
      "iteration 68400 / 100000: loss 0.093639\n",
      "iteration 68500 / 100000: loss 0.105112\n",
      "iteration 68600 / 100000: loss 0.079581\n",
      "iteration 68700 / 100000: loss 0.088107\n",
      "iteration 68800 / 100000: loss 0.082653\n",
      "iteration 68900 / 100000: loss 0.084865\n",
      "iteration 69000 / 100000: loss 0.088647\n",
      "iteration 69100 / 100000: loss 0.085800\n",
      "iteration 69200 / 100000: loss 0.081763\n",
      "iteration 69300 / 100000: loss 0.107089\n",
      "iteration 69400 / 100000: loss 0.095998\n",
      "iteration 69500 / 100000: loss 0.084275\n",
      "iteration 69600 / 100000: loss 0.082742\n",
      "iteration 69700 / 100000: loss 0.087277\n",
      "iteration 69800 / 100000: loss 0.088909\n",
      "iteration 69900 / 100000: loss 0.082932\n",
      "iteration 70000 / 100000: loss 0.086668\n",
      "iteration 70100 / 100000: loss 0.069810\n",
      "iteration 70200 / 100000: loss 0.076735\n",
      "iteration 70300 / 100000: loss 0.081493\n",
      "iteration 70400 / 100000: loss 0.075646\n",
      "iteration 70500 / 100000: loss 0.086817\n",
      "iteration 70600 / 100000: loss 0.077583\n",
      "iteration 70700 / 100000: loss 0.083033\n",
      "iteration 70800 / 100000: loss 0.069186\n",
      "iteration 70900 / 100000: loss 0.079540\n",
      "iteration 71000 / 100000: loss 0.063776\n",
      "iteration 71100 / 100000: loss 0.068232\n",
      "iteration 71200 / 100000: loss 0.073881\n",
      "iteration 71300 / 100000: loss 0.102088\n",
      "iteration 71400 / 100000: loss 0.090540\n",
      "iteration 71500 / 100000: loss 0.086542\n",
      "iteration 71600 / 100000: loss 0.085215\n",
      "iteration 71700 / 100000: loss 0.081062\n",
      "iteration 71800 / 100000: loss 0.087821\n",
      "iteration 71900 / 100000: loss 0.103933\n",
      "iteration 72000 / 100000: loss 0.094580\n",
      "iteration 72100 / 100000: loss 0.070268\n",
      "iteration 72200 / 100000: loss 0.090070\n",
      "iteration 72300 / 100000: loss 0.082794\n",
      "iteration 72400 / 100000: loss 0.084348\n",
      "iteration 72500 / 100000: loss 0.078696\n",
      "iteration 72600 / 100000: loss 0.081423\n",
      "iteration 72700 / 100000: loss 0.079834\n",
      "iteration 72800 / 100000: loss 0.081785\n",
      "iteration 72900 / 100000: loss 0.106435\n",
      "iteration 73000 / 100000: loss 0.072672\n",
      "iteration 73100 / 100000: loss 0.066571\n",
      "iteration 73200 / 100000: loss 0.076877\n",
      "iteration 73300 / 100000: loss 0.081728\n",
      "iteration 73400 / 100000: loss 0.084774\n",
      "iteration 73500 / 100000: loss 0.078955\n",
      "iteration 73600 / 100000: loss 0.081187\n",
      "iteration 73700 / 100000: loss 0.091349\n",
      "iteration 73800 / 100000: loss 0.071401\n",
      "iteration 73900 / 100000: loss 0.087735\n",
      "iteration 74000 / 100000: loss 0.098449\n",
      "iteration 74100 / 100000: loss 0.091071\n",
      "iteration 74200 / 100000: loss 0.086347\n",
      "iteration 74300 / 100000: loss 0.086698\n",
      "iteration 74400 / 100000: loss 0.080852\n",
      "iteration 74500 / 100000: loss 0.079942\n",
      "iteration 74600 / 100000: loss 0.091862\n",
      "iteration 74700 / 100000: loss 0.070576\n",
      "iteration 74800 / 100000: loss 0.077163\n",
      "iteration 74900 / 100000: loss 0.076237\n",
      "iteration 75000 / 100000: loss 0.092263\n",
      "iteration 75100 / 100000: loss 0.103670\n",
      "iteration 75200 / 100000: loss 0.093821\n",
      "iteration 75300 / 100000: loss 0.100043\n",
      "iteration 75400 / 100000: loss 0.090213\n",
      "iteration 75500 / 100000: loss 0.076874\n",
      "iteration 75600 / 100000: loss 0.078728\n",
      "iteration 75700 / 100000: loss 0.104308\n",
      "iteration 75800 / 100000: loss 0.085141\n",
      "iteration 75900 / 100000: loss 0.070569\n",
      "iteration 76000 / 100000: loss 0.105866\n",
      "iteration 76100 / 100000: loss 0.099323\n",
      "iteration 76200 / 100000: loss 0.086081\n",
      "iteration 76300 / 100000: loss 0.087450\n",
      "iteration 76400 / 100000: loss 0.084406\n",
      "iteration 76500 / 100000: loss 0.083295\n",
      "iteration 76600 / 100000: loss 0.082686\n",
      "iteration 76700 / 100000: loss 0.082491\n",
      "iteration 76800 / 100000: loss 0.091185\n",
      "iteration 76900 / 100000: loss 0.079799\n",
      "iteration 77000 / 100000: loss 0.085067\n",
      "iteration 77100 / 100000: loss 0.079507\n",
      "iteration 77200 / 100000: loss 0.077321\n",
      "iteration 77300 / 100000: loss 0.078058\n",
      "iteration 77400 / 100000: loss 0.092372\n",
      "iteration 77500 / 100000: loss 0.075901\n",
      "iteration 77600 / 100000: loss 0.092653\n",
      "iteration 77700 / 100000: loss 0.080974\n",
      "iteration 77800 / 100000: loss 0.078626\n",
      "iteration 77900 / 100000: loss 0.077063\n",
      "iteration 78000 / 100000: loss 0.076002\n",
      "iteration 78100 / 100000: loss 0.097479\n",
      "iteration 78200 / 100000: loss 0.097692\n",
      "iteration 78300 / 100000: loss 0.096333\n",
      "iteration 78400 / 100000: loss 0.089545\n",
      "iteration 78500 / 100000: loss 0.079067\n",
      "iteration 78600 / 100000: loss 0.091709\n",
      "iteration 78700 / 100000: loss 0.075201\n",
      "iteration 78800 / 100000: loss 0.091603\n",
      "iteration 78900 / 100000: loss 0.088090\n",
      "iteration 79000 / 100000: loss 0.087227\n",
      "iteration 79100 / 100000: loss 0.099909\n",
      "iteration 79200 / 100000: loss 0.079981\n",
      "iteration 79300 / 100000: loss 0.099146\n",
      "iteration 79400 / 100000: loss 0.090905\n",
      "iteration 79500 / 100000: loss 0.092393\n",
      "iteration 79600 / 100000: loss 0.089659\n",
      "iteration 79700 / 100000: loss 0.093140\n",
      "iteration 79800 / 100000: loss 0.072127\n",
      "iteration 79900 / 100000: loss 0.091207\n",
      "iteration 80000 / 100000: loss 0.088622\n",
      "iteration 80100 / 100000: loss 0.085957\n",
      "iteration 80200 / 100000: loss 0.083895\n",
      "iteration 80300 / 100000: loss 0.097079\n",
      "iteration 80400 / 100000: loss 0.113379\n",
      "iteration 80500 / 100000: loss 0.071017\n",
      "iteration 80600 / 100000: loss 0.075385\n",
      "iteration 80700 / 100000: loss 0.072018\n",
      "iteration 80800 / 100000: loss 0.076274\n",
      "iteration 80900 / 100000: loss 0.091326\n",
      "iteration 81000 / 100000: loss 0.081433\n",
      "iteration 81100 / 100000: loss 0.076230\n",
      "iteration 81200 / 100000: loss 0.090230\n",
      "iteration 81300 / 100000: loss 0.085778\n",
      "iteration 81400 / 100000: loss 0.085715\n",
      "iteration 81500 / 100000: loss 0.097466\n",
      "iteration 81600 / 100000: loss 0.085031\n",
      "iteration 81700 / 100000: loss 0.085912\n",
      "iteration 81800 / 100000: loss 0.074503\n",
      "iteration 81900 / 100000: loss 0.093939\n",
      "iteration 82000 / 100000: loss 0.084114\n",
      "iteration 82100 / 100000: loss 0.086276\n",
      "iteration 82200 / 100000: loss 0.095211\n",
      "iteration 82300 / 100000: loss 0.077946\n",
      "iteration 82400 / 100000: loss 0.103636\n",
      "iteration 82500 / 100000: loss 0.072615\n",
      "iteration 82600 / 100000: loss 0.078267\n",
      "iteration 82700 / 100000: loss 0.103684\n",
      "iteration 82800 / 100000: loss 0.070717\n",
      "iteration 82900 / 100000: loss 0.078083\n",
      "iteration 83000 / 100000: loss 0.089186\n",
      "iteration 83100 / 100000: loss 0.096935\n",
      "iteration 83200 / 100000: loss 0.104909\n",
      "iteration 83300 / 100000: loss 0.096917\n",
      "iteration 83400 / 100000: loss 0.122869\n",
      "iteration 83500 / 100000: loss 0.083718\n",
      "iteration 83600 / 100000: loss 0.079602\n",
      "iteration 83700 / 100000: loss 0.077476\n",
      "iteration 83800 / 100000: loss 0.087968\n",
      "iteration 83900 / 100000: loss 0.081003\n",
      "iteration 84000 / 100000: loss 0.081429\n",
      "iteration 84100 / 100000: loss 0.095746\n",
      "iteration 84200 / 100000: loss 0.092177\n",
      "iteration 84300 / 100000: loss 0.080812\n",
      "iteration 84400 / 100000: loss 0.105512\n",
      "iteration 84500 / 100000: loss 0.076841\n",
      "iteration 84600 / 100000: loss 0.096781\n",
      "iteration 84700 / 100000: loss 0.090560\n",
      "iteration 84800 / 100000: loss 0.076081\n",
      "iteration 84900 / 100000: loss 0.087289\n",
      "iteration 85000 / 100000: loss 0.099890\n",
      "iteration 85100 / 100000: loss 0.070164\n",
      "iteration 85200 / 100000: loss 0.073790\n",
      "iteration 85300 / 100000: loss 0.075991\n",
      "iteration 85400 / 100000: loss 0.085793\n",
      "iteration 85500 / 100000: loss 0.082930\n",
      "iteration 85600 / 100000: loss 0.083285\n",
      "iteration 85700 / 100000: loss 0.069502\n",
      "iteration 85800 / 100000: loss 0.084133\n",
      "iteration 85900 / 100000: loss 0.095259\n",
      "iteration 86000 / 100000: loss 0.076945\n",
      "iteration 86100 / 100000: loss 0.112124\n",
      "iteration 86200 / 100000: loss 0.093743\n",
      "iteration 86300 / 100000: loss 0.087965\n",
      "iteration 86400 / 100000: loss 0.085256\n",
      "iteration 86500 / 100000: loss 0.085355\n",
      "iteration 86600 / 100000: loss 0.074402\n",
      "iteration 86700 / 100000: loss 0.079222\n",
      "iteration 86800 / 100000: loss 0.082694\n",
      "iteration 86900 / 100000: loss 0.091402\n",
      "iteration 87000 / 100000: loss 0.077139\n",
      "iteration 87100 / 100000: loss 0.082061\n",
      "iteration 87200 / 100000: loss 0.081872\n",
      "iteration 87300 / 100000: loss 0.090258\n",
      "iteration 87400 / 100000: loss 0.074501\n",
      "iteration 87500 / 100000: loss 0.092321\n",
      "iteration 87600 / 100000: loss 0.089750\n",
      "iteration 87700 / 100000: loss 0.070861\n",
      "iteration 87800 / 100000: loss 0.091681\n",
      "iteration 87900 / 100000: loss 0.087898\n",
      "iteration 88000 / 100000: loss 0.073128\n",
      "iteration 88100 / 100000: loss 0.115486\n",
      "iteration 88200 / 100000: loss 0.075889\n",
      "iteration 88300 / 100000: loss 0.101339\n",
      "iteration 88400 / 100000: loss 0.092813\n",
      "iteration 88500 / 100000: loss 0.083807\n",
      "iteration 88600 / 100000: loss 0.086234\n",
      "iteration 88700 / 100000: loss 0.088584\n",
      "iteration 88800 / 100000: loss 0.106173\n",
      "iteration 88900 / 100000: loss 0.123273\n",
      "iteration 89000 / 100000: loss 0.087071\n",
      "iteration 89100 / 100000: loss 0.091355\n",
      "iteration 89200 / 100000: loss 0.085607\n",
      "iteration 89300 / 100000: loss 0.087850\n",
      "iteration 89400 / 100000: loss 0.098250\n",
      "iteration 89500 / 100000: loss 0.078644\n",
      "iteration 89600 / 100000: loss 0.091845\n",
      "iteration 89700 / 100000: loss 0.087940\n",
      "iteration 89800 / 100000: loss 0.099612\n",
      "iteration 89900 / 100000: loss 0.090677\n",
      "iteration 90000 / 100000: loss 0.071332\n",
      "iteration 90100 / 100000: loss 0.089413\n",
      "iteration 90200 / 100000: loss 0.079649\n",
      "iteration 90300 / 100000: loss 0.075562\n",
      "iteration 90400 / 100000: loss 0.089658\n",
      "iteration 90500 / 100000: loss 0.087479\n",
      "iteration 90600 / 100000: loss 0.084475\n",
      "iteration 90700 / 100000: loss 0.098515\n",
      "iteration 90800 / 100000: loss 0.078741\n",
      "iteration 90900 / 100000: loss 0.082352\n",
      "iteration 91000 / 100000: loss 0.095076\n",
      "iteration 91100 / 100000: loss 0.099069\n",
      "iteration 91200 / 100000: loss 0.083896\n",
      "iteration 91300 / 100000: loss 0.081686\n",
      "iteration 91400 / 100000: loss 0.075778\n",
      "iteration 91500 / 100000: loss 0.089608\n",
      "iteration 91600 / 100000: loss 0.081972\n",
      "iteration 91700 / 100000: loss 0.098922\n",
      "iteration 91800 / 100000: loss 0.079170\n",
      "iteration 91900 / 100000: loss 0.095725\n",
      "iteration 92000 / 100000: loss 0.078591\n",
      "iteration 92100 / 100000: loss 0.099972\n",
      "iteration 92200 / 100000: loss 0.085867\n",
      "iteration 92300 / 100000: loss 0.078680\n",
      "iteration 92400 / 100000: loss 0.087355\n",
      "iteration 92500 / 100000: loss 0.087425\n",
      "iteration 92600 / 100000: loss 0.078031\n",
      "iteration 92700 / 100000: loss 0.088517\n",
      "iteration 92800 / 100000: loss 0.079408\n",
      "iteration 92900 / 100000: loss 0.071579\n",
      "iteration 93000 / 100000: loss 0.099811\n",
      "iteration 93100 / 100000: loss 0.082936\n",
      "iteration 93200 / 100000: loss 0.078009\n",
      "iteration 93300 / 100000: loss 0.124242\n",
      "iteration 93400 / 100000: loss 0.080665\n",
      "iteration 93500 / 100000: loss 0.086229\n",
      "iteration 93600 / 100000: loss 0.085660\n",
      "iteration 93700 / 100000: loss 0.080396\n",
      "iteration 93800 / 100000: loss 0.076739\n",
      "iteration 93900 / 100000: loss 0.082411\n",
      "iteration 94000 / 100000: loss 0.084865\n",
      "iteration 94100 / 100000: loss 0.094244\n",
      "iteration 94200 / 100000: loss 0.135948\n",
      "iteration 94300 / 100000: loss 0.079053\n",
      "iteration 94400 / 100000: loss 0.075207\n",
      "iteration 94500 / 100000: loss 0.074330\n",
      "iteration 94600 / 100000: loss 0.076408\n",
      "iteration 94700 / 100000: loss 0.075416\n",
      "iteration 94800 / 100000: loss 0.084034\n",
      "iteration 94900 / 100000: loss 0.088221\n",
      "iteration 95000 / 100000: loss 0.080431\n",
      "iteration 95100 / 100000: loss 0.085821\n",
      "iteration 95200 / 100000: loss 0.111148\n",
      "iteration 95300 / 100000: loss 0.083509\n",
      "iteration 95400 / 100000: loss 0.083780\n",
      "iteration 95500 / 100000: loss 0.077287\n",
      "iteration 95600 / 100000: loss 0.084249\n",
      "iteration 95700 / 100000: loss 0.098368\n",
      "iteration 95800 / 100000: loss 0.081977\n",
      "iteration 95900 / 100000: loss 0.074829\n",
      "iteration 96000 / 100000: loss 0.083047\n",
      "iteration 96100 / 100000: loss 0.075102\n",
      "iteration 96200 / 100000: loss 0.096205\n",
      "iteration 96300 / 100000: loss 0.095920\n",
      "iteration 96400 / 100000: loss 0.083703\n",
      "iteration 96500 / 100000: loss 0.098750\n",
      "iteration 96600 / 100000: loss 0.071824\n",
      "iteration 96700 / 100000: loss 0.097569\n",
      "iteration 96800 / 100000: loss 0.101409\n",
      "iteration 96900 / 100000: loss 0.092047\n",
      "iteration 97000 / 100000: loss 0.086509\n",
      "iteration 97100 / 100000: loss 0.079416\n",
      "iteration 97200 / 100000: loss 0.110457\n",
      "iteration 97300 / 100000: loss 0.088956\n",
      "iteration 97400 / 100000: loss 0.082853\n",
      "iteration 97500 / 100000: loss 0.080347\n",
      "iteration 97600 / 100000: loss 0.096857\n",
      "iteration 97700 / 100000: loss 0.076004\n",
      "iteration 97800 / 100000: loss 0.075451\n",
      "iteration 97900 / 100000: loss 0.073974\n",
      "iteration 98000 / 100000: loss 0.082954\n",
      "iteration 98100 / 100000: loss 0.068941\n",
      "iteration 98200 / 100000: loss 0.091821\n",
      "iteration 98300 / 100000: loss 0.085256\n",
      "iteration 98400 / 100000: loss 0.084996\n",
      "iteration 98500 / 100000: loss 0.071527\n",
      "iteration 98600 / 100000: loss 0.082149\n",
      "iteration 98700 / 100000: loss 0.080071\n",
      "iteration 98800 / 100000: loss 0.092096\n",
      "iteration 98900 / 100000: loss 0.111182\n",
      "iteration 99000 / 100000: loss 0.077139\n",
      "iteration 99100 / 100000: loss 0.080126\n",
      "iteration 99200 / 100000: loss 0.078794\n",
      "iteration 99300 / 100000: loss 0.081091\n",
      "iteration 99400 / 100000: loss 0.069689\n",
      "iteration 99500 / 100000: loss 0.072385\n",
      "iteration 99600 / 100000: loss 0.073482\n",
      "iteration 99700 / 100000: loss 0.079292\n",
      "iteration 99800 / 100000: loss 0.087442\n",
      "iteration 99900 / 100000: loss 0.084658\n"
     ]
    }
   ],
   "source": [
    "net_converge = TwoLayerNet(input_size, 80, num_classes)\n",
    "stats_converge = net.train(X_train, y_train, X_val, y_val,\n",
    "                    num_iters=100000, batch_size=200,\n",
    "                    learning_rate=0.01,\n",
    "                    learning_rate_decay=0.8175,\n",
    "                    reg=0.1, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "net_converge Test accuracy:  0.0581\n",
      "best_net hyperparams are : \n",
      "Hidden Size: 80, \n",
      "Learning Rate: 0.01, \n",
      "LR Decay: 0.8175, \n",
      "Regularization: 0.1\n",
      "Validation set accuracy:  0.0638\n"
     ]
    }
   ],
   "source": [
    "test_acc_converge = (net_converge.predict(X_test) == y_test).mean()\n",
    "print('net_converge Test accuracy: ', test_acc_converge)\n",
    "\n",
    "val_acc_converge = (net_converge.predict(X_val) == y_val).mean()\n",
    "print(\"best_net hyperparams are : \\nHidden Size: 80, \\nLearning Rate: 0.01, \\nLR Decay: 0.8175, \\nRegularization: 0.1\")\n",
    "\n",
    "print('Validation set accuracy: ', val_acc_converge)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run on the test set\n",
    "When you are done experimenting, you should evaluate your final trained network on the test set; you should get above 96.3%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy:  0.976\n"
     ]
    }
   ],
   "source": [
    "test_acc = (best_net.predict(X_test) == y_test).mean()\n",
    "print('Test accuracy: ', test_acc)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
